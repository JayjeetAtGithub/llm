D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ : Seamless Integration of
Database Systems and Fast Storage via CXL
Sangjin Lee1Alberto Lerner1Philippe Bonnet2Philippe Cudré-Mauroux1
1University of Fribourg2University of Copenhagen
Switzerland Denmark
ABSTRACT
Flash memory is the de facto standard for data persistence in data-
intensive systems. Despite its bene ￿ts, this type of memory has at
least one severe disadvantage: it is o ￿ered only as part of tightly
closed Solid-State Drives (SSDs). To access an SSD, applications
need to resort to one of many possible I/O frameworks, which
themselves are wrappers around a block interface abstraction, the
NVMe standard. These levels of indirection impact how applications
are structured and prevent them from bene ￿ting from the full power
of Flash-based devices.
In this paper, we argue that SSDs should instead interact with ap-
plications via CXL. CXL is a new technology driven by an Intel-led
consortium that allows systems to maintain coherence between a
host’s memory and memory from attached peripherals. With CXL,
a device can expose a range of Flash-backed addresses through the
server’s memory. One implementation option is to allow applica-
tions to read and write to that range and let the device convert
them to Flash operations. In our SSD, however, we pick a di ￿erent
option. The device exposes what we call a D￿￿￿￿￿￿￿ K￿￿￿￿￿ (DBK)
through a CXL-backed memory range. Read/writes against a kernel
would trigger database-centric computations that the kernel would
perform inside the device. We show examples of DBKs to support
di￿erent database functionalities and discuss their bene ￿ts. We
believe that CXL and D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ can support a new gen-
eration of heterogeneous database platforms with unprecedented
e￿ciency, performance, and functionality.
1 INTRODUCTION
Storage layers API galore. Applications nowadays can access
many di ￿erent memory types, ranging from caches, DRAM (local
and remote), Persistent Memory (ditto), and NAND Flash. This
breadth of alternatives is necessary because each memory type sup-
ports a storage layer with a distinct size, speed, latency, persistence,
and cost trade-o ￿. Since no single memory type can outperform all
the others in all criteria, having many storage layers gives applica-
tions some necessary ￿exibility.
Dealing with most of those storage layers is relatively straight-
forward. For instance, caches and DRAM are transparent to appli-
cations; they see these layers as a continuous memory region that
can be directly read and written with simple load s and store s in-
structions. However, this simplicity is sacri ￿ced when applications
require persistent memory. The storage layers providing persistence
come with much heavier abstractions.
This paper is published under the Creative Commons Attribution 4.0 International
(CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their
personal and corporate Web sites with the appropriate attribution, provided that you
attribute the original work to the authors and CIDR 2024. 14th Annual Conference on
Innovative Data Systems Research (CIDR ’24). January 14-17, 2024, Chaminade, USA.
CPUCXLPCIe5Database SystemDatabaseKernels
Memory
Expanded Memory
SSDFigure 1: CXL allows a host’s memory to be expanded by a
peripheral device. Both host and device can update the mem-
ory, i.e., CXL guarantees coherence. D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ use
the memory to provide services to the database by changing
the semantics of speci ￿c regions of CXL memory. Writing to
a particular area may use a low-latency data path—useful
for logging, for instance. Reading from another area may
return the result of a table scan instead of the table itself.
Reading/Writing to a third area may decompress/compress
the contents along the way.
Currently, the best persistent memory option is arguably NAND
Flash, the underlying storage medium of SSDs. To use fast SSDs,
and thus Flash, an application must resort to NVMe and, most
likely, an additional I/O framework to issue fast block reads and
writes [ 9]. These layers force applications to be structured around
asynchronous API calls instead of the much simpler load/store
instructions. Even with these APIs, properly coupling a database
system with an SSD requires much e ￿ort and may not unlock the
device’s entire performance [15].
CXL as a storage layer uni ￿er.Recently, a technology called
Compute Express Link (CXL) [ 7] emerged that can bring back
simplicity. CXL promises to allow memory of any type sitting on
peripheral cards or even in remote machines to be accessed directly
by applications as if they were local DRAM. At its heart, CXL is a
cache coherence protocol [ 31]. For decades, these protocols have
allowed multi-socket servers to o ￿er applications a uni ￿ed view
of memory. These protocols, however, have been closely guarded
and have all been proprietary. CXL’s most signi ￿cant advantage is,
arguably, that it is public. It promises to support interoperability
across di ￿erent manufacturers’ devices.
This allows third parties to build, for instance, so-called memory
expanders , PCIe form factor daughter cards that contribute extra
memory to a host, be it Intel- or AMD-based, or any future server
that would support the protocol [ 24,29]. Figure 1 (top) depicts this
scenario. CXL characteristics are already known [ 33], and major
hyperscalers, such as Microsoft and Meta, have already announced
potential adoption plans [21, 23].CIDR’24, January 14-17, 2024, Chaminade, USA Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudré-Mauroux
SSD storage can join CXL but do more than memory expan-
sion. The CXL standard, however, does not clearly address how to
incorporate a persistence storage layer such as the one provided by
an SSD. Flash has widely di ￿erent characteristics than DRAM and
requires completely di ￿erent maintenance procedures. Even so, one
can imagine that a CXL memory expander could simply mask these
di￿erences and use Flash to back a CXL memory address range.
An I/O against a Flash-backed CXL address would be somehow
converted into an I/O against Flash. Put di ￿erently, an application
would not need intermediate frameworks to access this device and
could thus have a more natural structure. This semantics is viable
and is one of the CXL-Flash integration alternatives proposed by
this paper and some prototypes [14, 28].
This paper, however, goes beyond this level of integration. We
introduce a new CXL-Flash coupling option that allows parts of
a database system to be implemented in the device, in what is
commonly referred to as Near-Data Processing (NDP) [ 2]. Storage
devices have been steadily gaining such capabilities [ 10,11,18,
19]. We call our new abstraction D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ (DBK).DBK
leverage the CXL technology to export an address space to the
database system. However, instead of implementing a 1:1 mapping
into Flash, I/Os against this area trigger computations inside the
device. Figure 1 also depicts this possibility.
TheDBKabstraction allows moving di ￿erent aspects of database
systems into the device. For instance, early predicate execution
can be implemented as a DBK. The device could take a predicate
description through an address within the CXL-backed window
and present a materialized view of the result in the rest of the area.
As with other kernels, we expect the execution to consume fewer
resources (e.g., transfers), to be faster, and to liberate the host CPU
for other tasks. We will discuss how di ￿erent classes of DBKs can
extend the database functionality into the device.
To execute DBKs, we proposed a new SSD architecture. This SSD
is more modular than a traditional one and opens access to some
of the device’s internal interfaces. The interfaces are designed to
shield non-specialized programmers from unnecessary intricacies
while allowing them to develop NDP database functionality.
The structure of this paper follows its contributions, which are
summarized below:
•We start by brie ￿y explaining the main CXL tenets and present
techniques to extend the use of CXL to storage devices (Section 2).
•We introduce the concept of D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ as a means to ex-
ecute data-intensive tasks in a near-data-processing fashion and
show a device architecture to support such kernels (Section 3).
•We present the di ￿erent types of CXL features that can be lever-
aged by DBKs (Section 4).
•We show examples of DBKin the context of database systems
(Section 5).
•We discuss the viability of implementing D￿￿￿￿￿￿￿ K￿￿￿￿￿￿
devices in practice (Section 6).
•We enumerate the research questions that require further in-
vestigation to realize D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ supporting devices
(Section 7).
We discuss related work in Section 8 and conclude in Section 9.2 BACKGROUND & MOTIVATION
Memory Coherence and Caching. Applications perceive mem-
ory as a continuous range of addresses that can be freely accessed.
Providing this level of abstraction in the modern memory hierarchy
is not trivial. The issue is that modern CPUs have many cores, each
with a private cache. When several cores access a given memory
address simultaneously, the data at the given address may be copied
and possibly updated in multiple caches. The discipline that gov-
erns how cores access data copies for the same memory address is
called Memory (or Cache) Coherence.
Formally, coherence can be de ￿ned in various ways through
invariants [ 31]. A simple de ￿nition is the following: (1) writes to
thesame memory location are serialized; and (2) every write
is eventually made visible to all cores1. Informally, a typical im-
plementation of coherence allows many copies of a piece of data
to exist in di ￿erent caches, provided no core modi ￿es them. If a
core wants to modify its copy, it must acquire an exclusive version,
invalidating all existing copies before proceeding.
Such an implementation relies on two components. The Direc-
tory Controller keeps track of which memory addresses are cached,
by whom, and in which state. The Cache Controller requests cache
addresses to the directory controller and receives invalidation re-
quests from it. Figure 2 (left) depicts a simpli ￿ed implementation
of this architecture. Note that this implementation scales well to
multi-socket systems, by associating a Directory Controller to each
socket and a Cache controller to each cache. The memory addresses
are assigned to Directory Controllers in such a way that each Cache
Controller can tell if a request should be directed to the local or a
remote Directory Controller. Figure 2 (left) depicts this scenario.
CXL versions and Device Types. The￿rst version of CXL, called
1.1, extends Memory Coherence to caches located on local peripher-
als. We are starting to see the ￿rst products emerging in the market
that support that CXL version, e.g., servers [ 12] and memory ex-
panders [ 28]. Two additional versions of CXL are already rati ￿ed.
CXL 2.0 enables single-level switching, i.e., Memory Coherence is
supported across multiple hosts and multiple devices connected
through a single switch. CXL 3.0 extends Memory Coherence to
multiple switches over various interconnects and fabrics protocols.
CXL also adopts the concept of a Directory and a Cache control-
lers—although it uses di ￿erent terminology— dividing the message
types that make up the protocol into two. The messages originating
from the Cache Controller form the subprotocol named cxl.cache .
The messages originating from the Directory controller form the
cxl.mem sub-protocol. A peripheral can implement only the cxl.
cache protocol as a Type 1 device , both protocols as a Type 2 device ,
or only the cxl.mem protocol as a Type 3 device .
Figure 2 (right) depicts a Type 3 device. It is suitably called a
memory expander because its goal is to provide additional memory
to a server without caching the latter’s memory. In other words,
only CPU cores on the server side will cache contents of the memory
the device is providing. For that reason, it only needs to implement
cxl.mem . The ￿gure shows that the device implements a Directory
Controller—called a Device Controller here.
1Note the emphasis on a single memory location. The discipline that governs the order
of accesses to multiple addresses is a di ￿erent one: Memory Consistency [ 25]. Memory
coherence and consistency are orthogonal concepts.D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ : Seamless Integration of Database Systems and Fast Storage via CXL CIDR’24, January 14-17, 2024, Chaminade, USA
CacheControllerDirectoryController
Cache
Coreload/storesGet/PutData/Ack
Memory
DirectoryControllerData/Ack
Memory
symmetricinterconnectCPU 1CPU 2
CacheControllerDirectoryController
Cache
Coreload/storesGet/PutData/Ack
Memory
DeviceControllerData/CmPMemory
CXL.memasymmetricinterconnectCPU 1CXL MemoryExpander Device
MemRd/MemWr①②③①②⑤③③④Figure 2: (Left) Coherence across sockets: To access or modify the contents of a memory address, a core brings a copy of it to its
cache 1 . This can be triggered by issuing a load or astore instruction. Upon receiving the instruction, the Cache Controller
issues a request to either get a copy or put (write) its copy of the modi ￿ed content from/back to memory 2 . The Directory
Controller receives this message and executes the required memory access, either sending a copy of the read data to the Cache
Controller or acknowledging that the modi ￿ed data was written 3 . The Cache Controller can then signal to the core that the
instruction is complete. Note that if the address required were held by a remote Directory Controller, the Cache Controller
would have targeted it instead 3 . (Right) Coherence with a memory expander device: The Cache Controller asks or sends a
cache line as before but is unaware of who is backing that address. Upon noticing that the request is for the expanded memory
area, the Directory Controller issues the proper command to the Device Controller 3 , which in turn interacts with the local
memory 4 and responds. It is the Directory Controller that sends the cache line or the acknowledgment back as if the line
accessed was local 5 .
Curiously, CXL imposes a hierarchy of Directory Controllers.
A Cache controller cannot talk directly with a Device Controller
because it does not know whether it is accessing a range of memory
that it manages. The Directory Controller on the host mediates all
communications. This type of coherence protocol is called asymmet-
ric. Note that in Figure 2 (left), we portray a symmetric coherence
protocol; there is no hierarchy between the Directory Controllers
of each CPU. There have been debates on the relative merits of the
approaches. CXL proponents argue that asymmetric protocols are
simpler. Symmetric protocol proponents argue that more balanced
systems may be built if every peripheral that o ￿ers memory to
the system controls its own memory. It seems, however, that the
Industry decided to proceed with asymmetric protocols.
A naive type 3 Flash-based CXL device. The standard recognizes
memory expanders, but nothing in it constrains what type of mem-
ory can back the address range the expander adds to the Directory
Controller. In fact, recent versions of the standard even stipulate
ranges of tolerable latency for transactions (request-response mes-
sage pairs) issued against the device. Some of these ranges are well
within the response times of NAND Flash-based devices. Naturally,
one can conceive of a Type 3 device whose address range would be
backed by that kind of memory and, therefore, be persistent (e.g.,
Samsung’s recent Memory-Semantics SSD Prototype [28]).
Although the exercise of building such a storage device is inter-
esting, we think it would deliver subpar response times. As we will
substantiate shortly, the device would only be noti ￿ed of a write
operation when it is about to complete, leaving very little time
to hide the Flash-memory latency. Instead, we claim that a device
with this functionality is better realized by a Type 2 device that
is allowed to hold a cache—and implement a cache controller—of
its own. Cache Coherence here is a means to give early notice of
on-going write operations to the NAND-based device.3 DEVICE ARCHITECTURE
We propose a storage device that supports simple and e ￿cient ac-
cess to expanded memory (including Flash—but not only) through
memory reads and writes, and rich semantics associated to opera-
tions on a given memory range.
CXL Integration and Storage Options. Our storage device is a
CXL Type 2 device. It exposes memory regions to the server through
cxl.mem , and it caches memory from the host through cxl.cache ,
as depicted in Figure 3. We provide a simple API to map physical
addresses from the device onto process memory. Once this mapping
is done, applications access these memory ranges “as usual.”
Internally, however, the device o ￿ers a powerful indirection
mechanism. It associates a range of addresses with a kernel. A
kernel is a function that provides well-de ￿ned semantics for reads and
writes . A kernel that implements memory expansion semantics will
simply redirect reads/writes to a selected memory type. We will
provide such a kernel with the device that can opt between DRAM
or NAND-Flash as backing memory. As Figure 3 shows, the device
can still present itself to the system as an NVMe device and o ￿er a
traditional data path. Nothing prevents a legacy application from
using it that way.
The device can also associate standard or application-speci ￿c
kernels to a chosen memory range. We focus on kernels that are
relevant for data-intensive applications that we denote as D￿￿￿￿￿￿￿
K￿￿￿￿￿￿ . These kernels can provide much more than 1:1 address
mapping. They can host functionality that would otherwise be
performed by the database system on the server host. D￿￿￿￿￿￿￿
K￿￿￿￿￿￿ can manipulate memory directly but can also rely on the
help of local Direct Memory Access (DMA) engines, capable of
e￿ciently transferring data in, out, and across the storage options,
including fast memory, such as SRAM, in addition to DRAM and
NAND-Flash.CIDR’24, January 14-17, 2024, Chaminade, USA Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudré-Mauroux
Fast Staging Memory
FlashChannel
DMA
DRAMMemory
DMA
MemoryController
CacheController
Fast Staging Memory
FlashChannel
DRAMMemory
NVMeControllercxl.cachemessagescxl.memmessagesNVMemessages
SSDSemantics
ExpanderSemantics
DMAkernel areaﬁxed kernels
…
DMA……
database kernels
 Message Routerrequeststo cacheinvali-dationsmemreadsmemwritespagereadspagewrites
Figure 3: A D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ supporting device. The device
can be accessed as a conventional SSD or through CXL. In the
latter case, the messages to a given memory address range
will be directed to its assigned kernel. The kernel can choose
which kind of storage type to use and how.
Note that a kernel does not need to map that range onto storage
addresses directly. Instead, it can tie an address range to a virtually
materialized result of a given computation . In other words, if we
imagine that the device is holding a table, a kernel can expose only
some of these table’s rows, as if it had applied a predicate on behalf
of the application. The kernel can also expose data that is not stored
— if it knows how to calculate it from the data that is. We will discuss
more examples in Section 5.
Why use a Type 2 device? So far, we have only discussed memory
accesses in what could be perfectly accomplished by a Type 3 device.
However, we propose developing D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ on a Type
2 device. As explained above, this type of device can contribute
memory to the system and also cache data from the system locally.
To do so, it implements a Cache Controller that interacts with the
system’s Directory Controller via cxl.cache . Interestingly, Type
2 devices release control of their memory range to the Directory
Controller (on the host), which forces the device to notify the Di-
rectory Controller if it wishes to cache its own memory. This is
called Host Bias [7].
In our device, a kernel has the option to request shared cached
lines on any portion of the address ranges it exposes. The bene ￿t
of this arrangement is subtle but powerful. If a core on the host
wishes to access a cached memory address in exclusive mode—e.g.,
it wishes to write a new entry in an exposed area—the device can
be noti ￿ed of this intent through a cache invalidation message.
Figure 4 illustrates this case This early noti ￿cation of an intent to
write gives the device much more time to prepare for the write than
it would have if it learned about the write as it was requested.OwningCacheControllerDirectoryControllerSharingCacheControllersDeviceControllerask ExclusiveInvalidade sharersackExclusivegrantedputMemWrCmpackearly noticeof upcomingwrite
Type 3DeviceType 2DeviceCore  Reques-ting ExclusiveOwership store
cxl.cachetrafﬁccxl.memtrafﬁcFigure 4: A Type 3 device would not learn about a write oper-
ation until the Directory Controller requested it. In contrast,
if the same device could cache memory as well, i.e., if it were
a Type 2 device, it would learn about the early intent to write.
The reason is that, to give a core exclusive access to a memory
address, the Directory Controller must invalidate all accesses
given before. The invalidation is an early signal to the Flash-
based Type 2 device that it should prepare to hear a write
request for that address in the short future, giving it ample
time to prepare.
4 DATABASE KERNEL TYPES
The discussion above about choosing a Type 2 device for its moni-
toring capabilities suggests that there are other CXL mechanisms
than coherence that could be attractive for kernel development. We
divide the kernels in categories according to the CXL features they
use and discuss these categories next.
Classic Kernels. These are kernels that expose a memory address
range backed by device’s DRAM to the database system. There
could be processes inside the device that issue load and store
instructions against these addresses without di ￿erentiating whether
an address sits on the host system or the device DRAM. Because
these kernels rely on the coherence mechanisms of CXL, we call
them classic CXL kernels .
In a classic kernel, the memory semantics and its backing imple-
mentation are the same as those of traditional memory. Ultimately,
these kernels allow extending the database functionality into the
device, simply by moving wholesale processes into it. We will dis-
cuss more speci ￿c examples in the following section, but generally
speaking, any database process one would spin in a multi-socket
machine—where remote access to memory occurs transparently—
could technically be spun inside of the device as well. What de ￿nes
these kernels is access to coherent memory.D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ : Seamless Integration of Database Systems and Fast Storage via CXL CIDR’24, January 14-17, 2024, Chaminade, USA
Advanced Kernels. Database kernels, however, can leverage other
aspects of the CXL protocol than coherence to build more advanced
kernels . We comment on at least three features that enable new
kernel functionality, summarized in Table 1.
The￿rst feature pertains to how CXL is built upon an extremely
low-latency messaging system. Arguably, the most important CXL
component that supports this feature is the Arbiter . The Arbiter
is a hardware component that sits on the lowest part of the CXL
stack and decides which type of message— cxl.io ,cxl.mem , or
cxl.cache —will use the PCIe bus next. In practice, the Arbiter will
prioritize cache coherence tra ￿c, even, for instance, during heavy
DMA operations in the PCIe bus. The very presence of the Arbiter
is what di ￿erentiates PCIe Gen 5 and cxl.io . Another CXL aspect
that makes it low latency is that its messages are small. They occur
in￿its, or 68 bytes in CXL 1.1 (although ￿its will grow in later CXL
standard iterations) [ 30]. We have seen above how this combination
of low latency and small messages enable the protocol to monitor
the status of data regions at a cache line granularity and prepare
for upcoming memory modi ￿cation events before they have been
concluded. Simply put, the coherence tra ￿c can sometimes predict
the memory tra ￿c, and it does so with low latency. There could be
kernels that use this prediction mechanism.
The second feature that can unlock advanced kernel possibilities
is memory independence. The standard does not dictate the type of
memory a device associates with exported address ranges. Servers
supporting CXL on the market at the time of this writing invariably
use DDR5 as the memory standard. However, some CXL-enabled
devices that started appearing can o ￿er DDR4 or even high band-
width memory (HBM). Other CXL-enable devices are built with
FPGAs, which carry some SRAM variations. Moreover, CXL devices
can o ￿er persistence by backing addresses with persistent memory
or with NAND Flash memory. A DBKmay use any type of memory
available in the device or even a combination thereof, for instance,
implementing some sort of memory layering inside the device with
anti-caching semantics [ 8]. The kernel is free to implement a con-
tract (semantics) that the application relies on when issuing reads
and writes against that kernel memory. This contract may even
vary across di ￿erent addresses of the same kernel.
The third CXL feature that enables advanced D￿￿￿￿￿￿￿ K￿￿￿￿￿￿
is perhaps the most powerful. The standard neither dictates the
kind of memory to back up addresses o ￿ered by a CXL device, nor
does it mandate that there should be some memory backing the
address! This allows an address to hold the results of a dynamic
computation, performed only when the address is accessed. A typi-
cal usage of this kernel is data compression. Such a kernel would
accept uncompressed writes to a memory region but persist them
in a compressed way. In turn, reading from that region would de-
compress the necessary addresses only. Another interesting kernel
using this feature is to allow a given data structure to be seen as
column-oriented through one memory region but row-oriented
through another region. Most importantly, if either gets updated,
the kernel responsible for these two regions would invalidate the
corresponding cached lines from both regions.5 KERNEL EXAMPLES
The previous section discussed how certain CXL features, both
centered on coherence and not, can unlock several useful D￿￿￿￿￿￿￿
K￿￿￿￿￿￿ . In this section, we provide a functional description of a
few of them.
Bu￿er Manager Extensions. Perhaps the most intuitive kernel
to add to a system is one that expands the amount of memory it
can use. Technically, there is no need for a D￿￿￿￿￿￿￿ K￿￿￿￿￿ to
achieve this. Any type 3 device can be attached to the system whose
memory could be promptly used by the system’s Bu ￿er Manager
simply through a larger bu ￿er frame.
Flushing bu ￿ers in this arrangement, however, could be less than
e￿cient. If the device o ￿ering the memory has storage capabilities,
the database system may not have the means to transfer data from
the expanded memory into the persistent area easily. It would, most
likely, treat the CXL device as two, the volatile and the persistent
storage device areas. It would stage data from the expanded memory
and send it to a persistent area—when this data movement could
very well be performed intra-device. The goal of a Bu ￿er Manager
Extension kernel is to enable such optimizations. In other words,
the kernel would implement a Flush operation to move data from
DRAM into Flash.
Note that this kernel may allow for exciting data placement
possibilities. The system may know upfront that certain pages are
being retrieved that may be updated, e.g., as part of a SQL UPDATE
command, while others would not, e.g., in a SQL query. Pages
not already loaded in the system may be allocated accordingly,
with likely writable pages being allocated from the device memory
pool. The bene ￿t of such a data placement scheme is to reduce I/O
bandwidth.
Query Execution Worker. The idea of pushing predicates down
is as old as query optimization. Naturally, this type of optimization
was one of the ￿rst to be attempted in-storage [ 16]. With kernels,
we can support not only this type of scan and ￿lter operation but
also an extensive array of access paths, as the query operations
that interact with the base tables/indexes storage are called. For
instance, a kernel could implement an indexed lookup access. It
would entail an index tree traversal and a base table page read. The
kernel could implement the tree traversal in the device bene ￿ting
from cached data, saving index data transfers between the host and
the device.
As with other typical query workers, the operators this worker
would be executing would communicate with each other via pages
pinned in the Bu ￿er Manager. These pages may or may not reside
inside the device. A traditional query worker on the system pro-
cesses the rest. The coherence feature of CXL allows either type of
worker to access a common set of pages.
A Fast Transaction Logging Kernel. In the two previous kernels,
we took advantage of the memory coherence feature of CXL to
ship some of the database functionality to the device. In this kernel,
we mostly rely on CXL’s fast communication messaging system
in a situation that does not strictly require coherence. Transaction
logging typically entails a sensitive operation in a database system,
as every transaction has to be re ￿ected in a local (and persistent) log
￿le. Quite often, the speed with which the transaction is persistedCIDR’24, January 14-17, 2024, Chaminade, USA Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudré-Mauroux
CXL Features Potential Kernel Functionality Kernel Examples
Coherence Allows di ￿erent processes to access memory Bu ￿er manager extensions, query execution worker, etc.
coherently.
Flit-based messaging Support low-latency communication for Fast transaction logging, etc.
coherence tra ￿c.
Backing-memory freedom Allows mixing memory types within the Data placement for LSM compression, etc.
same memory region.
Non-backed addressing Enables view materialization mechanisms. Compression/decompression, data transposition, coherent
views, etc.
Table 1: CXL features and the kernel functionality they enable.
is the main bottleneck in a database. Transaction logging is an ideal
operation to o ￿oad to a DBK, in the sense that it can use a type
of memory with low latency and with a battery-backed option for
persistence. A dedicated database kernel exposing an append-only
abstraction on top of CXL could streamline this set of operations in
various ways. For instance, the kernel can stage log entries on SRAM
and quickly destage them asynchronously to Flash. Comparable
semantics can be obtained using a dedicated storage device (such
as our own X-SSD system [ 17]), but CXL and kernels bring many
advantages in this context, including the upfront notice of an intent
to write (cf. Section 3).
Data Placement Kernel. In column stores, data is traditionally
stored in two very di ￿erent memory regions: a write-optimized
store that ￿rst receives all updates, and a compressed, read-optimized
store [ 32]. In HTAP systems, the areas that store di ￿erent data
representations can be even more disparate [ 26]. Moreover, in LSM-
tree-based key-value stores, the writing activity in the upper layers
of the tree is more frequent than in the lower ones, and these in-
terfere with compaction work [ 3]. As discussed in the previous
section, the address ranges exported by a type 3 device may be
backed by more than one memory type. Areas with more intensive
write activity could be initially persisted in SLC Flash memories,
for instance. Areas with more read activity could be moved to MLC,
TLC, or QLC Flash memory, which sacri ￿ce write performance in
the name of more economical reads. Currently, this ￿ne-grained
data placement is not visible to an SSD user, even if it exists in
some devices [ 34], but with D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ , it can. A kernel
can place di ￿erent portions of their memory range under distinct
Service Level Agreement, so to speak, just by assigning each portion
to a di ￿erent type of backing memory.
Coherent Virtual Views. This is perhaps the most powerful type
ofD￿￿￿￿￿￿￿ K￿￿￿￿￿ . It can equate memory accesses with per-
forming computations. The idea with this kernel is that it exports
a memory address range but does not back it up with memory. It
associates a computation with the region and performs the com-
putation as a side-e ￿ect of accessing that region. One example
would be to use this functionality to compress data when writing
and decompressing when reading, both transparently. As the name
implies, we say the kernel implements a view over the data.
The views may explore coherence in a unique way. Suppose
a view exports a column-oriented representation of an area that
is, in fact, stored in a row-oriented format. Each virtual view be
accessible through a di ￿erent memory range. Updating one of thememory regions implies updating the other as well. If, however,
either area is being cached anywhere in the system, these cache
lines need to be invalidated. The kernel can control how the areas
of the di ￿erent regions are associated. When one area gets updated,
besides asking for exclusive access over that data structure, the
kernel asks for the same access over the virtual structure. This
technique was pioneered by CCKit [27] and PLayer [5].
6 PRELIMINARY VIABILITY ANALYSIS
We seek a platform that supports CXL to develop a D￿￿￿￿￿￿￿
K￿￿￿￿￿￿ storage device. This platform inevitably needs special-
ized hardware because PCIe and CXL protocol messages require
low latency. Moreover, memory management requires specialized
hardware for the same reason. For mostly everything else, there
are alternatives that can use the software in e ￿cient ways. This
combination of hardware and software makes FPGAs a particularly
promising platform. We already have access to a platform that ￿ts
this pro ￿le.
One of the most challenging aspects of developing using FPGAs
is predicting the necessary area (how many logic units in the FPGA
fabric) a given design will have. Therefore, our ￿rst experiments
were to prototype a hardware design that connects the PCIe/CXL
areas of the card with the memory controller areas. The rationale
behind this design is that it can approximate the data path we wish
the card to support. The data path is certainly one of the components
that will consume the larger area in our design. Table 2 shows the
results of this experiment for a Type 2 and a Type 3 design. As
expected, the Type 2 design uses more area since it implements a
Cache Controller (cf. Figure 2).
Case IP(s)Logic Unit Counts % of Total
Logic Units Ideal Real Total
1CXL Type 2 179K 213K251K 27.5%2 DDRs + User Logic 31K 38K
2CXL Type 3 141K 167K204K 22.5%2 DDRs + User Logic 30K 37K
Table 2: The table shows how many logic units a primary
type 2 or 3 device design uses. Each design includes CXL IP,
two channels of DDR 4, and user logic.
Fortunately, the FPGA is comfortably sized; our data path oc-
cupies only slightly more than 1/4thof the available area, leaving
ample space for the other components.D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ : Seamless Integration of Database Systems and Fast Storage via CXL CIDR’24, January 14-17, 2024, Chaminade, USA
Area, however, is not the only concern. The data path should
provide adequate bandwidth to be e ￿ective. In practice, the maxi-
mum bandwidth of a PCIe card is capped at the width of the PCIe
connection. In our case, this is 64GB/s (a Gen5 x16 connection).
This bandwidth can roughly support 2 DDR4 memory controllers,
but it is a somewhat high bandwidth for an FPGA card. For compar-
ison, this bandwidth is equivalent to ￿ve 100Gbps network ports.
Therefore, we analyze whether a preliminary design could achieve
such bandwidth. Figure 5 shows the result of this experiment. The
data path in that ￿gure connects (1) the PCIe/CXL controller with
(2) two memory controllers.
Figure 5: Floorplan of the FPGA fabric. The ￿gure shows the
FPGA fabric in the center (large blue rectangle) and the spe-
cialized hard blocks in the periphery. The PCIe/CXL block
is located at the left-center of the FPGA fabric (1). The two
DDR4 channels are located at the bottom-center (2). The cir-
cuit placement in the FPGA connecting these areas is shown
in a green-magenta color range. That range represents the
density of the circuit. In particular, the magenta areas are
close to saturating the resources in that area.
The FPGA ￿oorplan shows that the data path is viable but there
are congested areas within our data path. In essence, this may
mean that the FPGA synthesizer may take longer to compile circuit
de￿nitions, trying to place and route them on the FPGA. We believe
the drawbacks are minor and that we have a suitable platform to
develop a D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ -supporting device.7 RESEARCH AGENDA
We believe D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ is a foundation for future database
systems incorporating in-storage processing capabilities. The work
presented here is but the start of several fundamental research
directions that need to be explored, including:
Following CXL evolution. Our current proposal is based on CXL
1.1, which is the version that is about to become commercially
available. However, the future versions of the CXL protocol are
already speci ￿ed. The additional features speci ￿ed for versions
2.0 and 3.0 can unlock further possibilities for DBK. Of particular
interest is the possibility of integrating a host with remote storage,
which CXL 3.0 and beyond will allow.
Hardware Support for Application Logic. We would like D￿￿￿￿
￿￿￿￿ K￿￿￿￿￿￿ to be an inviting and performant environment for
software development. We can achieve so with a combination of
user and pre-installed functions [ 11]. Examples of pre-installed
functions are sorting, merging, ￿ltering, transposition, etc. Some
functions could even deal with serialization and deserialization of
traditional ￿le formats. Since these internal functions are stable and
generic, they may be implemented in hardware. The user functions,
in contrast, could be developed in a software environment and a
general-purpose language. To support the latter, the device should
dedicate one or more cores to run application software. We discuss
how to develop in this environment next.
A Database Kernel Development Kit (DSK). Admittedly, the de-
velopment of kernels in our current proposal requires skills that are
only available to SSD and FPGA design specialists. The integration
with Flash is, for now, too low level, and, for performance, some of
it must be implemented via hardware. However, there is no funda-
mental impediment to shifting the development techniques towards
a more software-centric approach. This may require implementing
clearer software interfaces and wrapping hardware aspects of this
integration in something akin to function calls.
Additional Memory Technologies. The current D￿￿￿￿￿￿￿ K￿￿￿
￿￿￿￿ proposal makes SRAM, DRAM, and NAND-Flash memories
available for kernel development. In the future, it should be possible
to incorporate other types of memory such as HBM—and, perhaps
future formats of persistent memory that replace Optane—should
they become more commonly available on development platforms.
Safety and Security Aspects. With many kernels running in a
device, crash safety and security issues may arise. The kernels
should not interfere with one another, and despite being integrated
into the device, they should be isolated in a way that does not
corrupt or otherwise hamper the proper functioning of the device.
Fostering Interoperability. One factor that could signi ￿cantly im-
prove adoption is if di ￿erent vendors supported D￿￿￿￿￿￿￿ K￿￿￿￿￿￿
and competed by o ￿ering di ￿erent cost vs. performance tradeo ￿s.
Similarly, it would be interesting if a database system boot pro-
cess could check whether the storage over which it is running
provides D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ and adjust accordingly. With such
interoperability features, it should be possible for something akin
to a marketplace of DBKto emerge.CIDR’24, January 14-17, 2024, Chaminade, USA Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudré-Mauroux
8 RELATED WORK
To the best of our knowledge, this is the ￿rst work that leverages
CXL to provide in-storage database functionality. Storage, however,
has been historically gaining processing capabilities [4, 11, 19].
Leveraging that potential for query processing was described by
Do et al. [ 10]. More recently, Lerner & Bonnet characterized the
architectural alternatives to do so [ 18]. There are many examples
of functionalities that were pushed into storage: joins and ￿lters [ 6,
13,16], transaction log acceleration [ 17], LSM compaction [ 22], and
device pro ￿ling [ 20], to cite a few. These are excellent kernel candi-
dates, and if implemented so, they will bene ￿t from the uniform
and transparent interface that CXL can o ￿er.
Some works started to speculate about how to use CXL memory
expanders to integrate a host’s memory either with storage [ 14,28]
or directly with a database system [ 1]—but never both, as D￿￿￿￿
￿￿￿￿ K￿￿￿￿￿￿ do. Abishek et al. have discussed mechanisms to
maintain cache coherence across virtually materialized views [ 27].
The mechanism is very powerful, and D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ can take
full advantage of it within a storage device.
9 CONCLUSION
In this paper, we introduced D￿￿￿￿￿￿￿ K￿￿￿￿￿￿ , the ￿rst platform
to embed database functionalities deep into storage devices using
coherence technology. The cornerstone of this database-device inte-
gration is the use of CXL, and in particular its caching capabilities.
The device uses coherence tra ￿c to monitor requests, prepare ahead
of time, and ultimately answer database queries more e ￿ciently.
While realizing the full potential of DBKs will take years, we are
already excited about the new database architectural possibilities
that this new technology opens.
ACKNOWLEDGMENTS
This work has received funding from the Swiss State Secretariat
for Education (SERI) in the context of the SmartEdge EU project
(Grant agreement No. 101092908).
REFERENCES
[1]Minseon Ahn et al .2022. Enabling CXL Memory Expansion for In-Memory
Database Management Systems. In DaMoN . https://doi.org/10.1145/3533737.
3535090
[2]Rajeev Balasubramonian, Jichuan Chang, Troy Manning, Jaime H Moreno,
Richard Murphy, Ravi Nair, and Steven Swanson. 2014. Near-data processing:
Insights from a micro-46 workshop. IEEE Micro 34, 4 (2014), 36–42.
[3]Oana Balmau, Florin Dinu, Willy Zwaenepoel, Karan Gupta, Ravishankar Chand-
hiramoorthi, and Diego Didona. 2019. SILK: Preventing Latency Spikes in
Log-Structured Merge Key-Value Stores. In USENIX . https://www.usenix.org/
conference/atc19/presentation/balmau
[4]Antonio Barbalace and Jaeyoung Do. 2021. Computational Storage: Where Are We
Today?. In CIDR . https://www.cidrdb.org/cidr2021/papers/cidr2021_paper29.pdf
[5]Richard Braun, Abishek Ramdas, Michal Friedman, and Gustavo Alonso. 2023.
PLayer: Expanding Coherence Protocol Stack with a Persistence Layer. In DIMES
Workshop . https://doi.org/10.1145/3609308.3625270
[6]Wei Cao et al .2020. POLARDB Meets Computational Storage: E ￿ciently Support
Analytical Workloads in {Cloud-Native }Relational Database. In FAST . https:
//www.usenix.org/system/ ￿les/fast20-cao_wei.pdf
[7]CXL Consortium. 2023. Compute Express Link Speci ￿cation. https://www.
computeexpresslink.org/download-the-speci ￿cation.
[8]Justin DeBrabant, Andrew Pavlo, Stephen Tu, Michael Stonebraker, and Stan
Zdonik. 2013. Anti-caching: A new approach to database management system
architecture. 6, 14 (2013), 1942–1953. https://doi.org/10.14778/2556549.2556575
[9]Diego Didona, Jonas Pfe ￿erle, Nikolas Ioannou, Bernard Metzler, and Animesh
Trivedi. 2022. Understanding Modern Storage APIs: A Systematic Study of libaio,
SPDK, and io_uring. In SYSTOR . https://doi.org/10.1145/3534056.3534945[10] Jaeyoung Do, Yang-Suk Kee, Jignesh M. Patel, Chanik Park, Kwanghyun Park,
and David J. DeWitt. 2013. Query Processing on Smart SSDs: Opportunities and
Challenges. In SIGMOD . https://doi.org/10.1145/2463676.2465295
[11] Niclas Hedam, Morten Tychsen Clausen, Philippe Bonnet, Sangjin Lee, and Ken
Friis Larsen. 2023. Delilah: EBPF-O ￿oad on Computational Storage. In DaMoN .
https://doi.org/10.1145/3592980.3595319
[12] Intel. 2023. Sapphire Rapids Family. https://ark.intel.com/content/www/us/en/
ark/products/codename/126212/products-formerly-sapphire-rapids.html.
[13] Insoon Jo, Duck-Ho Bae, Andre S Yoon, Jeong-Uk Kang, Sangyeun Cho, Daniel DG
Lee, and Jaeheon Jeong. 2016. YourSQL: a high-performance database system
leveraging in-storage computing. In VLDB . https://doi.org/10.14778/2994509.
2994512
[14] Myoungsoo Jung. 2022. Hello Bytes, Bye Blocks: PCIe Storage Meets Compute
Express Link for Memory Expansion (CXL-SSD). In HotStorage . https://doi.org/
10.1145/3538643.3539745
[15] Aarati Kakaraparthy, Jignesh M. Patel, Kwanghyun Park, and Brian P. Kroth.
2019. Optimizing Databases by Learning Hidden Parameters of Solid State Drives.
(2019). https://doi.org/10.14778/3372716.3372724
[16] Sungchan Kim, Hyunok Oh, Chanik Park, Sangyeun Cho, Sang-Won Lee, and
Bongki Moon. 2016. In-storage processing of database scans and joins. In Infor-
mation Sciences . https://doi.org/10.1016/j.ins.2015.07.056
[17] Sangjin Lee, Alberto Lerner, André Ryser, Kibin Park, Chanyoung Jeon, Jinsub
Park, Yong Ho Song, and Philippe Cudré-Mauroux. 2022. X-SSD: A Storage
System with Native Support for Database Logging and Replication. In SIGMOD .
https://doi.org/10.1145/3514221.3526188
[18] Alberto Lerner and Philippe Bonnet. 2021. Not Your Grandpa’s SSD: The Era of Co-
Designed Storage Devices. In SIGMOD . https://doi.org/10.1145/3448016.3457540
[19] Alberto Lerner, Rana Hussein, André Ryser, Sangjin Lee, and Philippe Cudré-
Mauroux. 2020. Networking and Storage: The Next Computing Elements in
Exascale Systems?. In IEEE Data Engineering Bulletin . https://exascale.info/
assets/pdf/lerner20debull.pdf
[20] Alberto Lerner, Jaewook Kwak, Sangjin Lee, Kibin Park, Yong Ho Song, and
Philippe Cudré-Mauroux. 2020. It Takes Two: Instrumenting the Interaction
between In-Memory Databases and Solid-State Drives. In CIDR . https://www.
cidrdb.org/cidr2020/papers/p19-lerner-cidr20.pdf
[21] Huaicheng Li et al .2023. Pond: CXL-Based Memory Pooling Systems for Cloud
Platforms. In ASPLOS . https://doi.org/10.1145/3575693.3578835
[22] Minje Lim, Jeeyoon Jung, and Dongkun Shin. 2021. LSM-tree Compaction
Acceleration Using In-storage Processing. https://doi.org/10.1109/ICCE-
Asia53811.2021.9641965
[23] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agarwal,
Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanaujia,
and Prakash Chauhan. 2023. TPP: Transparent Page Placement for CXL-Enabled
Tiered-Memory. In ASPLOS . https://doi.org/10.1145/3582016.3582063
[24] Micron. 2023. CZ120 memory expansion module. https://www.micron.com/
solutions/server/cxl.
[25] Scott Owens, Susmit Sarkar, and Peter Sewell. 2009. A better x86 memory model:
x86-TSO. In TPHOLs . https://doi.org/10.1007/978-3-642-03359-9_27
[26] Fatma Özcan, Yuanyuan Tian, and Pinar Tözün. 2017. Hybrid transac-
tional/analytical processing: A survey. In SIGMOD . 1771–1775. https://doi.org/
10.1145/3035918.3054784
[27] Abishek Ramdas. 2023. CCKit: FPGA acceleration in symmetric coherent heteroge-
neous platforms . Ph. D. Dissertation. ETH Zurich.
[28] Samsung. 2023. Memory Semantics SSD. https://samsungmsl.com/ms-ssd/.
[29] Samsung. 2023. Samsung Develops Industry’s First CXL DRAM Supporting CXL
2.0. https://semiconductor.samsung.com/news-events/news/samsung-develops-
industrys- ￿rst-cxl-dram-supporting-cxl-2-0/.
[30] Debendra Das Sharma, Robert Blankenship, and Daniel S Berger. 2023. An
Introduction to the Compute Express Link (CXL) Interconnect. arXiv preprint
arXiv:2306.11227 (2023).
[31] Daniel Sorin, Mark Hill, and David Wood. 2011. A primer on memory consistency
and cache coherence . Morgan & Claypool Publishers. https://doi.org/10.1007/978-
3-031-01764-3
[32] Michael Stonebraker et al .2005. C-Store: A Column-oriented DBMS. In VLDB .
https://doi.org/10.5555/1083592.1083658
[33] Yan Sun, Yifan Yuan, Zeduo Yu, Reese Kuper, Ipoom Jeong, Ren Wang, and
Nam Sung Kim. 2023. Demystifying CXL Memory with Genuine CXL-Ready
Systems and Devices. In arXiv . https://arxiv.org/abs/2303.15375.
[34] Xuebin Zhang, Jiangpeng Li, Hao Wang, Kai Zhao, and Tong Zhang. 2016. Reduc-
ing Solid-State Storage Device Write Stress through Opportunistic In-place Delta
Compression. In FAST . https://www.usenix.org/conference/fast16/technical-
sessions/presentation/zhang-xuebinDemystifying CXL Memory with
Genuine CXL-Ready Systems and Devices
Yan Sun
University of Illinois
Urbana, U.S.A.
yans3@illinois.eduYifan Yuan
Intel Labs
Hillsboro, U.S.A.
yifan.yuan@intel.comZeduo Yu
University of Illinois
Urbana, U.S.A.
zeduoyu2@illinois.eduReese Kuper
University of Illinois
Urbana, U.S.A.
rkuper2@illinois.edu
Chihun Song
University of Illinois
Urbana, U.S.A.
chihuns2@illinois.eduJinghan Huang
University of Illinois
Urbana, U.S.A.
jinghan4@illinois.eduHouxiang Ji
University of Illinois
Urbana, U.S.A.
hj14@illinois.eduSiddharth Agarwal
University of Illinois
Urbana, U.S.A.
sa10@illinois.edu
Jiaqi Lou
University of Illinois
Urbana, U.S.A.
jiaqil6@illinois.eduIpoom Jeong
University of Illinois
Urbana, U.S.A.
ipoom@illinois.eduRen Wang
Intel Labs
Hillsboro, U.S.A.
ren.wang@intel.comJung Ho Ahn
Seoul National University
Seoul, Republic of Korea
gajh@snu.ac.kr
Tianyin Xu
University of Illinois
Urbana, U.S.A.
tyxu@illinois.eduNam Sung Kim
University of Illinois
Urbana, U.S.A.
nskim@illinois.edu
ABSTRACT
1The ever-growing demands for memory with larger capacity and
higher bandwidth have driven recent innovations on memory ex-
pansion and disaggregation technologies based on Compute eX-
press Link (CXL). Especially, CXL-based memory expansion tech-
nology has recently gained notable attention for its ability not
only to economically expand memory capacity and bandwidth but
also to decouple memory technologies from a speci ￿c memory
interface of the CPU. However, since CXL memory devices have
not been widely available, they have been emulated using DDR
memory in a remote NUMA node. In this paper, for the ￿rst time,
we comprehensively evaluate a true CXL-ready system based on
the latest 4th-generation Intel Xeon CPU with three CXL memory
devices from di ￿erent manufacturers. Speci ￿cally, we run a set of
microbenchmarks not only to compare the performance of true CXL
memory with that of emulated CXL memory but also to analyze
the complex interplay between the CPU and CXL memory in depth.
This reveals important di ￿erences between emulated CXL memory
and true CXL memory, some of which will compel researchers to
revisit the analyses and proposals from recent work. Next, we iden-
tify opportunities for memory-bandwidth-intensive applications
to bene ￿t from the use of CXL memory. Lastly, we propose a CXL-
memory-aware dynamic page allocation policy, Caption to more
e￿ciently use CXL memory as a bandwidth expander. We demon-
strate that Caption can automatically converge to an empirically
favorable percentage of pages allocated to CXL memory, which
1This work has been accepted by a conference. The authoritative version of this
work will appear in the Proceedings of the IEEE/ACM International Symposium
on Microarchitecture (MICRO), 2023. Please refer to https://doi.org/10.1145/3613424.
3614256 for the o ￿cial version of this paper.improves the performance of memory-bandwidth-intensive appli-
cations by up to 24% when compared to the default page allocation
policy designed for traditional NUMA systems.
KEYWORDS
Compute eXpress Link, tiered-memory management, measurement
1 INTRODUCTION
Emerging applications have demanded memory with even larger
capacity and higher bandwidth at lower power consumption. How-
ever, as the current memory technologies have almost reached their
scaling limits, it has become more challenging to meet these de-
mands cost-e ￿ciently. Especially, when focusing on the memory
interface technology, we observe that DDR5 requires 288 pins per
channel [ 44], making it more expensive to increase the number
of channels for higher bandwidth under the CPU’s package pin
constraint. Besides, various signaling challenges in high-speed par-
allel interfaces, such as DDR, make it harder to further increase
the rate of data transfers. This results in super-linearly increasing
energy consumption per bit transfer [ 55] and reducing the number
of memory modules (DIMMs) per channel to one for the maximum
bandwidth [ 33]. As the capacity and bandwidth of memory are
functions of the number of channels per CPU package, the number
of DIMMs per channel, and the rate of bit transfers per channel,
DDR has already shown its limited bandwidth and capacity scala-
bility. This calls for alternative memory interface technologies and
memory subsystem architectures.
Among them, Compute eXpress Link (CXL) [ 73] has emerged
as one of the most promising memory interface technologies. CXL
is an open standard developed through a joint e ￿ort by major
hardware manufacturers and hyperscalers. As CXL is built on thearXiv:2303.15375v4  [cs.PF]  5 Oct 2023Host CPUMemory ctrl.DDRDDRCXL memory module CXL/PCIeCXL.ioCXL.memCXLcontrollerCXL/PCIeMemory ctrl.Mem.deviceMem.deviceMem.device···Mem.deviceMem.deviceMem.device···Figure 1: CXL memory module architecture.
standard PCIe, which is a serial interface technology, it can o ￿er
much higher bit-transfer rates per pin ( e.g., PCIe 4.0: 16 Gbps/lane
vs. DDR4-3200: 3.2 Gbps/pin) and consumes lower energy per bit
transfer ( e.g., PCIe 4.0: 6 pJ/bit [ 75] vs. DDR4: 22 pJ/bit [ 56]), but at
the cost of much longer link latency ( e.g., PCIe 4.0: ⇠40 ns [ 73] vs.
DDR4: <1 ns [ 15]). Compared to PCIe, CXL implements additional
features that enable the CPU to communicate with devices and their
attached memory in a cache-coherent fashion using load and store
instructions. Figure 1 illustrates a CXL memory device consisting
of a CXL controller and memory devices. Consuming ⇠3⇥fewer
pins than DDR5, a CXL memory device based on PCIe 5.0 ⇥8 may
expand memory capacity and bandwidth of systems cost-e ￿ciently.
Furthermore, with the CXL controller between the CPU and mem-
ory devices, CXL decouples memory technologies from a speci ￿c
memory interface technology supported by the CPU. This grants
memory manufacturers unprecedented ￿exibility in designing and
optimizing their memory devices. Besides, by employing retimers
and switches, a CPU with CXL support can easily access memory in
remote nodes with lower latency than traditional network interface
technologies like RDMA, e ￿ciently facilitating memory disaggre-
gation. These advantages position memory-related extension as
one of the primary target use cases for CXL [ 25,32,65,67], and
major hardware manufacturers have announced CXL support in
their product roadmaps [4, 25, 35, 65, 67].
Given its promising vision, CXL memory has recently attracted
signi ￿cant attention with active investigation for datacenter-scale
deployment [ 59,64]. Unfortunately, due to the lack of commercially
available hardware with CXL support, most of the recent research
on CXL memory has been based on emulation using memory in a
remote NUMA node in a multi-socket system, since CXL memory is
exposed as such [ 6,59,64]. However, as we will reveal in this paper,
there are fundamental di ￿erences between emulated CXL memory
and true CXL memory. That is, the common emulation-based prac-
tice of using a remote NUMA node to explore CXL memory may
give us misleading performance characterization results and/or lead
to suboptimal design decisions.
This paper addresses a pressing need to understand the capabili-
ties and performance characteristics of true CXL memory, as well
as their impact on the performance of (co-running) applications
and the design of OS policies to best use CXL memory. To this
end, we take a system based on the CXL-ready 4th-generation Intel
Xeon CPU [ 35] and three CXL memory devices from di ￿erent man-
ufacturers (§3). Then, for the ￿rst time, we not only compare the
performance of true CXL memory with that of emulated CXL mem-
ory, but also conduct an in-depth analysis of the complex interplaybetween the CPU and CXL memory. Based on these comprehensive
analyses, we make the following contributions.
CXL memory <remote NUMA memory (§4). We reveal that true
CXL memory exhibits notably di ￿erent performance Characteristics
from emulated CXL memory. (C1) Depending on CXL controller
designs and/or memory technologies, true CXL memory devices
give a wide range of memory access latency and bandwidth val-
ues.(C2) True CXL memory can give up to 26% lower latency and
3–66% higher bandwidth e ￿ciency than emulated CXL memory,
depending on memory access instruction types and CXL memory
devices. This is because true CXL memory has neither caches nor
CPU cores that modify caches, although it is exposed as a NUMA
node. As such, the CPU implements an on-chip hardware structure
to facilitate fast cache coherence checks for memory accesses to
the true CXL memory. These are important di ￿erences that may
change conclusions made by prior work on the performance charac-
teristics of CXL memory and consequently the e ￿ectiveness of the
proposals at the system level. (C3)The sub-NUMA clustering (SNC)
mode provides LLC isolation among SNC nodes (§3) by directing
the CPU cores within an SNC node to evict their L2 cache lines from
its local memory exclusively to LLC slices within the same SNC
node. However, when CPU cores access CXL memory, they end up
breaking the LLC isolation, as L2 cache lines from CXL memory
can be evicted to LLC slices in any SNC nodes. Consequently, ac-
cessing CXL memory can bene ￿t from e ￿ectively 2–4 ⇥larger LLC
capacity than accessing local DDR memory, notably compensating
for the longer latency of accessing CXL memory for cache-friendly
applications.
Naïvely used CXL memory considered harmful (§5). Using
a system with a CXL memory device, we evaluate a set of appli-
cations with diverse memory access characteristics and di ￿erent
performance metrics ( e.g., response time and throughput). Subse-
quently, we present the following Findings. (F1)Simple applications
(e.g., key-value-store) demanding `s-scale latency are highly sensi-
tive to memory access latency. Consequently, allocating pages to
CXL memory increases the tail latency of these applications by 10–
82% compared to local DDR memory. Besides, the state-of-the-art
CXL-memory-aware page placement policy for a tiered memory
system [ 64] actually increases tail latency even further when com-
pared to statically partitioning pages between DDR memory and
CXL memory. This is due to the overhead of page migration. (F2)
Complex applications ( e.g., social network microservices) exhibit-
ing<s-scale latency experience a marginal increase in tail latency
even when most of pages are allocated to CXL memory. This is
because the longer latency of accessing CXL memory contributes
marginally to the end-to-end latency of such applications. (F3)Even
for memory-bandwidth-intensive applications, naïvely allocating
50% of pages to CXL memory based on the default OS policy may
result in lower throughput, despite higher aggregate bandwidth
delivered by using both DDR memory and CXL memory.
CXL-memory-aware dynamic page allocation policy (§6). To
showcase the usefulness of our characterizations and ￿ndings de-
scribed above, we propose Caption ,aCXL-memory- aware dy-
namic page alloca tion policy for the OS to more e ￿ciently use
the bandwidth expansion capability of CXL memory. Speci ￿cally,
2CXL memory controller
Memory device(DDR4 or DDR5) Memory I/FMemory ctrl.Memory device(DDR4 or DDR5) Memory ctrl.Memory I/FConfiguration space regs/MMIO spaceCXL link layerCXL transaction layerCXL.memlink layerCXL.iolink layerCXL.memtransaction layerCXL.iotransaction layerDevice-specific buffer/logic RxTxCXL ARB/MUXFlex Bus Physical Layer (PHY)CXL IPFigure 2: CXL.mem controller architecture.
Caption begins by determining the bandwidth of manufacturer-
speci ￿c CXL memory devices. Subsequently, Caption periodically
monitors various CPU counters, such as memory access latency
experienced by (co-running) applications and assesses the band-
width consumed by them at runtime. Lastly, based on the moni-
tored CPU counter values, Caption estimates memory-subsystem
performance over periods. When a given application demands an
allocation of new pages, Caption considers the history of memory
subsystem performance and the percentage of pages allocated to
CXL memory in the past. Then, it adjusts the percentage of the
pages allocated to CXL memory to improve the overall system
throughput using a simple greedy algorithm. Our evaluation shows
that Caption improves the throughput of a system co-running a
set of memory-bandwidth-intensive SPEC CPU2017 benchmarks
by 24%, compared with the default static page allocation policy set
by the OS.
2 BACKGROUND
2.1 Compute eXpress Link (CXL)
PCIe is the industry standard for a high-speed serial interface be-
tween a CPU and I/O devices. Each lane of the current PCIe 5.0 can
deliver 32 GT/s ( e.g.,⇠64 GB/s with 16 lanes). Built on the physical
layer of PCIe, the CXL standard de ￿nes three separate protocols:
CXL.io ,CXL.cache , and CXL.mem .CXL.io uses protocol features
of the standard PCIe, such as transaction-layer packet (TLP) and
data-link-layer packet (DLLP), to initialize the interface between a
CPU and a device [ 20].CXL.cache andCXL.mem use the aforemen-
tioned protocol features for the device to access the CPU’s memory
and for the CPU to access the device’s memory, respectively.
TheCXL.mem protocol accounts only for memory accesses from
the CPU to the device facilitated by the Home Agent (HA) and the
CXL controller on the CPU and the device, respectively [ 70]. The
HA handles the CXL.mem protocol and transparently exposes CXL
memory to the CPU as memory in a remote NUMA node. That is,
the CPU can access CXL memory with load and store instructions in
the same way as it accesses memory in a remote NUMA node. This
has an advantage over other memory expansion technologies, such
as RDMA, which involves the device’s DMA engine and thus has
di￿erent memory access semantics. Lastly, when the CPU accesses
CXL memory, it caches data from/to the CXL memory in every
level of its cache hierarchy. This has been impossible with any other
memory extension technologies except for persistent memory.2.2 CXL-ready Systems and Memory Devices
CXL requires hardware support from both the CPU and devices.
Both the latest 4th-generation Intel Xeon Scalable Processor (Sap-
phire Rapids) and the latest 4th-generation AMD EPYC Processor
(Genoa) are among the ￿rst server-class commodity CPUs to sup-
port the CXL 1.1 standard [ 4,35]. Figure 2 depicts a typical archi-
tecture of CXL.mem controllers. It primarily consists of (1) PCIe
physical layer, (2) CXL link layer, (3) CXL transaction layer, and (4)
memory controller blocks. (2), (3), and other CXL-related compo-
nents are collectively referred to as CXL IP in this paper. As of today,
in addition to some research prototypes, multiple CXL memory de-
vices have been designed by major hardware manufacturers, such
as Samsung [ 25], SK Hynix [ 77], Micron [ 65], and Montage [ 67].
To facilitate more ￿exible memory functionality and near-memory
computing capability, Intel also enables the CXL protocol in its
latest Agilex-I series FPGA [ 40], integrated with hard CXL IPs to
support the CXL.io ,CXL.cache , and CXL.mem [41]. Lastly, unlike
a true NUMA node typically based on a large server-class CPU, a
CXL memory device does not have any CPU cores, caches, or long
interconnects between the CXL IP and the memory controller in
the device.
3 EVALUATION SETUP
3.1 System and Device
Systems. We use a server to evaluate the latest commercial hard-
ware supporting CXL memory (Table 1). The server consists of two
Intel Sapphire Rapids (SPR) CPU sockets. One socket is populated
with eight 4800 MT/s DDR5 DRAM DIMMs (128 GB) across eight
memory channels. The other socket is populated with only one
4800 MT/s DDR5 DRAM DIMM to emulate the bandwidth and ca-
pacity of CXL memory. The Intel SPR CPU integrates four CPU
chiplets, each with up to 15 cores and two DDR5 DRAM channels.
A user can choose to use the 4 chiplets as a uni ￿ed CPU, or each
chiplet (or two chiplets) as a NUMA node in the SNC mode. Such
￿exibility is to give users strong isolation of shared resources, such
as LLC, among applications. Lastly, we turn o ￿the hyper-threading
feature and set the CPU core clock frequency to 2.1 GHz for more
predictable performance.
CXL memory devices. We take three CXL memory devices (‘CXL
memory devices’ in Table 1), each featuring di ￿erent CXL IPs (ASIC-
based hard IP and FPGA-based soft IP) and DRAM technologies
(DDR5-4800, DDR4-2400, and DDR4-3200). Since the CXL protocol
itself does not prescribe the underlying memory technology, it can
seamlessly and transparently accommodate not only DRAM but
also persistent memory, ￿ash [ 72], and other emerging memory
technologies. Consequently, various CXL memory devices may
exhibit di ￿erent latency and bandwidth characteristics.
3.2 Microbenchmark
To characterize the performance of CXL memory, we use two
microbenchmarks. First, we use Intel Memory Latency Checker
(MLC) [ 42], a tool used to measure memory latency and bandwidth
for various usage scenarios. Second, we use a microbenchmark
dubbed memo (measuring e￿ciency of memory subsystems). It
shares some features with Intel MLC, but we develop it to give more
3Table 1: System con ￿gurations.
Dual-socket server system
Component Desription
OS (kernel) Ubuntu 22.04.2 LTS (Linux kernel v6.2)
CPU2⇥Intel®Xeon 6430 CPUs @2.1 GHz [37], 32 cores
and 60 MB LLC per CPU, Hyper-Threading disabled
MemorySocket 0: 8 ⇥DDR5-4800 channels
Socket 1: 1 ⇥DDR5-4800 channel (emulated CXL memory)
CXL memory devices
Device CXL IP Memory technology Max. bandwidth
CXL-A Hard IP DDR5-4800 38.4 GB/s per channel
CXL-B Hard IP 2 ⇥DDR4-2400 19.2 GB/s per channel
CXL-C Soft IP DDR4-3200 25.6 GB/s per channel
control over characterizing memory subsystem performance in di-
verse ways. For instance, it can measure the latency and bandwidth
of a speci ￿c memory access instruction ( e.g., AVX-512 non-temporal
load and store instructions).
3.3 Benchmark
Latency-sensitive applications. We run Redis [69], a popular
high-performance in-memory key-value store, with YCSB [19]. We
use a uniform distribution of keys, ensuring maximum stress on the
memory subsystem, unless we explicitly specify the use of other
distributions. We also run DeathStarBench ( DSB)[28], an open-
source benchmark suite designed to evaluate the performance of
microservices. It uses Docker to launch components of a microser-
vice, including machine learning (ML) inference logic, web backend,
load balancer, caching, and storage. Speci ￿cally, we evaluate three
DSBworkloads: (1) compose posts , (2)read user timelines , and
(3)mixed workloads (10% of compose posts , 30% of read user
timelines , and 60% of read home timelines ) as a social network
framework. Lastly, we run FIO[7], an open-source tool used for
benchmarking storage devices and ￿le systems, to evaluate the
latency impact of using CXL memory for OS page cache. The page
cache is supported by the standard Linux storage subsystem, which
holds recently accessed storage data ( e.g.,￿les) in unused main
memory space to reduce the number of accesses to slow storage
devices.
Throughput applications. First, we run an inference application
based on a deep learning recommendation model ( DLRM ) with the
same setup as MERCI [58]. The embedding reduction step in DLRM
inference is known to have a large memory footprint and is re-
sponsible for 50–70% of the inference latency [ 58]. Second, we take
the SPECrate CPU2017 benchmark suite [ 13], which is commonly
used to evaluate the throughput of systems in datacenters. Then we
assess misses per kilo instructions (MPKI) of every benchmark and
run the four benchmarks with the highest MPKI: (1) fotonik3d , (2)
mcf, (3)roms , and (4) cactuBSSN . We run multiple instances of a
single benchmark or two di ￿erent benchmarks.
4 MEMORY LATENCY AND BANDWIDTH
CHARACTERISTICS
In this section, we ￿rst evaluate the latency and bandwidth of ac-
cessing di ￿erent memory devices: an emulated CXL memory devicebased on DDR5 memory in a remote NUMA node (DDR5-R), and
three true CXL memory devices (CXL-A, CXL-B, and CXL-C). We
conduct this evaluation to understand the performance characteris-
tics of various CXL memory devices for di ￿erent memory access
instruction types. Next, we investigate interactions between the
Intel SPR CPU’s cache hierarchy and the CXL memory devices.
4.1 Latency
Figure 3 presents the measured latency values of accessing both
emulated and true CXL memory devices. The ￿rst group of bars
shows average (unloaded idle) memory access latency values mea-
sured by Intel MLC that performs pointer-chasing ( i.e., getting the
memory address of a load from the value of the preceding load) in a
memory region larger than the total LLC capacity of the CPU. This
e￿ectively measures the latency of serialized memory accesses. The
remaining four groups of bars show the average memory access
latency values measured by memo for four memory access instruc-
tion types: (1) temporal load ( ld), (2) non-temporal load ( nt-ld ),
(3) temporal store ( st), and (4) non-temporal store ( nt-st ). For
these groups, we ￿rst execute clflush to￿ush all cache lines from
the cache hierarchy and then mfence to ensure the completion
of￿ushing the cache lines. Then, we execute 16 memory access
instructions back to back to 16 random memory addresses in a
cacheable memory region. To measure the execution time of these
16 memory access instructions, we execute rdtsc , which reads
the current value of the CPU’s 64-bit time-stamp counter into a
register immediately before and after executing the 16 memory
access instructions, followed by an appropriate fence instruction.
This e ￿ectively measures the latency of random parallel memory
accesses for each memory access instruction type for a given mem-
ory device. To obtain a representative latency value, we repeat the
measurement 10,000 times and choose the median value to exclude
outliers caused by TLB misses and OS activities.
Analyzing the latency values shown in Figure 3, we make the
following Observations.
(O1) The full-duplex CXL and UPI interfaces reduce memory
access latency. memo gives emulated CXL memory 76% lower ld
latency than Intel MLC. This di ￿erence arises because serialized
memory accesses by Intel MLC cannot exploit the full-duplex capa-
bility of the UPI interface that connects NUMA nodes. In contrast,
random parallel memory accesses by memo can send memory com-
mands/addresses and receive data in parallel through the full-duplex
UPI interface, e ￿ectively halving the average latency cost of going
Figure 3: Random memory access latency of various memory
devices (DDR5-R, CXL-A, CXL-B, and CXL-C), measured by
Intel MLC and memo .
4through the UPI interface. Since true CXL memory is also based
on the full-duplex interface ( i.e., PCIe), it enjoys the same bene ￿t
as emulated CXL memory. Nonetheless, with memo , CXL-A gets a
3 percentage points more ldlatency reduction than for DDR5-R.
(O3) explains the reason for this additional latency reduction.
(O2) The latency of accessing true CXL memory devices is
highly dependent on a given CXL controller design. Figure 3
shows that CXL-A exhibits only 35% longer ldlatency than DDR5-
R, while CXL-B and CXL-C present almost 2 ⇥and 3 ⇥longer ld
latency, respectively. Even with the same DDR4 DRAM technology,
CXL-C based on DDR4-3200 gives 67% longer ldlatency than CXL-
B based on DDR4-2400.
(O3) Emulated CXL memory can give longer memory access
latency than true CXL memory. When issuing memory requests
to emulated CXL memory, the local CPU must ￿rst check with
the remote CPU, which is connected through the inter-chip UPI
interface, for cache coherence [ 62,66]. Moreover, the memory re-
quests must travel through a long intra-chip interconnect within
the remote CPU to reach its memory controllers [ 83]. These over-
heads increase with more CPU cores, i.e., more caches and a longer
interconnect path. For example, the ldlatency values of DDR5-R
with 26- and 40-core Intel SPR CPUs are 29% lower and 19% higher,
respectively, than those of DDR5-L with the 32-core Intel SPR CPU
used for our primary evaluations. In contrast, true CXL memory
has neither caches nor CPU cores that modify caches, although it
is exposed as a remote NUMA node. As such, the CPU implements
an on-chip hardware structure to facilitate fast cache coherence
checks for memory accesses to the true CXL memory. Moreover,
true CXL memory features a short intra-chip interconnect within
the CXL controller to reach its memory controllers.
Speci ￿cally for DDR5-R and CXL-A, memo provides 76% and 79%
lower ldlatency values, respectively, than Intel MLC. Although
both DDR5-R and CXL-A bene ￿t from (O1), CXL-A gives a more
ldlatency reduction than DDR5-R. This arises from the following
di￿erences between memo and Intel MLC. As Intel MLC accesses
memory serially, the local CPU performs the aforementioned cache
coherence checks one by one. By contrast, memo accesses memory in
parallel. In such a case, memory accesses to emulated CXL memory
incur a burst of cache coherence checks that need to go through the
inter-chip UPI interface, leading to cache coherence tra ￿c conges-
tion. This, in turn, increases the time required for cache coherence
checks. However, memory accesses to true CXL memory su ￿er
notably less from this overhead because the CPU checks cache
coherence through its local on-chip structure described earlier. This
contributes to a further reduction in the ldlatency for true CXL
memory. Also note that DDR5-R, based on a 40-core Intel SPR CPU,
presents 4% longer ldlatency than CXL-A due to a higher overhead
of cache coherence checks.
The overhead of cache coherence checks becomes even more
prominent for stbecause of two reasons. First, the stlatency
is much higher than the ldlatency in general. For example, the
latency of stto DDR5-R is 2.3 ⇥longer than that of ldfrom DDR5-
L. This is because of the cache write-allocate policy in Intel CPUs.
When stexperiences an LLC miss, the CPU ￿rst reads 64-byte data
from memory to a cache line ( i.e., implicitly executing ld), and
then writes modi ￿ed data to the cache line [ 43]. This overhead isincreased for both emulated CXL memory and true CXL memory,
as the overhead of traversing the UPI and CXL interfaces is doubled.
Yet, the latency of stto emulated CXL memory increases more
than that of stto emulated CXL memory, when compared to ldor
nt-ld . This is because the emulated CXL memory incurs a higher
overhead for cache coherence checks than the true CXL memory,
as discussed earlier.
Lastly, although both nt-ld andnt-st bypass caches and di-
rectly access memory, the local CPU accessing emulated CXL mem-
ory still needs to check with the remote CPU for cache coher-
ence [ 39]. This explains why the nt-ld latency values of all the
memory devices are similar to those of ldthat needs to be served
by memory in Figure 3. Unlike st, however, nt-st does not read
64-byte data from the memory since it does not allocate a cache line
by its semantics, which eliminates the memory access and cache
coherence overheads associated with implicit ld. Therefore, the
absolute values of nt-st latency across all the memory devices are
smaller than those of stlatency. Furthermore, nt-st can also o ￿er
shorter latency than ldandnt-ld because the CPU issuing nt-st
sends the address and data simultaneously. In contrast, the CPU
issuing ldornt-ld sends the address ￿rst and then receive the data
later, which makes signals go through the UPI and CXL interfaces
twice. With shorter latency for a cache coherency check, nt-st
to true CXL memory can be shorter than nt-st to emulated CXL
memory. For instance, CXL-A exhibits a 25% lower latency than
DDR5-R for nt-st . Note that nt-st behaves di ￿erently depending
on whether the allocated memory region is cacheable or not [ 39],
and we conduct our experiment with a cacheable memory region.
4.2 Bandwidth
The sequential memory access bandwidth represents the maximum
throughput of the memory subsystem when all the CPU cores sends
memory requests in this paper. Nonetheless, it notably varies across
(1) CXL controller designs, (2) memory access instruction types, and
(3) DRAM technologies ( i.e., DDR5-4800, DDR4-3200, and DDR4-
2400 in this paper). As such, for fair and insightful comparison,
we use bandwidth e ￿ciency as a metric, normalizing the mea-
sured bandwidth to the theoretical maximum bandwidth. Figure 4
presents the bandwidth e ￿ciency values of DDR5-R, CXL-A, CXL-
B, and CXL-C for various read/write ratios and di ￿erent memory
instruction types, respectively. Analyzing these values, we make
the following Observations.
(O4) The bandwidth is strongly dependent on the e ￿ciency of
CXL controllers. The maximum sequential bandwidth values that
can be provided by the DDR5-4800, DDR4-3200, and DDR4-2400
DRAM technologies are 38.4 GB/s, 25.6 GB/s, and 19.2 GB/s per
channel, respectively. Nonetheless, Figure 4a shows that DDR5-R,
CXL-A, CXL-B, and CXL-C provides only 70%, 46%, 47%, and 20%
of the theoretical maximum bandwidth, respectively, for ‘All Read’.
DDR5-R and CXL-A are based on the same DDR5-4800 DRAM tech-
nology, yet the bandwidth e ￿ciency of DDR5-R is 23 percentage
points higher than that of CXL-A. We speculate that the lower e ￿-
ciency of the CXL-A’s memory controller for memory read accesses
contributes to this bandwidth e ￿ciency gap, as both DDR5-R and
CXL-A exhibit similar ldlatency values.
5(a) MLC with various read and write ratios
(b)memo with di ￿erent memory instruction types
Figure 4: E ￿ciency of maximum sequential memory access
bandwidth across di ￿erent memory types.
As the write ratio increases, however, CXL-A starts to provide
higher bandwidth e ￿ciencies. For example, Figure 4a shows that
the bandwidth e ￿ciency of CXL-A for ‘2:1-RW’ is 23 percentage
points higher than that of DDR5-R. We speculate that the CXL-
A’s memory controller is designed to handle interleaved memory
read and write accesses more e ￿ciently than the DDR5-R’s and
CXL-B’s memory controllers. This is supported by (1) the fact that
stinvolves both memory read and write accesses due to implicit
ldwhen it incurs a cache miss ( cf. (O3)), and (2) the bandwidth
e￿ciency of CXL-A for all the other memory access instruction
types is lower than that of DDR5-R and CXL-B. This also implies
that the higher bandwidth e ￿ciency of CXL-A for stis not solely
attributed to a unique property of true CXL memory.
Figure 4b shows that the bandwidth e ￿ciency of CXL-B is higher
than that of CXL-A, except for st, although the latency values of
CXL-B is higher than those of CXL-A. Speci ￿cally, the bandwidth
e￿ciency values of CXL-B for ld,nt-ld , and nt-st are 1, 1, and 6
percentage points higher than that of CXL-A, respectively. We spec-
ulate that the recently developed third-party DDR5 memory con-
trollers may not be as e ￿cient as the mature and highly optimized
DDR4 memory controller used in CXL-B for read- or write-only
memory accesses. Note that CXL-C is based on DDR4-3200 DRAM
technology, but it generally exhibits poor bandwidth e ￿ciency due
to the FPGA-based implementation of the CXL controller. The band-
width e ￿ciency values of CXL-C for ld,nt-ld ,st, and nt-st are
26, 26, 3, and 20 percentage points lower, respectively, than those of
CXL-B, which is based on the same DDR4 DRAM technology but
provides 25% lower theoretical maximum bandwidth per channel.
(O5) True CXL memory can o ￿er competitive bandwidth ef-
￿ciency for the store compared to emulated CXL memory.
Figure 4b shows that styields lower bandwidth e ￿ciency values
than ldacross all the memory devices due to the overheads of
implicit ldand cache coherence checks ( cf. (O3)). Speci ￿cally, st
to DDR5-R, CXL-A, CXL-B, and CXL-C o ￿ers 74%, 31%, 59%, and15% lower bandwidth e ￿ciency values, respectively, than ldfrom
DDR5-R, CXL-A, CXL-B, and CXL-C. This suggests that emulated
CXL memory experiences a notably more bandwidth e ￿ciency
degradation than true CXL memory partly because it su ￿ers more
from the overhead of cache coherence checks. As a result, the band-
width e ￿ciency values of CXL-A and CXL-B for stare 12 and 1
percentage points higher, respectively, than DDR5-R. For nt-st ,
the bandwidth e ￿ciency gap between emulated CXL memory and
true CXL memory is noticeably reduced compared to nt-ld . Specif-
ically, the bandwidth e ￿ciency gap between DDR5-R and CXL-A
fornt-ld is 26 percentage points, whereas it is reduced to 6 percent-
age points for nt-st . CXL-B provides almost the same bandwidth
e￿ciency as DDR5-R for nt-st .
4.3 Interaction with Cache Hierarchy
Starting with the Intel Skylake CPU, Intel has adopted non-inclusive
cache architecture [ 34]. Suppose that a CPU core with non-inclusive
cache architecture incurs an LLC miss that needs to be served by
memory. Then it loads data from the memory into a cache line in
the CPU core’s (private) L2 cache rather than the (shared) LLC, in
contrast to a CPU core with inclusive cache architecture. When
evicting the cache line in the L2 cache, the CPU core will place it
into the LLC. That is, the LLC serves as a victim cache. The SNC
mode (§3.1), however, restricts where the CPU core places evicted
L2 cache lines within the LLC to provide LLC isolation among SNC
nodes. The LLC comprises as many slices as the number of CPU
cores, and L2 cache lines in an SNC node are evicted only to LLC
slices within the same SNC node when data in the cache lines are
from the local DDR memory of that SNC node (light-green lines in
Figure 5). In contrast, we notice that L2 cache lines can be evicted
to any LLC slices within any SNC nodes when the data are from
remote memory, including both emulated CXL memory and true
CXL memory (red-dashed lines in Figure 5). As such, CPU cores
accessing CXL memory break LLC isolation among SNC nodes in
SNC mode. This makes such CPU cores bene ￿t from 2–4 ⇥larger
LLC capacity than the ones accessing local DDR memory, notably
compensating for the slower access latency of CXL memory.
CoreCh 1Mem.ctrl.Ch 1Ch 0Mem.ctrl.SNC-3SNC-1SNC-2SNC-0
Ch 1Ch 0Mem.ctrl.Ch 1Ch 0Mem.ctrl.Ch 0
DDRDDRDDRDDRCoreCoreCoreCoreCoreCoreCXL memory
LegendLocal DDR pathCXL.mempathCXL.memdataLocal DDR dataCXL Ctrl.DDRMem. ctrl.Mem. ctrl.LLCLLCLLCLLCLLCLLCLLCLLCCoreCoreCoreCoreCoreCoreLLCLLCLLCLLCLLCLLCLLCCore
CoreCoreCoreCoreLLCLLCLLCLLCLLCCoreLLCCoreMem.ctrl.CoreCoreCoreCoreCoreLLCLLCLLCLLCLLCLLCCoreLLCCoreLLCCoreDDRDDRCoreCoreLLC
CoreLLCCoreLLCDDRDDR
Figure 5: Di ￿erence in L2 cache line eviction paths between
local DDR memory and CXL memory in SNC mode.
6DDR5:CXL-A = 100:075:2550:5025:750:100
050100150200
020406080100p99 Latency (us)Target QPS (Kilo-QPS)(a) [Redis] YCSB-A (RD:UPD = 5:5)
1.E+01.E+11.E+21.E+31.E+41.E+5
012345p99 Latency (ms)
Target QPS (Kilo-QPS)(b) [DSB] compose posts
1.E+01.E+11.E+21.E+31.E+41.E+5
010203040p99 Latency (ms)
Target QPS (Kilo-QPS)(c) [DSB] read user timelines
1.E+01.E+11.E+21.E+31.E+41.E+5
0481216p99 Latency (ms)
Target QPS (Kilo-QPS)(d) [DSB] mixed workloads
Figure 6: 99th-percentile (p99) latency values of Redis with various percentages of pages allocated to CXL memory and DSB
with 100% of only ‘caching and storage’ pages allocated to either CXL memory or DDR memory.
To verify this, we run a single instance of Intel MLC on an idle
CPU and measure the average latency of randomly accessing 32 MB
bu￿ers allocated to DDR5-L and CXL-A, respectively, in the SNC
mode. The total LLC capacity of the CPU with four SNC nodes
in our system is ⇠60 MB. A 32 MB bu ￿er is larger than the total
LLC capacity of a single SNC node but smaller than that of all
four SNC nodes. This shows that accessing the bu ￿er allocated to
CXL-A gives an average memory access latency of 41 ns, whereas
accessing the bu ￿er allocated to DDR5-L o ￿ers an average memory
access latency of 76.8 ns. The shorter latency of accessing the bu ￿er
allocated to CXL-A evidently shows that the CPU cores accessing
CXL memory can bene ￿t from larger e ￿ective LLC capacity than
the CPU cores accessing local DDR memory in the SNC mode.
(O6) CXL memory interacts with the CPU’s cache hierar-
chy di ￿erently compared to local DDR memory. As discussed
above, the CPU cores accessing CXL memory are exposed to a
larger e ￿ective LLC capacity in the SNC mode. This often signi ￿-
cantly impacts LLC hit/miss and interference characteristics that
the CPU cores experience, and thus a ￿ecting the performance of ap-
plications (§5.3). Therefore, we must consider this attribute of CXL
memory when analyzing the performance of applications using
CXL memory, especially in the SNC mode.
5 IMPACT OF USING CXL MEMORY ON
APPLICATION PERFORMANCE
To study the impact of using CXL memory on the performance
of applications (§3.3), we take CXL-A, which provides the most
balanced latency and bandwidth characteristics among the three
CXL memory devices. We use numactl in Linux to allocate memory
pages of a given program, either fully or partially, to CXL memory,
exploiting the fact that the OS recognizes CXL memory as memory
in a remote NUMA node by the OS. Speci ￿cally, numactl allows
users to: (1) bind a program to a speci ￿c memory node ( membind
mode); (2) allocate memory pages to the local NUMA node ￿rst,
and then other remote NUMA nodes only when the local NUMA
node runs out of memory space ( preferred mode); or (3) allocate
memory pages evenly across a set of nodes ( interleaved mode).
A recent Linux kernel patch [ 85] enhances the interleaved mode
to facilitate ￿ne-grained control over the allocation of a speci ￿c
percentage of pages to a chosen NUMA node. For example, we canchange the percentage of pages allocated to CXL memory from
the default 50% to 25%. That is, 75% of pages are allocated to local
DDR5 memory.
In this section, we will vary the percentage of pages allocated
to CXL memory and analyze its impact on performance using
application-speci ￿c performance metrics, setting the stage for our
CXL memory-aware dynamic page allocation policy (§6). Note that
we enable the SNC mode to use only two local DDR5 memory
channels along with one CXL memory channel. This is because
our system can accommodate only one CXL memory device, and it
needs to make a meaningful contribution to the total bandwidth
of the system. In such a setup, the local DDR5 memory with two
channels provides ⇠2⇥higher bandwidth for stand⇠3.4⇥higher
bandwidth for ldthan CXL memory. As future platforms accommo-
date more CXL memory devices, we may connect up to four CXL
memory devices to a single CPU socket with eight DDR5 memory
channels, providing the same DDR5 to CXL channel ratio as our
setup.
5.1 Latency
Redis. Figure 6a shows the 99C⌘-percentile (p99) latency values
ofRedis with YCSB workload A(50% read and 50% update) while
varying the percentage of pages allocated to CXL memory or lo-
cal DDR5 memory (referred to as DDR memory hereafter). First,
allocating 100% of pages to CXL memory (CXL 100%) signi ￿cantly
increases the p99 latency compared to allocating 100% of pages
to DDR memory (DDR 100%), especially at high target QPS val-
ues. For example, CXL 100% results in 10%, 73%, and 105% higher
p99 latency than DDR 100% at 25 K, 45 K, and 85 K target QPS,
respectively. Second, as more pages are allocated to CXL memory,
the p99 latency increases proportionally. For instance, at 85 K tar-
get QPS, allocating 25%, 50%, and 75% of pages to CXL memory
results in p99 latency increases of 9%, 23%, and 45%, respectively,
compared to DDR 100%. Finally, as expected, allocating 100% of
pages to DDR memory results in the lowest and most stable p99
latency. Explaining the substantial di ￿erence in the p99 latency
values for various percentages of pages allocated to CXL memory
and di ￿erent target QPS values, we note that Redis typically oper-
ates with response time at a `s scale, making it highly sensitive to
memory access latency (§3.3). Therefore, allocating more pages to
7Figure 7: Impact of TPP on latency of Redis compared with
statically allocating 25% of (random) pages to CXL memory.
We show the distributions up to the p99 latency.
CXL memory and/or increasing the target QPS makes Redis more
frequently access CXL memory with almost 2 ⇥longer latency than
DDR memory, resulting in higher p99 latency.
Redis+TPP. We conduct an experiment to assess whether the
most recent transparent page placement (TPP) [ 64] can minimize
the impact of using CXL memory on the p99 latency. The most
recent publicly available release [ 63] only o ￿ers an enhanced page
migration policy, and it does not automatically place pages in CXL
memory. Thus, we begin by allocating 100% of pages requested by
Redis to CXL memory and let TPP automatically migrate pages to
DDR memory until the percentage of the pages allocated to CXL
memory becomes 25%, based on the DDR to CXL bandwidth ratio in
our setup. Then we measure the latency values of Redis . Figure 7
compares two distributions of the measured latency values. The
￿rst one is from using TPP, while the second one is from statically
allocating 25% of pages to CXL memory.
TPP migrates a large number of pages to DDR memory in the
beginning phase, requiring the CPU to (1) copy pages from one
memory device to another and (2) update the OS page table entries
associated with the migrated pages [ 89]. Since (1) and (2) incur high
overheads, we measure the p99 latency only after 75% of pages are
migrated to DDR memory. As shown in Figure 7, TPP generally
gives higher latency, resulting in 174% higher p99 latency than
statically allocating 25% of pages to CXL memory. This is because
TPP constantly migrates a small percentage of pages between DDR
memory and CXL memory over time, based on its metric assessing
hotness/coldness of the pages. Although TPP has a feature that re-
duces ping-pong behavior ( i.e., pages are constantly being promoted
and demoted between DDR memory and CXL memory), migrating
pages incurs the overheads from (1) and (2) above. (1) blocks the
memory controllers from serving urgent memory read requests
from latency-sensitive applications [ 57], and (2) also requires a
considerable number of CPU cycles and memory accesses.
DSB. Figure 6b–6d present the p99 latency values of (b) compose
posts , (c)read user timelines , and (d) mixed workloads . Ta-
ble 2 summarizes the components of the benchmarks, their working
set sizes and characteristics, and allocated memory devices. In our
experiment, we allocate 100% of the pages pertinent to the caching
and storage components with large working sets to either DDR
memory (DDR 100%) or CXL memory (CXL 100%). Meanwhile, we
always allocate 100% of the pages associated with the remaining
components, such as nginx front-end and analytic docker images
(e.g., logic in Table 2), to DDR memory, since these components
are more sensitive to memory access latency than the caching andTable 2: Components of DSBsocial network benchmark.
Name Working set Intensiveness Allocated mem. type
Frontend 83 MB Compute DDR memory
Logic 208 MB Compute DDR memory
Caching & Storage 628 MB Memory CXL memory
12345
110100100010000
4k8k16k32k64k128k256k512kPercent Increasep99 Latency (us)Block Size (Bytes)DDR5CXL-APercent Increase
Figure 8: p99 latency values of FIOfor various block sizes,
and percentage values of increase in p99 latency by allocating
page cache to CXL.
storage components. For example, nginx spends 60% of CPU cycles
on the CPU front-end, which is dominated by fetching instructions
from memory [ 28]. Therefore, pages of such components should
be allocated to DDR memory.
This experiment shows that all three benchmarks, compose posts ,
read user timelines , and mixed workloads are not sensitive to
long latency of accessing CXL memory as they exhibit little di ￿er-
ence in p99 latency values between CXL 100% and DDR 100%. This
is because of two reasons. First, most of the p99 latency in these
benchmarks is contributed by the front-end and/or logic compo-
nents ( i.e., DDR 100%). This makes the latency of accessing CXL
memory amortized by the these components, and thus the p99 la-
tency is much less dependent on the latency of accessing databases
(i.e., CXL 100%). Second, the p99 latency of DSBis at a <s scale and
two orders of magnitude longer than that of Redis . Therefore, it is
not as sensitive to memory access latency as that of Redis .
Note that CXL 100% provides lower p99 latency values than
DDR 100% for mixed workloads when the QPS range is between
5 K and 11 K. This is because mixed workloads is far more memory-
bandwidth-intensive than compose posts andread user timelines .
Speci ￿cally, when we measure the average bandwidth consump-
tion by these three benchmarks in the QPS range that saturates the
throughput of the benchmarks, we observe that mixed workloads
consumes 32 GB/s while compose posts andread user timelines
consume only 7 GB/s and 10 GB/s, respectively. When a given ap-
plication consumes such high bandwidth in our setup, we observe
that the application’s throughput, which is inversely proportional
to its latency, becomes sensitive to the bandwidth available for the
application (§5.2). Lastly, as the QPS approaches to 11 K, the com-
pute capability of the CPU cores becomes the dominant bottleneck,
leading to a decrease in the p99 latency gap between DDR 100%
and CXL 100%.
FIO. Figure 8 presents the p99 latency values of FIOwith 4 GB
page cache allocated to either DDR memory or CXL memory for
various I/O block sizes. We use a Zipfian distribution for FIOto
evaluate the impact of using page cache on ￿le-system performance.
It shows that allocating the page cache to CXL memory gives only
⇠3% longer p99 latency than DDR5 memory for 4 KB block size.
8This is because the p99 latency for a 4 KB block size is primarily
dominated by the Linux kernel operations related to page cache
management, such as context switching, page cache lookup, send-
ing an I/O request through the ￿le system, block layer, and device
driver. However, with a 8 KB block size, the cost of Linux kernel
operations is amortized, as multiple 4 KB pages are brought from a
storage device by a single system call. Consequently, longer access
latency of CXL memory a ￿ects the p99 latency of FIOmore no-
tably, resulting in a ⇠4.5% increase in the p99 latency. Meanwhile,
as the block size increases beyond 8 KB, the page cache hit rate
decreases from 76% for 8 KB to 65% for 128 KB. As lower page cache
hit rates necessitate more page transfers from the storage device,
the storage access latency begins to dominate the p99 latency. In
such a case, the di ￿erence in memory access latency between DDR
memory and CXL memory exhibits a lower impact on p99 latency,
since Data Direct I/O (DDIO) [ 38] directly injects pages read from
the storage device into the LLC [ 3,26,27,93]. Lastly, we observe
another trend shift beyond 128 KB block size, which is mainly due
to the limited read and write bandwidth of CXL memory. As more
page cache entries are evicted from the LLC to memory as well as
from memory to the storage device, the limited bandwidth of CXL
memory increases the e ￿ective latency of I/O requests.
Key￿ndings. Based on our analyses above, we present the follow-
ing three key Findings. (F1)Allocating any percentage of pages to
CXL memory proportionally increases the p99 latency of simple
memory-intensive applications demanding `s-scale latency, since
such applications are highly sensitive to memory access latency.
(F2)Even an intelligent page migration policy may further increase
the p99 latency of such latency-sensitive applications because of
the overheads of migrating pages. (F3)Judiciously allocating cer-
tain pages to CXL memory does not increase the p99 latency of
complex applications exhibiting <s-scale latency. This is because
the long latency of accessing CXL memory marginally contributes
to the end-to-end latency of such applications and it is amortized
by intermediate operations between accesses to CXL memory.
5.2 Throughput
DLRM. Figure 9a shows the throughput of DLRM embedding re-
duction for various percentages of pages allocated to CXL mem-
ory. As the throughput of DLRM embedding reduction is bounded
by memory bandwidth [ 50,58,94], it begins to saturate over 20
threads when 100% of pages are allocated to DDR memory. In such
a case, we observe that allocating a certain percentage of pagesTable 3: Throughput of DLRM using only 1 SNC node versus all
4 SNC nodes, normalized to the throughput of DLRM running
on 1 SNC node allocating all the pages to local DDR memory.
1 SNC node 4 SNC nodes
DDR 100% CXL 100% DDR 100% CXL 100%
1 0.947 1 0.504
to CXL memory can improve the throughput further, as it supple-
ments to the bandwidth of DDR memory, increasing the total band-
width available for DLRM . For instance, when running 32 threads,
we observe that allocating 63% of pages to CXL memory can maxi-
mize the throughput of DLRM embedding reduction, providing 88%
higher throughput than DDR 100%. Note that a lower percentage of
pages will be allocated to CXL memory for achieving the maximum
throughput if the maximum bandwidth capability of a given CXL
memory device is lower ( e.g., CXL-C). This clearly demonstrates
the bene ￿t of CXL memory as a memory bandwidth expander.
Redis. Although Redis is a latency-sensitive application, its through-
put is also an important performance metric. Figure 9b shows the
maximum sustainable QPS for various percentages of pages allo-
cated to CXL memory. For example, for YCSB-A , allocating 25%,
50%, 75%, and 100% of pages to CXL memory provides 8%, 15%, 22%,
and 30% lower throughput than allocating 100% of pages to DDR
memory. As Redis does not fully utilize the memory bandwidth,
its throughput is bounded by memory access latency. Thus, similar
to its p99 latency trends (Figure 6a), allocating more pages to CXL
memory reduces the throughput of Redis .
Key￿ndings. Based on our analyses above, we present the follow-
ing key Finding. (F4)For memory-bandwidth-intensive applica-
tions, naïvely allocating 50% of pages to CXL memory based on the
OS default policy may result in lower throughput than allocating
100% of pages to DDR memory, even with higher total bandwidth
from using both DDR memory and CXL memory together. This
motivates us to develop a dynamic page allocation policy that can
automatically con ￿gure the percentage of pages allocated to CXL
memory at runtime based on the bandwidth capability of a given
CXL memory device and bandwidth consumed by co-running ap-
plications (§6).
5.3 Interaction with Cache Hierarchy
Previously, we discussed that accessing CXL memory breaks the
LLC isolation among SNC nodes (§4.3). To analyze the impact of
0246810121416
48121620242832Inference Throughput (M-queries/sec)Thread CountDDR5:CXL-A = 100:083:1762:3850:5037:6317:830:100
(a) [DLRM] embedding reduction
0.00.20.40.60.81.01.2
ABCDFMax QPS Normalized to DDR100%
WorkloadDDR5:CXL-A = 100:075:2550:5025:750:100(b) [Redis] YCSB-A
Figure 9: Impact of using CXL memory on throughput of Redis andDLRM for various ratios of page allocation to CXL memory.
9such an attribute of accessing CXL memory on application perfor-
mance, we evaluate two cases. In the ￿rst case (‘1 SNC node’ in
Table 3), only one SNC node ( i.e., SNC-0 in Figure 5) runs 8 DLRM
threads while the other three SNC nodes idle. In the second case (‘4
SNC nodes’ in Table 3), each SNC node runs 8 DLRM threads. Only
SNC-0 allocates 100% of its pages to either its DDR memory or CXL
memory, while the other three SNC nodes ( i.e., SNC-1, SNC-2, and
SNC-3) allocate 100% of their pages only to their respective local
DDR memory. The second case is introduced to induce interference
at the LLC among all the SNC nodes when SNC-0 with CXL 100%.
Table 3 shows that SNC-0 with CXL 100% in ‘1 SNC node’ o ￿ers
88% higher throughput than SNC-0 with CXL 100% in ‘4 SNC nodes. ’
This is because of the other three SNC nodes in ‘4 SNC nodes’
reduces the e ￿ective LLC capacity of SNC-0 with CXL 100% ( i.e.,
LLC slices from all the SNC nodes). Speci ￿cally, while the other
three SNC nodes evict LLC lines within their respective LLC slices,
they also inevitably evict many LLC lines from SNC-0 with CXL
100%. Although not shown in Table 3, SNC-0 with DDR 100% in ‘1
SNC node’ provides 2% higher throughput than each of the other
three SNC nodes in ‘4 SNC nodes’ when SNC-0 in ‘4 SNC nodes’ is
with CXL 100%. This is because SNC-0 with CXL 100% in ‘4 SNC
nodes’ pollutes the LLC slices of the other three SNC nodes with
cache lines evicted from the L2 caches of SNC-0, breaking the LLC
isolation among the SNC nodes. Lastly, in our previous evaluation
ofDLRM throughput (§5.2), when SNC-0 needs to run more than 8
threads of DLRM in the SNC mode, it makes the remaining threads
run on the CPU cores in the other three SNC nodes. Nonetheless,
the CPU cores in the other three SNC nodes continue to access the
DDR memory of SNC-0, and cache lines in the L2 caches of these
CPU cores are still evicted to the LLC slices of SNC-0 since the
cache lines were from the DDR memory of SNC-0.
6 CXL-MEMORY-AWARE DYNAMIC
PAGE ALLOCATION POLICY
We have demonstrated a potential of CXL memory as a band-
width expander, which can improve the performance of bandwidth-
intensive applications (§5.2). If the throughput of a given application
is limited by the bandwidth, allocating a higher percentage of pages
to CXL memory may alleviate bandwidth pressure on DDR memory,
and hence reduce average memory access latency. Intuitively, such
a percentage should be tuned for di ￿erent CXL memory devices
given their distinct bandwidth capabilities (§5.2). By contrast, if a
given application is not memory-bandwidth-bounded, allocating
a lower percentage of pages to CXL memory may lead to lower
average memory access latency and thus higher throughput. That
stated, to better utilize auxiliary memory bandwidth provided by
CXL memory, we present Caption , a dynamic page allocation pol-
icy.Caption automatically tunes the percentage of new pages to
be allocated by the OS to CXL memory based on three factors: (1)
bandwidth capability of CXL memory, (2) memory intensiveness of
co-running applications, and (3) average memory access latency.
Note that Caption , focusing on the page allocation ratio between
DDR memory and CXL memory, is orthogonal and complementary
toTPP.
MonitorEstimatorTuner
Collect/parsePredict throughputPMU-toolsPCM-*eBPFPerf. metricsBinary searchNext page allocation ratioLinear regression modelMachine learning modelAllocationratioThroughputFigure 10: Overview of Caption . The components in dotted
boxes can be used for better performance.
6.1 Policy Design
Caption consists of three runtime Modules (Figure 10). (M1) peri-
odically monitors some CPU counters related to memory subsystem
performance, and then (M2) estimates memory-subsystem perfor-
mance based on values of the counters. When a given application
requests an allocation of new pages, (M3) tunes the percentage of
the new pages allocated to CXL memory, aiming to improve the
throughput of the application. Subsequently, mempolicy [85] sets
the page allocation ratio between DDR memory and CXL memory
based on the percentage guided by (M3), and instructs the OS to
allocate the new pages based on the ratio.
(M1) Monitoring CPU counters related to memory subsys-
tem performance. We use Intel PCM [ 36] to periodically sample
various CPU counters related to memory subsystem performance,
as listed in Table 4. These CPU counters allow (M2) to estimate
overall memory-subsystem performance. In Figure 11, we run DLRM ,
of which the throughput is bounded by memory bandwidth. Then
we observe correlations between DLRM throughput and values of
those counters, as we vary the percentage of pages allocated to
CXL memory.
Figure 11a shows that DLRM throughput is proportional to the
consumed memory bandwidth. Yet, as the consumed memory band-
width exceeds a certain amount, the memory access latency rapidly
increases due to contention and resulting queuing delay at the mem-
ory controller [ 80], which, in turn, decreases the DLRM throughput.
Meanwhile, Figure 11b shows that DLRM throughput is inversely
proportional to L1 miss latency. The L1 miss latency is an impor-
tant memory-subsystem performance metric that simultaneously
captures both the cache friendliness and bandwidth intensiveness
(i.e., queuing delay at the memory controller) of given (co-running)
applications at the same time. At ￿rst, allocating more pages to CXL
memory reduces pressure on the DDR memory controller, thereby
decreasing the latency of accessing DDR memory and handling
L1 misses. However, at some point, the long latency of accessing
CXL memory begins to dominate that of handling L1 misses, and
Table 4: CPU counters pertinent to memory-subsystem perf.
Metric Tool Description
L1 miss latency pcm-latency Average L1 miss latency (ns)
DDR read latency pcm-latency DDR read latency (ns)
IPC pcm Instructions per cycle
100.40.60.81.01.21.4
10152025303540Norm. DLRM ThroughputSystem Bandwidth (GB/s)(a) System bandwidth
0.81.01.21.41.61.82.02.2
20406080100L1 Miss Latency (ns)(b) L1 miss latency
Figure 11: Correlations between throughput and various
counter values, as we increase the percentage of pages al-
located to CXL memory for DLRM . The system bandwidth is
the total consumed memory bandwidth, and The throughput
is normalized to DDR 100%.
the application throughput begins to decrease. Finally, IPC is an-
other important metric that implicitly measures the e ￿ciency of
the memory subsystem for the applications.
(M2) Estimating system throughput. To build a model that esti-
mates the system performance, we collect CPU counter values at
various DDR:CXL ratios while running DLRM with 24 threads. We
then build a linear-regression model that correlates these counter
values with DLRM throughput. Taking these counter values from
(M1), Caption periodically estimates (or infers) memory-subsystem
performance at runtime. In our current implementation of Caption ,
(M1) samples the counters every 1 second. To reduce the noise
among the values, we collect a moving average of the past 5 sam-
ples for each counter. The averaged value is then fed into (M2) for
performance estimation. Although we may use a machine-learning
(ML) model, we use the following simple linear model for the cur-
rent implementation of Caption :
.=V0+V1-1+V2-2+... (1)
where .represents the estimated memory-subsystem performance,
-=represents a counter value listed in Table 4, and V=represents
the-=’s weight obtained through multiple linear regression steps.
This linear model is simple enough to be used by the OS at a low
performance cost, yet e ￿ective enough to estimate the memory-
subsystem performance. In the current implementation of Caption ,
we￿nd that using PCMtoolkit is su ￿cient. Nonetheless, we may use
PMU tools and eBPF [ 24] to access more diverse counters, facilitat-
ing a more precise estimation of memory-subsystem performance.
(M3) Tuning the percentage of pages allocated to CXL mem-
ory. When a given application demands allocation of memory
pages, Caption (Algorithm 1) ￿rst compares the estimated memory-
subsystem performance value from the past period (line: 9–11) with
the current period (line: 3). If the memory-subsystem performance
in the current period has increased compared to the previous pe-
riod, Caption assumes that its previous decision, i.e., increasing
(or decreasing) the percentage of pages allocated to CXL memory,
was correct. Then it continues to incrementally increase (or de-
crease) the percentage by a ￿xed amount (line: 5). Otherwise, it
will begin to reverse the step by half (line: 4), which decreases (or
increases) the percentage and evaluate the decision in the future
period to determine a favorable percentage of pages allocated to
CXL memory. Note that the absolute value of the step variable hasAlgorithm 1: Caption tuning algorithm. state ,step and
ratio represent memory subsystem performance, unit of
tuning page allocation ratio, and ratio of page allocation
between DDR and CXL memory.
1while CAD4 do
2 2DAA _BC0C4  4BC8<0C>A ()
3 if2DAA _BC0C4 <?A4E _BC0C4 then
4 2DAA _BC4?  ?A4E _BC4? ⇥( 0.5)//reverse
5 2DAA _A0C8>  ?A4E _A0C8> +2DAA _BC4?
6 2⌘42: _A0C8> _1>D=3 ()
7 B4C_A0C8> (2DAA _A0C8> )
8 ifnew allocations then
9 ?A4E _BC0C4  2DAA _BC0C4
10 ?A4E _BC4?  2DAA _BC4?
11 ?A4E _A0C8>  2DAA _A0C8>
12 B;44? (CD=4 _8=C4AE0; )
the minimum limit ( e.g., 9% in our evaluation) to prevent it from
being close to zero. Lastly, inspired by conventional control theory,
Caption implements mechanisms to e ￿ciently handle very small
or sudden large changes in memory subsystem performance, even
though they are not described in Algorithm 1.
6.2 Evaluation
We have developed Caption after analyzing the various perfor-
mance characteristics and memory subsystem statistics of DLRM .
However, we expect that Caption should work well for other appli-
cations because the monitored L1 miss latency, DDR read latency,
and IPC counters are fundamental memory subsystem performance
metrics that are strongly correlated with the throughput of memory-
bandwidth-intensive applications; we believe CXL memory access
latency and bandwidth statistics are also useful for estimating mem-
ory subsystem performance, but we currently cannot access the
corresponding counters. To demonstrate this, we evaluate the ef-
￿cacy of Caption by co-running (1) SPEC-Mix , various mixes of
memory-intensive SPECrate CPU2017 benchmarks, and (2) Redis
andDLRM without measuring their performance characteristics and
memory-subsystem statistics in advance.
Figure 12 shows normalized measured throughput ( Throughput ),
normalized estimated memory-subsystem performance (Eq. (1),
Model Output ), and Pearson correlation coe ￿cient values. For DLRM
we simply sweep the percentage of pages allocated to CXL memory
over time. For SPEC-Mix , we let Caption automatically tune the per-
centage of pages allocated to CXL memory whenever a benchmark
completes its execution. The Pearson correlation method allows us
to quantify synchrony between time-series data [ 9]. The coe ￿cient
value can range from -1 and 1, indicating that both sets of data
trend the same direction when it is positive. We calculate the Pear-
son correlation coe ￿cient values to assess the e ￿ectiveness of the
estimation model, as Algorithm 1 depends on precisely determining
only the direction of performance changes after tuning the percent-
age of pages allocated to CXL memory. Figure 12 demonstrates that
the Pearson correlation coe ￿cient values mostly remains positive
for both DLRM and SPEC-Mix . This indicates that the estimation
110.01.02.03.04.00.00.51.01.52.01815222936435057647178859299106113120127134141148155162169176183190Model Output/Pearson CorrelationNormalized ThroughputThroughputModel OutputPearson Correlation
9%23%33%41%47%326496192Time (s)0128160(a) DLRM
-2.0-1.00.01.02.03.04.05.00.960.981.001.021.041.0619017926835744653562471380289198010691158124713361425151416031692178118701959204821372226231524042493Model Output/Pearson CorrelationNormalized ThroughputThroughputModel OutputPearson Correlation
29%41%38%33%512102415362560Time (s)020489%
(b) SPEC-Mix
Figure 12: Estimated memory-subsystem performance, mea-
sured application throughput, and Pearson coe ￿cient values
over time. The numbers represent the percentage values of
pages allocated to CXL memory.
model is adequate for Algorithm 1 to e ￿ectively tune both DLRM
andSPEC-Mix . It is important to note that the estimation model is
based on the weight values derived by ￿tting counter values from
DLRM exclusively in this paper. However, it has the potential for
further improvement by ￿tting counter values from a more diverse
range of applications.
Figure 13 evaluates the e ￿cacy of Caption for 16 instances of
individual SPEC benchmarks, two di ￿erent SPEC-Mix , and a mix
ofRedis andDLRM . For all the evaluation cases, Caption outper-
forms both 100% and 50% allocations to DDR memory while allocat-
ing substantial percentages of pages to CXL memory. Speci ￿cally,
Caption o￿ers 19%, 18%, 8%, and 20% higher throughput values for
fotonik3d ,mcf,roms , and cactuBSSN , respectively, than the best
static allocation policy ( i.e., 100% or 50% allocation to DDR memory),
allocating 29%–41% of pages to CXL memory in a steady state. For
the mixes of mcfandroms ,cactuBSSN androms , and Redis and
DLRM ,Caption provides 24%, 1%, and 4% higher throughput values
than the best static allocation policy, allocating 33%–41% of pages
to CXL memory. Since DLRM andRedis use di ￿erent throughput
metrics, we show a geometric mean value of normalized through-
put values of DLRM andRedis as a single throughput value. These
demonstrate that Caption captures the immense memory pres-
sure from co-running applications and tunes to the percentage of
pages to CXL memory that yields higher throughput than the static
allocation policies.
In Figure 13, we do not compare Caption with the static al-
location policy for DLRM andRedis individually. This is because
we have derived the estimation model after running DLRM (§6.1)
and demonstrated that allocating all the pages to DDR memory is
best for Redis (§5.2). However, our evaluation shows that Caption
presents 80% higher and 4% lower throughput values than allocat-
ing 100% and 50% of pages to DDR memory, respectively, for DLRM .
0.00.20.40.60.81.01.21.4
fotonik3dmcfcactuBSSNromsroms+mcfroms+cactuRedis+DLRMPerformanceNormalized to 50:50DDR5:CXL-A = 100:050:50Caption41%29%33%41%33%41%41%Figure 13: Throughput of each evaluated benchmark or mix,
normalized to that with the default static policy allocating
50% of pages to CXL memory. A number atop each bar is the
percentage of pages allocated to CXL memory by Caption .
ForRedis ,Caption is able to identify that allocating more mem-
ory to low-latency DDR memory is bene ￿cial, and thus o ￿ers 3.2%
higher throughput than allocating 50% of pages to DDR memory
but 8.6% lower throughput than allocating 100% of pages to DDR
memory. Albeit not perfect, Caption demonstrates its capability
of searching near-optimal percentage values of pages allocated to
CXL memory without any guidance from users and/or applications
for several workloads with notably di ￿erent characteristics. Lastly,
one of our primary goals is to emphasize the need for a dynamic
page allocation policy and show a potential of such a policy. Hence,
we leave further enhancement of Caption as future work.
7 RELATED WORK
With the rapid development of memory technologies, diverse het-
erogeneous memory devices have been introduced. These memory
devices are often di ￿erent from the standard DDR-based DRAM
devices, and each memory device o ￿ers unique characteristics and
trade-o ￿s. These include but are not limited to persistent mem-
ory, such as Intel Optane DIMM [ 84,88,90], remote/disaggregated
memory [ 12,21,30,46,54,60,76], and even byte-addressable
SSD [ 1,8]. These heterogeneous memory devices in the memory
hierarchy of datacenters have been applied to diverse domains of
applications. For example, in a tiered memory/ ￿le system, pages
can be dynamically placed, cached, and migrated across di ￿erent
memory devices, based on their hotness and persistency require-
ments [ 2,5,14,22,23,31,48,49,51,61,64,68,78,79,81,82,89,97].
Besides, database or key-value store can leverage these memory de-
vices for faster and more scalable data organization and retrieval [ 10,
16,17,45,47,53,92,95,98]. Solutions similar to Caption were pro-
posed in the context of HBM [ 18] and storage system [ 87]. While
they have been extensively pro ￿led and studied, CXL memory, as
a new member in the memory tier, still has unclear performance
characteristics and indications, especially its interaction with CPUs.
This leads to new challenges and opportunities for applying CXL
memory to the aforementioned domains. This paper aims to bridge
the gap of CXL memory understanding, and thus enable the wide
adoption of CXL memory in the community. Lastly, our Caption
is speci ￿cally optimized for CXL memory, making the most out of
the memory bandwidth available for a given system.
Since the inception of the concept in 2019, CXL has been heav-
ily discussed and invested by researchers and practitioners. For
instance, Meta envisioned using CXL memory for memory tiering
12and swapping [ 64,86]; Microsoft built a CXL memory prototype sys-
tem for memory disaggregation exploration [ 11,59]. Most of them
used NUMA servers to emulate the behavior of CXL memory. There
are also e ￿orts in building software-based CXL simulators [ 91,96].
Gouk et al. built a CXL memory prototype on FPGA-based RISC-
V CPU [ 29]. There are also a body of work focusing on certain
particular applications [ 52,71,74]. Di￿erent from the prior stud-
ies, this paper presents the ￿rst comprehensive study on true CXL
memory and compares it with emulated CXL memory using the
commercial high-performance CPU and CXL devices with both
microbenchmarks and widely-used applications, which can bet-
ter help the design space exploration of both CXL-memory based
software systems and simulators.
8 CONCLUSION
In this paper, we have taken a ￿rst step to analyze the device-
speci ￿c characteristics of true CXL memory and compared them
with NUMA-based emulations, a common practice in CXL research.
Our analysis revealed key di ￿erences between emulated and true
CXL memory, with important performance implications. Our analy-
sis also identi ￿ed opportunities to e ￿ectively use CXL memory as a
memory bandwidth expander for memory-bandwidth-intensive ap-
plications, which leads to the development of a CXL-memory-aware
dynamic page allocation policy and demonstrated its e ￿cacy.
ACKNOWLEDGMENTS
We would like to thank Robert Blankenship, Miao Cai, Bhushan
Chitlur, Pekon Gupta, David Koufaty, Chidamber Kulkami, Henry
Peng, Andy Rudo ￿, Deshanand Singh, and Alexander Yu. This work
was supported in part by grants from Samsung Electronics, PRISM,
one of the seven centers in JUMP 2.0, a Semiconductor Research
Corporation (SRC) program sponsored by DARPA, and NRF funded
by the Korean Government MSIT (NRF-2018R1A5A1059921). Nam
Sung Kim has a ￿nancial interest in Samsung Electronics and Neu-
roRealityVision.
REFERENCES
[1]Ahmed Abulila, Vikram Sharma Mailthody, Zaid Qureshi, Jian Huang, Nam Sung
Kim, Jinjun Xiong, and Wen-mei Hwu. 2019. FlatFlash: Exploiting the Byte-
Accessibility of SSDs within a Uni ￿ed Memory-Storage Hierarchy. In Proceedings
of the 24th ACM International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS’19) .
[2]Neha Agarwal and Thomas F. Wenisch. 2017. Thermostat: Application-
Transparent Page Management for Two-Tiered Main Memory. In Proceedings of
the 22nd ACM International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS’17) .
[3]Mohammad Alian, Yifan Yuan, Jie Zhang, Ren Wang, Myoungsoo Jung, and
Nam Sung Kim. 2020. Data Direct I/O Characterization for Future I/O System
Exploration. In Proceedings of the IEEE International Symposium on Performance
Analysis of Systems and Software (ISPASS’20) .
[4]AMD. accessed in 2023. 4th Gen AMD EPYC ™Processor Architecture. https:
//www.amd.com/en/campaigns/epyc-9004-architecture.
[5]Thomas E. Anderson, Marco Canini, Jongyul Kim, Dejan Kosti ć, Youngjin Kwon,
Simon Peter, Waleed Reda, Henry N. Schuh, and Emmett Witchel. 2020. Assise:
Performance and Availability via Client-local NVM in a Distributed File System.
InProceedings of the 14th USENIX Symposium on Operating Systems Design and
Implementation (OSDI’20) .
[6]Moiz Arif, Kevin Assogba, M. Mustafa Ra ￿que, and Sudharshan Vazhkudai. 2023.
Exploiting CXL-Based Memory for Distributed Deep Learning. In Proceedings of
the 51st International Conference on Parallel Processing (ICPP’22) .
[7]Jens Axboe. accessed in 2023. Flexible I/O Tester. https://github.com/axboe/ ￿o.
[8]Duck-Ho Bae, Insoon Jo, Youra Adel Choi, Joo-Young Hwang, Sangyeun Cho,
Dong-Gi Lee, and Jaeheon Jeong. 2018. 2B-SSD: The Case for Dual, Byte- andBlock-Addressable Solid-State Drives. In Proceedings of the ACM/IEEE 45th Annual
International Symposium on Computer Architecture (ISCA’18) .
[9]Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Noise
Reduction in Speech Processing . Springer.
[10] Lawrence Benson, Hendrik Makait, and Tilmann Rabl. 2021. Viper: An E ￿cient
Hybrid PMem-DRAM Key-Value Store. Proceedings of the VLDB Endowment
(2021).
[11] Daniel S. Berger, Daniel Ernst, Huaicheng Li, Pantea Zardoshti, Monish Shah,
Samir Rajadnya, Scott Lee, Lisa Hsu, Ishwar Agarwal, Mark D. Hill, and Ricardo
Bianchini. 2023. Design Tradeo ￿s in CXL-Based Memory Pools for Public Cloud
Platforms. IEEE Micro (2023).
[12] Shai Bergman, Priyank Faldu, Boris Grot, Lluís Vilanova, and Mark Silberstein.
2022. Reconsidering OS Memory Optimizations in the Presence of Disaggregated
Memory. In Proceedings of the ACM SIGPLAN International Symposium on Memory
Management (ISMM’22) .
[13] James Bucek, Klaus-Dieter Lange, and Jóakim v. Kistowski. 2018. SPEC CPU2017:
Next-Generation Compute Benchmark. In Companion of the ACM/SPEC Interna-
tional Conference on Performance Engineering (ICPE’18) .
[14] Irina Calciu, M. Talha Imran, Ivan Puddu, Sanidhya Kashyap, Hasan Al Maruf,
Onur Mutlu, and Aasheesh Kolli. 2021. Rethinking Software Runtimes for Dis-
aggregated Memory. In Proceedings of the 26th ACM International Conference on
Architectural Support for Programming Languages and Operating Systems (ASP-
LOS’21) .
[15] Daniel W. Chang, Gyungsu Byun, Hoyoung Kim, Minwook Ahn, Soojung Ryu,
Nam S. Kim, and Michael Schulte. 2013. Reevaluating the Latency Claims of
3D Stacked Memories. In Proceedings of the 18th Asia and South Paci ￿c Design
Automation Conference (ASP-DAC’18) .
[16] Youmin Chen, Youyou Lu, Kedong Fang, Qing Wang, and Jiwu Shu. 2020. uTree:
a Persistent B+-Tree with Low Tail Latency. Proceedings of the VLDB Endowment
(2020).
[17] Youmin Chen, Youyou Lu, Fan Yang, Qing Wang, Yang Wang, and Jiwu Shu. 2020.
FlatStore: An E ￿cient Log-Structured Key-Value Storage Engine for Persistent
Memory. In Proceedings of the 25th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems (ASPLOS’20) .
[18] Chiachen Chou, Aamer Jaleel, and Moinuddin Qureshi. 2017. BATMAN: Tech-
niques for Maximizing System Bandwidth of Memory Systems with Stacked-
DRAM. In Proceedings of the International Symposium on Memory Systems (MEM-
SYS’17) .
[19] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell
Sears. 2010. Benchmarking Cloud Serving Systems with YCSB. In Proceedings of
the 1st ACM Symposium on Cloud Computing (SoCC’10) .
[20] CXL Consortium. accessed in 2023. Compute Express Link (CXL). https://www.
computeexpresslink.org.
[21] Aleksandar Dragojevi ć, Dushyanth Narayanan, Orion Hodson, and Miguel Castro.
2014. FaRM: Fast Remote Memory. In Proceedings of the 11th USENIX Conference
on Networked Systems Design and Implementation (NSDI’14) .
[22] Zhuohui Duan, Haikun Liu, Xiaofei Liao, Hai Jin, Wenbin Jiang, and Yu Zhang.
2019. HiNUMA: NUMA-Aware Data Placement and Migration in Hybrid Memory
Systems. In Proceedings of the IEEE 37th International Conference on Computer
Design (ICCD’19) .
[23] Padmapriya Duraisamy, Wei Xu, Scott Hare, Ravi Rajwar, David Culler, Zhiyi
Xu, Jianing Fan, Christopher Kennelly, Bill McCloskey, Danijela Mijailovic, Brian
Morris, Chiranjit Mukherjee, Jingliang Ren, Greg Thelen, Paul Turner, Carlos
Villavieja, Parthasarathy Ranganathan, and Amin Vahdat. 2023. Towards an
Adaptable Systems Architecture for Memory Tiering at Warehouse-Scale. In
Proceedings of the 28th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems (ASPLOS’23) .
[24] eBPF.io. accessed in 2023. eBPF Documentation. https://ebpf.io/what-is-ebpf/.
[25] Samsung Eletronics. accessed in 2023. Scalable Memory Development Kit v1.3.
https://github.com/OpenMPDK/SMDK.
[26] Alireza Farshin, Amir Roozbeh, Gerald Q. Maguire Jr., and Dejan Kosti ć. 2019.
Make the Most out of Last Level Cache in Intel Processors. In Proceedings of the
14th European Conference on Computer Systems (EuroSys’19) .
[27] Alireza Farshin, Amir Roozbeh, Gerald Q. Maguire Jr., and Dejan Kosti ć. 2020.
Reexamining Direct Cache Access to Optimize I/O Intensive Applications for
Multi-hundred-gigabit Networks. In Proceedings of the USENIX Annual Technical
Conference (ATC’20) .
[28] Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha Shetty, Priyal Rathi, Nayan Katarki,
Ariana Bruno, Justin Hu, Brian Ritchken, Brendon Jackson, Kelvin Hu, Meghna
Pancholi, Yuan He, Brett Clancy, Chris Colen, Fukang Wen, Catherine Leung,
Siyuan Wang, Leon Zaruvinsky, Mateo Espinosa, Rick Lin, Zhongling Liu, Jake
Padilla, and Christina Delimitrou. 2019. An Open-Source Benchmark Suite for
Microservices and Their Hardware-Software Implications for Cloud & Edge
Systems. In Proceedings of the 24th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems (ASPLOS’19) .
[29] Donghyun Gouk, Miryeong Kwon, Hanyeoreum Bae, Sangwon Lee, and Myoung-
soo Jung. 2023. Memory Pooling with CXL. IEEE Micro (2023).
13[30] Juncheng Gu, Youngmoon Lee, Yiwen Zhang, Mosharaf Chowdhury, and Kang G.
Shin. 2017. E ￿cient Memory Disaggregation with INFINISWAP. In Proceedings
of the 14th USENIX Conference on Networked Systems Design and Implementation
(NSDI’17) .
[31] Manish Gupta, Vilas Sridharan, David Roberts, Andreas Prodromou, Ashish
Venkat, Dean Tullsen, and Rajesh Gupta. 2018. Reliability-Aware Data Place-
ment for Heterogeneous Memory Architecture. In Proceedings of the 24th IEEE
International Symposium on High Performance Computer Architecture (HPCA’18) .
[32] Rambus Incorporated. accessed in 2023. Memory Interface Chips – CXL Memory
Interconnect Initiative. https://www.rambus.com/memory-and-interfaces/cxl-
memory-interconnect/.
[33] Intel Corporation. accessed in 2023. 4th Gen Intel Xeon Processor Scalable Family,
sapphire rapids. https://www.intel.com/content/www/us/en/developer/articles/
technical/fourth-generation-xeon-scalable-family-overview.html.
[34] Intel Corporation. accessed in 2023. Di ￿erence of Cache Memory Be-
tween CPUs for Intel Xeon E5 Processors and Intel Xeon Scalable Proces-
sors. https://www.intel.com/content/www/us/en/support/articles/000027820/
processors/intel-xeon-processors.html.
[35] Intel Corporation. accessed in 2023. Intel Launches 4th Gen Xeon Scalable
Processors, Max Series CPUs. https://www.intel.com/content/www/us/
en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-
gpus.html#gs.o28z2f.
[36] Intel Corporation. accessed in 2023. Intel Performance Counter Monitor. https:
//github.com/intel/pcm.
[37] Intel Corporation. accessed in 2023. Intel Xeon Gold 6430 Proces-
sor. https://ark.intel.com/content/www/us/en/ark/products/231737/intel-xeon-
gold-6430-processor-60m-cache-2-10-ghz.html.
[38] Intel Corporation. accessed in 2023. Intel ®Data Direct I/O (DDIO). https:
//www.intel.com/content/www/us/en/io/data-direct-i-o-technology.html.
[39] Intel Corporation. accessed in 2023. Intel ®64 and IA-32 Architectures Optimiza-
tion Reference Manual. https://cdrdv2-public.intel.com/671488/248966-046A-
software-optimization-manual.pdf.
[40] Intel Corporation. accessed in 2023. Intel ®Agilex ™7 FPGA I-Series Devel-
opment Kit. https://www.intel.com/content/www/us/en/products/details/fpga/
development-kits/agilex/i-series/dev-agi027.html.
[41] Intel Corporation. accessed in 2023. Intel ®FPGA Compute Express Link
(CXL) IP. https://www.intel.com/content/www/us/en/products/details/fpga/
intellectual-property/interface-protocols/cxl-ip.html.
[42] Intel Corporation. accessed in 2023. Intel ®Memory Latency Checker
v3.10. https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-
memory-latency-checker.html.
[43] Intel Corporation. accessed in 2023. Performance Analysis Guide
for Intel ®Core ™i7 Processor and Intel ®Xeon ™5500 processors.
https://www.intel.com/content/dam/develop/external/us/en/documents/
performance-analysis-guide-181827.pdf.
[44] JEDEC – Global Standards for the Microelectronics Industry. accessed in 2023.
Main Memory: DDR4 & DDR5 SDRAM. https://www.jedec.org/category/
technology-focus-area/main-memory-ddr3-ddr4-sdram.
[45] Olzhas Kaiyrakhmet, Songyi Lee, Beomseok Nam, Sam H. Noh, and Young-ri
Choi. 2019. SLM-DB: Single-Level Key-Value Store with Persistent Memory.
InProceedings of the 17th USENIX Conference on File and Storage Technologies
(FAST’19) .
[46] Anuj Kalia, David Andersen, and Michael Kaminsky. 2020. Challenges and
Solutions for Fast Remote Persistent Memory Access. In Proceedings of the 11th
ACM Symposium on Cloud Computing (SoCC’20) .
[47] Sudarsun Kannan, Nitish Bhat, Ada Gavrilovska, Andrea C. Arpaci-Dusseau, and
Remzi H. Arpaci-Dusseau. 2018. Redesigning LSMs for Nonvolatile Memory with
NoveLSM. In Proceedings of the USENIX Annual Technical Conference (ATC’18) .
[48] Sudarsun Kannan, Ada Gavrilovska, Vishal Gupta, and Karsten Schwan. 2017.
HeteroOS: OS Design for Heterogeneous Memory Management in Datacenter. In
Proceedings of the ACM/IEEE 44th Annual International Symposium on Computer
Architecture (ISCA’17) .
[49] Sudarsun Kannan, Yujie Ren, and Abhishek Bhattacharjee. 2021. KLOCs: Kernel-
Level Object Contexts for Heterogeneous Memory Systems. In Proceedings of
the 26th ACM International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS’21) .
[50] Liu Ke, Udit Gupta, Benjamin Youngjae Cho, David Brooks, Vikas Chandra, Utku
Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Meng
Li, Bert Maher, Dheevatsa Mudigere, Maxim Naumov, Martin Schatz, Mikhail
Smelyanskiy, Xiaodong Wang, Brandon Reagen, Carole-Jean Wu, Mark Hemp-
stead, and Xuan Zhang. 2020. RecNMP: Accelerating Personalized Recommenda-
tion with near-Memory Processing. In Proceedings of the ACM/IEEE 47th Annual
International Symposium on Computer Architecture (ISCA’20) .
[51] Jonghyeon Kim, Wonkyo Choe, and Jeongseob Ahn. 2021. Exploring the Design
Space of Page Management for Multi-Tiered Memory Systems. In Proceedings of
the USENIX Annual Technical Conference (ATC’21) .
[52] Kyungsan Kim, Hyunseok Kim, Jinin So, Wonjae Lee, Junhyuk Im, Sungjoo Park,
Jeonghyeon Cho, and Hoyoung Song. 2023. SMT: Software-De ￿ned MemoryTiering for Heterogeneous Computing Systems With CXL Memory Expander.
IEEE Micro (2023).
[53] Wonbae Kim, Chanyeol Park, Dongui Kim, Hyeongjun Park, Young ri Choi, Alan
Sussman, and Beomseok Nam. 2022. ListDB: Union of Write-Ahead Logs and
Persistent SkipLists for Incremental Checkpointing on Persistent Memory. In
Proceedings of the 16th USENIX Symposium on Operating Systems Design and
Implementation (OSDI’22) .
[54] Vamsee Reddy Kommareddy, Simon David Hammond, Clayton Hughes, Ahmad
Samih, and Amro Awad. 2019. Page Migration Support for Disaggregated Non-
Volatile Memories. In Proceedings of the International Symposium on Memory
Systems (MEMSYS’19) .
[55] Donghyuk Lee, Mike O’Connor, and Niladrish Chatterjee. 2018. Reducing Data
Transfer Energy by Exploiting Similarity within a Data Transaction. In Proceed-
ings of the 24th IEEE International Symposium on High Performance Computer
Architecture (HPCA’18) .
[56] Sukhan Lee, Hyunyoon Cho, Young Hoon Son, Yuhwan Ro, Nam Sung Kim, and
Jung Ho Ahn. 2018. Leveraging Power-Performance Relationship of Energy-
E￿cient Modern DRAM Devices. IEEE Access (2018).
[57] Sukhan Lee, Kiwon Lee, Minchul Sung, Mohammad Alian, Chankyung Kim,
Wooyeong Cho, Reum Oh, Seongil O, Jung Ho Ahn, and Nam Sung Kim. 2018.
3D-Xpath: High-Density Managed DRAM Architecture with Cost-E ￿ective Al-
ternative Paths for Memory Transactions. In Proceedings of the 27th ACM Interna-
tional Conference on Parallel Architectures and Compilation Techniques (PACT’18) .
[58] Yejin Lee, Seong Hoon Seo, Hyunji Choi, Hyoung Uk Sul, Soosung Kim, Jae W.
Lee, and Tae Jun Ham. 2021. MERCI: E ￿cient Embedding Reduction on Com-
modity Hardware via Sub-Query Memoization. In Proceedings of the 26th ACM
International Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS’21) .
[59] Huaicheng Li, Daniel S. Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko
Novakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, Mark D.
Hill, Marcus Fontoura, and Ricardo Bianchini. 2023. Pond: CXL-Based Memory
Pooling Systems for Cloud Platforms. In Proceedings of the 28th ACM International
Conference on Architectural Support for Programming Languages and Operating
Systems (ASPLOS’23) .
[60] Kevin Lim, Jichuan Chang, Trevor Mudge, Parthasarathy Ranganathan, Steven K.
Reinhardt, and Thomas F. Wenisch. 2009. Disaggregated Memory for Expan-
sion and Sharing in Blade Servers. In Proceedings of the ACM/IEEE 36th Annual
International Symposium on Computer Architecture (ISCA’09) .
[61] Lei Liu, Shengjie Yang, Lu Peng, and Xinyu Li. 2019. Hierarchical Hybrid Memory
Management in OS for Tiered Memory Systems. IEEE Transactions on Parallel
and Distributed Systems (2019).
[62] Kevin Loughlin, Stefan Saroiu, Alec Wolman, Yatin A Manerkar, and Baris Kasikci.
2022. MOESI-prime: Preventing Coherence-Induced Hammering in Commodity
Workloads. In Proceedings of the ACM/IEEE 49th Annual International Symposium
on Computer Architecture (ISCA’22) .
[63] Hasan Al Maruf. accessed in 2023. Transparent Page Placement for Tiered-
Memory. https://lore.kernel.org/all/cover.1637778851.git.hasanalmaruf@fb.
com/.
[64] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agarwal,
Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanaujia,
and Prakash Chauhan. 2023. TPP: Transparent Page Placement for CXL-Enabled
Tiered-Memory. In Proceedings of the 28th ACM International Conference on Archi-
tectural Support for Programming Languages and Operating Systems (ASPLOS’23) .
[65] Micron. accessed in 2023. CZ120 memory expansion module. https://www.
micron.com/solutions/server/cxl.
[66] Daniel Molka, Daniel Hackenberg, Robert Schöne, and Wolfgang E Nagel. 2015.
Cache Coherence Protocol and Memory Performance of the Intel Haswell-EP Ar-
chitecture. In Proceedings of the 44th International Conference on Parallel Processing
(ICPP’15) .
[67] Montage Technology. accessed in 2023. CXL Memory eXpander Controller
(MXC). https://www.montage-tech.com/MXC.
[68] Amanda Raybuck, Tim Stamler, Wei Zhang, Mattan Erez, and Simon Peter. 2021.
HeMem: Scalable Tiered Memory Management for Big Data Applications and
Real NVM. In Proceedings of the ACM SIGOPS 28th Symposium on Operating
Systems Principles (SOSP’21) .
[69] Redis Ltd. accessed in 2023. Redis. https://redis.io/.
[70] Robert Blankenship. 2020. Compute Express Link (CXL): Memory and Cache
Protocols. https://snia.org/sites/default/ ￿les/SDC/2020/130-Blankenship-CXL-
1.1-Protocol-Extensions.pdf.
[71] Seokhyun Ryu, Sohyun Kim, Jaeyung Jun, Donguk Moon, Kyungsoo Lee, Jungmin
Choi, Sunwoong Kim, Hyungsoo Kim, Luke Kim, Won Ha Choi, Moohyeon Nam,
Dooyoung Hwang, Hongchan Roh, and Youngpyo Joo. 2023. System Optimization
of Data Analytics Platforms using Compute Express Link (CXL) Memory. In
Proceedings of the IEEE International Conference on Big Data and Smart Computing
(BigComp’23) .
[72] Samsung Semiconductor. accessed in 2023. Memory-Semantic SSD. https://
samsungmsl.com/ms-ssd/.
14[73] Debendra Das Sharma. 2022. Compute Express Link (CXL): Enabling Heteroge-
neous Data-Centric Computing With Heterogeneous Memory Hierarchy. IEEE
Micro (2022).
[74] Joonseop Sim, Soohong Ahn, Taeyoung Ahn, Seungyong Lee, Myunghyun Rhee,
Jooyoung Kim, Kwangsik Shin, Donguk Moon, Euiseok Kim, and Kyoung Park.
2023. Computational CXL-Memory Solution for Accelerating Memory-Intensive
Applications. IEEE Computer Architecture Letters (2023).
[75] Tom Simon. 2021. Low Power High Performance PCIe SerDes IP for Sam-
sung Silicon - SemiWiki. https://semiwiki.com/events/305345-low-power-high-
performance-pcie-serdes-ip-for-samsung-silicon/.
[76] Arjun Singhvi, Aditya Akella, Dan Gibson, Thomas F. Wenisch, Monica Wong-
Chan, Sean Clark, Milo M. K. Martin, Moray McLaren, Prashant Chandra, Rob
Cauble, Hassan M. G. Wassel, Behnam Montazeri, Simon L. Sabato, Joel Scherpelz,
and Amin Vahdat. 2020. 1RMA: Re-Envisioning Remote Memory Access for
Multi-Tenant Datacenters. In Proceedings of the Annual Conference of the ACM
Special Interest Group on Data Communication on the Applications, Technologies,
Architectures, and Protocols for Computer Communication (SIGCOMM’20) .
[77] SK hynix Inc. 2022. SK hynix Introduces Industry’s First CXL-based
Computational Memory Solution (CMS) at the OCP Global Summit.
https://news.skhynix.com/sk-hynix-introduces-industrys- ￿rst-cxl-based-
cms-at-the-ocp-global-summit/.
[78] Jingbo Su, Jiahao Li, Luofan Chen, Cheng Li, Kai Zhang, Liang Yang, and Yinlong
Xu. 2023. Revitalizing the Forgotten On-Chip DMA to Expedite Data Movement
in NVM-based Storage Systems. In Proceedings of the 21st USENIX Conference on
File and Storage Technologies (FAST’23) .
[79] Kshitij Sudan, Karthick Rajamani, Wei Huang, and John B. Carter. 2012. Tiered
Memory: An Iso-Power Memory Architecture to Address the Memory Power
Wall. IEEE Trans. Comput. (2012).
[80] Amin Tootoonchian, Aurojit Panda, Chang Lan, Melvin Walls, Katerina Argyraki,
Sylvia Ratnasamy, and Scott Shenker. 2018. ResQ: Enabling SLOs in Network
Function Virtualization. In Proceedings of 15th USENIX Symposium on Networked
Systems Design and Implementation (NSDI’18) .
[81] Majed Valad Beigi, Bahareh Pourshirazi, Gokhan Memik, and Zhichun Zhu. 2020.
DeepSwapper: A Deep Learning Based Page Swap Management Scheme for
Hybrid Memory Systems. In Proceedings of the 29th ACM International Conference
on Parallel Architectures and Compilation Techniques (PACT’20) .
[82] Evangelos Vasilakis, Vassilis Papaefstathiou, Pedro Trancoso, and Ioannis Sourdis.
2020. Hybrid2: Combining Caching and Migration in Hybrid Memory Systems.
InProceedings of the 26th IEEE International Symposium on High Performance
Computer Architecture (HPCA’20) .
[83] Markus Velten, Robert Schöne, Thomas Ilsche, and Daniel Hackenberg. 2022.
Memory Performance of AMD EPYC Rome and Intel Cascade Lake SP Server
Processors. In Proceedings of the ACM/SPEC on International Conference on Per-
formance Engineering (ICPE’22) .
[84] Zixuan Wang, Xiao Liu, Jian Yang, Theodore Michailidis, Steven Swanson, and
Jishen Zhao. 2020. Characterizing and Modeling Non-Volatile Memory Sys-
tems. In Proceedings of the 53rd Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO’20) .
[85] Johannes Weiner. accessed in 2023. [PATCH] mm: mempolicy: N:M inter-
leave policy for tiered memory nodes. https://lore.kernel.org/linux-mm/YqD0%
2FtzFwXvJ1gK6@cmpxchg.org/T/.
[86] Johannes Weiner, Niket Agarwal, Dan Schatzberg, Leon Yang, Hao Wang, Blaise
Sanouillet, Bikash Sharma, Tejun Heo, Mayank Jain, Chunqiang Tang, and Dim-
itrios Skarlatos. 2022. TMO: Transparent Memory O ￿oading in Datacenters. In
Proceedings of the 27th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems (ASPLOS’22) .
[87] Kan Wu, Zhihan Guo, Guanzhou Hu, Kaiwei Tu, Ramnatthan Alagappan, Rathijit
Sen, Kwanghyun Park, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau.
2021. The Storage Hierarchy is Not a Hierarchy: Optimizing Caching on Modern
Storage Devices with Orthus. In Proceedings of the 19th USENIX Conference on
File and Storage Technologies (FAST’21) .
[88] Lingfeng Xiang, Xingsheng Zhao, Jia Rao, Song Jiang, and Hong Jiang. 2022.
Characterizing the Performance of Intel Optane Persistent Memory: A Close
Look at Its on-DIMM Bu ￿ering. In Proceedings of the 17th European Conference
on Computer Systems (EuroSys’22) .
[89] Zi Yan, Daniel Lustig, David Nellans, and Abhishek Bhattacharjee. 2019. Nimble
Page Management for Tiered Memory Systems. In Proceedings of the 24th ACM
International Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS’19) .
[90] Jian Yang, Juno Kim, Morteza Hoseinzadeh, Joseph Izraelevitz, and Steven Swan-
son. 2020. An Empirical Guide to the Behavior and Use of Scalable Persistent
Memory. In Proceedings of the 18th USENIX Conference on File and Storage Tech-
nologies (FAST’20) .
[91] Yiwei Yang, Pooneh Safayenikoo, Jiacheng Ma, Tanvir Ahmed Khan, and Andrew
Quinn. 2023. CXLMemSim: A pure software simulated CXL.mem for performance
characterization. arXiv preprint arXiv:2303.06153 (2023).
[92] Ting Yao, Yiwen Zhang, Jiguang Wan, Qiu Cui, Liu Tang, Hong Jiang, Chang-
sheng Xie, and Xubin He. 2020. MatrixKV: Reducing Write Stalls and WriteAmpli ￿cation in LSM-tree Based KV Stores with Matrix Container in NVM. In
Proceeings of the USENIX Annual Technical Conference (ATC’20) .
[93] Yifan Yuan, Mohammad Alian, Yipeng Wang, Ren Wang, Ilia Kurakin, Charlie
Tai, and Nam Sung Kim. 2021. Don’t Forget the I/O When Allocating Your
LLC. In Proceedings of the IEEE/ACM 48th International Symposium on Computer
Architecture (ISCA’21) .
[94] Chaoliang Zeng, Layong Luo, Qingsong Ning, Yaodong Han, Yuhang Jiang, Ding
Tang, Zilong Wang, Kai Chen, and Chuanxiong Guo. 2022. FAERY: An FPGA-
accelerated Embedding-based Retrieval System. In Proceedings of the 16th USENIX
Symposium on Operating Systems Design and Implementation (OSDI’22) .
[95] Wenhui Zhang, Xingsheng Zhao, Song Jiang, and Hong Jiang. 2021.
ChameleonDB: A Key-Value Store for Optane Persistent Memory. In Proceedings
of the 26th European Conference on Computer Systems (EuroSys’21) .
[96] Xu Zhang. accessed in 2023. gem5-CXL. https://github.com/zxhero/gem5-CXL.
[97] Shengan Zheng, Morteza Hoseinzadeh, and Steven Swanson. 2019. Ziggurat: A
Tiered File System for Non-Volatile Main Memories and Disks. In Proceedings of
the 17th USENIX Conference on File and Storage Technologies (FAST’19) .
[98] Xinjing Zhou, Lidan Shou, Ke Chen, Wei Hu, and Gang Chen. 2019. DPTree:
Di￿erential Indexing for Persistent Memory. Proceedings of VLDB Endowment
(2019).
15TPP: Transparent Page Placement for CXL-Enabled
Tiered-Memory
Hasan Al Maruf
University of Michigan
USAHao Wang
NVIDIA
USAAbhishek Dhanotia
Meta Inc.
USA
Johannes Weiner
Meta Inc.
USANiket Agarwal
NVIDIA
USAPallab Bhattacharya
NVIDIA
USA
Chris Petersen
Meta Inc.
USAMosharaf Chowdhury
University of Michigan
USAShobhit Kanaujia
Meta Inc.
USA
Prakash Chauhan
Meta Inc.
USA
ABSTRACT
The increasing demand for memory in hyperscale applications has
led to memory becoming a large portion of the overall datacen-
ter spend. The emergence of coherent interfaces like CXL enables
main memory expansion and o ￿ers an e ￿cient solution to this prob-
lem. In such systems, the main memory can constitute di ￿erent
memory technologies with varied characteristics. In this paper, we
characterize memory usage patterns of a wide range of datacenter
applications across the server ￿eet of Meta. We, therefore, demon-
strate the opportunities to o ￿oad colder pages to slower memory
tiers for these applications. Without e ￿cient memory management,
however, such systems can signi ￿cantly degrade performance.
We propose a novel OS-level application-transparent page place-
ment mechanism (TPP) for CXL-enabled memory. TPP employs a
lightweight mechanism to identify and place hot/cold pages to ap-
propriate memory tiers. It enables a proactive page demotion from
local memory to CXL-Memory. This technique ensures a memory
headroom for new page allocations that are often related to request
processing and tend to be short-lived and hot. At the same time, TPP
can promptly promote performance-critical hot pages trapped in
the slow CXL-Memory to the fast local memory, while minimizing
both sampling overhead and unnecessary migrations. TPP works
transparently without any application-speci ￿c knowledge and can
be deployed globally as a kernel release.
We evaluate TPP with diverse memory-sensitive workloads in
the production server ￿eet with early samples of new x86 CPUs with
CXL 1.1 support. TPP makes a tiered memory system performant as
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro ￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speci ￿c permission
and/or a fee. Request permissions from permissions@acm.org.
ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada
©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9918-0/23/03. . . $15.00
https://doi.org/10.1145/3582016.3582063an ideal baseline (<1% gap) that has all the memory in the local tier.
It is 18% better than today’s Linux, and 5–17% better than existing
solutions including NUMA Balancing and AutoTiering. Most of the
TPP patches have been merged in the Linux v5.18 release while the
remaining ones are just pending for more discussion.
CCS CONCEPTS
•Software and its engineering !Operating systems ;Memory
management ;•Hardware !Emerging architectures ;Mem-
ory and dense storage .
KEYWORDS
Datacenters, Operating Systems, Memory Management, Tiered-
Memory, CXL-Memory, Heterogeneous System
ACM Reference Format:
Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket
Agarwal, Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shob-
hit Kanaujia, and Prakash Chauhan. 2023. TPP: Transparent Page Placement
for CXL-Enabled Tiered-Memory. In Proceedings of the 28th ACM Interna-
tional Conference on Architectural Support for Programming Languages and
Operating Systems, Volume 3 (ASPLOS ’23), March 25–29, 2023, Vancouver,
BC, Canada. ACM, New York, NY, USA, 14pages. https://doi.org/10.1145/
3582016.3582063
1 INTRODUCTION
The surge in memory needs for datacenter applications [ 12,61],
combined with the increasing DRAM cost and technology scaling
challenges [ 49,54] has led to memory becoming a signi ￿cant infras-
tructure expense in hyperscale datacenters. Non-DRAM memory
technologies provide an opportunity to alleviate this problem by
building tiered memory subsystems and adding higher memory
capacity at a cheaper $/GB point [ 5,19,38,39,46]. These technolo-
gies, however, have much higher latency vs. main memory and
can signi ￿cantly degrade performance when data is ine ￿ciently
placed in di ￿erent levels of the memory hierarchy. Additionally,
prior knowledge of application behavior and careful application
742
ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.CPUDRAMDRAMDRAMDRAMDRAMDRAM(a) Without CXLCPUDRAMDRAMNVMDDRLPDDRCXL….Memory Technology(b) With CXL
Figure 1: CXL decouples memory from compute.
tuning is required to e ￿ectively use these technologies. This can be
prohibitively resource-intensive in hyperscale environments with
varieties of rapidly evolving applications.
Compute Express Link (CXL) [ 7] mitigates this problem by pro-
viding an intermediate latency operating point with DRAM-like
bandwidth and cache-line granular access semantics. CXL protocol
allows a new memory bus interface to attach memory to the CPU
(Figure 1). From a software perspective, CXL-Memory appears to
a system as a CPU-less NUMA node where its memory character-
istics (e.g., bandwidth, capacity, generation, technology, etc.) are
independent of the memory directly attached to the CPU. This
allows ￿exibility in memory subsystem design and ￿ne-grained
control over the memory bandwidth and capacity [ 9,10,24]. Addi-
tionally, as CXL-Memory appears like the main memory, it provides
opportunities for transparent page placement on the appropriate
memory tier. However, Linux’s memory management mechanism
is designed for homogeneous CPU-attached DRAM-only systems
and performs poorly on CXL-Memory system. In such a system,
as memory access latency varies across memory tiers (Figure 2),
application performance greatly depends on the fraction of memory
served from the fast memory.
To understand whether memory tiering can be bene ￿cial, we
need to understand the variety of memory access behavior in
existing datacenter applications. For each application, we want
to know how much of its memory remains hot, warm, and cold
within a certain period and what fraction of its memory is short- vs.
long-lived. Existing Idle Page Tracking (IPT) based characterization
tools [ 11,17,69] do not ￿t the bill as they require kernel modi ￿ca-
tions that is often not possible in productions. Besides, continuous
access bit sampling and pro ￿ling require excessive CPU and mem-
ory overhead. This may not scale with large working sets. More-
over, applications often have di ￿erent sensitivity towards di ￿erent
types of memory pages (e.g., anon page, ￿le page cache, shared
memory, etc.) which existing tools do not account. To this end, we
build Chameleon, a robust and lightweight user-space tool, that
uses existing CPU’s Precise Event-Based Sampling (PEBS) mecha-
nism to characterize an application’s memory access behavior (§ 3).
Chameleon generates a heat-map of memory usage on di ￿erent
types of pages and provides insights into an application’s expected
performance with multiple temperature tiers.
We use Chameleon to pro ￿le a variety of large memory-bound
applications across di ￿erent service domains running in our produc-
tion and make the following observations. (1)Meaningful portions
of application working sets can be warm/cold. We can o ￿oad that
to a slow tier memory without signi ￿cant performance impact. (2)
A large fraction of anon memory (created for a program’s stack,
heap, and/or calls to mmap) tends to be hotter, while a large fraction
of￿le-backed memory tends to be relatively colder. (3)Page accessCacheMain MemoryCXL-MemoryNVM Disaggregated MemorySSDHDDRegister0.2ns1-40ns80-140ns170-250ns300-400ns2-4μs10-40μs3-10msAttached to CPUNetwork AttachedCPU IndependentFigure 2: Latency characteristics of memory technologies.
patterns remain relatively stable for meaningful time durations
(minutes to hours). This is enough to observe application behavior
and make page placement decisions in kernel-space. (4)With new
(de)allocations, actual physical page addresses can change their
behavior from hot to cold and vice versa fairly quickly. Static page
allocations can signi ￿cantly degrade performance.
Considering the above observations, we design an OS-level trans-
parent page placement mechanism – TPP, to e ￿ciently place pages
in a tiered-memory systems so that relatively hot pages remain in
fast memory tier and cold pages are moved to the slow memory tier
(§5). TPP has three prime components: (a)a lightweight reclama-
tion mechanism to demote colder pages to the slow tier node; (b)
decoupling the allocation and reclamation logic for multi-NUMA
systems to maintain a headroom of free pages on fast tier node; and
(c)a reactive page promotion mechanism that e ￿ciently identi ￿es
hot pages trapped in the slow memory tier and promote them to
the fast memory tier to improve performance. We also introduce
support for page type-aware allocation across the memory tiers –
preferably allocate sensitive anon pages to fast tier and ￿le caches
to slow tier. With this optional application-aware setting, TPP can
act from a better starting point and converge faster for applications
with certain access behaviors.
We choose four production workloads that constitute signi ￿cant
portion of our server ￿eet and run them on a system that support
CXL 1.1 speci ￿cation (§ 6). We ￿nd that TPP provides the similar
performance behavior of all memory served from the fast memory
tier. For some workloads, this holds true even when local DRAM is
only 20% of the total system memory. TPP moves all the e ￿ective
hot memory to the fast memory tier and improves default Linux’s
performance by up to 18%. We compare TPP against NUMA Bal-
ancing [ 22] and AutoTiering [ 47], two state-of-the-art solutions for
tiered memory. TPP outperforms both of them by 5–17%.
We make the following contributions in this paper:
•We present Chameleon, a lightweight user-space memory char-
acterization tool. We use it to understand workload’s memory
consumption behavior and assess the scope of tiered-memory
in hyperscale datacenters (§ 3). We open source Chameleon .
•We propose TPP for e ￿cient memory management on a tiered-
memory system (§ 5).We publish the source code of TPP .A
major portion of it has been merged to Linux kernel v5.18. Rest
of it is under an upstream discussion.
•We evaluate TPP on a CXL-enabled tiered-memory systems
with real production workloads (§ 6) for years. TPP makes tiered
memory systems as performant as an ideal system with all
memory in local tier. For datacenter applications, TPP improves
743TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada
default Linux’s performance by up to 18%. It also outperforms
NUMA Balancing and AutoTiering by 5–17%.
To the best of our knowledge, we are the ￿rst to characterize
and evaluate an end-to-end practical CXL-Memory system that can
be readily deployed in hyperscale datacenters.
2 MOTIVATION
Increased Memory Demand in Datacenter Applications. To
build low-latency services, in-memory computation has become a
norm in datacenter applications. This has led to rapid growth in
memory demands across the server ￿eet. With new generations
of CPU and DRAM technologies, memory is becoming the more
prominent portion of rack-level power and total cost of ownership
(TCO). (Figure 3).14.619.819.720.728.833.315.625.521.621.231.837.1010203040Gen0Gen1Gen2Gen3Gen4Gen5Percentage (%)PowerCostFigure 3: Memory as a percentage of rack TCO and power
across di ￿erent hardware generations of Meta.
Scaling Challenges in Homogeneous Server Designs. In to-
day’s server architectures, memory subsystem design is completely
dependent on the underlying memory technology support in the
CPUs. This has several limitations: (a) memory controllers only
support a single generation of memory technology which limits
mix-and-match of di ￿erent technologies with di ￿erent cost-per-GB
and bandwidth vs. latency pro ￿les; (b) memory capacity comes at
power-of-two granularity which limits ￿ner grain memory capacity
sizing; (c) there are limited bandwidth vs. capacity points per DRAM
generation (Figure 4). This forces higher memory capacity in order
to get more bandwidth on the system. Such tight coupling between
CPU and memory subsystem restricts the ￿exibility in designing ef-
￿cient memory hierarchies and leads to stranded compute, network,
and/or memory resources. Prior bus interfaces that allow memory
expansion are also proprietary to some extent [ 3,18,44] and not
commonly supported across all the CPUs [ 6,14,23]. Besides, high
latency characteristics and lack of coherency limit their viability in
hyperscalers.
CXL for Designing Tiered-Memory Systems. CXL [ 7] is an
open, industry-supported interconnect based on the PCI Express
(PCIe) interface. It enables high-speed, low latency communication
between the host processor and devices (e.g., accelerators, memory
bu￿ers, smart I/O devices, etc.) while expanding memory capac-
ity and bandwidth. CXL provides byte addressable memory in the
same physical address space and allows transparent memory allo-
cation using standard memory allocation APIs. It allows cache-line
granularity access to the connected devices and underlying hard-
ware maintains coherency and consistency. With PCIe 5.0, CPU
to CXL interconnect bandwidth will be similar to the cross-socket
interconnects (Figure 5) on a dual-socket machine. CXL-Memory
access latency is also similar to the NUMA access latency. CXL adds11448881611.21.41.61.822.23.605101520Gen0Gen1Gen2Gen3Gen4Gen5Gen6Gen7x TimesCapacity (GB)Bandwidth (GB/s)Figure 4: Memory bandwidth and capacity increase over time.
around 50-100 nanoseconds of extra latency over normal DRAM
access. This NUMA-like behavior with main memory-like access
semantics makes CXL-Memory a good candidate for the slow-tier
in datacenter memory hierarchies.
CXL solutions are being developed and incorporated by leading
chip providers [ 1,4,9,21,24,25]. All the tools, drivers, and OS
changes required to support CXL are open sourced so that any-
one can contribute and bene ￿t directly without relying on single
supplier solutions. CXL relaxes most of the memory subsystem
limitations mentioned earlier. It enables ￿exible memory subsystem
designs with desired memory bandwidth, capacity, and cost-per-GB
ratio based on workload demands. This helps scale compute and
memory resources independently and ensure a better utilization of
stranded resources.
Scope of CXL-Based Tiered-Memory Systems. Datacenter
workloads rarely use all of the memory all the time [ 2,15,48,70,73].
Often an application allocates a large amount of memory but ac-
cesses it infrequently [ 48,56]. We characterize four popular appli-
cations in our production server ￿eet and ￿nd that 55-80% of an
application’s allocated memory remains idle within any two min-
utes interval (§ 3.2). Moving this cold memory to a slower memory
tier can create space for more hot memory pages to operate on
the fast memory tier and improve application-level performance.
Besides, it also allows reducing TCO by ￿exible server design with
smaller fast memory tier and larger but cheaper slow memory tier.
As CXL-attached slow memory can be of any technology (e.g.,
DRAM, NVM, LPDRAM, etc.), for the sake of generality, we will
call a memory directly attached to a CPU as local memory and a
CXL-attached memory as CXL-Memory.
Lightweight Characterization of Datacenter Applications.
In a hyperscaler environment, di ￿erent types of rapidly evolving
applications consume a production server’s resources. Applications
can have di ￿erent requirements, e.g., some can be extremely la-
tency sensitive, while others can be memory bandwidth sensitive.
Sensitivity towards di ￿erent memory page types can also vary for
di￿erent applications. To understand the scope of tiered-memory
system for existing datacenter applications, we need to characterize
their memory usage pattern and quantify the opportunity of mem-
ory o ￿oading at di ￿erent memory tiers for di ￿erent page types.
This insight can help system admins decide on the ￿exible and
optimized memory con ￿gurations to support di ￿erent workloads.
Existing memory characterization tools fall short of providing
these insights. For example, access bit-based mechanism [ 11,17,
69] cannot track detailed memory access behavior because it tells
whether a given page is accessed within a certain period of time.
Even if a page gets multiple accesses within a tracking cycle, IPT-
based tools will count that as a single access. Moreover, IPT only
744ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.CPU0CPU1Interconnect32 GB/s perlinkDRAMDRAM38.4 GB/s per channel~100 ns~180 ns(a) Without CXLCPU0CXL64 GB/s per x16 linkDRAMDRAM~100 ns~170-250 ns38.4 GB/s per channel(b) With CXL
Figure 5: CXL-System compared to a dual-socket server.
provides information in the physical address space – we cannot
track memory allocation/deallocation if a physical page is re-used
by multiple virtual pages. The overhead of such tools signi ￿cantly
increases with the application’s memory footprint size. Even for
application’s with 10s of GB working set size, the overhead of
IPT-based tool can be 20–90% [ 50].Similarly, complex PEBS-based
user-space tools [ 35,63,65,74] lead to high CPU overheads (more
than 15% per core) and often slow down the application. None of
these tools generate page type-aware heat map.
3 CHARACTERIZING DATACENTER
APPLICATIONS
To understand the scope of tiered-memory in hyperscale applica-
tions, we develop Chameleon, a light-weight user-space memory
access behavior characterization tool. The objective of developing
Chameleon is to allow users hop on any existing datacenter produc-
tion machine and readily deploy it without disrupting the running
application(s) and modifying the underline kernel. Chameleon’s
overhead needs to be comparatively lower such that it does not
notably a ￿ect a production application’s behavior. Chameleon’s
prime use case is to understand an application’s memory access
behavior, i.e., what fraction of an application’s memory remains
hot-warm-cold, how long a page survive on a speci ￿c temperature
tier, how frequently they get accessed and so on. In practice, to char-
acterize a certain type of application, we expect to run Chameleon
for a few hours on a tiny fraction of servers in the whole ￿eet.
Considering the above objectives, we design Chameleon with
two primary components – a Collector and a Worker– running si-
multaneously on two di ￿erent threads. Collector utilizes the PEBS
mechanism of modern CPUs to collect hardware-level performance
events related to memory accesses. Worker uses the sampled infor-
mation to generate insights.
Collector. Collector samples last level cache (LLC) misses
for demand loads (event MEM_LOAD_RETIRED.L3_MISS )
and optionally TLB misses for demand stores (event
MEM_INST_RETIRED.STLB_MISS_STORES ). Sampled records
provide us with the PID and virtual memory address for the
memory access events. Like any other sampling mechanism,
accuracy of PEBS depends on the sampling rate – frequent samples
provide higher accuracy. High sampling rate, however, incurs
higher performance overhead directly on application threads and
demands more CPU resources for Chameleon’s Worker thread . In
our￿eets, sampling rate is con ￿gured as one sample for every
200 events, which appears as a good trade-o ￿between overhead
and accuracy. Note the choice of the sampling event on the store
side is due to hardware limitations, i.e., there is no precise event
for LLC-missed stores, likely because stores are marked completeonce TLB translation is done in modern high-end CPUs. We have
conveyed the concern on this limitation to major x86 vendors in
multiple occasions.
To improve ￿exibility , the Collector divides all CPU cores into a
set of groups and enables sampling on one or more group(s) at a time
(Figure 6a). After each mini_interval (by default, 5 seconds), the
sampling thread rotates to the next core group. This duty-cycling
helps further tune the trade-o ￿between overhead and accuracy.
It also allows sampling di ￿erent events on di ￿erent core groups.
For example, for latency-critical applications, one can choose to
sample half of the cores at a time. On the other hand, for store-
heavy applications, one can enable load sampling on half of the
cores and store sampling on the other half at the same time.
The Collector reads the sampling bu ￿er and writes into one of
the two hash tables. After each interval (by default, 1 minute),
the Collector wakes up the Worker to process data in current hash
table and moves to the other hash table for storing next interval’s
sampled data.
Worker. The Worker (Figure 6b) runs on a separate thread to
read page access information and generate insights on memory
access behavior. It considers the address of a sampled record as a
virtual page access where the page size is de ￿ned by the OS. This
makes it generic to systems with any page granularities (e.g., 4KB
base page, 2MB huge page, etc.). To generate statistics on both
virtual- and physical-spaces, the Worker ￿nds the corresponding
physical page mapped to the sampled virtual page. This address
translation can cause high overhead if the target process’s working
set size is extremely large (e.g., terabyte-scale). One can con ￿gure
the Worker to disable the physical address translation and charac-
terize an application only on its virtual-space access pattern.
For each page, a 64-bit bitmap tracks its activeness within an
interval. If a page is active within an interval, the corresponding bit
is set. At the end of each interval, the bitmap is left-shifted one bit to
track for a new interval. One can use multiple bits for one interval
to capture the page access frequency, at the cost of supporting
shorter history. After generating the statistics and reporting them,
the Worker sleeps (Figure 6c).
To characterize an application deployed on hundreds of thou-
sands of servers, we run Chameleon on a small set of servers only for
a few hours. During our analysis, we chose servers where system-
wide CPU and memory usage does not go above 80%. In such produc-
tion environments, we do not notice any service-level performance
impact while running Chameleon. CPU overhead is within 3–5%
of a single core. However, on a synthetic workload that is memory
bandwidth sensitive and uses all the CPU cores so that Chameleon
needs to contend for CPU, we lose 7% performance due to pro ￿ling.
3.1 Production Workload Overview
We use Chameleon to characterize most popular memory-bound
applications running for years across our production server ￿eet
serving live tra ￿c on four diverse service domains. These workloads
constitute a signi ￿cant portion of the server ￿eet and represent a
wide variety of our workloads [ 67,68].Web implements a Virtual
Machine to serve web requests. Web1 is a HipHop Virtual Machine
(HHVM)-based and Web2 is a Python-based service. Cache is a
large distributed-memory object caching service lying between the
745TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS ’23, March 25–29, 2023, Vancouver, BC, CanadaMemory NodeStore SampledLoad SampledUnsampled
Core Group 1Core Group 2Core Group 3Core Group NSample StoreSample LoadCollectorRotate sampling event collection to next CPU core group after each mini-interval(a) CollectorHistory GeneratorVirtual Address RangePhysical Address000110…010100…110010…StorageReportHotness TrackerVA to PA Translator/proc/$PID/map/proc/$PID/pagemap(b) WorkerInterval NCollectorwrites to hashtable-0WorkerSleepsWorkerRunsreads from hashtable-1Interval N+1Collectorwrites to hashtable-1WorkerSleepsWorkerRunsreads from hashtable-0(c) Work ￿ow within a Cycle
Figure 6: Overview of Chameleon components (left) and work ￿ow (right)
020406080100Web1Web2Cache1Cache2WarehouseAds1Ads2Ads3Memory Utilization (%)Cold10 Min Hot5 Min Hot2 Min Hot1 Min Hot
Figure 7: Application memory usage over last N mins.
web and database tiers for low-latency data-retrieval. Data Ware-
house is a uni ￿ed computing engine for parallel data processing
on compute clusters. This service manages and coordinates the ex-
ecution of long and complex batch queries on data across a cluster.
Adsare compute heavy workloads that retrieve in-memory data
and perform machine learning computations.
3.2 Page Temperature
In datacenter applications, a signi ￿cant amount of allocated mem-
ory remains cold beyond a few minutes of intervals (Figure 7). Web,
Cache, and Ads use 95–98% of the system’s total memory capac-
ity, but within a two-minute interval, they use 22–80% of the total
allocated memory on average.
Data Warehouse is a compute-heavy workload where a speci ￿c
computation operation can span even terabytes of memory. This
workload consumes almost all the available memory within a server.
Even here, on average, only 20% of the accessed memory is hot
within a two-minute interval.
Observation: A signi ￿cant portion of a datacenter application’s
accessed memory remain cold for minutes. Tiered memory sys-
tem can be a good ￿t for such cold memory if page placement
mechanism can move these cold pages to a lower memory tier.
3.3 Temperature Across Di ￿erent Page Types
Applications consume di ￿erent types of pages based on applica-
tion logic and execution demand. However, the fraction of anons
(anonymous pages) remain hot is higher than the fraction of ￿les
(￿le pages). For Web, within a two-minute interval, 35–60% of the
total allocated anons remain hot; for ￿les, in contrast, it is only
3–14% of the total allocation (Figure 8).
Cache applications use tmpfs [27] for a fast in-memory lookup.
Anons are used mostly for processing queries. As a result, ￿le
pages contribute signi ￿cantly to the total hot memory. However, for
Cache1, 40% of the anons get accessed within every two minutes,020406080100AnonFileAnonFileAnonFileAnonFileAnonFileAnonFileAnonFileAnonFileWeb1Web2Cache1Cache2W.houseAds1Ads2Ads3% of AllocationCold10 Min Hot2 Min Hot1 Min Hot
Figure 8: Anon pages tends to be hotter than ￿le pages.
while the fraction for ￿le is only 25%. For Cache2, the fraction
of anon and ￿le usage is almost equal within a two-minute time
window. However, within a minute interval, even for Cache2, higher
fraction of anons (43%) remain hot over ￿le pages (30%).
Data Warehouse and Ads use anon pages for computation. The
￿le pages are used for writing intermediate computation data to the
storage device. As expected, almost all of hot memories are anons
where almost all of the ￿les remain cold.
Observation: A large fraction of anon pages is hot, while ￿le
pages are comparatively colder within short intervals.
Due to space constraints, we focus on a subset of these applica-
tions. In our analysis, we ￿nd the similar behavior from the rest of
the applications.
3.4 Usage of Di ￿erent Page Types Over Time
When the Web service starts, it loads the virtual machine’s binary
and bytecode ￿les into memory. As a result, at the beginning, ￿le
caches occupy a signi ￿cant portion of the memory. Overtime, anon
usage slowly grows and ￿le caches get discarded to make space for
the anon pages (Figure 9a).
Cache applications mostly use ￿le caches for in-memory look-
ups. As a result, ￿le pages consume most of the allocated memory.
For Cache1 and Cache2 (Figure 9b-9c), the fraction of ￿les hov-
ers around 70–82%. While the fraction of anon and ￿le is almost
steady, if at any point, anon usage grows, ￿le pages are discarded
to accommodate newly allocated anons.
For Data Warehouse workload, anon pages consume most of the
allocated memory – 85% of the total allocated memory are anons
and rest of the 15% are ￿le pages (Figure 9d). The usage of anon
and￿le pages mostly remains steady.
Observation: Although anon and ￿le usage may vary over time,
applications mostly maintain a steady usage pattern. Smart page
placement mechanisms should be aware of page type when making
placement decisions.
746ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.
00.20.40.60.81
050100150200250300Memory Utilization (%)Time (minute)TotalAnonFile(a) Web100.20.40.60.81
0200400600800Memory Utilization (%)Time (minute)TotalAnonFile(b) Cache100.20.40.60.81
0200400600800Memory Utilization (%)Time (minute)TotalAnonFile(c) Cache200.20.40.60.81
0255075100125Memory Utilization(%)Time (minute)TotalAnonFile(d) Data Warehouse
Figure 9: Memory usage over time for di ￿erent applications
00.20.40.60.8100.20.40.60.81Memory Utilization (%)Throughput (%)FileAnon
(a) Web100.20.40.60.8100.20.40.60.81Memory Utilization (%)Throughput (%)FileAnon
(b) Cache100.20.40.60.8100.20.40.60.81Memory Utilization (%)Throughput (%)FileAnon
(c) Cache200.20.40.60.8100.20.40.60.81Memory Utilization (%)Throughput (%)FileAnon
(d) Data Warehouse
Figure 10: Workloads’ sensitivity towards anons and ￿les varies. High memory capacity utilization provides high throughput.
3.5 Impact of Page Types on Performance
Figure 10shows what fractions of di ￿erent page types are used
to achieve a certain application-level throughput. Memory-bound
application’s throughput improves with high memory utilization.
However, workloads have di ￿erent levels of sensitivity toward
di￿erent page types. For example, Web’s throughput improves with
the higher utilization of anon pages (Figure 10a).
For Cache, tmpfs is allocated during initialization period. Be-
sides, Cache1 uses a ￿xed amount of anons throughout its life cycle.
As a result, we cannot observe any noticeable relation between
anon or ￿le usage and the application throughput (Figure 10b).
However, for Cache2, we can see high throughput is achieved with
comparatively higher utilization of anons (Figure 10c). Similarly,
Data Warehouse application maintains a ￿xed amount of ￿le pages.
However, it consumes di ￿erent amount of anons at di ￿erent execu-
tion period and the highest throughput is achieved when the total
usage of anons reaches to its highest point (Figure 10d).
Observation: Workloads have di ￿erent levels of sensitivity to-
ward di ￿erent page types that varies over time.
3.6 Page Re-access Time Granularity
Cold pages often get re-accessed at a later point. Figure 11shows
the fraction of pages that become hot after remaining cold for a
certain interval. For Web, almost 80% of the pages are re-accessed
within a ten-minute interval. This indicates Web mostly repurposes
pages allocated at an earlier time. Same goes for Cache – randomly
o￿oading cold memory can impact performance as a good chunk
of colder pages get re-accessed within a ten-minute window.
However, Data Warehouse shows di ￿erent characteristics. For
this workload, anons are mostly newly allocated – within a ten-
minute interval, only 20% of the hot ￿le pages are previously ac-
cessed. Rest of them are newly allocated.020406080100Web1Cache1Cache2WarehouseMemory Utilization (%)10 Min5 Min2 Min1 MinFigure 11: Fraction of pages re-accessed at di ￿erent intervals.
Observation: Cold page re-access time varies for workloads.
Page placement on a tiered memory system should be aware of this
and actively move hot pages to lower memory nodes to avoid high
memory access latency.
From above observations, tiered memory subsystems can be
a good ￿t for datacenter applications as there exists signi ￿cant
amount of cold memory with steady access patterns.
4 DESIGN PRINCIPLES OF TPP
With the advent of CXL technologies, hyperscalers are embracing
CXL-enabled heterogeneous tiered-memory system where di ￿er-
ent memory tier has di ￿erent performance characteristics [ 41,52].
For performance optimization in such systems, transparent page
placement mechanism (TPP) is needed to handle pages with varied
hotness characteristics on appropriate temperature tiers. To de-
sign TPP for next-generation tiered-memory systems, we consider
following questions:
•What is an ideal layer to implement TPP functionalities?
•How to detect page temperature?
•What abstraction to provide for accessing CXL-Memory?
In this section, we discuss the rationale and trade-o ￿s behind
design choices for TPP.
747TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada
Implementation Layer. Application-transparent page place-
ment mechanism can be employed both in the user- and kernel-
space. In user-space, a Chameleon-like tool can be used to detect
page temperatures and perform NUMA migrations using user-space
APIs (e.g., move_pages() ). To identify what to migrate, the migra-
tion tool needs to implement user-space page lists and history
management. This technique entails overheads due to user-space to
kernel-space context switching. It also adds processing overheads
due to history management in user space. Besides, there are mem-
ory overheads due to page information management in user-space
that may not scale with large working sets. While this is acceptable
for pro ￿ling tools that run for short intervals on a small sample of
servers in the ￿eet, it can be prohibitively expensive when they run
continuously on all production ￿eet. Considering these, we design
TPP as a kernel feature as we think it’s less complex to implement
and more performant over user-space mechanisms.
Page Temperature Detection. There are several di ￿erent tech-
niques that can potentially be used for page temperature detection
including PEBS, Page Poisoning, and NUMA Balancing. PEBS can
be utilized in kernel-space to detect page temperature. However, as
PEBS counters are not standardized across CPU vendors, a generic
precise event-based kernel implementation for page temperature
detection that works across all hardware platforms is not feasi-
ble. Additionally, limited number of perf counters are supported
in CPUs and are generally required to be exposed in user-space.
More importantly, as mentioned earlier, even with optimizations,
PEBS-based pro ￿ling is not good enough as an always-running
component of TPP for high pressure workloads.
Sampling and poisoning a few pages within a memory re-
gion to track their access events is another well-established ap-
proach [ 16,22,29,31] for ￿nding hot/cold pages. To detect a page ac-
cess, IPT-based [ 16,29] approach needs to clear the page’s accessed
bitand￿ush the corresponding TLB entry. This requires monitor-
ingaccessed bits at high frequency which results in unacceptable
slowdowns [ 29,31]. Thermostat [ 31] solves this problem by sam-
pling at 2MB page granularity which makes it e ￿ective speci ￿cally
for huge-pages. One of our design goals behind TPP is that it should
be agnostic to page size. In our production environment, application
owners generally pre-allocate 2MB and 1GB pages and use them to
allocate text regions (code), static data structures, and slab pools
that serve request allocations. In most cases, these large pages are
hot and should never get demoted to CXL-Memory.
NUMA Balancing (also known as AutoNUMA) [ 22] is transpar-
ent to OS page sizes. It generates a minor page fault when the
sampled page gets accessed. Periodically incurring page faults on
most frequently accessed pages can lead to high overheads. To ad-
dress this, when designing TPP, we chose to only leverage minor
page fault as a temperature detection mechanism for CXL-Memory.
As CXL-Memory is expected to hold warm and cold pages, this will
keep the overhead of temperature detection low. For cold page de-
tection in local memory node, we ￿nd Linux’s existing LRU-based
age management mechanism is lightweight and quite e ￿cient.
Empirically, we do not see any potential bene ￿t to leverage a
more sophisticated page temperature detection mechanism. In our
experiments (§ 6), using kernel LRUs for on-the- ￿y pro ￿ling works
well. Combining LRUs and NUMA Balancing, we can detect mosthot pages in CXL-Memory at virtually zero overhead as presented
in Figure 14and Table 1.
Memory Abstraction for CXL-Memory. One can use CXL-
Memory as a swap-space to host colder memory using existing in-
memory swapping mechanisms [ 8,13,26,30]. TMO [ 73] is one such
swap-based mechanism that detects cold pages in local memory
and moves them to swap-space that is referred to as (z)swap pool.
However, we do not plan to use CXL-Memory as an in-memory
swap device. In such a case, we will e ￿ectively lose CXL’s most
important feature, i.e., load/store access semantics at cache-line
granularity. With swap abstraction, every access to a (z)swapped
page will incur a major page fault and we have to read the whole
page from CXL-Memory. This will signi ￿cantly increase the ef-
fective access latency far over 200ns and make CXL-Memory less
attractive. We chose to build TPP so that applications can lever-
age load-store semantics when accessing warm or cold data from
CXL-Memory.
While the swap semantics of TMO are not desirable in CXL-
Memory page placement context, the memory saving through
feedback-driven reclamation is still valuable. We think TMO as
an orthogonal and complimentary tool to TPP as it operates one
layer above TPP (§ 6.3.2). TMO runs in user-space and keeps push-
ing for memory reclamation, while TPP runs in kernel-space and
optimizes page placement for allocated memory between local and
CXL-Memory tiers.
5 TPP FOR CXL-MEMORY
An e ￿ective page placement mechanism should e ￿ciently o ￿oad
cold pages to slower CXL-Memory while aptly identify trapped hot
pages in CXL-node and promote them to the fast memory tier. As
CXL-Memory is CPU-less and independent of the CPU-attached
memory, it should be ￿exible enough to support heterogeneous
memory technologies with varied characteristics. Page allocation
to a NUMA node should not frequently halt due to the slower recla-
mation mechanism to free up spaces. Besides, an e ￿ective policy
should be aware of an application’s sensitivity toward di ￿erent
page types.
Considering the datacenter workload characteristics and our de-
sign objectives, we propose TPP– a smart OS-managed mechanism
for tiered-memory system. TPP places ‘hotter’ pages in local mem-
ory and moves ‘colder’ pages in CXL-Memory. TPP’s design-space
can be divided across four main areas – (a)lightweight demotion
to CXL-Memory, (b)decoupled allocation and reclamation paths,
(c)hot-page promotion to local nodes, and (d)page type-aware
memory allocation.
5.1 Migration for Lightweight Reclamation
Linux tries to allocate a page to the memory node local to a CPU
where the process is running. When a CPU’s local memory node
￿lls up, default reclamation pages-out to swap device. In such a
case, in a NUMA system, new allocations to local node halts and
takes place on CXL-node until enough pages are freed up. The
slower the reclamation is, the more pages end up being allocated to
the CXL-node. Besides, invoking paging events in the critical path
worsens the average page access latency and impacts application
performance.
748ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.CPUdemotionwatermarkallocationwatermarkx% of capacitymigrates to CXL-Memoryhigh watermarkLocal NodeCXL Nodeinitiates reclamation21CXLheadroom for allocationFigure 12: TPP decouples the allocation and reclamation log-
ics for local memory node. It uses migration for demotion.
To enable a light-weight page reclamation for local nodes, after
￿nding the reclamation-candidates, instead of invoking swapping
mechanism, we put them in to a separate demotion list and try to
migrate them to the CXL-node asynchronously ( 1in Figure 12).
Migration to a NUMA node is orders of magnitude faster than
swapping. We use Linux’s default LRU-based mechanism to select
demotion candidates. However, unlike swapping, as demoted pages
are still available in-memory, along with inactive ￿le pages, we scan
inactive anon pages for reclamation candidate selection. As we start
with the inactive pages, chances of hot pages being migrated to
CXL-node during reclamation is very low unless the local node’s
capacity is smaller than the hot portion of working set size. If a
migration during demotion fails (e.g., due to low memory on the
CXL-node), we fall back to the default reclamation mechanism for
that failed page. As allocation on CXL-node is not performance
critical, CXL-nodes use the default reclamation mechanism (e.g.,
pages out to the swap device).
If there are multiple CXL-nodes, the demotion target is chosen
based on the node distances from the CPU. Although other complex
algorithms can be employed to dynamically choose the demotion
target based on a CXL-node’s state, this simple distance-based static
mechanism turns out to be e ￿ective.
5.2 Decoupling Allocation and Reclamation
Linux maintains three watermarks ( min,low,high ) for each mem-
ory zone within a node. If the total number of free pages for a
node goes below low_watermark , Linux considers the node is un-
der memory pressure and initiates page reclamation for that node.
In our case, TPP demotes them to CXL-node. New allocation to
local node halts till the reclaimer frees up enough memory to satisfy
thehigh_watermark . With high allocation rate, reclamation may
fail to keep up as it is slower than allocation. Memory retrieved by
the reclaimer may ￿ll up soon to satisfy allocation requests. As a
result, local memory allocations halt frequently, more pages end up
in CXL-node which eventually degrades application performance.
In a multi-NUMA system with severe memory constraint, we
should proactively maintain a reasonable amount of free memory
headroom on the local node. This helps in two ways. First, new
allocation bursts (that are often related to request processing and,
therefore, both short-lived and hot) can be directly mapped to the
local node. Second, local node can accept promotions of trapped
hot pages on CXL-nodes.
To achieve that, we decouple the logic of ‘reclamation stop’ and
‘new allocation happen’ mechanism. We continue the asynchronous
background reclamation process on local node until its total number
of free pages reaches demotion_watermark , while new allocationCPUdemotionwatermarkallocationwatermarkremoteaccesspromotes active LRU pageshigh watermarkLocal NodeCXL NodeCXL123move to active LRU listheadroom for allocationcheck page active stateFigure 13: TPP promotes a page considering its activity state.
can happen if the free page count satis ￿es a di ￿erent watermark
–allocation_watermark (2in Figure 12). Note that demotion
watermark is always set to a higher value above the allocation and
low watermark so that we always reclaim more to maintain the
free memory headroom.
How aggressively one needs to reclaim often depends on the
application behavior and available resources. For example, if an
application has high page allocation demand, but a large fraction
of its memory is infrequently accessed, aggressive reclamation can
help maintain free memory headroom. On the other hand, if the
amount of frequently accessed pages is larger than the local node’s
capacity, aggressive reclamation will thrash hot memory across
NUMA nodes. Considering these, to tune the aggressiveness of
the reclamation process on local nodes, we provide a user-space
sysctl interface ( /proc/sys/vm/demote_scale_factor ) to con-
trol the free memory threshold for triggering the reclamation on
local nodes. By default, its value is empirically set to 2% that means
reclamation starts as soon as only a 2% of the local node’s capacity is
available to consume. One can use workload monitoring tools [ 73]
to dynamically adjust this value.
5.3 Page Promotion from CXL-Node
Due to increased memory pressure on local nodes, new pages may
often get allocated to CXL-nodes. Besides, demoted pages may
also become hot later as discussed in § 3.6. Without any promotion
mechanism, hot pages will always be trapped in CXL-nodes and
hurt application performance. To promote such pages, we augment
Linux’s NUMA Balancing [ 22].
NUMA Balancing for CXL-Memory. In NUMA Balancing,
a kernel task routinely samples a subset of a process’s memory
(by default, 256MB of pages) on each memory node. When a CPU
accesses a sampled page, a minor page-fault is generated (known as
NUMA hint fault). Pages that are accessed from a remote CPU are
migrated to that CPU’s local memory node (known as promotion).
In a CXL-System, it is not reasonable to promote a local node’s hot
memory to other local or CXL-nodes. Besides, as sampling pages
to￿nd a local node’s hot memory generates unnecessary NUMA
hint fault overheads, we limit sampling only to CXL-nodes.
While promoting a CXL-node’s trapped hot pages, we ignore
the allocation watermark checking for the target local node. This
creates more memory pressure to initiate the reclamation of com-
paratively colder pages on that node. If a system has multiple local
nodes, we select the node where the task is running. When applica-
tions share multiple memory nodes, we choose local node with the
lowest memory pressure.
749TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada
Ping-Pong due to Opportunistic Promotion. When a NUMA
hint fault happens on a page, default NUMA balancing instantly
promotes the page without checking its active state. As a result,
pages with very infrequent accesses can still be promoted to the
local node. Once promoted, these type of pages may shortly become
the demotion candidate if the local nodes are always under pres-
sure. Thus, promotion tra ￿c generated from infrequently accessed
pages can easily ￿ll up the local node’s free space and generate a
higher demotion tra ￿c for CXL-nodes. This unnecessary tra ￿c
can negatively impact an application’s performance.
Apt Identi ￿cation of Trapped Hot Pages. To solve this ping-
pong issue, instead of instant promotion, we check a page’s age
through its position in the LRU list maintained by the OS. If the
faulted page is in inactive LRU, we do not consider the page for
promotion instantly as it might be an infrequently accessed page.
We consider the faulted page as a promotion candidate only if it
is found in the active LRUs ( 1in Figure 13). This signi ￿cantly
reduces the promotion tra ￿c.
However, OS uses LRU lists for reclamation. If a memory node is
not under pressure and reclamation does not kick in, then pages in
inactive LRU list do not automatically move to the active LRU list.
As CXL-nodes may not always be under pressure, faulted pages
may often be found in the inactive LRU list and, therefore, bypass
the promotion ￿lter. To address this, whenever we ￿nd a faulted
page on the inactive LRU list, we mark the page as accessed and
move it to the active LRU list immediately ( 2in Figure 13). If the
page still remains hot during the next NUMA hint fault, it will be
in the active LRU, and promoted to the local node ( 3in Figure 13).
This helps TPP add some hysteresis to page promotion. Besides,
as Linux maintains separate LRU lists for anon and ￿le pages, dif-
ferent page types have di ￿erent promotion rates based on their
respective LRU sizes and activeness. This speeds up the convergence
of hot pages across memory nodes.
5.4 Page Type-Aware Allocation
The page placement mechanism we described above is generic
to all page types. However, some applications can further bene ￿t
from page type-aware allocation policy. For example, production
applications often perform lots of ￿le I/O during the warm up phase
and generate ￿le caches that are accessed infrequently. As a result,
cold￿le caches eventually end up on CXL-nodes. Not only that,
local memory node being occupied by the inactive ￿le caches often
forces anons to be allocated on the CXL-nodes that may need to be
promoted back later.
To resolve these unnecessary page migrations, we allow an ap-
plication allocating caches (e.g., ￿le cache, tmpfs, etc.) to the CXL-
nodes preferrably, while preserving the allocation policy for anon
pages. When this allocation policy is enabled, page caches generated
at any point of an application’s life cycle will be initially allocated
to the CXL-node. If a page cache becomes hot enough to be selected
as a promotion candidate, it will be eventually promoted to the
local node. This policy helps applications with infrequent cache
accesses run on a system with a small amount of local memory and
large but cheap CXL-Memory while maintaining the performance.Table 1: TPP is e ￿ective over its counterparts. It reduces mem-
ory access latency and improves application throughput.
Workload/Throughput (%)
(normalized to Baseline)Default
LinuxTPPNUMA
BalancingAutoTiering
Web1 (2:1) 83.5 99.5 82.8 87.0
Cache1 (2:1) 97.0 99.9 93.7 92.5
Cache1 (1:4) 86.0 99.5 90.0 Fails
Cache2 (2:1) 98.0 99.6 94.2 94.6
Cache2 (1:4) 82.0 95.0 78.0 Fails
Data Warehouse (2:1) 99.3 99.5 – –
5.5 Observability into TPP to Assess
Performance
Promotion- and demotion-related statistics can help better under-
stand the e ￿ectiveness of the page placement mechanism and de-
bug issues in production environments. To this end, we introduce
multiple counters to track di ￿erent demotion and promotion re-
lated events and make them available in the user-space through
/proc/vmstat interface.
To characterize the demotion mechanism, we introduce counters
to track the distribution of successfully demoted anon and ￿le pages.
To understand the promotion behavior, we add new counters to
collect information on the numbers of sampled pages, the number
of promotion attempts, and the number of successfully promoted
pages for each of the memory types.
To track the ping-pong issue mentioned in § 5.3, we utilize the
unused 0x40 bit in the page ￿ag to introduce PG_demoted ￿ag for
demoted pages. Whenever a page is demoted its PG_demoted bit is
set and gets cleared upon promotion. We also count the number of
demoted pages that become promotion candidates. High value of
this counter means TPP is thrashing across NUMA nodes. We add
counters for each of the promotion failure scenario (e.g., local node
having low memory, abnormal page references, system-wide low
memory, etc.) to reason about where and how promotion fails.
6 EVALUATION
We integrate TPP on Linux kernel v5.12. We evaluate TPP with a
subset of representable production applications mentioned in § 3.1
serving live tra ￿c on tiered memory systems across our server ￿eet.
We explore the following questions:
•How e ￿ective TPP is in distributing pages across memory tiers
and improving application performance? (§ 6.1)
•What are the impacts of TPP components? (§ 6.2)
•How it performs against state-of-the-art solutions? (§ 6.3)
For each experiment, we use application-level throughput as the
metric for performance. In addition, we use the fraction of memory
accesses served from the local node as the key low-level supporting
metric. We compare TPP against default Linux (§ 6.1), default NUMA
Balancing [ 22], AutoTiering [ 47], and TMO [ 73] (§6.3) (Table 1).
None of our experiments involve swapping to disks, the whole
system has enough memory to support the workload. We use the
default local-node ￿rst, then CXL-node allocation policy for Linux.
Experimental Setup. We deploy a number of pre-production
x86 CPUs with FPGA-based CXL-Memory expansion card that
support CXL 1.1 speci ￿cation. Memory attached to the expansion
card shows up to the OS as a CPU-less NUMA node. Although our
750ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.
00.20.40.60.81
0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(a) Web100.20.40.60.81
0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(b) Cache100.20.40.60.81
0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(c) Cache200.20.40.60.81
0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(d) Data Warehouse
Figure 14: Fast cold page demotion and e ￿ective hot page promotion allow TPP to serve most of the tra ￿cs from the local node.
current FPGA-based CXL cards have around 250ns higher latency
than our eventual target, we use them for the functional validation.
We have con ￿rmation from two major x86 CPU vendors that the
access latency to CXL-Memory is similar to the remote latency on
a dual-socket system. For performance evaluation, we primarily
use dual-socket systems and con ￿gure them to mimic our target
CXL-enabled system’s characteristics (one memory node with all
active CPU cores and one CPU-less memory node) according to the
guidance of our CPU vendors. For baseline, we disable the memory
node and CPU cores on a socket while enabling su ￿cient memory
on another socket. Here, single memory node serves the whole
working set.
We use two memory con ￿gurations where the ratio of local node
and CXL-Memory capacity is (a)2:1 (§ 6.1.1) and (b)1:4 (§ 6.1.2).
Con￿guration (a)is similar to our current CXL-enabled production
systems where local node is supposed to serve all the hot working
set. We use con ￿guration (b)to stress-test TPP on a constrained
memory scenario – only a fraction of the total hot working set
can￿t on the local node and hot pages are forced to move to the
CXL-node.
6.1 E ￿ectiveness of TPP
6.1.1 Default Production Environment (2:1 Configuration). Web.
Web1 performs lots of ￿le I/O during initialization and ￿lls up the
local node. Default Linux is 44⇥slower than TPP during freeing up
the local node. As a result, new allocation to the local node halts
and anons get allocated to the CXL-node and stay there forever. In
default Linux, on average, only 22% of total memory accesses are
served from the local node (Figure 14a). As a result, throughput
drops by 16.5%.
Active and faster demotion helps TPP move colder ￿le pages to
CXL-node and allow more anon pages to be allocated in the local
node. Here, 92% of the total anon pages are served from the local
node. As a result, local node serves 90% of total memory accesses.
Throughput drop is only 0.5%.
Cache. Cache applications maintain a steady ratio of anon and
￿le pages throughout their life-cycle. Almost all the anon pages get
allocated to the local node from the beginning and served from there.
For Cache1, with default Linux, only 8% of the total hot pages are
trapped in CXL-node. As a result, application performance remains
very close to the baseline – performance regression is only 3%. TPP
can even minimize this performance gap (throughput is 99.9% of
the baseline) by promoting all the trapped hot ￿les and improving
the fraction of tra ￿c served from the local node (Figure 14b).00.20.40.60.81
0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(a) Cache100.20.40.60.81
0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(b) Cache2
Figure 15: E ￿ectiveness of TPP under memory constraint.
Although most of the Cache2’s anon pages reside in the local
node on a default Linux kernel, all of them are not always hot – only
75% of the total anon pages remain hot within a two-minute interval.
TPP can e ￿ciently detect the cold anon pages and demote them to
the CXL-node. This allows the promotion of more hot ￿le pages.
On default Linux, local node serves 78% of the memory accesses
(Figure 14c) and cause 2% throughput regression. TPP improves the
fraction of local tra ￿c to 91%. Throughput regression is only 0.4%.
Data Warehouse. This workload generates ￿le caches to store
the intermediate processing data. File caches mostly remain cold.
Besides, only one third of the total anon pages remain hot. Our
default production con ￿guration is good enough to serve all the
hot memory from the local node. Default Linux and TPP perform
the same (0.5–0.7% throughput drop).
TPP improves the fraction of local tra ￿c by moving relatively
hotter anon pages to the local node. With TPP, 94% of the total
anon pages reside on the local node, while the default kernel hosts
only 67%. To make space for the hot anon pages, cold ￿le pages
are demoted to the CXL-node. TPP allows 4% higher local node
memory accesses over Linux (Figure 14d).
6.1.2 Large Memory Expansion with CXL (1:4 Configuration). Ex-
treme setups like 1:4 con ￿guration allow ￿exible server design with
DRAM as a small-sized local node and CXL-Memory as a large
but cheaper memory. As in our production, such a con ￿guration is
impractical for Web and Data Warehouse, we limit our discussion
to the Cache applications. Note that TPP is e ￿ective even for Web
and Data Warehouse in such a setup and performs very close to
the baseline.
Cache1. In a 1:4 con ￿guration, on a default Linux kernel, ￿le
pages consume almost all the local node’s capacity. 85% of the total
anon pages get trapped to the CXL-node and throughput drops by
14%. Because of the apt promotion, TPP can move almost all the
CXL-node’s hot anon pages (97% of the total hot anon pages within
751TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada
01020304050
220240260280300Increasein Avg. Memory Latency (ns)CXL-Memory Latency (ns)DefaultTPP
(a) Avg. Memory Access Latency0123456
220240260280300Throughput Loss (%)CXL-Memory Latency (ns)DefaultTPP
(b) Impact on Throughput
Figure 16: TPP bene ￿ts CXL-node with varied latency traits.
05001000150020002500
050010001500Allocation Rate (MB/s)Time (minute)w/ Decouplingw/o Decoupling(a) New Allocation0.0010.010.1110100100010000
050010001500Promotion Rate (KB/s)Time (minute)w/ Decouplingw/o Decoupling
(b) Promotion to Toptier Node
Figure 17: Impact of decoupling allocation and reclamation.
a minute) to the local node. This forces less latency-sensitive ￿le
pages to be demoted to the CXL-node. Eventually, TPP stabilizes
the tra ￿c between the two nodes and local node serves 85% of
the total memory accesses. This helps Cache1 achieve the baseline
performance – even though the local node’s capacity is only 20% of
the working set size, throughput regression is only 0.5% (Figure 15a).
Cache2. Similar to Cache1, on default Linux, Cache2 experiences
18% throughput loss. Only 14% of the anon pages remain on the
local node (Figure 15b). TPP can bring back almost all the remote
hot anon pages (80% of the total anon pages) to local node while
demoting the cold ones to the CXL-node. As Cache2 accesses lots
of caches, and caches are now mostly in CXL-node, 41% of the
memory tra ￿c comes from the CXL-node. Yet, throughput drop is
only 5% with TPP.
6.1.3 TPP with Varied CXL-Memory Latencies. The variation in
CXL-Memory’s access latency does not impact TPP much. We run
Cache2 with 2:1 con ￿guration where CXL-Memory has di ￿erent
latency characteristics (Figure 16). In all cases, due to TPP’s bet-
ter hot page identi ￿cation and e ￿ective promotion, only a small
portion (4–5%) of hot pages are served from the CXL-node. On the
other hand, for default Linux, 22–25% hot pages remain trapped in
the CXL-node. This increases the average memory access latency
by7⇥for default Linux (Figure 16a). This, eventually, impact the
application-level performance, default Linux experiences 2.2 2.8⇥
higher throughput loss over TPP (Figure 16b).
6.2 Impact of TPP Components
In this section, we discuss the contribution of TPP components. As
a case study, we use Cache1 with 1:4 con ￿guration.
Allocation and Reclamation Decoupling. Without this fea-
ture, reclamation on local node triggers at a later phase. With high
memory pressure and delayed reclamation on local node, the bene-
￿t of TPP disappears as it fails to promote CXL-node pages. Besides,00.20.40.60.8101020304050607080Memory Access (%)Time (minute)Local (Active LRU)CXL-Node (Active LRU)Local (Default)CXL-Node (Default)query startsmemory accesstrafficconvergessteepchange in traffic for defaultwarm upFigure 18: Restricting the promotion candidate based on their
age reduces unnecessary promotion tra ￿c.
Table 2: Page-type aware allocation helps applications.
Application Con￿guration% of Memory Access Tra ￿cThroughput
w.r.t Baseline Local Node CXL-node
Web1 2:1 97% 3% 99.5%
Cache1 1:4 85% 15% 99.8%
Cache2 1:4 72% 28% 98.5%
00.20.40.60.81
0200400600800Local Traffic (%)Time (minute)TPPNUMA BalancingAutoTiering(a) Web1 on 2:1 Con ￿guration00.20.40.60.81
0200400600800Local Traffic (%)Time (minute)TPPNuma BalancingAutoTiering (2:1)(b) Cache1 on 1:4 Con ￿guration
Figure 19: TPP outperforms existing page placement mecha-
nism. Note that AutoTiering can’t run on 1:4 con ￿guration
For Cache1, TPP on 1:4 con ￿guration performs better than
AutoTiereing on 2:1.
newly allocated pages are often short-lived (less than a minute life-
time) and de-allocated even before being selected as a promotion
candidate. Trapped CXL-node hot pages worsen the performance.
Without the decoupling feature, allocation maintains a steady
rate that is controlled by the rate of reclamation (Figure 17a). As a
result, any burst in allocations puts pages to the CXL-node. When
allocation and reclamation is decoupled, TPP allows more pages on
the local node – allocation rate to local node increases by 1.6⇥at
the95C⌘percentile.
As local node is always under memory pressure, new allocations
consume freed up pages instantly and promotion fails as the target
node becomes low on memory after serving allocation requests.
For this reason, without the decoupling feature, promotion almost
halts most of the time (Figure 17b). Trapped pages on the CXL-
node generates 55% of the memory tra ￿c which leads to a 12%
throughput drop. With the decoupling feature, promotion maintains
a steady rate of 50KB/s on average. During the surge in remote
memory usage, the promotion goes as high as 1.2MB/s in the 99C⌘
percentile. This helps TPP to maintain the throughput of baseline.
Active LRU-Based Hot Page Detection. Considering active
LRU pages as promotion candidates helps TPP add hysteresis to
page promotion. This reduces the page promotion rate by 11⇥. The
number of demoted pages that subsequently get promoted is also
reduced by 50%. Although the demotion rate drops by 4%, as we are
not allowing unnecessary promotion tra ￿cs to waste local memory
node, now there are more e ￿ective free pages in local node. As a
752ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.
Table 3: TMO enhances TPP’s memory reclamation process
and improves page migration by generating more free space.
Web1 on 2:1 Con ￿guration TPP-only TPP with TMO
Migration Failure Rate (pages/sec) 20 5
CXL-node’s Memory Tra ￿c (%) 3.1% 2.7%
Table 4: TPP improves TMO by e ￿ectively turning the swap
action into a two-stage demote-then-swap process.
Web1 on 2:1 Con ￿guration TMO-only TMO with TPP
Process Stall (Normalized to Threshold) 70% 40%
Memory Saving (% of Total Capacity) 13.5% 16.5%
result, promotion success rate improves by 48%. Thus, reduced but
successful promotions provide enough free spaces in local node for
new allocations and improve the local node’s memory access by 4%.
Throughput also improves by 2.4%. The time requires to converge
the tra ￿c across memory tiers is almost similar – to reach the peak
tra￿c on local node, TPP with active LRU-based promotion takes
extra ￿ve minutes (Figure 18).
Cache Allocation to Remote Node Policy. For Web and Cache,
preferring the ￿le cache allocation to CXL-node can provide all-
local performances even with a small-sized local node (Table 2).
TPP is e ￿cient enough to keep most of the e ￿ective hot pages on
the local node. Throughput drop over the baseline is only 0.2–2.5%.
6.3 Comparison Against Existing Solutions
We compare TPP against Linux’s default NUMA Balancing, Au-
toTiering, and TMO. We use Web1 and Cache1, two representative
workloads of two di ￿erent service domains, and evaluate them on
target production setup (2:1 con ￿guration) and memory-expansion
setup (1:4 con ￿guration), respectively. We omit Data Warehouse
as it does not show any signi ￿cant performance drop even with
default Linux (§ 6.1).
6.3.1 TPP against NUMA Balancing and AutoTiering. Web1. NUMA
Balancing helps when the reclaim mechanism can provide with
enough free pages for promotion. When the local node is low on
memory, NUMA Balancing stops promoting pages and performs
even worse than default Linux because of its extra scanning and
failed promotion tries. Due to 42⇥slower reclamation rate than
TPP, NUMA Balancing experiences 11⇥slower promotion rate.
Local node can serve only 20% of the memory tra ￿c (Figure 19a).
As a result, throughput drops by 17.2%. Due to the unnecessary
sampling, its system-wide CPU overhead is 2% higher that TPP.
AutoTiering has a faster reclamation mechanism – it migrates
pages with low access frequencies to CXL-node. However, with a
tightly-coupled allocation-reclamation path, it maintains a ￿xed-
size bu ￿er to support promotion under pressure. This reserved
bu￿er eventually ￿lls up during a surge in CXL-node page accesses.
At that point, AutoTiering also fails to promote pages and end up
serving 70% of the tra ￿c from the CXL-node. Throughput drops
by 13% from the baseline.
Note that TPP experiences only a 0.5% throughput drop.
Cache1. In 1:4 con ￿guration, with more memory pressure on
local node, NUMA Balancing e ￿ectively stops promoting pages.
Only 46% of the tra ￿cs are accessed from the local node (Figure 19b).
Throughput drops by 10%.We can not setup AutoTiering with 1:4 con ￿guration. It fre-
quently crashes right after the warm up phase, when query ￿res.
We run Cache1 with AutoTiering on 2:1 con ￿guration. TPP out
performs AutoTiering even with a 46% smaller local node – TPP
can serve 10% more tra ￿c from local node and provides 7% better
throughput over AutoTiering.
6.3.2 Comparison between TPP and TMO. TPP vs. TMO. TMO
monitors application stalls during execution time because of in-
su￿cient system resources (CPU, memory, and IO). Based on the
pressure stall information (PSI), it decides on the amount of memory
that needs to be o ￿oaded to the swap space. As mentioned in § 4,
using TMO for CXL-Memory’s (z)swap-based abstraction is beyond
our design goal. For the sake of argument, if we con ￿gure CXL-
Memory as a swap-space for TMO, it will be only populated during
reclamation. New page allocation can never happen there. Besides,
without any fast promotion mechanism, aggressive reclamation
can hurt application’s performance; especially, when reclaimed
pages are re-accessed through costly swap-ins. As a result, TMO
throttles and can not populate most of the CXL-Memory capacity.
For Web1, Cache1, and Data Warehouse in 2:1 con ￿guration, TMO
can only consume 45%, 61%, and 7% of the CXL-Memory capacity,
respectively. On the other hand, TPP can use CXL-Memory for both
allocation and reclamation purposes. For same applications, TPP’s
CXL-Memory usage is 83%, 92%, and 87%, respectively.
TPP with TMO. We run TMO with TPP and observe they are
orthogonal and augment each other’s behavior for Web1 on 2:1
con￿guration. TPP keeps most hot pages in local node; it gets
slightly better when TMO is enabled (Table 3).TMO creates more
free memory space in the system by swapping out cold pages both
from local and CXL-Memory nodes. The presence of some memory
headroom in the system makes it easier for TPP to move pages
around and leads to fewer page migration failures. As TPP-driven
migration fails less frequently, page placement is more optimized,
resulting in even fewer accesses to the CXL-node.
TMO is still able to save memory without noticeable performance
overhead, as shown in Table 4.This is because TPP makes (z)swap a
two-stage process – TMO-driven reclamation in local node will ￿rst
demote victim pages to CXL-Memory before getting (z)swapped out
eventually. This improves victim page selection process – semi-hot
pages now get a second chance for staying in local memory when
drifting down CXL-node’s LRU list. As a result, TPP reduces the
amount of process stall in TMO originated from major page faults
(i.e., memory and IO pressure in [ 73]) by 30%. As TMO throttles
itself based on the process stall metric, this improves memory saving
by 3% (2GB in absolute terms).
7 DISCUSSION AND FUTURE RESEARCH
TPP makes us production-ready to onboard our ￿rst generation of
CXL-enabled tiered-memory system. We, however, foresee research
opportunities with technology evolution.
Tiered Memory for Multi-tenant Clouds. In a typical cloud,
when multiple tenants co-exist on a single host machine, TPP can
e￿ectively enable them to competitively share di ￿erent memory
tiers. When local memory size dominates the total memory capac-
ity of the system, this may not cause much problem. However, if
753TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada
applications with di ￿erent priorities have di ￿erent QoS require-
ments, TPP may provide sub-optimal performance. Integrating a
well-designed QoS-aware memory management mechanism over
TPP can address this problem.
Allocation Policy for Memory Bandwidth Expansion. For
memory bandwidth-bound applications, CPU to DRAM memory
bandwidth often becomes the bottleneck. CXL’s additional memory
bandwidth can help by spreading memory across the top-tier and
remote node. Instead of only placing cold pages into CXL-Memory,
which draw very low bandwidth consumption, the optimal solu-
tion should place the right amount of bandwidth-heavy, latency-
insensitive pages to CXL-Memory. The methodology to identify
the ideal fraction of such working sets may even require hardware
support. We want to explore transparent memory management for
memory bandwidth-expansion use case in our future work.
Hardware Support for E ￿ective Page Placement. Hardware
features can further enhance performance of TPP. A memory-side
cache and its associated prefetcher on the CXL ASIC might help
reduce the e ￿ective latency of CXL-Memory. Hardware support
for data movement between memory tiers can help reduce page
migration overheads. While in our environment we do not see
a high migration overheads, others may chose to put provision
systems more aggressively with very small amount of local memory
and high amount of CXL-Memory. For our use cases, in steady state,
the migration bandwidth is 4–16 MB/s (1–4K pages/second) which
is far lower than CXL link bandwidth and also unlikely to cause
any meaningful CPU overhead due to page movement.
8 RELATED WORK
Tiered Memory System. With the emergence of low-latency non-
DDR technologies, heterogeneous memory systems are becoming
popular. There have been signi ￿cant e ￿orts in using NVM to ex-
tend main memory [ 5,31,37,38,45,46,58,60,63]. CXL enables an
intermediate memory tier with DRAM-like low-latency in the hier-
archy and brings a paradigm shift in ￿exible and performant server
design. Industry leaders are embracing CXL-enabled tiered memory
system in their next-generation datacenters [ 1,4,9,10,21,24,25].
Page Placement for Tiered Memory. Prior work explored
hardware-assisted [ 57,59,62] and application-guided [ 20,25,38,72]
page placement for tiered memory systems, which may not often
scale to datacenter use cases as they require hardware support or
application redesign from the ground up.
Application-transparent page placement approaches often pro-
￿le an application’s physical [ 29,31,36,45,48,78] or virtual address-
space [ 53,63,75,77] to detect page temperature. This causes high
performance-overhead because of frequent invocation of TLB in-
validations or interrupts. We ￿nd existing in-kernel LRU-based
page temperature detection is good enough for CXL-Memory. Prior
study also explored machine learning directed decisions [ 36,48],
user-space APIs [ 53,63], and swapping [ 31,48] to move pages
across the hierarchy, which are either resource or latency intensive.
In-memory swapping [ 8,13,26,30] can be used to swap-out
cold pages to CXL-node. In such cases, CXL-node access requires
page-fault and swapped-out pages are immediately brought back
to main memory when accessed. This makes in-memory swapping
ine￿ective for workloads that access pages at varied frequencies.When CXL-Memory is a part of the main memory, less frequently
accessed pages can be on CXL-node without any page-fault over-
head upon access.
Solutions considering NVM to be the slow memory tier [ 28,43,
47,76] are conceptually close to our work. Nimble [ 76] is optimized
for huge page migrations. During migration, it employs page ex-
change between memory tiers. This worsens the performance as a
demotion needs to wait for a promotion in the critical path. Sim-
ilar to TPP, AutoTiereing [ 47] and work from Huang et al. [ 28]
use background migration for demotion and optimized NUMA bal-
ancing [ 22] for promotion. However, their timer-based hot page
detection causes computation overhead and is often ine ￿cient, es-
pecially when pages are infrequently accessed. Besides, none of
them consider decoupling allocation and reclamation paths. Our
evaluation shows, this is critical for memory-bound applications to
maintain their performance under memory pressure.
Disaggregated Memory. Memory disaggregation exposes ca-
pacity available in remote hosts as a pool of memory shared among
many machines. Most recent memory disaggregation e ￿orts [ 32–
34,40,42,51,55,56,64,66,71] are speci ￿cally designed for RDMA
over In ￿niBand or Ethernet networks where latency characteris-
tics are orders-of-magnitude higher than CXL-Memory. Memory
managements of these systems are orthogonal to TPP– one can
use both CXL- and network-enabled memory tiers and apply TPP
and memory disaggregation solutions to manage memory on the
respective tiers.
9 CONCLUSION
We analyze datacenter applications’ memory usage behavior using
Chameleon, a lightweight and robust user-space working set char-
acterization tool, to ￿nd the scope of CXL-enabled tiered-memory
system. To realize such a system, we design TPP, an OS-level trans-
parent page placement mechanism that works without any prior
knowledge on applications’ memory access behavior. We evaluate
TPP using diverse production workloads and ￿nd TPP improves
application’s performance on default Linux by 18%. TPP also out-
performs NUMA Balancing and AutoTiering, two state-of-the-art
tiered-memory management mechanisms, by 5–17%.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for insightful feedback that
helped improve the paper. Hasan Al Maruf and Mosharaf Chowd-
hury were partly supported by National Science Foundation grants
(CNS-1845853, CNS-2104243) and gifts from VMware and Meta.
REFERENCES
[1]A Milestone in Moving Data. https://newsroom.intel.com/editorials/milestone-
moving-data/ .
[2]Alibaba Cluster Trace 2018. https://github.com/alibaba/clusterdata/blob/master/
cluster-trace-v2018/trace_2018.md .
[3]AMD In ￿nity Architecture. https://www.amd.com/en/technologies/in ￿nity-
architecture .
[4]AMD Joins Consortia to Advance CXL. https://community.amd.com/t5/amd-
business-blog/amd-joins-consortia-to-advance-cxl-a-new-high-speed-
interconnect/ba-p/418202 .
[5]Baidu feed stream services restructures its in-memory database with intel optane
technology. https://www.intel.com/content/www/us/en/customer-spotlight/
stories/baidu-feed-stream-case-study.html .
[6]CCIX. https://www.ccixconsortium.com/ .
[7]Compute Express Link (CXL). https://www.computeexpresslink.org/ .
754ASPLOS ’23, March 25–29, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.
[8]Creating in-memory RAM disks. https://cloud.google.com/compute/docs/disks/
mount-ram-disks .
[9]CXL and the Tiered-Memory Future of Servers. https://www.lenovoxperience.
com/newsDetail/283yi044hzgcdv7snkrmmx9ovpq6aesmy9u9k7ai2648j7or .
[10] CXL Roadmap Opens Up the Memory Hierarchy. https://www.nextplatform.
com/2021/09/07/the-cxl-roadmap-opens-up-the-memory-hierarchy/ .
[11] DAMON: Data Access MONitoring Framework for Fun and Memory Management
Optimizations. https://www.linuxplumbersconf.org/event/7/contributions/659/
attachments/503/1195/damon_ksummit_2020.pdf .
[12] Facebook and Amazon are causing a memory shortage. https:
//www.networkworld.com/article/3247775/facebook-and-amazon-are-causing-
a-memory-shortage.html .
[13] Frontswap. https://www.kernel.org/doc/html/latest/vm/frontswap.html .
[14] Gen-Z. https://genzconsortium.org/ .
[15] Google Cluster Trace 2019. https://github.com/google/cluster-data/blob/master/
ClusterData2019.md .
[16] Idle Memory Tracking. https://www.kernel.org/doc/Documentation/vm/idle_
page_tracking.txt .
[17] Idle page tracking-based working set estimation. https://lwn.net/Articles/460762/ .
[18] Intel ®Xeon ®Processor Scalable Family Technical Overview. https:
//www.intel.com/content/www/us/en/developer/articles/technical/xeon-
processor-scalable-family-technical-overview.html .
[19] Introducing new product innovations for SAP HANA, Expanded AI collaboration
with SAP and more. https://azure.microsoft.com/en-us/blog/introducing-new-
product-innovations-for-sap-hana-expanded-ai-collaboration-with-sap-and-
more/ .
[20] Memkind. https://memkind.github.io/memkind/ .
[21] Micron Exits 3DXPoint, Eyes CXL Opportunities. https://www.eetimes.com/
micron-exits-3d-xpoint-market-eyes-cxl-opportunities .
[22] NUMA Balancing (AutoNUMA). https://mirrors.edge.kernel.org/pub/linux/
kernel/people/andrea/autonuma/autonuma_bench-20120530.pdf .
[23] OpenCAPI. https://opencapi.org/ .
[24] Reimagining Memory Expansion for Single Socket Servers with CXL.
https://www.computeexpresslink.org/post/cxl-consortium-upcoming-
industry-events .
[25] Samsung Unveils Industry-First Memory Module Incorporating New CXL Inter-
connect Standard. https://news.samsung.com/global/samsung-unveils-industry-
￿rst-memory-module-incorporating-new-cxl-interconnect-standard .
[26] The zswap compressed swap cache. https://lwn.net/Articles/537422/ .
[27] Tmpfs. https://www.kernel.org/doc/html/latest/ ￿lesystems/tmpfs.html .
[28] Top-tier memory management. https://lwn.net/Articles/857133/ .
[29] Using DAMON for proactive reclaim. https://lwn.net/Articles/863753/ .
[30] zram. https://www.kernel.org/doc/Documentation/blockdev/zram.txt .
[31] N. Agarwal and T. F. Wenisch. Thermostat: Application-transparent page man-
agement for two-tiered main memory. SIGPLAN , 2017.
[32] M. K. Aguilera, N. Amit, I. Calciu, X. Deguillard, J. Gandhi, S. Novakovi ć, A. Ra-
manathan, P. Subrahmanyam, L. Suresh, K. Tati, R. Venkatasubramanian, and
M. Wei. Remote regions: a simple abstraction for remote memory. In USENIX
ATC, 2018.
[33] E. Amaro, C. Branner-Augmon, Z. Luo, A. Ousterhout, M. K. Aguilera, A. Panda,
S. Ratnasamy, and S. Shenker. Can far memory improve job throughput? In
EuroSys , 2020.
[34] I. Calciu, M. T. Imran, I. Puddu, S. Kashyap, H. A. Maruf, O. Mutlu, and A. Kolli.
Rethinking software runtimes for disaggregated memory. In ASPLOS , 2021.
[35] Y. Chen, I. B. Peng, Z. Peng, X. Liu, and B. Ren. ATMem: Adaptive data placement
in graph applications on heterogeneous memories. In CGO , 2020.
[36] T. D. Doudali, S. Blagodurov, A. Vishnu, S. Gurumurthi, and A. Gavrilovska. Kleio:
A hybrid memory page scheduler with machine intelligence. In HPDC , 2019.
[37] J. Du and Y. Li. Elastify cloud-native spark application with PMEM. Persistent
Memory Summit, 2019.
[38] S. R. Dulloor, A. Roy, Z. Zhao, N. Sundaram, N. Satish, R. Sankaran, J. Jackson,
and K. Schwan. Data tiering in heterogeneous memory systems. In EuroSys ,
2016.
[39] A. Eisenman, D. Gardner, I. AbdelRahman, J. Axboe, S. Dong, K. Hazelwood,
C. Petersen, A. Cidon, and S. Katti. Reducing DRAM footprint with NVM in
Facebook. In EuroSys , 2018.
[40] Y. Gao, Q. Li, L. Tang, Y. Xi, P. Zhang, W. Peng, B. Li, Y. Wu, S. Liu, L. Yan, F. Feng,
Y. Zhuang, F. Liu, P. Liu, X. Liu, Z. Wu, J. Wu, Z. Cao, C. Tian, J. Wu, J. Zhu,
H. Wang, D. Cai, and J. Wu. When cloud storage meets RDMA. In NSDI , 2021.
[41] D. Gouk, S. Lee, M. Kwon, and M. Jung. Direct access, High-Performance memory
disaggregation with DirectCXL. In USENIX ATC , 2022.
[42] J. Gu, Y. Lee, Y. Zhang, M. Chowdhury, and K. G. Shin. E ￿cient memory disag-
gregation with In ￿niswap. In NSDI , 2017.
[43] D. Hansen. Migrate pages in lieu of discard. https://lwn.net/Articles/860215/ .
[44] B. Holden, D. Anderson, J. Trodden, and M. Daves. HyperTransport 3.1 Interconnect
Technology . 2008.
[45] S. Kannan, A. Gavrilovska, V. Gupta, and K. Schwan. Heteroos: Os design for
heterogeneous memory management in datacenter. In ISCA , 2017.[46] H. T. Kassa, J. Akers, M. Ghosh, Z. Cao, V. Gogte, and R. Dreslinski. Improving
performance of ￿ash based Key-Value stores using storage class memory as a
volatile memory extension. In USENIX ATC , 2021.
[47] J. Kim, W. Choe, and J. Ahn. Exploring the design space of page management for
Multi-Tiered memory systems. In USENIX ATC , 2021.
[48] A. Lagar-Cavilla, J. Ahn, S. Souhlal, N. Agarwal, R. Burny, S. Butt, J. Chang,
A. Chaugule, N. Deng, J. Shahid, G. Thelen, K. A. Yurtsever, Y. Zhao, and P. Ran-
ganathan. Software-de ￿ned far memory in warehouse-scale computers. In
ASPLOS , 2019.
[49] S.-H. Lee. Technology scaling challenges and opportunities of memory devices.
In2016 IEEE International Electron Devices Meeting (IEDM) , 2016.
[50] Y. Lee, Y. Kim, and H. Y. Yeom. Lightweight memory tracing for hot data identi ￿-
cation. Cluster Computing , 2020.
[51] Y. Lee, H. A. Maruf, M. Chowdhury, A. Cidon, and K. G. Shin. Hydra : Resilient
and highly available remote memory. In FAST , 2022.
[52] H. Li, D. S. Berger, S. Novakovic, L. Hsu, D. Ernst, P. Zardoshti, M. Shah, S. Ra-
jadnya, S. Lee, I. Agarwal, M. D. Hill, M. Fontoura, and R. Bianchini. Pond:
CXL-Based Memory Pooling Systems for Cloud Platforms. In ASPLOS , 2023.
[53] Y. Li, S. Ghose, J. Choi, J. Sun, H. Wang, and O. Mutlu. Utility-based hybrid
memory management. In CLUSTER , 2017.
[54] C. A. Mack. Fifty years of moore’s law. IEEE Transactions on Semiconductor
Manufacturing , 2011.
[55] H. A. Maruf and M. Chowdhury. E ￿ectively Prefetching Remote Memory with
Leap. In USENIX ATC , 2020.
[56] H. A. Maruf, Y. Zhong, H. Wong, M. Chowdhury, A. Cidon, and C. Waldspurger.
Memtrade: A disaggregated-memory marketplace for public clouds. arXiv
preprint arXiv:2108.06893 , 2021.
[57] M. R. Meswani, S. Blagodurov, D. Roberts, J. Slice, M. Ignatowski, and G. H.
Loh. Heterogeneous memory architectures: A HW/SW approach for mixing
die-stacked and o ￿-package memories. In HPCA , 2015.
[58] V. Mishra, J. L. Benjamin, and G. Zervas. MONet: heterogeneous memory over
optical network for large-scale data center resource disaggregation. Journal of
Optical Communications and Networking , 2021.
[59] J. C. Mogul, E. Argollo, M. Shah, and P. Faraboschi. Operating system support
for nvm+dram hybrid main memory. In HotOS , 2009.
[60] M. Oskin and G. H. Loh. A software-managed approach to die-stacked dram. In
PACT , 2015.
[61] J. Ousterhout, P. Agrawal, D. Erickson, C. Kozyrakis, J. Leverich, D. Mazières,
S. Mitra, A. Narayanan, G. Parulkar, M. Rosenblum, S. M. Rumble, E. Stratmann,
and R. Stutsman. The case for RAMClouds: Scalable high-performance storage
entirely in DRAM. SIGOPS Oper. Syst. Rev. , 2010.
[62] L. E. Ramos, E. Gorbatov, and R. Bianchini. Page placement in hybrid memory
systems. In ICS, 2011.
[63] A. Raybuck, T. Stamler, W. Zhang, M. Erez, and S. Peter. HeMem: Scalable tiered
memory management for big data applications and real nvm. In SOSP , 2021.
[64] Z. Ruan, M. Schwarzkopf, M. K. Aguilera, and A. Belay. AIFM: High-performance,
application-integrated far memory. In OSDI , 2020.
[65] H. Servat, A. J. Peña, G. Llort, E. Mercadal, H.-C. Hoppe, and J. Labarta. Au-
tomating the application data placement in hybrid memory systems. In IEEE
International Conference on Cluster Computing (CLUSTER) , 2017.
[66] Y. Shan, Y. Huang, Y. Chen, and Y. Zhang. LegoOS: A disseminated, distributed
OS for hardware resource disaggregation. In OSDI , 2018.
[67] A. Sriraman and A. Dhanotia. Accelerometer: Understanding Acceleration Oppor-
tunities for Data Center Overheads at Hyperscale . 2020.
[68] A. Sriraman, A. Dhanotia, and T. F. Wenisch. SoftSKU: Optimizing server archi-
tectures for microservice diversity @scale. In ISCA , 2019.
[69] Vladimir Davydov. Idle Memory Tracking. https://lwn.net/Articles/639341/ .
[70] M. Vuppalapati, J. Miron, R. Agarwal, D. Truong, A. Motivala, and T. Cruanes.
Building an elastic query engine on disaggregated storage. In NSDI , 2020.
[71] C. Wang, H. Ma, S. Liu, Y. Li, Z. Ruan, K. Nguyen, M. D. Bond, R. Netravali,
M. Kim, and G. H. Xu. Semeru: A Memory-Disaggregated managed runtime. In
OSDI , 2020.
[72] W. Wei, D. Jiang, S. A. McKee, J. Xiong, and M. Chen. Exploiting program
semantics to place data in hybrid memory. In PACT , 2015.
[73] J. Weiner, N. Agarwal, D. Schatzberg, L. Yang, H. Wang, B. Sanouillet, B. Sharma,
T. Heo, M. Jain, C. Tang, and D. Skarlatos. TMO: Transparent memory o ￿oading
in datacenters. In ASPLOS , 2022.
[74] K. Wu, Y. Huang, and D. Li. Unimem: Runtime data managementon non-volatile
memory-based heterogeneous main memory. In SC, 2017.
[75] K. Wu, J. Ren, and D. Li. Runtime data management on non-volatile memory-
based heterogeneous memory for task-parallel programs. In SC, 2018.
[76] Z. Yan, D. Lustig, D. Nellans, and A. Bhattacharjee. Nimble page management
for tiered memory systems. In ASPLOS , 2019.
[77] L. Zhang, R. Karimi, I. Ahmad, and Y. Vigfusson. Optimal data placement for
heterogeneous cache, memory, and storage systems. In SIGMETRICS , 2020.
[78] P. Zhou, V. Pandey, J. Sundaresan, A. Raghuraman, Y. Zhou, and S. Kumar. Dy-
namic tracking of page miss ratio curve for memory management. In ASPLOS ,
2004.
755Enabling CXL Memory Expansion for In-Memory Database
Management Systems
Minseon Ahn
Donghun Lee
Jungmin Kim
minseon.ahn@sap.com
domg.hun.lee@sap.com
jungmin.kim@sap.com
SAP Labs Korea
Seoul, South KoreaOliver Rebholz
oliver.rebholz@sap.com
SAP SE
Walldorf, Baden-Württemberg
GermanyAndrew Chang
Jongmin Gim
Jaemin Jung
Vincent Pham
Krishna T. Malladi
Yang Seok Ki
andrew.c1@samsung.com
gim.jongmin@samsung.com
j.jaemin@samsung.com
tung1.pham@samsung.com
k.tej@samsung.com
yangseok.ki@samsung.com
Samsung Semiconductor Inc.
San Jose, California, USA
ABSTRACT
Limited memory volume is always a performance bottleneck in an
in-memory database management system (IMDBMS) as the data
size keeps increasing. To overcome the physical memory limita-
tion, heterogeneous and disaggregated computing platforms are
proposed, such as Gen-Z, CCIX, OpenCAPI, and CXL. In this work,
we introduce ￿exible CXL memory expansion using a CXL type 3
prototype and evaluate its performance in an IMDBMS. Our evalu-
ation shows that CXL memory devices interfaced with PCIe Gen5
are appropriate for memory expansion with nearly no throughput
degradation in OLTP workloads and less than 8% throughput degra-
dation in OLAP workloads. Thus, CXL memory is a good candidate
for memory expansion with lower TCO in IMDBMSs.
CCS CONCEPTS
•Hardware !Emerging interfaces ;•Information systems
!Database management system engines .
KEYWORDS
CXL, Compute Express Link, In-Memory Database, DBMS, Data-
base Management Systems
ACM Reference Format:
Minseon Ahn, Donghun Lee, Jungmin Kim, Oliver Rebholz, Andrew Chang,
Jongmin Gim, Jaemin Jung, Vincent Pham, Krishna T. Malladi, and Yang
Seok Ki. 2022. Enabling CXL Memory Expansion for In-Memory Database
Management Systems. In Data Management on New Hardware (DaMoN’22),
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro ￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speci ￿c permission and/or a
fee. Request permissions from permissions@acm.org.
DaMoN’22, June 13, 2022, Philadelphia, PA, USA
©2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9378-2/22/06. . . $15.00
https://doi.org/10.1145/3533737.3535090June 13, 2022, Philadelphia, PA, USA. ACM, New York, NY, USA, 5 pages.
https://doi.org/10.1145/3533737.3535090
1 INTRODUCTION
Modern applications’ growing data needs large DRAM capacity, par-
ticularly for in-memory database management systems (IMDBMS).
Furthermore, considering data growth after initial deployment,
the DRAM capacity of an on-premise server is usually overprovi-
sioned. This results in an increased total cost of ownership (TCO)
for IMDBMS servers. Emerging new technologies like NVMe [ 11],
and RDMA [ 12,13,17] provide a ￿exible and integrated memory
view larger than the physical memory. However, this needs appli-
cation changes as well as resulting in longer latency. To overcome
such limitations on physical memory expansion and to provide
more ￿exible solutions, heterogeneous and disaggregated comput-
ing platforms, such as Gen-Z [ 2], CCIX [ 4], OpenCAPI [ 1], and most
recently CXL (Compute Express Link) [ 5], are proposed. With wide
adoption across the industry, CXL is the most promising candidate
to mitigate memory overprovisioning issues.
CXL is a new class of interconnect for device connectivity, an
open industry standard led by Intel ®, and cache coherent interface
using PCIe, enabling memory expansion and heterogeneous mem-
ory for disaggregated computing platforms. CXL has an alternate
protocol that runs across the standard PCIe 5.0 physical layer, con-
sisting of three protocols; (1) CXL.io for discovery, con ￿guration,
register access, and interrupt, (2) CXL.cache for device access to pro-
cessor memory, and (3) CXL.memory for processor access to device
attached memory. There are three types of CXL devices. Type 1 is a
CXL device without host-managed device memory like NIC using
CXL.io and CXL.cache. Type 2 is a CXL device with host-managed
device memory like GPU or external computing units using all 3
CXL protocols. Type 3 is a CXL device only with host-managed
device memory using CXL.memory. A typical application of type 3
is memory expansion.
DaMoN’22, June 13, 2022, Philadelphia, PA, USA Minseon Ahn, et al.
Figure 1: CXL prototype diagram
In this work, we introduce CXL type 3 memory expansion for
IMDBMS [ 10]. First, we propose a CXL-based solution for ￿exible
memory expansion in our IMDBMS to address the overprovisioning
issue. Second, we introduce a prototype of CXL type 3 memory
devices and a complete working system. Third, we evaluate the
performance with common database benchmark tests. Our evalua-
tion shows that CXL memory expansion has nearly no throughput
degradation with TPC-C, one of OLTP workloads, and less than
8% throughput degradation with TPC-DS, one of OLAP workloads.
This is a promising solution given the limited capability of the pro-
totype. With PCIe Gen5 performance at the ￿nal product stage, CXL
memory expansion is expected to provide competitive solutions to
optimize TCO in IMDBMSs without large performance penalty.
The remainder of this paper is organized as follows: Section 2
discusses memory expansion in IMDBMSs. Section 3 introduces
CXL memory expansion devices. The performance evaluation is
addressed in Section 4. Section 5 represents the related work and
Section 6 concludes the paper.
2 MEMORY EXPANSION IN IMDBMS
IMDBMSs widely support hybrid transactional and analytical pro-
cessing (HTAP) [ 16]. In this work, SAP HANA in-memory database
platform [ 10] is used as a base platform. It adopts the columnar
storage [ 15], storing the data of each column in the read-optimized
main storage and maintaining the separate delta storage for opti-
mized writes. Additionally, a portion of memory is allocated for the
operational data to keep the intermediate results while processing
a query.
There are two options to enable the memory expansion using
CXL memory devices in our IMDBMS. In the ￿rst, the additional
memory space of the CXL device could be uniformly integrated with
the host memory space. This allows both operational memory and
main storage to be allocated in CXL memory. However, the random
accesses to the operational data can degrade overall performance
owing to longer access latency of the CXL device compared to the
host DRAMs. In the second option, the CXL memory device is used
only for the main storage. The delta storage and the operational
data are stored in the host DRAMs. Like the approach used in the
persistent memory [ 9], a prefetching scheme can e ￿ectively hide
the longer latency of the CXL device when the main storage is
sequentially accessed. In this work, we use the second option in
our IMDBMS to take advantage of the prefetch.
3 CXL MEMORY EXPANSION
This section introduces a prototype of CXL type 3 memory ex-
pansion devices in E3.S form factor. It implements CXL.mem and
CXL.io commands de ￿ned in CXL1.1 speci ￿cation, carrying 128GBDRAM as media and supporting a theoretical bandwidth of 16GB/s
with PCIe Gen4x8 as the bottleneck. As shown in Fig. 1, the proto-
type consists of a custom FPGA board and a single-channel-based
DDR3 DIMM module. The FPGA is composed of CXL PHY link
supporting the connection to CPU, CXL protocol engine manag-
ing CXL.mem and CXL.io, and memory controller for the DIMM
module. DDR3 DIMM can be replaced with DDR4 or DDR5 DIMMs
supporting multiple channels within a single CXL memory device
in the future. On the FPGA, the SerDes technology in our prototype
runs at 16 Gbps, same as PCIe Gen4 speed. It can be upgraded to
32 Gbps, PCIe Gen5 speed, with ASIC implementation. Because
CXL and PCIe share the same physical layer, this memory device
conveniently plugs into existing PCIe slots.
Our CXL device is recognized as a memory-only NUMA node or
a DAX device. When CXL memory appears on the system memory
map along with the host DRAMs, CPUs can directly load/store
from and to the device memory through the host CXL interface
without ever touching the host memory. To highlight CXL.mem
protocol bene ￿t of low latency, the translation logic between CXL
protocol to DRAM media is kept to a minimum. CXL physical
and link layers perform con ￿guration and link negotiation with
the PCIe root complex. CXL protocol layer unpacks CXL ￿its into
command, address, and data ￿elds for the internal data path. In
this prototype, host physical addresses are directly mapped onto
the device memory space, removing the need for translation. CXL
read and write commands are handled by CXL protocol engine and
the memory controller performs 64B read and write transactions
to DRAM. Because the target throughput is 16GB/s, a single DDR
channel is su ￿cient to match the performance. However, to get the
best throughput, multiple outstanding transactions are required to
mitigate the latency to and from the DDR interface. To maximize
device memory bandwidth, CXL.mem read and write are arbitrated
fairly in the ￿rst-come ￿rst-server order. Writes are completed
when data is written into DDR memory. Responses for read and
write are returned to the host in the same order of their completion.
4 PERFORMANCE EVALUATION
4.1 System Con ￿guration
The evaluation was done on Intel’s Sapphire Rapids (SPR) customer
reference board with C0 stepping CPUs. For all con ￿gurations
except CXL emulation in the single-node test and the scale-out
con￿guration, we enable one socket per node. For CXL emulation
and the scale-up con ￿gurations, we enable two sockets. Each socket
has 512 GB DDR5 4800 MHz memory, one 64 GB DIMM per chan-
nel. CXL memory extensions are set up as DAX devices because
the current release of our IMDBMS does not support the memory-
only NUMA node yet. Thus, the memory space is recognized with
persistent memory features [ 9] and the main storage is moved to
CXL memory. The persistent memory features do not add any over-
head when reading the main storage because there is no additional
instruction required.
As the IMDBMS used for our experiments accepts only one DAX
path per socket due to the limitation of the test version, we stripe
multiple CXL memory devices using the device mapper [ 6] to see
the impact of increased bandwidth beyond the current 16GB/s limit.
We use TPC-C with 100 warehouses for OLTP workloads, increasingEnabling CXL Memory Expansion for In-Memory Database Management Systems DaMoN’22, June 13, 2022, Philadelphia, PA, USA
(a) Baseline
 (b) 1CXL
 (c) 2CXL
 (d) CXL Emulation
Figure 2: System con ￿guration for single-node test
(a) TPC-C
 (b) TPC-DS
Figure 3: Benchmark performance in single-node test
the number of client connections to maximize the performance. The
number of client connections is set to 176, 352, and 704, which are
respectively 4, 8, and 16 times of 44 physical cores of a single CPU in
the system. We also use TPC-DS with SF=100 for OLAP workloads,
increasing the number of parallel requests up to 32 in the client to
see the performance scalability.
4.2 Evaluation of CXL Memory Devices
Single-node Test. We test a single-socket machine to evaluate the
performance of CXL memory expansion. In this experiment, we
have 4 con ￿gurations as shown in Fig. 2: (1) the baseline without
CXL memory expansion, (2) with 1 CXL device, (3) with 2 CXL
devices striped, and (4) CXL emulation in SPR by setting up the
main storage in the memory of the remote socket as a DAX device
after CPU a ￿nity is set to CPU0 only, assuming that the access
latency to the remote memory through UPI is similar to the future
CXL memory expansion. The baseline allocates the main storage in
the host DRAM, while the other con ￿gurations with CXL memory
devices have it in the CXL memory expansion area. CXL emulation
allocates the main storage in the remote memory.
Fig. 3 shows the normalized throughput to the maximum among
all con ￿gurations. TPC-C has nearly no performance di ￿erence be-
tween the baseline and the other CXL con ￿gurations. As mentioned
in Section 2, the latency of sequential accesses to the main storage
is completely hidden by the prefetching scheme. Pro ￿le results
using Intel ®VTune ™Pro￿ler [7] show low memory bandwidth
bound in TPC-C. Thus, CXL memory expansion has no performance
degradation in OLTP workloads. However, the average throughput
degradation in TPC-DS is 27% in 1CXL, 18% in 2CXL, and 8% in CXL
emulation. Larger performance degradation is mainly caused by the
limited bandwidth of the current CXL prototype with PCIe Gen4x8.
However, we expect that the degradation in TPC-DS would be lessthan 8% in the future CXL product as CXL memory bandwidth is
increased with PCIe Gen5x16 (64 GB/s), which is more than UPI
connections in CXL emulation.
Comparison between scale-up and scale-out. To study further
bene ￿ts of CXL memory expansion, we compare the performance
of a scale-up with 2 CPUs and a 2-node scale-out system as shown
in Fig. 4. First, the scale-up baseline has no CXL memory expansion.
Second, scale-up+2CXL has two CXL memory devices, one per
socket. Third, scale-up+4CXL has four CXL memory devices, two
per socket with striping to increase the bandwidth. The scale-up
baseline has the main storage in the host DRAM, while the scale-up
with the CXL memory has it in the CXL memory. We use the default
NUMA-aware location to achieve balanced memory usage across
NUMA nodes in the scale-up con ￿gurations. In the scale-out, we
prepare two nodes enabled with 1 CPU in each node to make a fair
comparison. Then, we connect them with 10G Ethernet. We use
hash partitioning on the ￿rst columns of the primary keys in all
tables, which are used in all the join conditions.
As shown in Fig. 5, TPC-C has no signi ￿cant performance di ￿er-
ence between the scale-up baseline and the scale-up CXL con ￿gu-
rations because of low memory bandwidth bound. Comparing the
performance between the scale-up and the scale-out, the scale-up
con￿gurations outperform the scale-out before 704 connections
(16 * 44 physical cores). We observe that the performance of the
scale-up decreases for 704 connections due to the overhead caused
by too many client connections in a single machine. The average
throughput degradation for TPC-DS compared to the scale-up base-
line is 39% in scale-up+2CXL and 16% in scale-up+4CXL due to the
bandwidth limitation of the prototype. However, scale-up+4CXL
shows slightly better throughput than the scale-out. Once CXL
memory bandwidth is increased with PCIe Gen5, the scale-up with
CXL memory is expected to have much better performance thanDaMoN’22, June 13, 2022, Philadelphia, PA, USA Minseon Ahn, et al.
(a) Scale-up
 (b) Scale-up+2CXL
 (c) Scale-up+4CXL
 (d) Scale-out
Figure 4: System con ￿guration for scale-up and scale-out
(a) TPC-C
 (b) TPC-DS
Figure 5: Performance comparison between scale-up and scale-out
the scale-out. Therefore, CXL memory expansion is a good solution
to increasing the memory capacity with lower TCO, if the system
provides su ￿cient computing resources.
5 RELATED WORK
Providing a ￿exible and integrated memory view larger than the
physical memory is one of the important topics in IMDBMSs to
overcome the capacity limitation. Guz et. al. [ 11] proposes NVMe-
SSD disaggregation using NVMf (NVMe-over-fabrics) [ 3]. Koh et.
al. [12] introduces a disaggregated memory system integrated with
the hypervisor for cloud computing. Adding the disaggregated
memory support to the memory management in the KVM hyper-
visor minimizes the overhead of remote direct memory accesses
(RDMA). Korolija et. al. [ 13] proposes Farview, a disaggregated
and network-attached memory using an FPGA-based smart NIC.
Taranov et. al. [ 17] proposes CoRM, an RDMA-accelerated shared
memory system.
As more ￿exible solutions for the heterogeneous and disaggre-
gated computing platform, several technical standards for cache
coherent interconnects have been devised. CCIX (cache coherent
interconnect for accelerators) [ 4] is a protocol to enable coherent
interconnects widely used in ARM-based System-on-Chips, while
OpenCAPI [ 1] is an open standard for Cache Accelerator Processor
Interface developed for IBM Power CPUs. Gen-Z [ 2] was an open
system interconnect to provide cache coherent memory accesses,
and now it is merged to CXL. NVLink [ 8] is also a cache coherentinterconnect mainly for NVidia GPUs. It is also supported in IBM
Power CPUs. Lutz et. al. [ 14] shows that fast interconnects like
NVLink 2.0 can overcome the limits of the current GPUs, such as
on-board memory capacity and interconnect bandwidth, thus re-
sulting in better performance in CPU-GPU hash joins with a larger
data size than the amount of GPU memory.
6 CONCLUSION
This work proposes a ￿exible CXL-based memory expansion with
potentially lower TCO in an IMDBMS as one of the signi ￿cant
use cases of CXL memory. The evaluation results using common
database benchmark tests proved the feasibility of CXL memory
expansion in an IMDBMS. OLTP workloads have nearly no through-
put degradation with CXL memory devices. Even though OLAP
workloads have a certain amount of throughput degradation, its
performance on the real CXL product can be dramatically improved
once CXL devices are operating at PCIe Gen5 speed. Furthermore,
considering that the current experiments were done using the pre-
production SPR (C0 stepping) CPU and slow DDR3 DIMMs in CXL
memory expansion, more performance improvement is anticipated
with the mass-production SPR and CXL memory.
REFERENCES
[1]2014. OpenCAPI Consortium . https://opencapi.org/
[2]2016. Gen-Z Consortium . https://genzconsortium.org/
[3]2016. NVM Express over Fabric 1.0 . https://nvmexpress.org/
[4]2017. CCIX Consortium . https://www.ccixconsortium.com/Enabling CXL Memory Expansion for In-Memory Database Management Systems DaMoN’22, June 13, 2022, Philadelphia, PA, USA
[5]2019. Compute Express Link . https://www.computeexpresslink.org/
[6]2021. Device Mapper . https://www.kernel.org/doc/html/latest/admin-guide/
device-mapper/index.html
[7]2021. Intel ®VTune ™Pro￿ler. https://www.intel.com/content/www/us/en/
developer/tools/oneapi/vtune-pro ￿ler.html
[8]2022. NVLink . https://www.nvidia.com/en-us/data-center/nvlink/
[9]Mihnea Andrei, Christian Lemke, Günter Radestock, Robert Schulze, Carsten
Thiel, Rolando Blanco, Akanksha Meghlan, Muhammad Sharique, Sebastian
Seifert, Surendra Vishnoi, et al .2017. SAP HANA adoption of non-volatile
memory. Proceedings of the VLDB Endowment 10, 12 (2017), 1754–1765.
[10] Franz Färber, Norman May, Wolfgang Lehner, Philipp Große, Ingo Müller, Hannes
Rauhe, and Jonathan Dees. 2012. The SAP HANA Database–An Architecture
Overview. IEEE Data Eng. Bull. 35, 1 (2012), 28–33.
[11] Zvika Guz, Harry Li, Anahita Shayesteh, and Vijay Balakrishnan. 2017. NVMe-
over-fabrics performance characterization and the path to low-overhead ￿ash
disaggregation. In Proceedings of the 10th ACM International Systems and Storage
Conference . 1–9.
[12] Kwangwon Koh, Kangho Kim, Seunghyub Jeon, and Jaehyuk Huh. 2018. Disag-
gregated cloud memory with elastic block management. IEEE Trans. Comput. 68,1 (2018), 39–52.
[13] Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin Taranov,
Dejan Miloji čić, and Gustavo Alonso. 2021. Farview: Disaggregated memory
with operator o ￿-loading for database engines. arXiv preprint arXiv:2106.07102
(2021).
[14] Clemens Lutz, Sebastian Breß, Ste ￿en Zeuch, Tilmann Rabl, and Volker Markl.
2020. Pump up the volume: Processing large data on GPUs with fast interconnects.
InProceedings of the 2020 ACM SIGMOD International Conference on Management
of Data . 1633–1649.
[15] Hasso Plattner. 2014. The impact of columnar in-memory databases on enter-
prise systems: implications of eliminating transaction-maintained aggregates.
Proceedings of the VLDB Endowment 7, 13 (2014), 1722–1729.
[16] Iraklis Psaroudakis, Florian Wolf, Norman May, Thomas Neumann, Alexan-
der Böhm, Anastasia Ailamaki, and Kai-Uwe Sattler. 2014. Scaling up mixed
workloads: a battle of data freshness, ￿exibility, and scheduling. In Technology
Conference on Performance Evaluation and Benchmarking . Springer, 97–112.
[17] Konstantin Taranov, Salvatore Di Girolamo, and Torsten Hoe ￿er. 2021. CoRM:
Compactable Remote Memory over RDMA. In Proceedings of the 2021 International
Conference on Management of Data . 1811–1824.Near to Far: An Evaluation of Disaggregated Memory
for In-Memory Data Processing
Andreas Geyer
andreas.geyer@tu-dresden.de
Technische Universität Dresden
Dresden, GermanyJohannes Pietrzyk
johannes.pietrzyk@tu-dresden.de
Technische Universität Dresden
Dresden, GermanyAlexander Krause
alexander.krause@tu-dresden.de
Technische Universität Dresden
Dresden, Germany
Dirk Habich
dirk.habich@tu-dresden.de
Technische Universität Dresden
Dresden, GermanyWolfgang Lehner
wolfgang.lehner@tu-dresden.de
Technische Universität Dresden
Dresden, GermanyChristian Färber
christian.faerber@intel.com
Intel Deutschland GmbH
Feldkirchen, Germany
Thomas Willhalm
thomas.willhalm@intel.com
Intel Deutschland GmbH
Feldkirchen, Germany
Abstract
E￿cient in-memory data processing relies on the availabil-
ity of su ￿cient resources, be it CPU time or available main
memory. Traditional approaches are coping with resource
limitations by either adding more processors or RAM sticks
to a single server (scale-up) or by adding multiple servers
to a network cluster (scale-out). Further, the In ￿niBand in-
terconnect enables Remote Direct Memory Access (RDMA)
and thus enhances the possibilities of resource sharing be-
tween distinct servers. Resource disaggregation means the
(dynamic) sharing of available hardware, e. g., through the
network. This paradigm is now further enhanced by the
speci ￿cation of Compute Express Link (CXL). In this pa-
per, we systematically evaluate the implications of memory
expansion as a form of resource disaggregation from the
perspective of in-memory data processing through the lo-
cal Ultrapath Interconnect (UPI), RDMA via In ￿niBand, and
PCIe attached memory via CXL. Our results show that CXL
yields behavior that is comparable to UPI and outperforms
the inherently asynchronous RDMA connection. Further, we
found that handling UPI-attached memory as a type of disag-
gregated resource can yield additional performance bene ￿ts.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for pro ￿t or commercial advantage and that
copies bear this notice and the full citation on the ￿rst page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speci ￿c
permission and/or a fee. Request permissions from permissions@acm.org.
DIMES ’23, October 23, 2023, Koblenz, Germany
©2023 Copyright held by the owner/author(s). Publication rights licensed
to ACM.
ACM ISBN 979-8-4007-0300-3/23/10. . . $15.00
h￿ps://doi.org/10.1145/3609308.3625271CCS Concepts: •Information systems !Main memory
engines ;Online analytical processing engines ;•Hardware
!Emerging interfaces .
Keywords: Memory Disaggregation, CXL, RDMA, UPI
ACM Reference Format:
Andreas Geyer, Johannes Pietrzyk, Alexander Krause, Dirk Habich,
Wolfgang Lehner, Christian Färber, and Thomas Willhalm. 2023.
Near to Far: An Evaluation of Disaggregated Memory for In-Memory
Data Processing. In 1st Workshop on Disruptive Memory Systems
(DIMES ’23), October 23, 2023, Koblenz, Germany. ACM, New York,
NY, USA, 7 pages. h￿ps://doi.org/10.1145/3609308.3625271
1 Introduction
The continuous evolution of hardware is an inherent charac-
teristic of this technology. Up to now, main memory and CPU
have been the most decisive drivers, but now other compo-
nents like networking and respecting protocols have become
innovation drivers as well. Today, we already see several
hardware architectures, e. g., scale-up or scale-out solutions
to cope with the ever increasing demand for more compu-
tational power and storage space. Over time, multi-socket
systems have emerged within the scale-up architecture and
introduced the now well-known Non-Uniform Memory Ac-
cess (NUMA) e ￿ect [8,9,13]. However, still all communica-
tion happens on the same machine and is – depending on
the interconnect and support by speci ￿c components [ 22]–
reasonably fast.
The dawn of In ￿niBand as a high speed interconnect en-
abled, e. g., cloud providers to more e ￿ciently share po-
tentially unused resources between individual machines.
In that ￿eld, resource disaggregation is becoming increas-
ingly prevalent. That is, instead of hosting multiple servers,
providers now stack up on dedicated resource cards, i. e.,
racks full of network-attached circuit boards holding either
16
DIMES ’23, October 23, 2023, Koblenz, Germany Geyer et al.CPUDRAMCPUDRAMUPICPUDRAMCPUDRAMPCIePCIeRDMACPUDRAM
DRAMPCIePCIeCXLCU 1CU 2CU 1CU 2CU 1ME(a)(b)(c)Figure 1. Schematic overview of di ￿erent memory expansion methods using distinct Compute Units and a Memory Expansion.
CPUs, DRAM, NVMe drives, accelerators like GPUs or FP-
GAs, and so on. Currently, disaggregation is realized through
Remote Direct Memory Access (RDMA), where two or more
servers are connected by, e. g., an In ￿niBand cable and share
their main memory with each other. Yet, the next big player
is just around the corner: Compute Express Link (CXL) [ 17].
This new PCIe based communication standard was ￿rst de-
scribed in 2019, was later fused with Gen-Z [ 18] and is now
just arriving at the market. CXL features multiple proto-
cols, such as CXL.io ,CXL.cache andCXL.mem , to provide,
e. g., cache-coherent memory access for a remotely attached
memory block. Through CXL, we are now able to combine
multiple resources through the network to a single software
composed system . However, this new architecture comes with
well-known problems: Physically distributed memory leads
to even more severe NUMA e ￿ects. That is because CXL-
attached memory extends the coherent virtual address space
by being exposed as just another NUMA node.
Contribution and Outline. In this paper, we systemati-
cally investigate the implications of resource disaggregation
through both RDMA and CXL from the perspective of in-
memory data processing. The most intriguing questions are:
(i) Can traditional solutions, that worked for scale-up servers,
be applied to disaggregated systems as well? (ii) What are
key similarities or di ￿erences? To answer these questions,
we￿rst conduct several microbenchmarks of representa-
tive in-memory processing operators from modern database
engines [ 4,14] and thus, Section 2 will introduce our exper-
imental methodology. Sections 3 and 4 show our ￿ndings,
clustered by operator complexity. Then, we extend our evalu-
ation focus to a recently proposed optimization technique in
Section 5. After this evaluation, we give an overview about
related work in Section 6. Finally, this paper closes with a
summary and a discussion of future work in Section 7.
2 Evaluation Methodology
Main memory is an important resource for data processing
and its availability can be increased through several methods.
In this paper, we systematically investigate memory expan-
sion through (i) scale-up with UPI, (ii) scale-out-like with
RDMA over In ￿niBand, and (iii) resource disaggregation
with a CXL attached DRAM card. To study the architectural
implications, we developed a comprehensive evaluation pro-
totype in C++ according to Figure 1. Further, we designed
di￿erent benchmarks, based on representative in-memorydata processing operators from database engines with both
a trivial (e. g., aggregation) and complex (e. g., hash join)
internal state to showcase, how memory distances and syn-
chronous vs. asynchronous access in ￿uence the processing
behavior. Currently, we do not have access to a single setup
that has both CXL and RDMA hardware. Thus, UPI and CXL
benchmarks, cf. Figure 1(a,c), are executed on a di ￿erent
physical setup than the RDMA benchmarks, cf. Figure 1b.
For every benchmark, we refer to local and remote data,
where local refers to data that is located on the same NUMA
node as the processing CPU core.
Hardware Setup. The benchmarks for Figure 1(a,c) are
executed on a single server with 4th-generation Intel®Xeon®
Scalable processors (code-named "Sapphire Rapids"), i. e., two
Intel®Xeon®Platinum 8480+ processors, with each having
56 physical cores with a base frequency of 2.0GHz. Main
memory consists of 16 DDR5 memory DIMMs with 16GB
each, which results in 128GBof memory per processor. Since
the processors do not support CXL type 3 devices, the Host
to Memory Expansion (ME) interface uses a CXL Type 2
link (not to be confused with CXL 2.0, cf. [ 1]) but utilizes
CXL.mem transactions. The experimental ME is realized us-
ing an Intel®Agilex ™I-Series FPGA Development Kit (DK-
DEV-AGI027R1BES), which supports PCIe 5.0 x16 CXL 1.1
connectivity and DDR4 memory. This card hosts 2x 8GBSR
DDR4-2666 components soldered to it and is hence exposing
16 GB of memory as a third NUMA-node to the system.
RDMA benchmarks are executed on a second server setup
consisting of two machines. One server is equipped with four
Intel®Xeon®Gold 6130 with 16 cores each and a base clock of
2.1GHz , the second server features four Intel®Xeon®Gold
5120 with 14 cores each and a base clock of 2.2GHz . On both
machines, each CPU is directly connected to 96GBof local
memory. A Mellanox ConnectX-4 card with up to 100GBit /s
(4xEDR) is connecting the NUMA socket 0 of both machines.
Therefore, we pin working threads and memory usage solely
to NUMA-node 0 on both machines when conducting the
RDMA experiments.
Ultrapath Interconnect Setup. UPI is an interconnect
between processors inside a NUMA machine, i. e., it connects
NUMA nodes and allows for cache-coherent data transfer
inside the coherent memory space. Figure 1a illustrates the
general setup. For these experiments, local data is placed on
NUMA socket 0 and remote data is placed on another node,
which is one NUMA-hop away, but still inside the very same
17Near to Far: An Evaluation of Disaggregated Memory for In-Memory Data Processing DIMES ’23, October 23, 2023, Koblenz, Germany
	
,%(#*$1,% & )+",
	
,%(#*$1,% & )+","')-")&.'().(-"&-%/"(!0%!-$	
Figure 2. Relative behavior of UPI, CXL and RDMA with additional remote bu ￿ers for the aggregate kernel. Threads for
remote bu ￿ers are ￿rst assigned to physical cores if available, otherwise to hyperthreads.
physical machine. This placement requires the processing
thread to access the data synchronously through UPI and to
transfer it into the local processor cache to process it.
Remote Direct Memory Access Setup. Figure 1b illus-
trates the connection of two servers via PCIe-attached In ￿ni-
Band (IB) adapters. In this setup, remote data is placed on a
physically di ￿erent machine than the local data. The PCIe
slot is directly attached to the NUMA socket, where the local
data is stored and accessing the remote data is performed
asynchronously through the IB verbs read/write.
Compute Express Link Setup. Our CXL setup involves
the experimental hardware prototype, which allows us to em-
ulate the basic CXL behavior. Local data is stored on the host
and remote data is placed on NUMA node 3, which is directly
connected to both NUMA sockets of the host system. Allocat-
ing memory on this speci ￿c node is performed through the
libnuma callnuma_alloc_on_node() and data access is as
simple as dereferencing a pointer. The compute capabilities
of the FPGA are not used in the conducted experiments.
Operator Types. To gain better insights into the e ￿ects
of the hardware from the perspective of in-memory data pro-
cessing, we selected typical query operators from modern
database engines [ 4,14] – as the prime representative for
in-memory data processing. We divide our operators into
two categories: (i) operators with trivial internal state and (ii)
operators with complex internal state. For the trivial internal
state operators, we decided to employ the classes of aggre-
gation and￿lteroperations. Both operations exhibit similar
properties, like a completely sequential data access on a sin-
gle column and therefore, only the results of the aggregation
kernel are reported. As an example for the aggregation, the
sum is chosen as it mostly depends on the access speed for
the data. The operators were equally executed in the fol-
lowing manner. A number of threads is spawned; for each
thread, exactly one bu ￿er￿lled with integer values between
0 and 100 is created. Every bu ￿er exceeds the cache size to
avoid caching-e ￿ects. All threads execute their respective
operator on their own bu ￿er. While the local bu ￿er holds
100 % of the data, the remote bu ￿ers are sized according to
the remote-to-local-bandwidth ratio. Generally, we reportrelative performance derived from the system bandwidth
calculated as the sum of all data sizes of the used bu ￿ers
divided by the time taken from starting all threads until the
last one is ￿nished, i. e., the synchronized wallclock time.
Contrary to the trivial state operators, we also imple-
mented the hash join as a computationally heavier operator
to represent the category of complex state operators. This
operator works on two columns and involves random access
to the hash table. Again, a number of threads is spawned,
but this time for each thread there is one large table in local
memory – the probe-table – and up to 10 smaller tables for
creating the lookup-tables, on remote memory. Therefore,
each thread has to perform a sequence of joins where the
larger local table is joined against the smaller remote table.
3 Evaluation of Trivial State Operators
We start our evaluation by looking at using either 50 % (cf.
Figure 2a) or 100 % (cf. Figure 2b) of the available physical
cores for local work and up to 8 threads for remote work.
These experiments highlight the scaling behavior with an
increasingly larger amount of o ￿oaded data for a given
interconnect type. Henceforth UPI denotes the link between
the NUMA nodes on the CXL hardware prototype and UPI2
refers to the link on the RDMA-supporting setup.
For each of the four experimental setups UPI, CXL, UPI2
and RDMA, the results are normalized to their respective
value at full local work and therefore, at 0 remote bu ￿ers.
Thus, 100 % for UPI and CXL is a di ￿erent absolute value
than 100 % for UPI2 and RDMA. This is mainly due to the fact
that the experiments for UPI2/RDMA had to be executed on
a di￿erent machine than the ones for UPI/CXL. In all setups,
the remote bu ￿er size that ￿ts their respective bandwidth
best is applied. Due to this setup, one line being above or
beneath another does not mean the absolute performance of
this technique is better or worse. In Figure 2, the results for
the usage of 50 % and100 % of the available physical cores
for the work on local bu ￿ers are shown. For 50 % (cf. Figure
2a) the overall system bandwidth for UPI increases to a cer-
tain degree, when more workers are added to process remote
bu￿ers. RDMA on the other hand decreases while UPI2 holds
18DIMES ’23, October 23, 2023, Koblenz, Germany Geyer et al.
	
	
	
	

	
	



(a)RDMA - scaling remote data with 5%and10 % of local data.				
					



			


(b)CXL - scaling remote data with 10 % and15 % of local data.
Figure 3. Comparing the relative aggregation bandwidth trend for di ￿erent interconnect types with varying amounts of local
and remote data bu ￿ers and adjusted remote data size in percent.
the performance of full local work. For CXL, the curve looks
similar to the one for UPI and thus, implies a similar scaling
behavior, which is signi ￿cantly better than the one shown by
RDMA. For 100 % (cf. Figure 2b) UPI, CXL and UPI2 behave
similarly, while being able to hold the performance of full
local work. Contrarily, RDMA is dropping fast throughout
the experiment because of hyperthreading e ￿ects. At this
point, some of the processing threads and RDMA infrastruc-
ture threads are scheduled on the same physical core and
thus induce triple usage of these cores and therefore, a per-
formance decrease. This is highlighted even more by the
mostly constant curve of UPI2 on the same machine. These
￿gures show that while RDMA is capable of mostly holding
its performance if there are resources available, it does not
make sense to use it in high-workload-scenarios. UPI and
CXL on the other hand scale well and o ￿er the possibility to
increase the system performance.
As these ￿gures only show a very selected view of the
whole search space, we o ￿er a more complete overview for
RDMA (cf. Figure 3a) and CXL (cf. Figure 3b) in the form of
heatmaps. These show the impact of quantity and size of re-
mote bu ￿ers by reporting the relative bandwidth, compared
to the local maximum per heatmap. Figure 3 shows each
combination for local (rows) and remote bu ￿ers (columns)
for RDMA, i. e., up to 16 each, and from 1 to 56 local and 0 to
28 remote bu ￿ers for CXL, with varying remote data sizes.
The heatmaps of Figure 3a show the relative performance
gains on the RDMA hardware for 5%and10 % remote bu ￿er
size compared to the ￿xed size local bu ￿er. With the right
fraction of remote data, it is possible to stay roughly at the
same level of full local work. But when exceeding the opti-
mal amount of remote data, the overall performance drops
signi ￿cantly as can be seen in the right heatmap of Figure 3a
that has an overall lighter color. As the threads that work
on the remote bu ￿ers are mostly waiting, their in ￿uence
by hyperthreading is not that large. The very light spot in
the lower right of each heatmap is a remarkable occurrence,
which clearly indicates the triple usage of one core, i. e., the
RDMA-communication-threads, local and remote working	!$! 	 "## 
Figure 4. Relative behavior of UPI, CXL, UPI2 and RDMA
with additional remote bu ￿ers for the hash-join kernel. Using
100 % of the available physical cores.
threads. This leads to the conclusion that RDMA can be used
for o ￿oading parts of the data, but its characteristics limit
its scalability in this use-case.
Figure 3b in comparison shows the same experiment for
CXL but di ￿erent results. The results clearly show that the
following two conclusions can be drawn: (i) As long as there
are physical cores available for processing remote bu ￿ers
(all rows except 56), we can either gain relative system band-
width or at least do not lose out on any. (ii) The staircase
pattern shows that a combination of remote and local bu ￿er
processing yields approximately the same performance as
the next step. Therefore, we can conclude that it is possible
to work on data located on CXL memory without sacri ￿cing
performance while gaining memory capacities. The lower
right corner of each heatmap staircase pattern exhibits an
overall lower bandwidth, which is because there are no phys-
ical cores left for processing and thus, hyperthreads are used,
meaning double work for the physical core on which this
hyperthread is scheduled. This e ￿ect becomes increasingly
prevalent with larger remote bu ￿ers, as there is also more
work to do for the hyperthread. This observation is indepen-
dent of the actual data placement. Additionally, there is a
shift to the left of the dark part of each heatmap visible with
19Near to Far: An Evaluation of Disaggregated Memory for In-Memory Data Processing DIMES ’23, October 23, 2023, Koblenz, Germany

		
			
$ 2+.)0*72+&$,&/1(2	


		
			
% 2+.)	0*72+&$,&/1(24(17/4.3$.'5+'3*"+2#70( ! 
860(1+-(.32-$,,1(-,$1)(1(-
Figure 5. Absolute behavior of UPI, CXL and RDMA with additional remote bu ￿ers for the hash-join kernel applying pipeline
grouping and prefetching strategies.
increasing remote data size, which is caused by the band-
width of the CXL device. Thus, it is clear that the o ￿oaded
amount of data is a key performance factor. Moreover, it
indicates that CXL scales with its available bandwidth and
therefore, increases the portion of e ￿ciently o ￿oadable data.
4 Evaluation of Complex State Operators
As already introduced (cf. Section 2), we also evaluated the
hash join as a complex state operator and show the results in
Figure 4, again with adjusting the remote table size according
to the relative bandwidth discrepancy. The relative system
bandwidth for the other experiments does not make sense
for the hash-join because the amount of work of each thread
varies according to the number of joins. Thus, we report the
relative slowdown compared to a single hash-join with in-
creasing number of joins that are executed sequentially. The
very naïve assumption would be a constant factor following
the "twice the work, twice the time" principle (depicted as
dashed, grey line), but this is not quite the case. In Figure 4,
again the results for 100 % of the physical cores working
in parallel on individual bu ￿ers is displayed. Even for the
complex state operator, we observe that UPI and CXL be-
have similarly and RDMA cannot keep up with their scaling
behavior. Counter-intuitively, UPI2 seems to perform worse
than RDMA, which is due to the normalization. Each curve is
again normalized to its performance for only one join. While
all approaches with direct synchronous data access can start
immediately, RDMA has to wait on the ￿rst data package
and thus, the ￿rst join for RDMA is slow and the subsequent
joins can pro ￿t from explicit prefetching. Remarkable is the
fact that all approaches are below the assumed slowdown.
This happens because with an increasing amount of executed
joins, the actual compute portion and local random access
into the hash table becomes the limiting factor and hardware
acceleration mechanisms like prefetching are able to hide a
lot of the introduced latency for retrieving the data from the
remote source. Due to the asynchronous character of RDMA,it cannot bene ￿t from these mechanisms that well. The plots
for50 % physical core usage show similar tendencies and are
thus omitted.
5 Evaluation of Advanced Techniques
Our initial memory expansion experiments with naïvely
using RDMA and CXL showed, that expectedly the network
is indeed a performance bottleneck. While CXL is able to
match the scaling performance of UPI, especially RDMA does
not scale well in the shown benchmarks. To overcome that,
we have developed a technique called pipeline grouping [ 5],
which aims to e ￿ciently overlap processing and network
transfer. We want to build upon this principle and investigate
the implications of explicit data access grouping for data
that resides in a coherent address space. To facilitate that,
we changed the access pattern for our UPI/UPI2 and CXL
experiments from a synchronous to an asynchronous pattern.
Meaning the data is explicitly copied from the remote source
to local memory for further usage. This allows us to group
the execution of multiple hash joins together and trigger data
transfer much earlier, allowing for an e ￿cient prefetching
while reducing the network tra ￿c by reducing redundant
data transfers. However, this comes with the cost of pinning
some threads - 5 in our case - for executing the copying
operation from the remote source to local memory.
For a better overall impression, the following benchmark
features up to 20 sequential hash joins, as introduced before.
The used small columns are distinct to each other, while one
large column is used for all of the 20 joins. Therefore, it can
be seen as up to 20 individual pipelines on the same base
column. This benchmark is executed in two variations: (i)
We place the smaller columns remote and the large column is
placed in local memory and (ii) the placements are inverted.
The results of this benchmark are reported in Figure 5. For
the usage of 50 % of the available physical cores as depicted
in Figure 5a, it is visible that again UPI and CXL behave
similarly. The throughput di ￿erence between the UPI and
20DIMES ’23, October 23, 2023, Koblenz, Germany Geyer et al.
CXL curves is easily explained with the limited bandwidth of
our prototypical CXL hardware setup. We are con ￿dent that
this gap will shrink dramatically, when ￿nal CXL products
are hitting the market. It is also visible that with a higher
number of queries the gap between all 4 curves grows smaller.
This is because more data can be reused and the absolute
data transfer is reduced and therefore, the limited bandwidth
has a lesser impact. RDMA and UPI2 on the other hand,
perform signi ￿cantly worse than UPI and CXL. At ￿rst sight,
this seems counter-intuitive to our evaluation of pipeline
groups [ 5], but going more into detail, instead of reducing
the redundant data transfer in parallel executions, we applied
it to sequential joins. Therefore, RDMA cannot bene ￿t that
well, as the amount of simultaneously requested data is not
reduced and thus, its lower bandwidth and higher latency is
a more dominating factor than for UPI and CXL. In addition
to that, the local memory bandwidth of the UPI2/RDMA
machine is signi ￿cantly lower than the one of the UPI/CXL
machine, which ￿attens the UPI2/RDMA curves even more.
Furthermore, while all benchmarks need additional threads
for the explicit data transfer (i. e., to force the asynchronous
prefetching according to [ 5]), the utilization of the RDMA
communication threads is much higher compared to UPI and
CXL and thus, in ￿uences the performance of the executing
physical cores much stronger. For the usage of 100 % of the
available physical cores, shown in Figure 5b, the behavior is
very similar, only the absolute values are larger. It is visible
that the bandwidth limitation, especially for RDMA, has a
higher in ￿uence for lesser query counts.
Overall, this benchmark has shown that, even though
direct access to the CXL memory is possible, it can be bene ￿-
cial to have an explicit asynchronous transfer from the CXL
memory to local memory. If data access grouping is applied,
it is less important which data is placed on what memory
region, but rather if synchronous or asynchronous access
shall be leveraged.
6 Related Work
Hardware disaggregation is by no means trivial and also
a hot contemporary topic as presented in [ 20]. In a lot of
cases, the naïve usage of disaggregation-strategies such as
RDMA or CXL provide usually a higher latency and less
throughput than state-of-the-art server architectures due
to a longer physical distance. From our point of view, the
most promising approaches for using RDMA are operator
push-down as implemented in Farview [ 10] and pipeline
grouping as introduced by us in [ 5] and applied in this paper.
Both approaches o ￿er appropriate techniques to reduce the
network transfer signi ￿cantly by either ￿ltering data before
transferring or avoiding redundant data transfers priorly.
However, as comprehensively evaluated in this paper and
discussed in [ 7], CXL is the better solution from the perspec-
tive of in-memory data processing. Because of this, there is alot of research ongoing with hardware disaggregation using
CXL. On the one hand, there are several papers [ 2,6,12,19,
21] discussing the possibilities CXL o ￿ers, from simple mem-
ory pooling to memory sharing and communication through
shared memory and how to use it to bypass limitations of
state-of-the-art-server architectures. Where [ 21] for example
o￿ers an approach to combine CXL-enabled memory and
SSDs at virtually no performance cost to reduce the TCO fur-
ther. On the other hand, papers like [ 7,11,16] laid their focus
on the evaluation of early CXL devices in di ￿erent scenarios,
similar to this paper. With [ 16] providing the ￿rst absolute
numbers of a CXL protoype device in microbenchmarks as
well as real-world scenarios and [ 11] using CXL as expan-
sion for the in-memory database system SAP HANA and
applying typical database benchmarks as TPC-C and TPC-H,
there are studies of CXL as memory expansion. While [ 3,15]
propose an approach for pushing down some computation to
the CXL memory device to reduce the network-bottleneck;
however, to the best our knowledge, none of them tried an
explicit data-transfer model as we did in this paper. There-
fore, it remains a future ￿eld of research to combine these
approaches which should be very interesting, especially with
CXL-enabled FPGAs.
7 Conclusion
E￿cient in-memory data processing relies on the availability
of su ￿cient resources, be it CPU time or available main mem-
ory. In order to meet the increasing resource demands, hard-
ware disaggregation is a promising concept for the future.
In line with ongoing research in this direction, we presented
a comprehensive evaluation of the implications of di ￿erent
memory expansion methods (UPI, RDMA, and CXL) from
the perspective of in-memory data processing in this paper.
While recent work on RDMA has seemingly been a promis-
ing way out of over-provisioning of memory resources and
high costs, in this paper we reproduced its limitations re-
garding scalability for memory expansion. In addition, we
were able to show that – for representative operators from
in-memory database engines and despite having an early
CXL prototype hardware – (i) CXL yields comparable behav-
ior to UPI and outperforms RDMA and (ii) CXL scales well.
Hence, we see CXL as the most promising memory expan-
sion technique for real world applications in the domain of
in-memory data processing.
Acknowledgments
This work was partly funded by (1) the German Research
Foundation (DFG) via a Reinhart Koselleck-Project (LE-1416/28-
1) and (2) the European Union’s Horizon 2020 research and in-
novation program under grant agreement No 957407 (DAPHNE).
We would like to thank our colleagues Oleg Struk, Otto
Bruggeman and Suprasad Mutalik Desai from Intel for their
support with the CXL machine setup.
21Near to Far: An Evaluation of Disaggregated Memory for In-Memory Data Processing DIMES ’23, October 23, 2023, Koblenz, Germany
References
[1]Minseon Ahn, Andrew Chang, Donghun Lee, Jongmin Gim, Jungmin
Kim, Jaemin Jung, Oliver Rebholz, Vincent Pham, Krishna T. Malladi,
and Yang-Seok Ki. 2022. Enabling CXL Memory Expansion for In-
Memory Database Management Systems. In DaMoN@SIGDMO . 8:1–
8:5.
[2]Daniel S. Berger, Daniel Ernst, Huaicheng Li, Pantea Zardoshti, Monish
Shah, Samir Rajadnya, Scott Lee, Lisa Hsu, Ishwar Agarwal, Mark D.
Hill, and Ricardo Bianchini. 2023. Design Tradeo ￿s in CXL-Based
Memory Pools for Public Cloud Platforms. IEEE Micro 43, 2 (2023),
30–38. h￿ps://doi.org/10.1109/MM.2023.3241586
[3]David Boles, Daniel Waddington, and David A. Roberts. 2023. CXL-
Enabled Enhanced Memory Functions. IEEE Micro 43, 2 (2023), 58–65.
h￿ps://doi.org/10.1109/MM.2022.3229627
[4]Peter A. Boncz, Martin L. Kersten, and Stefan Manegold. 2008. Breaking
the memory wall in MonetDB. Commun. ACM 51, 12 (2008), 77–85.
[5]Andreas Geyer, Alexander Krause, Dirk Habich, and Wolfgang Lehner.
2023. Pipeline Group Optimization on Disaggregated Systems. In
CIDR .
[6]Andreas Geyer, Daniel Ritter, Dong Hun Lee, Minseon Ahn, Johannes
Pietrzyk, Alexander Krause, Dirk Habich, and Wolfgang Lehner. 2023.
Working with Disaggregated Systems. What are the Challenges and
Opportunities of RDMA and CXL?. In BTW , Vol. P-331. 751–755.
[7]Donghyun Gouk, Miryeong Kwon, Hanyeoreum Bae, Sangwon Lee,
and Myoungsoo Jung. 2023. Memory Pooling With CXL. IEEE Micro
43, 2 (2023), 48–57. h￿ps://doi.org/10.1109/MM.2023.3237491
[8]Tim Kiefer, Benjamin Schlegel, and Wolfgang Lehner. 2013. BTW,
Vol. P-214. 185–204.
[9]Thomas Kissinger, Tim Kiefer, Benjamin Schlegel, Dirk Habich, Daniel
Molka, and Wolfgang Lehner. 2014. ERIS: A NUMA-Aware In-Memory
Storage Engine for Analytical Workload. In ADMS@VLDB . 74–85.
[10] Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin
Taranov, Dejan S. Milojicic, and Gustavo Alonso. 2022. Farview: Dis-
aggregated Memory with Operator O ￿-loading for Database Engines.
InCIDR .
[11] Donghun Lee, Thomas Willhalm, Minseon Ahn, Suprasad Mutalik De-
sai, Daniel Booss, Navneet Singh, Daniel Ritter, Jungmin Kim, and
Oliver Rebholz. 2023. Elastic Use of Far Memory for In-Memory Data-
base Management Systems. In DaMoN@SIGMOD . 35–43.
[12] Huaicheng Li, Daniel S. Berger, Lisa Hsu, Daniel Ernst, Pantea Zar-
doshti, Stanko Novakovic, Monish Shah, Samir Rajadnya, Scott Lee,
Ishwar Agarwal, Mark D. Hill, Marcus Fontoura, and Ricardo Bian-
chini. 2023. Pond: CXL-Based Memory Pooling Systems for CloudPlatforms. In Proceedings of the 28th ACM International Conference
on Architectural Support for Programming Languages and Operating
Systems, Volume 2 (Vancouver, BC, Canada) (ASPLOS 2023) . Asso-
ciation for Computing Machinery, New York, NY, USA, 574–587.
h￿ps://doi.org/10.1145/3575693.3578835
[13] Iraklis Psaroudakis, Tobias Scheuer, Norman May, Abdelkader Sellami,
and Anastasia Ailamaki. 2016. Adaptive NUMA-aware data placement
and task scheduling for analytical workloads in main-memory column-
stores. PVLDB 10, 2 (2016), 37–48.
[14] Mark Raasveldt and Hannes Mühleisen. 2019. DuckDB: an Embeddable
Analytical Database. In SIGMOD . 1981–1984.
[15] Joonseop Sim, Soohong Ahn, Taeyoung Ahn, Seungyong Lee,
Myunghyun Rhee, Jooyoung Kim, Kwangsik Shin, Donguk Moon,
Euiseok Kim, and Kyoung Park. 2023. Computational CXL-Memory
Solution for Accelerating Memory-Intensive Applications. IEEE Com-
puter Architecture Letters 22, 1 (2023), 5–8. h￿ps://doi.org/10.1109/
LCA.2022.3226482
[16] Yan Sun, Yifan Yuan, Zeduo Yu, Reese Kuper, Ipoom Jeong, Ren Wang,
and Nam Sung Kim. 2023. Demystifying CXL Memory with Genuine
CXL-Ready Systems and Devices. arXiv preprint arXiv:2303.15375
(2023).
[17] The CXL Consortium. 2019. Compute Express Link. h￿ps://www.
computeexpresslink.org/ . [Online; accessed 14-December-2022].
[18] The CXL Consortium. 2019. Compute Express Link. h￿ps://www.
computeexpresslink.org/projects-3 . [Online; accessed 14-December-
2022].
[19] Jacob Wahlgren, Maya Gokhale, and Ivy B. Peng. 2022. Evaluating
Emerging CXL-enabled Memory Pooling for HPC Systems. In 2022
IEEE/ACM Workshop on Memory Centric High Performance Computing
(MCHPC) . 11–20. h￿ps://doi.org/10.1109/MCHPC56545.2022.00007
[20] Ruihong Wang, Jianguo Wang, Stratos Idreos, M. Tamer Özsu, and
Walid G. Aref. 2022. The Case for Distributed Shared-Memory
Databases with RDMA-Enabled Memory Disaggregation. PVLDB 16, 1
(2022), 15–22.
[21] Qirui Yang, Runyu Jin, Bridget Davis, Devasena Inupakutika, and
Ming Zhao. 2022. Performance Evaluation on CXL-enabled Hybrid
Memory Pool. In 2022 IEEE International Conference on Networking,
Architecture and Storage (NAS) . 1–5. h￿ps://doi.org/10.1109/NAS55553.
2022.9925356
[22] Hao Yu, José E. Moreira, Parijat Dube, I-Hsin Chung, and Li Zhang.
2007. Performance Studies of a WebSphere Application, Trade, in
Scale-out and Scale-up Environments. In IPDPS . 1–8.
22CXL Memory as Persistent Memory for Disaggregated HPC:
A Practical Approach
Yehonatan Fridman
Ben-Gurion University, NRCN, Israel
fridyeh@post.bgu.ac.ilSuprasad Mutalik Desai, Navneet Singh
Intel, India
{suprasad.desai,navneet.singh}@intel.com
Thomas Willhalm
Intel, Germany
thomas.willhalm@intel.comGal Oren
Technion, NRCN, Israel
galoren@cs.technion.ac.il
ABSTRACT
In the landscape of High-Performance Computing (HPC), the quest
for e ￿cient and scalable memory solutions remains paramount.
The advent of Compute Express Link (CXL) introduces a promis-
ing avenue with its potential to function as a Persistent Memory
(PMem) solution in the context of disaggregated HPC systems. This
paper presents a comprehensive exploration of CXL memory’s
viability as a candidate for PMem, supported by physical experi-
ments conducted on cutting-edge multi-NUMA nodes equipped
with CXL-attached memory prototypes. Our study not only bench-
marks the performance of CXL memory but also illustrates the
seamless transition from traditional PMem programming models
to CXL, reinforcing its practicality.
To substantiate our claims, we establish a tangible CXL prototype
using an FPGA card embodying CXL 1.1/2.0 compliant endpoint
designs (Intel FPGA CXL IP). Performance evaluations, executed
through the STREAM and STREAM-PMem benchmarks, showcase
CXL memory’s ability to mirror PMem characteristics in App-Direct
andMemory Mode while achieving impressive bandwidth metrics
with Intel 4th generation Xeon (Sapphire Rapids) processors.
The results elucidate the feasibility of CXL memory as a per-
sistent memory solution, outperforming previously established
benchmarks. In contrast to published DCPMM results, our CXL-
DDR4 memory module o ￿ers comparable bandwidth to local DDR4
memory con ￿gurations, albeit with a moderate decrease in per-
formance. The modi ￿ed STREAM-PMem application underscores
the ease of transitioning programming models from PMem to CXL,
thus underscoring the practicality of adopting CXL memory.
The sources of this work are available at: https://github.com/
Scienti ￿c-Computing-Lab-NRCN/STREAMer .
KEYWORDS
CXL, Memory disaggregation, Persistent Memory (PMem), Intel
Optane DCPMM, HPC, STREAM, STREAM-PMem, STREAMer
1 INTRODUCTION
1.1 Current HPC Memory Solutions Limitations
As the era of Exa-Scale computing unfolds, the demand for ana-
lyzing, manipulating, and storing massive amounts of data inten-
si￿es [60]. Exascale systems are designed to meet these demands
and enable the execution of a broad spectrum of computations,
ranging from loosely to tightly coupled tasks, including CFD simu-
lations and deep learning optimizations [ 11]. Memory and storageresources play a crucial role in the performance and scalability of
these computations [ 32]. Memory factors such as capacity, latency,
and bandwidth are responsible for successfully handling extensive
tasks and delivering data to processing units promptly [ 45]. In sci-
enti￿c computing, storage devices hold signi ￿cance for preserving
diagnostics throughout computations [ 37]. Notably, the growing
frequency of failures in exascale machines emphasizes the signi ￿-
cance of storing vast data volumes to support recovery and bolster
fault tolerance [ 8,14].
However, the traditional memory and storage hierarchy in HPC
systems reveals notable gaps that impose critical constraints on
scienti ￿c computations [ 32]. From the vantage point of memory
architecture, DRAM has inherent limitations of bandwidth and
capacity that impact performance and prevent the processing of
large-scale problems [ 56,58]. From the storage perspective, tradi-
tional devices (such as HDDs and SSDs) provide large capacities
but exhibit very slow access times, leading to signi ￿cant overheads
for I/O-bound applications and fault tolerance mechanisms [ 37].
These gaps and limitations of traditional hardware highlight the
ongoing endeavors to expand the memory-storage hierarchy and
develop novel memory architectures and solutions. A notable ex-
ample is Non-Volatile RAM [ 59,71] (on which we elaborate in
subsection 1.2 ).
While High-Bandwidth Memory (HBM) [ 28] has been intro-
duced as a solution to enhance memory performance, it doesn’t
entirely alleviate the problem [ 29]. HBM memory modules are
stacked vertically, allowing for higher memory bandwidth due to
their increased parallelism. However, even with HBM, the memory
capacity remains limited compared to conventional DDR (Double
Data Rate) memory modules [ 67]. This limitation can still lead to
constraints in memory-intensive applications that require larger
memory spaces [ 44]. Moreover, while HBM addresses the band-
width issue to some extent, it doesn’t eliminate the underlying
problem of memory hierarchy [ 44]. The processor still needs to
access di ￿erent memory levels, and the latency of transferring data
between these levels can impact performance [ 44]. HBM improves
bandwidth between the processor and certain memory modules,
but the need to access di ￿erent levels of memory introduces latency
that can a ￿ect the execution of various tasks [ 44].
In general, it is possible to proclaim that the conventional ap-
proach of locating memory modules directly on the board poses
signi ￿cant challenges in the context of HPC systems [ 10]. This ar-
rangement restricts memory bandwidth due to the limited number
of connections between the processor and these modules [ 10]. As
1arXiv:2308.10714v1  [cs.DC]  21 Aug 2023,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren
a result, the data transfer rate between the processor and board-
mounted memory becomes a bottleneck, hindering the overall per-
formance of the system [ 10].
For an increase of memory capacity outside of the node, ad-
vanced communication technologies such as the Remote Direct
Memory Access (RDMA) based Message Passing Interface (MPI)
optimize inter-node communication [ 35]. However, these sophis-
ticated frameworks are not devoid of challenges [ 17]: MPI, a cor-
nerstone for distributed computing communication, contends with
latency and overhead issues during message transmission, dispro-
portionately a ￿ecting e ￿ciency for applications requiring frequent
communication. Furthermore, the management complexity esca-
lates with the cluster’s scale due to heightened contention for net-
work resources among a larger node count [ 7].
1.2 Persistent Memory in HPC
A proposed solution aimed at bridging the gap between memory
and storage is Persistent Memory (PMem) [ 33,49]. PMem imple-
mentations such as BBU (battery backed up) DIMM or Non-Volatile
RAM (NVRAM) aim to deliver rapid byte-addressable data access
alongside persistent data retention across power cycles. PMem tech-
nologies establish a new tier within the memory-storage hierarchy
by combining memory and storage characteristics [ 59,71]. Basic
solutions include battery-backed DRAM and have been accessible
from diverse vendors over a signi ￿cant timeframe, representing an
established concept [ 30,39,50,63]. However, these solutions face
challenges due to limited scalability and potential data loss risks.
The reliance on batteries introduces concerns regarding power fail-
ures, leading to potential data corruption or loss if batteries deplete.
Moreover, the approach’s scalability is hampered by the need for
individual batteries for each module, impacting cost-e ￿ectiveness
and overall system performance.
Yet, in recent years new PMem technologies have emerged,
with 3D-Xpoint [ 19] being the main technology and Intel Optane
DCPMM [ 21,72] the prominent product on the market. These mod-
ern PMem technologies o ￿er byte-addressable memory in larger
capacities compared to DRAM while maintaining comparable ac-
cess times [ 27]. Moreover, as these technologies are non-volatile
in nature, they enable data retrieval even in instances of power
failures. Moreover, PMem o ￿ers two con ￿guration options based
on these characteristics: (1) It can be utilized as main memory ex-
pansion, providing additional volatile memory, and (2) it can serve
as a persistent memory pool that can be accessed by applications
via a PMem-aware ￿le system [ 71] or be managed and accessed
directly by applications [ 27]. To simplify and streamline PMem pro-
gramming and management, the Persistent Memory Development
Kit (PMDK) was created [ 64].
During recent years, PMem has gained signi ￿cant traction in
HPC applications [ 15,48,55,62], with two direct use cases of PMem
for scienti ￿c applications that require no (or minimal) changes to
applications. The ￿rst use-case involves PMem as memory expan-
sion to support the execution of large scienti ￿c problems [ 48]. The
second use case involves leveraging PMem as a fast storage device
accessed by a PMem-aware ￿le system (mainly based on the POSIX
API), primarily for application diagnostics and checkpoint restart(C/R) mechanisms [ 38], but also for increasing the performance and
inherent fault tolerance of scienti ￿c applications [ 14].
In addition to the direct use cases of PMem in scienti ￿c applica-
tions, various frameworks and algorithms were developed to access
and manage data structures on PMem [ 4]. Among these are pri-
mary methods that are built on top of the PMDK library [ 14,31].
For example, persistent memory object storage frameworks such
as MOSIQS [ 31] and the NVM-ESR recovery model for exact state
reconstruction of linear iterative solvers using PMem [ 14].
Nevertheless, as HPC workloads advance, computing units
evolve, and onboard processing elements increase, the demand
for heightened memory bandwidth becomes essential [ 32]. Exist-
ing PMem solutions demonstrate notable shortcomings in meeting
these requirements, showing limitations in scalability beyond a
certain threshold [ 15]. Speci ￿cally, PMem devices exhibit limited
bandwidth. For instance, the bandwidth of Optane DCPMM for
reading and writing is multi-factor lower than that of DRAM [ 27].
This, in part, is connected with the hybrid and in-between proper-
ties of a PMem module [ 18], as schematically described in Table 1 .
Adding to these challenges, a signi ￿cant limitation arises from
the physical attachment of most PMem devices, like Optane
DCPMM, to the CPU board through memory DIMMs. This con ￿g-
uration restricts the potential for memory expansion, as PMem
contends for DIMM slots alongside conventional DRAM cards,
presenting a bottleneck to achieving optimal memory con ￿gura-
tions [ 51,69]. The HPC community as a whole — both the super
and cloud computing [ 61] — recognizes the drawbacks associated
with tight integrating memory and compute resources, particularly
in relation to capacity, bandwidth, elasticity, and overall system
utilization [ 10,57]. PMem technologies that are tightly coupled
with the CPU inherit these limitations. Now, as prominent PMem
technologies are phased out (Optane DCPMM, for example, as an-
nounced in 2022 [ 20,22]), there is an active and prominent pursuit
for the adoption of novel memory solutions in particular, and a
strive to achieve more disaggregated computing in general [ 36].
1.3 Dissagregated Memory with CXL
The emergence of discrete memory nodes housing DRAM and
network interface controllers (NICs) is anticipated to revolutionize
conventional memory paradigms, facilitating distributed and shared
memory access and reshaping HPC landscapes [ 10]. This shift aligns
with the concept of disaggregation, where compute resources and
memory units are decoupled for optimized resource utilization,
scalability, and adaptability.
The concept of memory disaggregation has been facilitated re-
cently by the development of advanced interconnect technologies,
exempli ￿ed by Compute Express Link (CXL) [ 66]. CXL is an open
standard to support cache-coherent interconnect between a va-
riety of devices [ 66]. After its introduction in 2019, the standard
has evolved and continues to be enhanced. CXL 1.1 de ￿nes the
protocol for three major device types [ 66]: Accelerators with cache-
only (type 1), cache with attached memory (type 2), and memory
expansion (type 3). CXL 2.0 expands the speci ￿cation – among
other capabilities – to memory pools using CXL switches on a de-
vice level. CXL 3.0 introduces fabric capabilities and management,
2CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,
Property As a main memory extension As a direct access to persistent memory
Volatility Volatile in memory extension mode Non-volatile in direct access mode
Access Cache-coherent memory expansion Transactional byte-addressable object store
Capacity Higher than main memory volume Lower than storage volume
Cost Cheaper than the main memory More expansive than storage
Performance Several factors below main memory bandwidth High bandwidth compared to storage
Table 1: Properties of PMem modules, either as a memory extension ( Memory Mode) or as a direct access PMem ( App-Direct ).
improved memory sharing and pooling with dynamic capacity ca-
pability, enhanced coherency, and peer-to-peer communication.
Bandwidth-wise, CXL 1.1 and 2.0 employ PCIe 5.0, achieving 32
GT/s for transfers up to 64 GB/s in each direction via a 16-lane link.
On the other hand, CXL 3.0 utilizes PCIe 6.0, doubling the speed to
64 GT/s, supporting 128 GB/s bi-directional communication via an
x16 link.
Since the market of CXL memory modules is emerging, several
vendors have announced products using the CXL protocol. For
example, Samsung [ 52] and SK Hynix [ 53] introduce CXL DDR5
modules, AsteraLabs [ 3] announced a CXL memory accelerator,
and Montage Technology [ 68] will o ￿er a CXL memory expander
controller.
Leveraging CXL, memory nodes will be interconnected through
high-speed links, enabling adaptive memory provisioning to com-
pute nodes in real time [ 70]. The practice of intra-rack disaggrega-
tion holds the potential to e ￿ectively address the memory demands
of applications while concurrently ensuring an adequate supply
of e￿cient remote memory bandwidth [ 46,47].Figure 1 demon-
strates the expected phase change from the processor’s point of
view, from previous years’ DDR4+PMem memory access, equipped
with NVMe SSDs via the PCIe Gen4, to the upcoming future of
DDR5 local memory equipped with local or remote NVMe SSDs
and CXL memory for memory expansion or persistency over the
new generations of PCIe.
Native DDR4Native DDR4Native DDR4Native DDR4PCIe Gen4ProcessorPMemPMemPMemPMemNative DDR5Native DDR5Native DDR5Native DDR5PCIe Gen5ProcessorNative DDR5Native DDR5Native DDR5Native DDR5NVMe SSDsCXL memory as PMemNVMe SSDsTodayCXL Future
Figure 1: The migration from PMem as hardware to CXL
memory as PMem in future systems.
Nevertheless, while the concept of memory disaggregation with
technologies like CXL holds signi ￿cant promise, it is important to
acknowledge that there are still challenges and considerations that
need to be addressed [ 2,16]; challenges and considerations that
resemble the ones of persistent memory integration in HPC [ 5].
For example, software and programming models need to evolve
to take advantage of disaggregated memory fully; Applications
and algorithms must be designed or adapted to work seamlessly
across distributed memory nodes; and e ￿cient data placement
and movement strategies are crucial to minimize the impact ofnetwork latencies and ensure that data-intensive workloads can
e￿ectively utilize CXL-based disaggregated memory resources, es-
pecially when cache-coherence or direct access is enabled. Notwith-
standing, when comparing CXL memory aspects to the ones of
PMem as non-volatile RAM (NVRAM), in general, it can be observed
(Table 2 ) that from the disaggregated HPC usage perspective, there
should be a prevalence to CXL over NVRAM considering band-
width, data transfer, and scalability, but also considering memory
coherency, integration, pooling and sharing.
1.4 Contribution
In this work, based on actual physical experiments with multi-
NUMA nodes and multi-core high-performance SOTA hardware
(subsection 2.1 ) and CXL-remote memory ( subsection 2.2 ), we claim
that it is not only possible to exemplify most persistent memory
modules characteristics (as described in Table 1 ) with CXL memory
fully but also that in terms of performance, we can achieve much
better bandwidth than previously published Optane DCPMM ones
(such in [ 26], which, for a single Optane DCPMM, discovers that
its max read bandwidth is 6.6 GB/s, whereas its max write band-
width is 2.3 GB/s). In fact, we show ( Figure 4 ) that by approaching
our CXL-DDR4 memory module – much cheaper than DDR5 – we
achieve comparable results to the local DDR4 module and exhibit
performance degradation of only about 60% in bandwidth in com-
parison to local DDR5 module access (noting that DDR4 has about
50% bandwidth of DDR5). Our tests were made in multiple con ￿g-
urations ( subsection 3.2 ) in relation to the memory distance from
the working threads using the well-known STREAM benchmark
(subsection 3.1 ).
In order to demonstrate the non-volatile properties of the mem-
ory as PMem, the CXL memory was located outside of the node, in
an FPGA device ( subsection 2.2 ), potentially backed by battery, like
previous battery-backed DIMMs. As many nodes can approach the
device, the battery-backed consideration is no longer considered
by us as a major overhead since it will be applied only once for the
memory modules and not in each compute node.
Moreover, besides the cache-coherent performance benchmarks
with STREAM [ 40,41], we retested the memory bandwidth in an
equivalent of the App-Direct approach with a modi ￿ed STREAM ap-
plication, named STREAM-PMem [ 12] when all of the main arrays
were allocated as a PMDK’s pmemobj and manipulated accordingly
[65].pmemobj provides an assurance that the condition of objects
will remain internally consistent regardless of when the program
concludes. Additionally, it o ￿ers a transaction function that can
encompass various modi ￿cations made to persistent objects. This
function ensures that either all of the modi ￿cations are successfully
applied or none of them take e ￿ect.
3,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren
Aspect CXL Memory NVRAM
Bandwidth &
Data TransferSigni ￿cantly higher bandwidth enabling fast data trans-
fers between processors and memory devices.Non-volatile storage with potential data transfer rate
limitations due to underlying interface and technology.
Memory
CoherencyProvides memory-coherent links, ensuring consistent
data across di ￿erent memory tiers.Requires additional mechanisms for memory coherency,
except with local RAM, when integrated with other
memory technologies.
Heterogeneous
Memory
IntegrationAllows seamless integration of various memory tech-
nologies within a uni ￿ed architecture.E￿ective for extending memory capacity, but inte-
gration may require additional considerations due to
unique characteristics.
Memory Pooling
and SharingFacilitates memory pooling and sharing, enabling e ￿-
cient resource utilization and dynamic allocation based
on workload requirements.Extends memory capacity, but inherent ￿exibility in
memory sharing and pooling may be limited.
Industry
StandardizationOpen industry standard supported by major technology
players, ensuring compatibility, interoperability, and
broader adoption.Solutions may vary, potentially leading to compatibility
challenges and limited integration options.
Scalability Architecture designed for scalability with multiple lanes
and protocols, catering to evolving data center needs.Scalability may be constrained by underlying technol-
ogy characteristics, such as DIMM count and RAM/N-
VRAM tradeo ￿.
Relevance to HPC Higher bandwidth, memory coherency, and memory
pooling capabilities enhance HPC workload perfor-
mance. Standardization compatibility in heterogeneous
environments and scalability cater to evolving demands.O￿ers non-volatility but is constrained by limitations in
bandwidth, coherency management, and scalability, af-
fecting its applicability to complex HPC memory needs.
Table 2: General comparison between common aspects of CXL memory and NVRAM for disaggregated HPC.
We stress that as our CXL memory module is located outside
of the node and can be backed by a battery, the ability to transac-
tionally and directly access the memory, exactly as previously done
with Optane DCPMM, while achieving even better performances,
is a key to our practical approach, which consider CXL memory as
a persistent memory for the future of disaggregated HPC.
Finally, we open-sourced the entire benchmarking methodology
as an easy-to-use and automated tool named STREAMer for future
CXL memory device evaluations for HPC purposes.
2 PHYSICAL EXPERIMENTAL SETUP
2.1 HPC hardware
Our HPC hardware experimental environment is based on 2 setups:
(1)Node equipped with two Intel 4C⌘generation Xeon (Sapphire
Rapids) processors with a base frequency of 2.1GHz and 48 cores
each, plus Hyper-Threading. BIOS was updated to support only 10
cores per socket. Each processor has one memory DIMM (64GB
DDR5 4800MHz DIMM). The system is equipped with a CXL pro-
totype device, implemented as DDR4 memory on a PCIe-attached
FPGA (see Figure 2).
(2)Node equipped with two Intel Xeon Gold 5215 processors with a
base frequency of 2.5GHz and 10 cores each, plus Hyper-Threading.
Each processor has total 96GB DRAM in 6 channels, 16GB DDR4
2666MHz DIMM per channel. (see Figure 3).
2.2 CXL prototype
We provide an in-depth overview of our CXL prototype’s imple-
mentation on an FPGA card [ 25].Figure 2 andFigure 4 grant a more
detailed view into the implementation of our CXL memory pool on
the FPGA card (while Figure 3 show the reference system, withoutany CXL attachment, with DDR4 main memory). The prototype
aims to harness the capabilities of the R-Tile Intel FPGA IP for CXL,
encompassing critical functionalities for CXL link establishment
and transaction layer management. This comprehensive solution
facilitates the construction of FPGA-based CXL 1.1/2.0 compliant
endpoint designs, including Type 1, Type 2, and Type 3 con ￿gura-
tions. It’s built upon a previously proven prototype, with necessary
slight modi ￿cations for PMem activity [ 34].
The architecture of our CXL implementation revolves around
a synergistic pairing of protocol Soft IP within the FPGA main
fabric die and the Hard IP counterpart, the R-Tile. This cohesive
arrangement ensures e ￿ective management of CXL link functions,
which are pivotal for seamless communication. Speci ￿cally, the
R-Tile interfaces with a CPU host via a PCIe Gen5x16 connection,
delivering a theoretical bandwidth of up to 64GB/s. As a key facet
of our implementation, the FPGA device is duly enumerated as a
CXL endpoint within the host system.
Complementing this link management, the Soft IP assumes the
mantle of transaction layer functions, vital for the successful execu-
tion of di ￿erent CXL endpoint types. For Type 3 con ￿gurations, the
CXL.mem transaction layer adeptly handles incoming CXL.mem
requests originating from the CPU host. It orchestrates the gener-
ation of host-managed device memory (HDM) requests directed
toward an HDM subsystem. Simultaneously, the CXL.io transaction
layer undertakes the responsibility of processing CXL.io requests.
These requests encompass both con ￿guration and memory space
inquiries initiated from the CPU host, seamlessly forwarding them
to their designated control and status registers. A noteworthy aug-
mentation is the User Streaming Interface, o ￿ering a conduit for
custom CXL.io features that can be seamlessly integrated into the
user design.
4CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,
64GB 4800 MHzMemory
UPIDDR5Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)CXL-attached memory
MemoryHostProcessorSocket0HostProcessorSocket164GB 4800 MHznumactl --membind=0/mnt/pmem0 numactl --membind=1/mnt/pmem1 numactl --membind=2/mnt/pmem2 PCIe5x16cores 0-9cores 10-19CXLDDR5
Figure 2: Setup #1 with DDR5 on-node memory and DDR4
CXL-attached memory.
16GB 2933 MHzDDR4MemoryMemoryHost Processor Socket0Host Processor Socket116GB 2933 MHznumactl --membind=0numactl --membind=1cores 0-9cores 10-19UPIDDR4Figure 3: Setup #2 with DDR4 on-node memory.
 Figure 4: Overview of CXL IP for Intel ®Agilex ®7 I-Series
FPGA [ 24], demonstrated in Figure 2 (setup #1).
Integral to our FPGA card is the inclusion of two onboard DDR4
memory modules, each boasting a capacity of 8GB and operating at
a clock frequency of 1333 MHz. These modules are accessible from
the host system as conventional memory resources. It is imperative
to highlight a distinctive attribute of this prototype con ￿guration:
the CXL link facilitates access to an identical memory volume. In
essence, this means that the same far memory segment can be made
available to two distinct NUMA nodes, eliminating any concerns
of address overlap. However, due to the absence of a uni ￿ed cache-
coherent domain, the onus of maintaining coherency between the
two NUMA nodes assigned to the shared far memory rests with
the applications leveraging this con ￿guration.
Notably, the bandwidth attainable from this prototype con ￿gura-
tion is subject to current implementation constraints and does not
re￿ect an intrinsic limitation of the CXL standard. Potential avenues
for enhancing bandwidth include several considerations. First, tran-
sitioning to a higher-speed FPGA, supporting DDR4 speeds of 3200
Mbps or even embracing the capabilities of DDR5 at 5600 Mbps,
could appreciably enhance throughput. Additionally, scaling the
resources allocated to the CXL IP by increasing the number of slices
is a viable strategy. Furthermore, expanding the FPGA’s capacity to
accommodate multiple independent DDR channels, possibly transi-
tioning from one channel to four, holds promise in augmenting the
prototype’s bandwidth potential.
In our discussion, the fact that the CXL memory device is DDR4
and not DDR5 is key, as usually, PMem is slower and cheaper than
the main memory. By using DDR4 CXL memory and not DDR5,
while main memory is DDR5, we keep on this important relation.
3 PERFORMANCE EVALUATION
3.1 STREAM and STREAM-PMem Benchmarks
The STREAM benchmark [ 42] is a synthetic benchmark program
that measures sustainable memory bandwidth for simple vectorkernels in high-performance computers. STREAM was developed
as a proxy for the basic computational kernels in scienti ￿c com-
putations [ 43] and includes Copy, Scale, Sum, and Triad kernels.
STREAM has a dedicated version to benchmark PMem modules by
allocating and accessing PMem via PMDK (STREAM-PMem [ 12]).
The excerpt presented in Listing 1 constitutes a portion of the
initial codebase that has since been extracted from the current
version of the code.
Listing 1: Original STREAM benchmark code at line 175-181.
1#ifndef STREAM_TYPE
2#define STREAM_TYPE double
3#endif
4static STREAM_TYPE a[STREAM_ARRAY_SIZE+OFFSET],
5 b[STREAM_ARRAY_SIZE+OFFSET],
6 c[STREAM_ARRAY_SIZE+OFFSET];
The content represented in Listing 1 has been substituted in
STREAM-PMem [ 12] with the code demonstrated in Listing 2 . The
code commences by accessing the memory pool. Furthermore, a
function named initiate is employed to initialize the three arrays.
Following this initialization, the code proceeds to execute the re-
maining segments of the STREAM benchmark code, mirroring the
structure of the original STREAM benchmark code.
Listing 2: Code that has replaced original code.
1PMEMobjpool *pop;
2POBJ_LAYOUT_BEGIN(array);
3POBJ_LAYOUT_TOID(array, double );
4POBJ_LAYOUT_END(array); //Declearing the arrays
5TOID( double ) a, b, c;
6void initiate() { //Initiating the arrays .
7 POBJ_ALLOC(pop, &a, double ,
(STREAM_ARRAY_SIZE+OFFSET)*sizeof(STREAM_TYPE),
NULL, NULL); //Same for band c.
8int main(){
5,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren
9 const char path[] =  .../ pool .obj ;
10 pop = pmemobj_create(path, LAYOUT_NAME, 10737418240,
0666);
11 if(pop == NULL)
12 pop = pmemobj_open(path, LAYOUT_NAME);
13 if(pop == NULL) {
14 perror(path);
15 exit(1); }
16 initiate();
17 //The rest ofthe STREAM benchmark after this .
18 }
In this work, we employ STREAM in those two versions to show-
case the shift from PMem to CXL. Throughout this demonstration,
we illustrate how programs designed for PMem can seamlessly
operate on CXL-enabled devices. Furthermore, we provide perfor-
mance assessments to anticipate the impact of CXL on performance
in relation to local RAM (DDR4 and DDR5) and local PMem-like
devices (emulation of remote sockets either for memory expansion
or as a direct access device, as done in [ 6,13]).
In contrast to previous research that primarily emphasizes
demonstrating the use of CXL memory for in-memory database
queries or ￿le system operations [ 1,34], STREAM memory access
involves accessing and manipulating large arrays, making it par-
ticularly applicable and signi ￿cant for scienti ￿c computations in
HPC systems. Moreover, STREAM is implemented with OpenMP
threads, which is the common shared-memory paradigm in scien-
ti￿c computing for parallelism [ 9].
3.2 Test Con ￿gurations
The methodology of this work is to employ STREAM and STREAM-
PMem in various CPU and memory con ￿gurations, taking into
account the availability of DRAM and CXL memory available on
the HPC setups, as will be described next. The results presented
inFigure 5 ,Figure 6 ,Figure 7 ,Figure 8 refer to STREAM execu-
tions with 100M array elements for Scale, Add, Copy, and Triad
operations correspondingly. For each STREAM method, the results
of our tests are presented in 2 classes (and a total of 5 groups), di-
vided conceptually for unique comparisons. We sub-divide those 5
groups into two classes. The ￿rst class (Class 1, (a)-(c)) refers to the
equivalent of the App-Direct mode in PMem in which we directly
access the local or remote memory (either in the alternative socket
or in the CXL memory), and the second class (Class 2, (a)-(b)) refers
to the Memory Mode in PMem, in which we increase the available
memory using other CC-NUMA nodes:
Class 1 — App-Direct :
(a)Local memory access as PMem: Con￿gurations within this
group involve accessing local memory (on-socket memory) in App-
Direct mode (thus benchmarking STREAM-PMem).
(b)Remote memory access as PMem: Con￿gurations within
this group involve computing cores on a single socket that access
remote memory in App-Direct mode (thus benchmarking STREAM-
PMem). The term "remote memory" in this context encompasses
both CXL-attached memory and on-node memory accessed from
the alternative CPU socket (i.e., memory accessed through the UPI).
(c)Remote memory as PMem (thread a ￿nity): Con￿gurations
within this group involve computing cores in both sockets thataccess remote memory in App-Direct mode (thus benchmarking
STREAM-PMem) using two distinct thread a ￿nity methods: close
andspread . The close method populates an entire socket ￿rst and
then adds cores from the second socket. The spread method, on the
opposite, adds cores alternately from both sockets.
Class 2 — Memory Mode :
(a)Remote CC-NUMA: Con￿gurations within this group involve
computing cores on a single socket that access remote memory as
CC-NUMA.
(b)Remote CC-NUMA (all cores): Con￿gurations within this
group involve cores on both CPU sockets accessing remote memory
as CC-NUMA. This includes con ￿gurations where both sockets
operate and access memory on one of them since these workloads
include remote accesses.
For better clarity, the data ￿ow for each test con ￿guration is
demonstrated in Figure 9. Each row in Figure 9contains the data
￿ow examinations of the test groups of the two classes. Thus, in each
of our test groups, for each of the STREAM operations (Figures 5,6,
7,8), the way to understand each trend, and its correspondence to
the relevant data ￿ow, is given in the trend itself by a combination
of three: symbol, color and memory annotation. The symbol is used
to distinguish between accessing on-node DDR4 ( s), on-node DDR5
(l) or CXL-attached DDR4 ( ⇥). The color implies the active com-
pute cores —- either in socket0, socket1, or both. The annotations
pmem #{0,1,2}ornuma #{0,1,2}accompanying each trend provide
an explanation of the accesses memory location: 0 for socket0; 1 for
socket1; and 2 for CXL memory. numa signi ￿es STREAM accessing
memory as NUMA memory expansion, while pmem represents
STREAM-PMem accessing memory using PMDK.
4 RESULTS AND ANALYSIS
Figure 5 ,Figure 6 ,Figure 7 andFigure 8 present STREAM results
for the Scale, Add, Copy, and Triad operations correspondingly,
and for the test con ￿gurations de ￿ned in subsection 3.2 as will
be described next. Figure 5a ,Figure 6a ,Figure 7a andFigure 8a
through Figure 5e ,Figure 6e ,Figure 7e andFigure 8e present results
for Class 1. (a)group though Class 2. (b)group correspondingly.
The results explain the costs associated with memory access
across varied con ￿gurations distinguished by parameters such as
memory type (on-node or CXL-attached), memory placement (local
to the socket, on the alternate CPU socket, or the CXL-attached
memory), access mode ( App-Direct vs.Memory Mode ), and thread
a￿nity (Close or Spread).
Next, we will examine and analyze the achieved results in rela-
tion to the con ￿guration classes and groups presented in subsec-
tion 3.2 :
Class 1 — App-Direct :
(a)Local memory access as PMem: It is possible to observe that
among all of the STREAM actions, the App-Direct access using
PMDK to the local DDR5 memory is saturated around 20-22 GB/s.
This test is a reference for the remote access presented in the follow-
ing group, either to a nearby remote socket or to the CXL memory
(with PMDK).
(b)Remote memory access as PMem: App-Direct access to the
emulated remote PMem (DDR5 on the alternate socket) results
in a decrease of 30% ( ⇠15 GB/s) of performance on average for
6CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,
(a) Class 1.a: Local memory access as PMem
(b) Class 1.b: Remote memory access as PMem
(c) Class 1.c: Remote memory as PMem (thread
a￿nity)
(d) Class 2.a: Remote CC-NUMA
(e) Class 2.b: Remote CC-NUMA (all cores)
Figure 5: SCALE — Various STREAM test con ￿gurations. Refer to Section 3.2for de ￿nition of test groups 1. (a), 1.(b), 1.(c), 2.(a),
2.(b)and legend clari ￿cations.
(a) Class 1.a: Local memory access as PMem
(b) Class 1.b: Remote memory access as PMem
(c) Class 1.c: Remote memory as PMem (thread
a￿nity)
(d) Class 2.a: Remote CC-NUMA
(e) Class 2.b: Remote CC-NUMA (all cores)
Figure 6: ADD — Various STREAM test con ￿gurations. Refer to Section 3.2for de ￿nition of test groups 1. (a), 1.(b), 1.(c), 2.(a),
2.(b)and legend clari ￿cations.
7,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren
(a) Class 1.a: Local memory access as PMem
(b) Class 1.b: Remote memory access as PMem
(c) Class 1.c: Remote memory as PMem (thread
a￿nity)
(d) Class 2.a: Remote CC-NUMA
(e) Class 2.b: Remote CC-NUMA (all cores)
Figure 7: COPY — Various STREAM test con ￿gurations. Refer to Section 3.2for de ￿nition of test groups 1. (a), 1.(b), 1.(c), 2.(a),
2.(b)and legend clari ￿cations.
(a) Class 1.a: Local memory access as PMem
(b) Class 1.b: Remote memory access as PMem
(c) Class 1.c: Remote memory as PMem (thread
a￿nity)
(d) Class 2.a: Remote CC-NUMA
(e) Class 2.b: Remote CC-NUMA (all cores)
Figure 8: TRIAD — Various STREAM test con ￿gurations. Refer to Section 3.2for de ￿nition of test groups 1. (a), 1.(b), 1.(c), 2.(a),
2.(b)and legend clari ￿cations.
8CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,(Class 1.a)
64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5Host Processor Socket0Host Processor Socket1/mnt/pmem0 
(Class 1.b)
64GB 4800 MHzUPIDDR5MemoryCXL-attached memory
Memory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)/mnt/pmem2 PCIe5x16CXL
64GB 4800 MHzUPIDDR5MemoryCXL-attached memory
Memory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)/mnt/pmem2 PCIe5x16CXL
64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHz/mnt/pmem1 cores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory
(Class 1.c)
64GB 4800 MHzUPIDDR5MemoryCXL-attached memory
Memory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)PCIe5x16CXL/mnt/pmem2 
64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5Host Processor Socket0Host Processor Socket1CXL-a�ached memory
/mnt/pmem0 
64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory
/mnt/pmem1 
(Class 2.a)
64GB 4800 MHzUPIDDR5MemoryCXL-attached memory
Memory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)PCIe5x16CXLnumactl --membind=2
64GB 4800 MHzUPIDDR5MemoryCXL-attached memory
Memory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)PCIe5x16CXLnumactl --membind=2
64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory
numactl --membind=116GB 2933 MHzDDR4MemoryMemoryHostProcessorSocket0HostProcessorSocket116GB 2933 MHznumactl --membind=1cores 0-9cores 10-19UPIDDR4
(Class 2.b)
64GB 4800 MHzUPIDDR5MemoryCXL-attached memory
Memory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)PCIe5x16CXLnumactl --membind=2
64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory16GB 2933 MHzDDR4MemoryMemoryHostProcessorSocket0HostProcessorSocket116GB 2933 MHzcores 0-9cores 10-19UPIDDR4
numactl --membind=0numactl --membind=0
64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory
numactl --membind=116GB 2933 MHzDDR4MemoryMemoryHostProcessorSocket0HostProcessorSocket116GB 2933 MHznumactl --membind=1cores 0-9cores 10-19UPIDDR4Figure 9: Data ￿ow demonstrations for the two classes ( App Direct andMemory Mode ). Each test group is evaluated in corre-
sponding to sub ￿gures of Figure 5 ,Figure 6 ,Figure 7 ,Figure 8 . Each row corresponds to a test group.9,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren
all STREAM operations, in comparison to local App-Direct access.
In the case of App-Direct access to remote CXL memory (DDR4),
we experience 50% decrease in performance in comparison to the
emulated PMem on DDR5. However, we note that DDR5 inherently
has about 50% higher bandwidth than DDR4, meaning that the
rest of the overhead – about 2-3 GB/s loss in bandwidth – can be
attributed to the CXL fabric.
(c)Remote memory as PMem (thread a ￿nity): As observed in
previous groups, local App-Direct accesses result in higher band-
width than remote accesses. In the case of close thread a ￿nity, after
populating the entire socket, adding remote accesses of compute
cores to the workload negatively impacts the bandwidth, whereas
adding local accesses contributes positively. With spread a￿nity,
the performance demonstrates an average between local and remote
accesses due to the inclusion of alternating accesses. Eventually,
when both sockets are operating with the entire core count, the
results converge for on-node DDR5 and remote CXL memory, sep-
arately. Notably, accessing remote CXL memory (DDR4) leads to a
50% observed degradation compared to on-node DDR5.
Class 2 — Memory Mode :
(a)Remote CC-NUMA: Evaluating DDR4 CC-NUMA, whether
on the remote socket or CXL-attached memory, yields comparable
￿gures (with average gaps of up to 2-5 GB/s). However, following a
small number of threads, a slight advantage is observed for access-
ing CXL memory. This advantage can be attributed to the larger
caches in Setup #1 utilizing CXL (Shappire Rapids), as opposed to
Setup #2 (Xeon Gold) with on-node DDR4 ( subsection 2.1 ). This
indicates that the CXL fabric overhead is constrained by the per-
formance reduction when transitioning back from Sapphire Rapids
to Xeon Gold. Moreover, the gap between the CC-NUMA to DDR5
and DDR4 (on-node or CXL-attached) stands on a factor of two, as
already observed in 1. (b)and 1. (c). In addition, in comparison to
the results of the App-Direct tests in 1. (b), it is observed that PMDK
overheads over CC-NUMA are 10%-15% (in all STREAM methods).
(b)Remote CC-NUMA (all cores): The observed gap between
DDR4 and DDR5 repeats here. Moreover, accessing on-node DDR4
using all cores converges to the same results as accessing DDR4
CXL memory.
To conclude, the analysis reveals that direct access to local DDR5
memory using PMDK saturates at 20-22 GB/s, while direct remote
access to emulated PMem and CXL memory results in 30% and
50% performance decreases, respectively, with about 2-3 GB/s band-
width loss attributed to CXL fabric. In terms of memory expansion,
accessing remote DDR4 CC-NUMA and DDR4 CXL-attached mem-
ory exhibit similar performance gaps of 2-3 GB/s, while DDR5
CC-NUMA maintains an advantage gap of a factor of 1.5 compared
to DDR4, and on-node DDR4 access converges with o ￿-node DDR4
access under varying thread a ￿nities.
5 CONCLUSIONS
In this study, we embarked on a comprehensive exploration of the
potential of CXL memory as a promising candidate for serving as a
persistent memory solution in the context of disaggregated HPC
systems. By conducting physical experiments on state-of-the-art
multi-NUMA nodes equipped with high-performance processors
and CXL-attached memory prototypes, we have provided empiricalevidence that supports the feasibility of using CXL memory to
exhibit all the characteristics of persistent memory modules while
achieving impressive performance metrics.
Our ￿ndings demonstrate that CXL memory has the capabil-
ity to outperform previously published benchmarks for Optane
DCPMM in terms of bandwidth. Speci ￿cally, by employing a CXL-
DDR4 memory module, which is a cost-e ￿ective alternative to
DDR5 memory, we achieved bandwidth results comparable to lo-
cal DDR4 memory con ￿gurations, with only a marginal decrease
of around 50% when compared to local DDR5 memory con ￿gura-
tions. These results, attained across various memory distances from
the working threads, were assessed through the well-established
STREAM benchmark underscoring the reliability and versatility of
CXL memory in the HPC landscape.
The shift from PMem to CXL was not only demonstrated through
performance evaluations but was also highlighted through the
modi ￿cation of the STREAM application into STREAM-PMem. We
showcased the seamless transition of programming models from
PMem to CXL, leveraging the PMDK’s pmemobj to ensure trans-
actional integrity and consistency of operations on persistent ob-
jects. Furthermore, the ability to access CXL memory directly and
transactionally, akin to Optane DCPMM, was underscored as a key
advantage for practical implementation.
Our study extends beyond theoretical considerations by imple-
menting a practical CXL prototype on an FPGA card. This prototype
embodies CXL 1.1/2.0 compliant endpoint designs, demonstrating
e￿ective link establishment and transaction layer management
through a combination of Soft and Hard IP components. The proto-
type’s performance, while constrained by current implementation
limitations, stands as a testament to the extensibility of this solu-
tion and o ￿ers a blueprint for potential enhancements, including
higher-speed FPGAs and increased resources.
6 FUTURE WORK
While this study provides valuable insights into the feasibility and
potential bene ￿ts of using CXL-enabled memory in HPC systems,
several avenues for future research and exploration remain:
•Scalability and Performance Optimization : Further investiga-
tion is warranted to explore the scalability of CXL-enabled memory
in larger HPC clusters, with more than one node accessing the CXL
memory. Optimizing communication protocols and memory access
patterns can help maximize memory disaggregation bene ￿ts.
•Hybrid Architectures : Combining di ￿erent memory technolo-
gies, such as DDR, PMem, and CXL memory, in a hybrid mem-
ory architecture could o ￿er a balanced solution that leverages the
strengths of each technology. Also, the CXL memory could also use
DDR5 and even Optane DCPMM, and as such, revisiting the results
with those CXL memories would be bene ￿cial.
•Real-World Applications : Extending the evaluation to real-
world HPC applications beyond benchmarks can provide a clearer
understanding of how CXL memory performs in practical scenarios.
•Fault Tolerance and Reliability : Investigating fault tolerance
mechanisms and data reliability in the context of CXL-enabled
memory is crucial, especially in large-scale distributed environ-
ments. Speci ￿cally, code systems that have previously been built
upon PMDK and Optane DCPMM presence in the HPC system.
10CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,
ACKNOWLEDGMENTS
This work was supported by Pazy grant 226/20, the Lynn and
William Frankel Center for Computer Science, and Intel Corpora-
tion (oneAPI Center of Excellence program). Computational support
was provided by the NegevHPC project [ 54] and Intel Developer
Cloud [ 23]. The authors would like to thank Gabi Dadush, Israel
Hen, and Emil Malka for their hardware support on NegevHPC.
The authors also want to thank Jay Mahalingam and Guy Tamir of
Intel for their great help in forming this collaboration.
REFERENCES
[1]Minseon Ahn, Andrew Chang, Donghun Lee, Jongmin Gim, Jungmin Kim, Jaemin
Jung, Oliver Rebholz, Vincent Pham, Krishna Malladi, and Yang Seok Ki. 2022.
Enabling CXL memory expansion for in-memory database management systems.
InProceedings of the 18th International Workshop on Data Management on New
Hardware . 1–5.
[2]Hasan Al Maruf and Mosharaf Chowdhury. 2023. Memory Disaggregation: Open
Challenges in the Era of CXL. In Workshop on HotTopics in System Infrastructure ,
Vol. 18.
[3]AsteraLabs. 2022. CXL Memory Accelerators .https://www.asteralabs.com/
products/cxl-memory-platform/
[4]Alexandro Baldassin, Joao Barreto, Daniel Castro, and Paolo Romano. 2021. Per-
sistent memory: A survey of programming support and implementations. ACM
Computing Surveys (CSUR) 54, 7 (2021), 1–37.
[5]Lawrence Benson, Marcel Weisgut, and Tilmann Rabl. 2023. What We Can Learn
from Persistent Memory for CXL. BTW 2023 (2023).
[6]Lars Bergstrom. 2011. Measuring NUMA e ￿ects with the STREAM benchmark.
arXiv preprint arXiv:1103.3225 (2011).
[7]David E Bernholdt, Swen Boehm, George Bosilca, Manjunath Gorentla Venkata,
Ryan E Grant, Thomas Naughton, Howard P Pritchard, Martin Schulz, and Geof-
froy R Vallee. 2020. A survey of MPI usage in the US exascale computing project.
Concurrency and Computation: Practice and Experience 32, 3 (2020), e4851.
[8]Andrés Bustos, Antonio Juan Rubio-Montero, Roberto Méndez, Sergio Rivera,
Francisco González, Xandra Campo, Hernán Asorey, and Rafael Mayo-García.
2023. Response of HPC hardware to neutron radiation at the dawn of exascale.
The Journal of Supercomputing (2023), 1–22.
[9]Leonardo Dagum and Ramesh Menon. 1998. OpenMP: An industry-standard API
for shared-memory programming. Computing in Science & Engineering 1 (1998),
46–55.
[10] Nan Ding, Pieter Maris, Hai Ah Nam, Taylor Groves, Muaaz Gul Awan, LeAnn
Lindsey, Christopher Daley, Oguz Selvitopi, Leonid Oliker, and Nicholas Wright.
2023. Evaluating the Potential of Disaggregated Memory Systems for HPC
applications. arXiv preprint arXiv:2306.04014 (2023).
[11] Thomas M Evans, Andrew Siegel, Erik W Draeger, Jack Deslippe, Marianne M
Francois, Timothy C Germann, William E Hart, and Daniel F Martin. 2022. A
survey of software implementations used by application codes in the Exascale
Computing Project. The International Journal of High Performance Computing
Applications 36, 1 (2022), 5–12.
[12] Svein Gunnar Fagerheim. 2021. Benchmarking Persistent Memory with Respect to
Performance and Programmability . Master’s thesis.
[13] Clément Foyer, Brice Goglin, and Andrès Rubio Proaño. 2023. A survey of soft-
ware techniques to emulate heterogeneous memory systems in high-performance
computing. Parallel Comput. (2023), 103023.
[14] Yehonatan Fridman, Yaniv Snir, Harel Levin, Danny Hendler, Hagit Attiya, and
Gal Oren. 2022. Recovery of Distributed Iterative Solvers for Linear Systems
Using Non-Volatile RAM. In 2022 IEEE/ACM 12th Workshop on Fault Tolerance for
HPC at eXtreme Scale (FTXS) . IEEE, 11–23.
[15] Yehonatan Fridman, Yaniv Snir, Matan Rusanovsky, K ￿r Zvi, Harel Levin, Danny
Hendler, Hagit Attiya, and Gal Oren. 2021. Assessing the use cases of persis-
tent memory in high-performance scienti ￿c computing. In 2021 IEEE/ACM 11th
Workshop on Fault Tolerance for HPC at eXtreme Scale (FTXS) . IEEE, 11–20.
[16] Andreas Geyer, Daniel Ritter, Dong Hun Lee, Minseon Ahn, Johannes Pietrzyk,
Alexander Krause, Dirk Habich, and Wolfgang Lehner. 2023. Working with
Disaggregated Systems. What are the Challenges and Opportunities of RDMA
and CXL? BTW 2023 (2023).
[17] William Gropp. 2012. MPI 3 and beyond: why MPI is successful and what
challenges it faces. In European MPI Users’ Group Meeting . Springer, 1–9.
[18] Shashank Gugnani, Arjun Kashyap, and Xiaoyi Lu. 2020. Understanding the
idiosyncrasies of real persistent memory. Proceedings of the VLDB Endowment
14, 4 (2020), 626–639.
[19] Frank T Hady, Annie Foong, Bryan Veal, and Dan Williams. 2017. Platform
storage performance with 3D XPoint technology. Proc. IEEE 105, 9 (2017), 1822–
1833.[20] Jim Handy and Tom Coughlin. 2023. Optane’s Dead: Now What? Computer 56, 3
(2023), 125–130.
[21] Takahiro Hirofuchi and Ryousei Takano. 2020. A prompt report on the perfor-
mance of intel optane dc persistent memory module. IEICE TRANSACTIONS on
Information and Systems 103, 5 (2020), 1168–1172.
[22] Intel. 2022. Migration from Direct-Attached Intel Optane Persistent Memory
to CXL-Attached Memory. https://www.intel.com/content/dam/www/central-
libraries/us/en/documents/2022-11/optane-pmem-to-cxl-tech-brief.pdf . [On-
line].
[23] Intel. 2023. Intel Developer Cloud. https://www.intel.com/content/www/us/en/
developer/tools/devcloud/overview.html . [Online].
[24] Intel. 2023. Intel ®FPGA Compute Express Link (CXL) IP. https:
//www.intel.com/content/www/us/en/products/details/fpga/intellectual-
property/interface-protocols/cxl-ip.html . [Online].
[25] Intel. 2023. Intel ®FPGA Compute Express Link (CXL) IP. https:
//www.intel.com/content/www/us/en/products/details/fpga/intellectual-
property/interface-protocols/cxl-ip.html . [Online].
[26] Joseph Izraelevitz, Hammurabi Mendes, and Michael L Scott. 2016. Lineariz-
ability of persistent memory objects under a full-system-crash failure model. In
International Symposium on Distributed Computing . Springer, 313–327.
[27] Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amirsaman
Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R Dulloor, et al .
2019. Basic performance measurements of the intel optane DC persistent memory
module. arXiv preprint arXiv:1903.05714 (2019).
[28] Hongshin Jun, Jinhee Cho, Kangseol Lee, Ho-Young Son, Kwiwook Kim, Hanho
Jin, and Keith Kim. 2017. Hbm (high bandwidth memory) dram technology and
architecture. In 2017 IEEE International Memory Workshop (IMW) . IEEE, 1–4.
[29] Hongshin Jun, Sangkyun Nam, Hanho Jin, Jong-Chern Lee, Yong Jae Park, and
Jae Jin Lee. 2016. High-bandwidth memory (HBM) test challenges and solutions.
IEEE Design & Test 34, 1 (2016), 16–25.
[30] Rajat Kateja, Anirudh Badam, Sriram Govindan, Bikash Sharma, and Greg Ganger.
2017. Viyojit: Decoupling battery and DRAM capacities for battery-backed DRAM.
ACM SIGARCH Computer Architecture News 45, 2 (2017), 613–626.
[31] Awais Khan, Hyogi Sim, Sudharshan S Vazhkudai, Jinsuk Ma, Myeong-Hoon
Oh, and Youngjae Kim. 2020. Persistent memory object storage and indexing
for scienti ￿c computing. In 2020 IEEE/ACM Workshop on Memory Centric High
Performance Computing (MCHPC) . IEEE, 1–9.
[32] Peter M Kogge and William J Dally. 2022. Frontier vs the Exascale Report: Why
so long? and Are We Really There Yet?. In 2022 IEEE/ACM International Workshop
on Performance Modeling, Benchmarking and Simulation of High Performance
Computer Systems (PMBS) . IEEE, 26–35.
[33] Benjamin C Lee, Engin Ipek, Onur Mutlu, and Doug Burger. 2010. Phase change
memory architecture and the quest for scalability. Commun. ACM 53, 7 (2010),
99–106.
[34] Donghun Lee, Thomas Willhalm, Minseon Ahn, Suprasad Mutalik Desai, Daniel
Booss, Navneet Singh, Daniel Ritter, Jungmin Kim, and Oliver Rebholz. 2023.
Elastic Use of Far Memory for In-Memory Database Management Systems. In
Proceedings of the 19th International Workshop on Data Management on New
Hardware . 35–43.
[35] Jiuxing Liu, Jiesheng Wu, Sushmitha P Kini, Pete Wycko ￿, and Dhabaleswar K
Panda. 2003. High performance RDMA-based MPI implementation over In ￿ni-
Band. In Proceedings of the 17th annual international conference on Supercomputing .
295–304.
[36] Ming Liu. 2023. Fabric-Centric Computing. In Proceedings of the 19th Workshop
on Hot Topics in Operating Systems . 118–126.
[37] Glenn K Lockwood, Damian Hazen, Quincey Koziol, R Shane Canon, Katie Anty-
pas, Jan Balewski, Nicholas Balthaser, Wahid Bhimji, James Botts, Je ￿Broughton,
et al. 2023. Storage 2020: A vision for the future of hpc storage. (2023).
[38] Luke Logan, Jay Lofstead, Xian-He Sun, and Anthony Kougkas. 2023. An Evalua-
tion of DAOS for Simulation and Deep Learning HPC Workloads. In Proceedings
of the 3rd Workshop on Challenges and Opportunities of E ￿cient and Performant
Storage Systems . 9–16.
[39] Krishna T Malladi, Manu Awasthi, and Hongzhong Zheng. 2016. DRAMPersist:
Making DRAM Systems Persistent. In Proceedings of the Second International
Symposium on Memory Systems . 94–95.
[40] John D. McCalpin. 1991-2007. STREAM: Sustainable Memory Bandwidth in High
Performance Computers . Technical Report. University of Virginia, Charlottesville,
Virginia. http://www.cs.virginia.edu/stream/ A continually updated technical
report. http://www.cs.virginia.edu/stream/.
[41] John D. McCalpin. 1995. Memory Bandwidth and Machine Balance in Current
High Performance Computers. IEEE Computer Society Technical Committee on
Computer Architecture (TCCA) Newsletter (Dec. 1995), 19–25.
[42] John D. McCalpin. 1995. STREAM Benchmark. https://www.cs.virginia.edu/
stream/ . [Online].
[43] John D McCalpin. 1995. Stream benchmark. Link: www. cs. virginia. edu/stream/ref.
html# what 22, 7 (1995).
[44] John D. McCalpin. 2023. Bandwidth Limits in the Intel Xeon Max (Sapphire
Rapids with HBM) Processors. In ISC 2023 IXPUG Workshop . 1–24.
11,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren
[45] Sally A McKee. 2004. Re ￿ections on the memory wall. In Proceedings of the 1st
conference on Computing frontiers . 162.
[46] George Michelogiannakis, Yehia Arafa, Brandon Cook, Liang Yuan Dai, Ab-
del Hameed Badawy, Madeleine Glick, Yuyang Wang, Keren Bergman, and John
Shalf. 2023. E ￿cient Intra-Rack Resource Disaggregation for HPC Using Co-
Packaged DWDM Photonics. arXiv preprint arXiv:2301.03592 (2023).
[47] George Michelogiannakis, Benjamin Klenk, Brandon Cook, Min Yee Teh,
Madeleine Glick, Larry Dennison, Keren Bergman, and John Shalf. 2022. A case
for intra-rack resource disaggregation in HPC. ACM Transactions on Architecture
and Code Optimization (TACO) 19, 2 (2022), 1–26.
[48] Vladimir Mironov, Igor Chernykh, Igor Kulikov, Alexander Moskovsky, Evgeny
Epifanovsky, and Andrey Kudryavtsev. 2019. Performance evaluation of the intel
optane dc memory with scienti ￿c benchmarks. In 2019 IEEE/ACM Workshop on
Memory Centric High Performance Computing (MCHPC) . IEEE, 1–6.
[49] Onur Mutlu. 2013. Memory scaling: A systems architecture perspective. In 2013
5th IEEE International Memory Workshop . IEEE, 21–25.
[50] Dushyanth Narayanan and Orion Hodson. 2012. Whole-system persistence. In
Proceedings of the seventeenth international conference on Architectural Support
for Programming Languages and Operating Systems . 401–410.
[51] Kevin Huang Nathan Pham. 2019. Analyzing the Performance of Intel Optane Per-
sistent Memory 200 Series in Memory Mode with Lenovo ThinkSystem Servers.
[52] Samsung Newsroom. 2022. Samsung Electronics Introduces Industry’s First 512GB
CXL Memory Module .https://news.samsung.com/global/samsung-electronics-
introduces-industrys- ￿rst-512gb-cxl-memory-module
[53] SK Hynix Newsroom. 2022. SK hynix Develops DDR5 DRAM CXLTM Memory
to Expand the CXL Memory Ecosystem .https://news.skhynix.com/sk-hynix-
develops-ddr5-dram-cxltm-memory-to-expand-the-cxl-memory-ecosystem/
[54] Rotem Industrial Park. 2019. NegevHPC Project. https://www.negevhpc.com .
[Online].
[55] Onkar Patil, Latchesar Ionkov, Jason Lee, Frank Mueller, and Michael Lang. 2019.
Performance characterization of a dram-nvm hybrid memory architecture for
hpc applications using intel optane dc persistent memory modules. In Proceedings
of the International Symposium on Memory Systems . 288–303.
[56] Ivy Peng, Ian Karlin, Maya Gokhale, Kathleen Shoga, Matthew Legendre, and
Todd Gamblin. 2021. A holistic view of memory utilization on HPC systems:
Current and future trends. In The International Symposium on Memory Systems .
1–11.
[57] Ivy Peng, Roger Pearce, and Maya Gokhale. 2020. On the memory underuti-
lization: Exploring disaggregated memory on hpc systems. In 2020 IEEE 32nd
International Symposium on Computer Architecture and High Performance Com-
puting (SBAC-PAD) . IEEE, 183–190.
[58] Milan Radulovic, Darko Zivanovic, Daniel Ruiz, Bronis R de Supinski, Sally A
McKee, Petar Radojkovi ć, and Eduard Ayguadé. 2015. Another trip to the wall:
How much will stacked dram bene ￿t hpc?. In Proceedings of the 2015 International
Symposium on Memory Systems . 31–36.
[59] Sadhana Rai and Basavaraj Talawar. 2023. Nonvolatile Memory Technologies:
Characteristics, Deployment, and Research Challenges. Frontiers of Quality
Electronic Design (QED) AI, IoT and Hardware Security (2023), 137–173.
[60] Daniel Reed, Dennis Gannon, and Jack Dongarra. 2022. Reinventing high perfor-
mance computing: challenges and opportunities. arXiv preprint arXiv:2203.02544
(2022).
[61] Chaoyi Ruan, Yingqiang Zhang, Chao Bi, Xiaosong Ma, Hao Chen, Feifei Li,
Xinjun Yang, Cheng Li, Ashraf Aboulnaga, and Yinlong Xu. 2023. Persistent
Memory Disaggregation for Cloud-Native Relational Databases. In Proceedings of
the 28th ACM International Conference on Architectural Support for Programming
Languages and Operating Systems, Volume 3 . 498–512.
[62] Andy Rudo ￿. 2017. Persistent memory: The value to hpc and the challenges. In
Proceedings of the Workshop on memory centric programming for hpc . 7–10.
[63] Arthur Sainio et al .2016. NVDIMM: changes are here so what’s next. Memory
Computing Summit (2016).
[64] Steve Scargall. 2020. Programming Persistent Memory: A Comprehensive Guide
for Developers. (2020).
[65] Steve Scargall and Steve Scargall. 2020. libpmemobj: A Native Transactional Ob-
ject Store. Programming Persistent Memory: A Comprehensive Guide for Developers
(2020), 81–109.
[66] Debendra Das Sharma, Robert Blankenship, and Daniel S Berger. 2023. An
Introduction to the Compute Express Link (CXL) Interconnect. arXiv preprint
arXiv:2306.11227 (2023).
[67] Galen M Shipman, Sriram Swaminarayan, Gary Grider, Jim Lujan, and R Joseph
Zerr. 2022. Early Performance Results on 4th Gen Intel (R) Xeon (R) Scalable
Processors with DDR and Intel (R) Xeon (R) processors, codenamed Sapphire
Rapids with HBM. arXiv preprint arXiv:2211.05712 (2022).
[68] Montage Technology. 2022. Montage Technology Delivers the World’s First CXL ™
Memory eXpander Controller .https://www.montage-tech.com/Press_Releases/
20220506
[69] TB Tristian and L Travis. 2019. Analyzing the performance of Intel Optane DC
persistent memory in app direct mode in Lenovo ThinkSystem servers.[70] Jacob Wahlgren, Maya Gokhale, and Ivy B Peng. 2022. Evaluating Emerging
CXL-enabled Memory Pooling for HPC Systems. In 2022 IEEE/ACM Workshop on
Memory Centric High Performance Computing (MCHPC) . IEEE, 11–20.
[71] Ying Wang, Wen-Qing Jia, De-Jun Jiang, and Jin Xiong. 2023. A Survey of Non-
Volatile Main Memory File Systems. Journal of Computer Science and Technology
38, 2 (2023), 348–372.
[72] Michèle Weiland, Holger Brunst, Tiago Quintino, Nick Johnson, Olivier I ￿rig,
Simon Smart, Christian Herold, Antonino Bonanni, Adrian Jackson, and Mark
Parsons. 2019. An early evaluation of intel’s optane dc persistent memory module
and its impact on high-performance scienti ￿c applications. In Proceedings of the
international conference for high performance computing, networking, storage and
analysis . 1–19.
12A Case Against CXL Memory Pooling
Philip Levis
Google
plevis@google .comKun Lin
Google
linkun@google .comAmy Tai
Google
amytai@google .com
Abstract
Compute Express Link (CXL) is a replacement for PCIe. With
much lower latency than PCIe and hardware support for cache
coherence, programs can efﬁciently access remote memory
over CXL. These capabilities have opened the possibility of
CXL memory pools in datacenter and cloud networks, consist-
ing of a large pool of memory that multiple machines share.
Recent work argues memory pools could reduce memory
needs and datacenter costs.
In this paper, we argue that three problems preclude CXL
memory pools from being useful or promising: cost, complex-
ity, and utility. The cost of a CXL pool will outweigh any
savings from reducing RAM. CXL has substantially higher
latency than main memory, enough so that using it will re-
quire substantial rewriting of network applications in complex
ways. Finally, from analyzing two production traces from
Google and Azure Cloud, we ﬁnd that modern servers are
large relative to most VMs; even simple VM packing algo-
rithms strand little memory, undermining the main incentive
behind pooling.
Despite recent research interest, as long as these three
properties hold, CXL memory pools are unlikely to be a
useful technology for datacenter or cloud systems.
CCS Concepts
•Networks !Data center networks ;•Information sys-
tems !Enterprise resource planning .
Keywords
datacenter networking, CXL memory pooling
ACM Reference Format:
Philip Levis, Kun Lin, and Amy Tai. 2023. A Case Against CXL
Memory Pooling. In The 22nd ACM Workshop on Hot Topics in
Networks (HotNets ’23), November 28–29, 2023, Cambridge, MA,
USA. ACM, New York, NY, USA, 7 pages. https://doi .org/10 .1145/
3626111 .3628195
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for third-party
components of this work must be honored. For all other uses, contact the
owner/author(s).
HotNets ’23, November 28–29, 2023, Cambridge, MA, USA
© 2023 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0415-4/23/11.
https://doi .org/10 .1145/3626111 .36281951Introduction
Memory is an expensive component of datacenter and cloud
servers: recent papers report its fraction of a server’s cost is
40% for Meta [ 14] and 50% for Azure [ 21]. Google faces
similar pressures [ 6]. The pressure to reduce RAM needs
and costs has motivated work in far memory [ 18], memory
compression [ 12], and Intel Optane memory, which trades off
performance for lower cost [ 17]. If a server has insufﬁcient
memory, it can have free cores but no available memory
(stranded cores); if it has too much memory it can have free
memory that cores do not use (stranded memory).
One approach to reduce RAM costs is to disaggregate mem-
ory through a shared pool. In this model, servers have their
own local RAM, which is sufﬁcient for average or expected
use. If a server needs more memory or has stranded cores,
it can allocate from a pool shared among several servers. A
memory pool needs to solve two major problems: latency and
cache coherence. Main memory in a larger server CPU has
a latency of 120-140ns; if a memory pool’s latency is much
higher, application performance will suffer.
The Compute Express Link (CXL) protocol promises to
provide low-latency, cache coherent access to remote mem-
ory. With claimed latencies in the hundreds of nanoseconds,
CXL can build a large memory pool shared across several
servers. Disaggregating storage from compute led to much
more efﬁcient and scalable datacenter storage [ 7]; disaggre-
gating memory from compute could have a similar impact,
enabling more efﬁcient and lower cost computing.
Unfortunately, this paper argues that CXL memory pool-
ing faces three major problems. Each of these problems, in
isolation, might limit potential use cases but is surmountable.
Together, however, they mean that CXL memory pools cost
more, require rewriting software, and do not reduce resource
stranding (e.g., unused memory).
The ﬁrst problem is cost. The primary beneﬁt of a CXL
memory pool is reducing the aggregate RAM needs of data-
center and cloud systems. Today, servers are provisioned so
they can keep all of their VMs or containers in memory even
when all of them maximize their footprint simultaneously (a
“sum-of-max” approach). Using a CXL pool can allow servers
to instead provision for expected use, and when VMs uses
their entire footprint the system can store cold data in a CXL
pool. This cost calculation, however, ignores infrastructure
costs. CXL requires a completely parallel network infras-
tructure to Ethernet, consisting of a top-of-rack (or top-of-N
server) CXL appliance, with direct, alternative cabling to all
of its servers.
The second problem is software complexity . Recent ex-
perimental results from real CXL hardware ﬁnd that many of
18
HotNets ’23, November 28–29, 2023, Cambridge, MA, USA Philip Levis, Kun Lin, and Amy Tai
CXL’s latency claims are best-case estimates. For example,
estimates in the Pond system are that CXL will add 70-90ns
over same-NUMA-node DRAM. Recent experimental results,
however, are that CXL adds 140ns for pointer chasing work-
loads [ 19]. This slowdown is for a directly-connected CXL
memory device, not a shared pool, which adds switching,
re-timers, and queueing. While loads and stores to a CXL de-
vice will be much slower than DRAM, hardware-accelerated
copies of 8kB blocks are close to DRAM speed [ 20]. There-
fore, achieving good performance involves rewriting software
to explicitly manage CXL memory, copying blocks into lo-
cal DRAM; this explicit, conditional, and pervasive mem-
ory management increases software complexity. Furthermore,
maintaining multiple copies reduces CXL’s memory savings.
The third problem is limited utility . The primary argument
for CXL memory is that memory that would otherwise be
stranded, i.e. memory that cannot be allocated to a VM be-
cause there are no more compute resources to support a VM,
can now be pooled and used by other servers. However, after
analyzing common server and VM shapes in a 2019 Google
cluster trace [ 22] and 2020 Azure Cloud trace [ 9] using a
methodology we developed to evaluate the conditions when
memory pooling can improve stranding, we conclude pooling
is rarely helpful. Modern servers are large (hundreds of cores
and terabytes of RAM), and VMs are small enough, that VMs
can be placed on a single server with little stranding . For the
traces we examined, the ratio of VM to server sizes must in-
crease by 32x (Google) and 8x (Azure) before pooling yields
even modest efﬁciency gains. To the best of our knowledge,
this is the ﬁrst methodology for estimating the potential of
memory pooling for resource packing.
In summary, as long as the cost, software complexity, and
lack of utility properties hold, sharing a large DRAM bank
between servers with CXL is a losing proposition. If one of
these issues goes away – CXL is cheap, CXL is nearly as fast
as main memory, or VM shapes become difﬁcult to pack into
servers – then CXL memory pools might prove to be useful.
2CXL Memory Pools
This section explains Compute Express Link (CXL) and how
CXL memory pools work. Readers familiar with CXL can
skip this section. Because many of the details of CXL are
extraneous to this paper, we gloss over them; an interested
reader can consult the speciﬁcations [3–5].
2.1 CXL
In this paper, we focus on CXL 1.1, the ﬁrst productized ver-
sion. Samsung [ 15], Intel [ 10], and Astera Labs [ 1] produce
CXL 1.1 devices, but none of them are generally available;
they are only for pilot use and commercial evaluation. Version
3.0 was published in August, 2022 [5].
CXL 1.1 uses the same physical layer (connections and
signaling) as PCIe. PCIe connections consist of one or more
parallel “lanes”. CXL 1.1 uses PCIe Gen5 signaling, which
provides 3.9GB/s per lane. CXL devices can have 1-32 lanes.
Figure 1: Processing path of a memory load (into a CXL
Request) in a dual-socket server to a memory pool con-
nected through. Queueing is possible at almost every
transition. The response goes through the same path;
lookups become updates.
A Samsung 128GB CXL memory device, for example, uses
8 lanes to support a maximum throughput of 35GB/s. [ 16]
CXL 3.0 uses PCIe Gen6 to double per-lane throughput. PCIe
Gen7 is expected to double throughput again (to 15GB/s),
but this approaches the practical limit for a differential pair
(224Gbps) due to gate switching latencies.
CXL differs from PCIe in two major ways: lower latency
and cache coherence. CXL strips out many of the protocol
overheads of PCIe to reduce latency. While PCIe Gen5 de-
vices have best-case round-trip-time latencies of over 500ns,
CXL devices can be as low as 150ns. This is the minimum
signaling latency: it does not include the time for a device to
generate a response (e.g., read from DRAM), any protocol
processing, or queueing delays. While CXL 3.0 increases the
throughput of CXL, it will not have lower latency [ 11] as
packetization delay is not signiﬁcant.1
CXL’s second improvement is hardware cache coherence.
This is useful for devices such as NICs or GPUs, but it is less
important for memory pools, which typically do not allow
servers to share memory.
2.2 Inside a CXL Pool
CXL memory pools can take many forms; in this section, we
focus on the cloud use case of multiple servers connecting to a
single device through separate physical links (called a “multi-
headed device”), as proposed by Pond. [ 13] We assume the
best-case use of a CXL pool, in which effectively all memory
accesses are to memory that is exclusive to a single server,
such that there are no cache coherence overheads.
Figure 1 decomposes the sources of latency when reading
from a CXL memory pool. First, there are the standard mem-
ory latency costs: a core must detect that the memory is not in
1For a 16-lane device at 62GB/s, a 256 byte CXL ﬂit takes 4ns.
19A Case Against CXL Memory Pooling HotNets ’23, November 28–29, 2023, Cambridge, MA, USA
any cache or local DRAM. In a dual-socket system (common
in cloud servers today), the CXL device might be connected
to either socket’s PCIe/CXL lanes, so there is potentially the
latency overhead of the CPU interconnect from one socket to
the other. The processor’s memory management unit (MMU)
must transform a memory request into a CXL request. This
enters the CXL root port, which dispatches it to the virtual
switch (VCS) and virtual PCI-to-PCI bridge (vPPB) of the
device; this dispatch is necessary because a port’s many lanes
can be allocated to many devices (e.g., 16 lanes can be al-
located to 4 different 4-lane M.2 SSDs). The read request
is packetized, encoded, and modulated onto the CXL link,
adding packetization and propagation delays.
On reception, the CXL read request has to be reassem-
bled from the parallel lanes, decoded and passed to the CXL
memory controller. After protocol processing. the controller
translates the request into DDR memory read requests. DDR
reads are striped across multiple DDR sockets to maximize
bandwidth. Once the data is assembled, the CXL device re-
sponds with a data response, which goes through a similar
switching, processing, and encoding as the request did.
At every step of this process, there can be queueing. E.g.,
CXL read requests can queue at the client, memory read
requests can queue at the device, DDR read requests can
queue in the DDR memory controller, etc.
2.3 Pond, an example CXL Pool
Pond is a recent proposal for using a CXL memory pool to
reduce RAM spending in cloud/VM systems [ 13]. Through
extensive analysis of Azure workloads, the paper ﬁnds that
a sizeable fraction of Azure memory is stranded: some VMs
request a low RAM-to-CPU ratio, such that some servers
have unused RAM but every core is in use. Pond argues
that by moving a fraction of every server’s memory to a
CXL pool and statistically multiplexing the memory, a pool
can reduce the total memory needed: memory that is unused
today can instead be used by another server. The tradeoff is
performance: since some VM memory is in the CXL pool, it
is slower. Through careful prediction of which applications
are latency sensitive and which pages are untouched or rarely
touched, Pond can reduce overall DRAM requirements by
7–9% with only 2% of VMs seeing performance degrade by
more than 5%.
2.4 The Case Against Memory Pools
We argue that, despite recent interest, CXL memory pools
are not an effective way to provision networked servers. They
seem like an exciting possibility and fruitful area of research,
but this rosy picture is built on three mistaken and simpliﬁed
assumptions: cost, complexity, and utility. The next three
sections examine each issue in detail.
3Cost
The ﬁrst obstacle for CXL memory pools is their cost. On
one hand, RAM is a large fraction of server cost, so a sharedpool to meet peak needs while reducing per-server memory
would reduce costs. We argue that such an analysis makes two
assumptions that do not hold in practice. First, it assumes that
memory is fungible and it is possible to cut a server’s RAM by
a small fraction (e.g., 7-9% in the Pond paper [ 13]). Second,
it ignores the cost of an additional cabling and networking
infrastructure.
3.1 Memory Provisioning
Cloud and datacenter servers are limited to discrete steps in
DRAM capacity; small reductions in memory do not necessar-
ily translate to cost savings. Modern server CPUs have 8 (In-
tel) or 12 (AMD) DDR channels. Because many applications
are memory bandwidth bound, servers always populate ev-
ery channel. DIMMs, however, only come in certain discrete
sizes (e.g., 32GB, 48GB, 64GB)2, and every channel must
have the same sized DIMM. For example, a modern AMD
Genoa CPU has 12 channels and 192 cores (384 vCPUs). A
Genoa server can be conﬁgured with 750GB (64GB DIMMS),
1.15TB (96GB DIMMS), or 1.5TB (128GB DIMMS), but no
intervening values.
Pond ﬁnds that allocating 25% of VM memory (on aver-
age) to a shared pool leads to only small slowdowns (1%
of VMs slow down by more than 5%). This is achievable,
e.g., by replacing 128GB DIMMs with 96GB DIMMS. While
achievable, allocating 25% of memory to a pool does not
reduce the amount of memory. The servers still need the same
amount of memory, just some of it is in a CXL-connected
memory appliance.
More importantly, Pond also ﬁnds that a CXL memory pool
can reduce total RAM by 7-9% without signiﬁcantly harming
performance. Some VM memory is unused, and by clustering
many servers worth of VMs together, Pond can aggregate
these savings. However, as one cannot shrink server RAM
by 7% or 9%, servers must cut their RAM by 25%, and the
memory pool takes the 7–9% out of this 25%. This, in turn,
requires targeting a very speciﬁc amount of memory in the
CXL pool device, which is difﬁcult given the need to populate
every socket to maximize throughput and the large jumps in
DIMM size. There are some speciﬁc conﬁgurations where
this can work out, but using them constrains the rest of the
system to speciﬁc amounts of memory, numbers of cores, and
degree of CXL pooling.
3.2 CXL Infrastructure
CXL memory pools are not free. We ﬁnd that their costs
exceed any savings from reducing RAM. When considering
cost tradeoffs, we consider consumer (MSRP) prices. Cloud
providers and hyperscalers receive steep discounts, but as we
are considering relative costs and tradeoffs between compo-
nents, we make a simplifying assumption that hyperscaler
discounts are similar across high-volume parts.
2Today, sizes that are not a power of two, such as 48GB, are rare; we assume
vendors would produce large numbers for a cloud provider if asked.
20HotNets ’23, November 28–29, 2023, Cambridge, MA, USA Philip Levis, Kun Lin, and Amy Tai
Figure 2: Minimum pool size for RAM cost savings to
equal switch cost as pool RAM cost decreases relative to
server RAM. Even if pool RAM is free, for a standard
4GB/core memory shape, the pool must be 24 nodes to
break even with just the switch cost.
Because there are no CXL memory pool devices today,
we do not know how much one costs. However, given the
speeds and processing involved, we propose that an Ethernet
switch is a good approximation. A CXL memory pool device
is effectively a high-speed switch, processing CXL packets,
managing cacheline state, reading and writing memory, and
sending responses back to servers. A standard CXL memory
device (e.g., a Astera Leo [ 1] or Intel device [ 10]) uses 16
lanes. At PCIe Gen5 speeds this is 480Gbps. A 16-server pool
therefore processes data at 7.6Tbps.
A modern, low-end, 32-port 200Gbps Ethernet switch such
as the Mellanox MSN3700-VS2F0 costs $38,500. [ 2] DDR5
RAM today is ⇡3$/GB. For the CXL pool device to break
even with its RAM savings, it must save 12.6TB of RAM.
Assuming Pond’s optimistic 9% reduction, to break even with
just the switch, the servers must have12.6)⌫
0.09= 140TB of RAM
in aggregate (using Pond would reduce this to 127TB). For a
32-node pool, 127TB, means 4TB per server. A dual-socket
AMD Genoa server, the standard next-generation system for
cloud providers, has 384 vCPUs. At 4TB/server, there is
>10GB of RAM per Genoa vCPU, more than high-RAM
VMs provide. You have to buy considerably more RAM for
Pond’s RAM savings to pay for themselves: you are better
off just buying less RAM.
What if pool RAM is cheaper than server RAM? E.g.,
it could be slower, more cost-efﬁcient DIMMs, or DDR4.
Figure 2 shows how pool RAM cost affects the minimum
pool size to break even. These results assume the Genoa setup
described above, reducing server RAM by 25% of RAM, and
being able to reduce aggregate RAM by 9%. Even if pool
RAM is completely free, for a standard 4GB/core memory
shape, the pool must be 24 nodes to break even. For memory-
optimized VMs (8GB/core), if the pool memory is half the
cost of server memory (50%), a pool size of 20 could break
even with the switch cost.
This accounting is only the capital expenditure of the pool
device: it doesn’t include the cost of the cabling, assembly,and maintenance to connect the servers to the pool, the cost
of the interface cards that connect to the cables, the space
costs of giving up rack slots to pools, or the energy costs of
the pool devices. It also does not consider any operational ex-
penditures for maintaining or managing this parallel network
infrastructure. We conclude that the costs of introducing CXL
devices into a datacenter network eclipse any cost savings of
reducing RAM.
4Software Complexity
The second major problem with a CXL memory pool is that
it will signiﬁcantly add to software complexity. Experimental
results from real hardware show that, for random accesses,
CXL devices are signiﬁcantly slower than the best case num-
bers suggested in standards documents. While CXL has high
latency, its high throughput means that transferring larger
blocks of memory (a few kB) can be competitive with DRAM.
This requires explicitly copying the remote memory into local
memory; the CXL pool stops being memory accessed directly
and instead becomes a far memory cache.
Today, CXL devices are not commercially available, and
NDAs preclude publishing results without prior approval.
The only CXL experimental results we are aware of are from
a series of versions of a paper by authors from UIUC and
Intel [ 19] (the Pond paper [ 13] assumes values reported in
standards). Because we do not have an agreement with CXL
device vendors, we base our conclusions on these published
experimental results.3
4.1 CXL Performance
CXL memory devices are high throughput. They are typically
16-lane CXL devices; at PCIe Gen5 speeds, this is 480Gbps.
A single DDR5-4800 DIMM (standard on new servers today)
is 300Gbps. Server CPUs have many DIMMs, but 480Gbps
is fast and it can support reasonable copy performance of
larger objects. For example, a copy from CXL memory to
local DDR by a single core can use 80% of the bandwidth of
two DIMMs.
However, CXL memory devices are also high latency. The
UIUC measurement results of a directly connected (no switch-
ing) CXL device show CXL loads have best case latencies
of 2x of local memory, substantially slower than a remote
NUMA node (1.5x) [ 19]. For a modern server CPU (e.g.,
Sapphire Rapids, as used in the paper), this means a memory
access jumps from 140ns for local memory to 280ns for CXL
memory. At 2.0-3.0 GHz in a multiple-issue superscalar pro-
cessor, this latency stalls the CPU for over 500 instructions.
Switched system with re-timers will have higher latency.
3The revision of the UIUC/Intel paper accepted to MICRO [ 19] reports
results from multiple CXL devices, which vary greatly in performance. We
focus on latencies from the highest performance device measured, CXL-A,
which is an ASIC.
21A Case Against CXL Memory Pooling HotNets ’23, November 28–29, 2023, Cambridge, MA, USA
4.2 Instructions or Transfers?
While CXL will perform poorly as a transparent far memory,
its throughput means it can read or write larger blocks with
good performance. For example, early Intel/UIUC results
show that synchronously copying 8kB from DRAM to CXL
memory can have 80% the throughput of DRAM-to-DRAM
copies when using the DSA memory copy accelerator. [20]
This involves explicitly copying CXL memory into local
memory. In this model, CXL memory is a far memory cache,
which processors can access faster than remote memory or
storage, but which programs must explicitly copy from. This
gets at a fundamental question with using CXL memory: how
does a program access it?
Instruction-level loads and stores operate on a cache line
granularity. Reasonable CXL performance, however, requires
8kB transfers. Unless an application can take an enormous
performance hit when accessing CXL memory, it cannot do
so transparently. Instead, it must do so explicitly, or the device
must act as a page-level cache (e.g., a far memory RAMdisk
partition).
Explicit copies require invasive changes to applications.
For example, suppose a program calculates the maximum
value over an array (tens of kB), and this array is in CXL
memory. The loop is fast, consisting of only 4 instructions. At
2 instructions per clock, it can process 2 bytes every clock. At
3GHz, 280ns is 840 ticks, and the loops processes 1680 bytes
in 280ns. A cache line is 64 bytes, so the processor must have
over 26 prefetches in ﬂight in order to keep the pipeline busy.
Processors do not prefetch so deeply, so this loop will spend
most of its time stalled on CXL reads.
In contrast, a program that explicitly copies from CXL
memory into local memory will perform much faster, because
it pays the 280ns latency once then operates on local, in-cache
memory. However, the problem is that this requires an explicit
memory copy to local memory. It requires rewriting programs
to conditionally copy if data is in CXL; CXL memory is not
transparent and requires pervasive changes to software. Fur-
thermore, it requires making copies of data, which increases
application memory use.
5Limited Utility
In this section we develop a methodology for estimating the
efﬁciency gains that can be recouped with memory pool-
ing. Our primary efﬁciency metric is utilization , deﬁned as
used capacity
total capacity. The main argument for memory pooling is that it
can improve utilization by reducing the amount of stranded
memory, in other words reducing ‘total capacity’.
Methodology. To approximate utilization improvement
from memory pooling, we need to estimate a cluster’s uti-
lization. Utilization is a multi-dimensional bin-packing prob-
lem [ 8,9,23,24], and to optimize efﬁciency, a datacenter
cluster scheduler should pack VMs onto physical machines
as tightly as possible. Of course, there are performance and
isolation considerations when packing workloads as tightly
as possible, so realistically, operators leave some fraction ofheadroom on each machine. Despite this, the optimal bin
packing utilization of a machine is still a good proxy for the
actual utilization an operator can achieve in a real deployment;
later in this section, we validate that the optimal packing ap-
proximates an event-driven packer within reasonable error
bounds. Crucially, using the optimal utilization does not pin
efﬁciency gains to a speciﬁc cluster scheduler implementation
and enables faster analysis than re-running a full cluster trace.
Therefore, we deﬁne a cluster’s utilization as its utilization
under the optimal packing of a VM workload on the cluster.
To determine the efﬁciency gain of pooling, we calculate
the utilization improvement from adding a pool to a cluster.
We model a cluster as a set of machines. We model pooling
as a large machine that is #-times the size of a machine.
#reﬂects the number of machines that share a CXL pool.
Note that modelling pooling as a large machine overestimates
the utilization gain from pooling, because the large machine
elides allocation boundaries. In a true pooling setup compute
resources must still be allocated to the machine boundary, and
memory resources must respect a machine and pool boundary.
Therefore any improvement due to pooling in this section
is a generous upper bound on what pooling can realistically
achieve.
Datacenter traces. For VM workloads and machine sizes
that are representative of real deployments, we analyze two
cluster traces, the 2019 Google cluster trace and 2020 Azure
Trace for Packing [ 9]. From each trace we derive a distribution
of VM demand and realistic machine sizes [ 22]. In the Google
trace, we use VMs and machines from Cell A and only include
VMs with production priority. In the Azure trace, we only
include VMs with high priority. Note that the Google trace
is a trace of internal workloads, and the Azure trace is for
cloud, or customer, VMs; we analyze both traces to see if the
two different settings affect the impact of pooling. We model
both machines and VMs as two-dimensional vectors of CPU
and memory.
Optimal packing. We use a vector bin-packing library
to calculate the optimal packing of each set of VMs. The
library takes a set of VMs and a machine size, and returns
theminimum number of machines it takes to pack the entire
set. Therefore, from a utilization perspective, ‘used capacity’
is always ﬁxed, because we must land all VMs, but ‘total
capacity’ is variable, because it depends how optimally the
VMs can be packed on the given machine sizes.
Our optimal packing packs a snapshot of the cluster, which
is a simpler problem than the packing problem in a real cluster
scheduler, because the snapshot is not bound to previous
placement decisions and does not take VM departures and
arrivals into consideration. We packed many snapshots and
studied the result distribution; all snapshots had trends similar
to Figures 3 and 4.
Validation. We validate that a cluster’s utilization under
optimal packing is close to the utilization under a reasonable,
live cluster scheduler. We implement a greedy bin packer that
replays a cluster trace and compare the cluster’s utilization
22HotNets ’23, November 28–29, 2023, Cambridge, MA, USA Philip Levis, Kun Lin, and Amy Tai
Figure 3: Google cluster trace: Pooling resources across
up to 16 machines yields does not yield utilization im-
provements until VMs are at least 32x larger.
Figure 4: Azure trace: Pooling resources across up to 16
machines does not yield utilization improvements until
VMs are at least 8x larger. These (cloud) VMs are much
larger than the VMs in the Google trace, but still not
large enough for pooling to matter.
after running the trace to the utilization of the optimal pack-
ing, at the snapshot of the cluster after the trace has been
played. We do this comparison for subsets of traces across all
8 cells available in the 2019 Google trace and ﬁnd that opti-
mal packing utilization is 0-17% better than the utilization of
the greedy bin packer, with a median difference of 5%. This
greedy bin packer is likely worse at packing workloads than
production-grade cluster schedulers, which means that the
optimal packing can even more closely approximate the uti-
lization of production-grade schedulers. These error bounds
suggest that the optimal packing is a reliable proxy for cluster
utilization in the following results.
Results. Figures 3 and 4 show the results, where utilization
is normalized to a 1x machine pooling factor and 1x VM sizes.
Taking the unmodiﬁed VM demands from the trace resulted
in no utilization gain from pooling of any size (ﬂat blue line)
in both datasets.
To see how sensitive this result is and how much packing
ﬂexibility there is, we inﬂate the sizes of VMs. For example,
for 8x we increase the core count and memory size of every
VM by a factor of 8. For the Google trace, we ﬁnd that pooling
has at most 1% utilization improvement, even when VMs are
inﬂated up to 16x. Weighed against the additional costs and
complexity of pooling outlined earlier in the paper, this smallimprovement renders pooling out of the question. In Figure 3,
pooling begins to have a beneﬁt with a 32x inﬂation factor,
and even then, it is modest, less than 5%. VMs must be 64x
larger in order for there to be signiﬁcant resource stranding at
a single server, which is what reduces stranding with pooling.
On the other hand, the Azure VMs (Figure 4) begin to see
utilization improve at around 8x VM sizes, suggesting that the
cloud VMs are larger than the internal Google VMs to begin
with, and that the pooling calculus may differ for internal
workloads versus public cloud workloads.
In general, if VMs can reach a certain size with respect to
physical machine size, pooling can help with the resulting
resource stranding. However, based on the analyzed traces,
most VMs are small and servers are large: while bin-packing
is an NP-hard problem, if the bins are large and almost all of
the objects are small, there is little leftover space in any bin.
As long as VM sizes remain small, pooling is untenable.
6Discussion
Compute Express Link provides numerous improvements to
PCIe, notably lower latency and cache coherence. For com-
plex, latency-sensitive peripherals such as NICs and GPUs,
CXL will allow much faster and more ﬁne-grained coordina-
tion between the CPU and these processors.
This paper examines another potential use of CXL, dis-
aggregation memory across servers through a shared CXL
memory pool. While there has been a lot of excitement and
interest in such an approach, there has been almost no experi-
mental data to verify its feasibility. Furthermore, analyses of
its potential beneﬁts ignore many of the practical deployment
issues and costs.
Disaggregation in datacenter and cloud systems was ﬁrst
proposed for hard disk drive storage, where seek times of
milliseconds outweigh any additional system or network la-
tency. Memory, however, is at the opposite of this spectrum.
Architectures move memory closer to compute because lo-
cality is key to performance. GPUs, TPUs, and IPUs all have
their own memory. For example, a 6 foot cable adds 12ns of
propagation delay in each direction, and at memory speeds
every such little increase matters.
In this paper, we described three reasons why CXL mem-
ory pools will not be useful in cloud and datacenter systems:
cost, software complexity, and a lack of utility. Each reason is
based on the best information we could ﬁnd and is grounded
in computing today. Future advances or marketplace shifts
may invalidate our assumptions and change the calculus to
make CXL pools attractive; that they could play a role as
far memory RAMdisks, which an OS copies into local mem-
ory. We look forward to and encourage research on such a
future, but at the same time do not want to mistake hopeful
possibilities for technical reality.
References
[1]Astera Labs. Leo cxl memory connectivity platform. https:
/ / www .asteralabs .com / products / cxl - memory - platform / leo - cxl -
memory-connectivity-platform/, 2023.
23A Case Against CXL Memory Pooling HotNets ’23, November 28–29, 2023, Cambridge, MA, USA
[2]CDW Corporation. Mellanox Spectrum-2 MSN3700 switch 32
ports. https://www .cdw.com/product/mellanox-spectrum-2-msn3700-
switch-32-ports-managed-rack-mountable/6415759, 2023.
[3]Compute Express Link Consortium, Inc. Compute Express Link (CXL)
Speciﬁcation, Revision 1.1, 2019.
[4]Compute Express Link Consortium, Inc. Compute Express Link (CXL)
Speciﬁcation, Revision 2.0, 2020.
[5]Compute Express Link Consortium, Inc. Compute Express Link (CXL)
Speciﬁcation, Revision 3.0, 2022.
[6]P. Duraisamy, W. Xu, S. Hare, R. Rajwar, D. Culler, Z. Xu, J. Fan,
C. Kennelly, B. McCloskey, D. Mijailovic, B. Morris, C. Mukher-
jee, J. Ren, G. Thelen, P. Turner, C. Villavieja, P. Ranganathan, and
A. Vahdat. Towards an adaptable systems architecture for memory tier-
ing at warehouse-scale. In Proceedings ofthe28th ACM International
Conference onArchitectural Support forProgramming Languages and
Operating Systems, Volume 3, ASPLOS 2023, page 727–741, New
York, NY, USA, 2023. Association for Computing Machinery.
[7]P. X. Gao, A. Narayan, S. Karandikar, J. Carreira, S. Han, R. Agar-
wal, S. Ratnasamy, and S. Shenker. Network requirements for re-
source disaggregation. In Proceedings ofthe12th USENIX Conference
onOperating Systems Design andImplementation , OSDI’16, page
249–264, USA, 2016. USENIX Association.
[8]R. Grandl, G. Ananthanarayanan, S. Kandula, S. Rao, and A. Akella.
Multi-resource packing for cluster schedulers. ACM SIGCOMM
Computer Communication Review, 44(4):455–466, 2014.
[9]O. Hadary, L. Marshall, I. Menache, A. Pan, E. E. Greeff, D. Dion,
S. Dorminey, S. Joshi, Y . Chen, M. Russinovich, et al. Protean: Vm allo-
cation service at scale. In Proceedings ofthe14th USENIX Conference
onOperating Systems Design andImplementation , pages 845–861,
2020.
[10] I. Intel. Intel FPGA Compute Express Link (CXL) IP. https://
www .intel.com/content/www/us/en/products/details/fpga/intellectual-
property/interface-protocols/cxl-ip .html, 2023.
[11] Ishwar Agarwal. CXL overview and evolution. In
Proceedings of HotChips 34, 2022.
[12] A. Lagar-Cavilla, J. Ahn, S. Souhlal, N. Agarwal, R. Burny, S. Butt,
J. Chang, A. Chaugule, N. Deng, J. Shahid, G. Thelen, K. A. Yurt-
sever, Y. Zhao, and P. Ranganathan. Software-deﬁned far memory
in warehouse-scale computers. In Proceedings oftheTwenty-Fourth
International Conference onArchitectural Support forProgramming
Languages andOperating Systems , ASPLOS ’19, page 317–330, New
York, NY, USA, 2019. Association for Computing Machinery.
[13] H. Li, D. S. Berger, L. Hsu, D. Ernst, P. Zardoshti, S. Novakovic,
M. Shah, S. Rajadnya, S. Lee, I. Agarwal, M. D. Hill, M. Fontoura,
and R. Bianchini. Pond: Cxl-based memory pooling systems for cloud
platforms. In Proceedings ofthe28th ACM International Conference
onArchitectural Support forProgramming Languages andOperating
Systems, Volume 2, ASPLOS 2023, page 574–587, New York, NY,
USA, 2023. Association for Computing Machinery.
[14] H. A. Maruf, H. Wang, A. Dhanotia, J. Weiner, N. Agarwal,
P. Bhattacharya, C. Petersen, M. Chowdhury, S. Kanaujia, and
P. Chauhan. Tpp: Transparent page placement for cxl-enabled tiered-
memory. In Proceedings ofthe28th ACM International Conference
onArchitectural Support forProgramming Languages andOperating
Systems, Volume 3, ASPLOS 2023, page 742–755, New York, NY,
USA, 2023. Association for Computing Machinery.
[15] S. Newsroom. Samsung Electronics Introduces Industry’s First 512GB
CXL Memory Module. https://news .samsung .com/global/samsung-
electronics-introduces-industrys-ﬁrst-512gb-cxl-memory-module,
2022.
[16] S. Newsroom. Samsung Develops Industry’s First CXL DRAM Sup-
porting CXL 2.0. https://news .samsung .com/global/samsung-develops-
industrys-ﬁrst-cxl-dram-supporting-cxl-2-0, 2023.
[17] I. B. Peng, M. B. Gokhale, and E. W. Green. System Evaluation
of the Intel Optane Byte-Addressable NVM. In Proceedings ofthe
International Symposium onMemory Systems , MEMSYS ’19, page304–315, New York, NY, USA, 2019. Association for Computing Ma-
chinery.
[18] Z. Ruan, M. Schwarzkopf, M. K. Aguilera, and A. Belay. Aifm:
High-performance, application-integrated far memory. In Proceedings
ofthe14th USENIX Conference onOperating Systems Design and
Implementation, OSDI’20, USA, 2020. USENIX Association.
[19] Y. Sun, Y. Yuan, Z. Yu, R. Kuper, I. Jeong, R. Wang, and N. S. Kim.
Demystifying cxl memory with genuine cxl-ready systems and devices,
October 2023.
[20] Y. Sun, Y. Yuan, Z. Yu, R. Kuper, I. Jeong, R. Wang, and N. S. Kim.
Demystifying cxl memory with genuine cxl-ready systems and devices,
v1, March 2023.
[21] The Next Platform. CXL And Gen-Z Iron Out A Coherent Interconnect
Strategy. https://www .nextplatform .com/2020/04/03/cxl-and-gen-z-
iron-out-a-coherent-interconnect-strategy/, 2020.
[22] M. Tirmazi, A. Barker, N. Deng, M. E. Haque, Z. G. Qin, S. Hand,
M. Harchol-Balter, and J. Wilkes. Borg: the Next Generation.
InProceedings oftheFifteenth European Conference onComputer
Systems (EuroSys’20), Heraklion, Greece, 2020. ACM.
[23] A. Verma, M. Korupolu, and J. Wilkes. Evaluating job packing in
warehouse-scale computing. In 2014 IEEE International Conference
onCluster Computing (CLUSTER), pages 48–56. IEEE, 2014.
[24] A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and
J. Wilkes. Large-scale cluster management at google with borg. In
Proceedings oftheTenth European Conference onComputer Systems ,
pages 1–17, 2015.
24Cache or Direct Access?
Revitalizing Cache in Heterogeneous Memory File System
Yubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei Hu
Huawei, China
Abstract
This paper revisits the value of cache in DRAM-PM hetero-
geneous memory /f_ile systems. The /f_irst contribution is a
comprehensive analysis of the existing /f_ile systems on het-
erogeneous memory, including cache-based and DAX-based
(direct access). We /f_ind that the DRAM cache still plays an im-
portant role in heterogeneous memory. The second contribu-
tion is a cache framework for heterogeneous memory, called
FLAC .FLAC integrates the cache with the virtual memory
management and proposes two technologies of zero-copy
caching and parallel-optimized cache management, which
deliver the bene /f_its of fast application-storage data trans-
fer and e ﬃcient DRAM-PM data synchronization/migration.
We further implement a library /f_ile system upon FLAC . Mi-
crobenchmarks show that FLAC provides a performance
increase of up to two orders of magnitude over existing /f_ile
systems in /f_ileread/write . With a real-world application,
FLAC achieves up to 77.4% and 89.3% better performance
than NOVA and EXT4, respectively.
CCS Concepts: •Software and its engineering →File
systems management ;•Information systems →Stor-
age class memory .
Keywords: Heterogeneous Memory, File Systems, Page Cache
ACM Reference Format:
Yubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei
Hu. 2023. Cache or Direct Access? Revitalizing Cache in Heteroge-
neous Memory File System. In 1st Workshop on Disruptive Memory
Systems (DIMES ’23), October 23, 2023, Koblenz, Germany. ACM, New
York, NY, USA, 7 pages. h/t_tps://doi.org/10.1145/3609308.3625272
1 Introduction
New persistent memory techniques ( e.g., 3DXPoint [ 12,20]
and CXL-based SSD [ 15]) and connection techniques ( e.g.,
CXL [ 4] and GenZ [ 5]) promise high performance, larger
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for pro /f_it or commercial advantage and that
copies bear this notice and the full citation on the /f_irst page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speci /f_ic
permission and/or a fee. Request permissions from permissions@acm.org.
DIMES ’23, October 23, 2023, Koblenz, Germany
© 2023 Copyright held by the owner/author(s). Publication rights licensed
to ACM.
ACM ISBN 979-8-4007-0300-3/23/10. . . $15.00
h/t_tps://doi.org/10.1145/3609308.3625272capacity, and energy e ﬃciency. These bring a new trend
of heterogeneous memory architecture which consists of
a volatile memory layer (DRAM) and a persistent memory
layer (PM, it can be the 3DXPoint or emerging CXL-based
memory storage devices). This work investigates an impor-
tant question: What kind of storage framework can maximize
the potential of heterogeneous memory? Currently, using
DRAM as cache and direct access (DAX) are two mainstream
solutions for heterogeneous memory /f_ile systems.
Caching pages in DRAM, such as VFS page cache, is a
common design in traditional /f_ile systems ( e.g., EXT4 and
XFS) to bridge the performance gap between fast DRAM
and slow persistent storage devices ( e.g., HDD and SSD).
However, many previous studies [ 8] in the past decade argue
that the DRAM cache incurs signi /f_icant software overhead
under the fast, full-memory architecture. Therefore, most
existing systems ( e.g., NOVA [ 31], SplitFS [ 17], ctFS [ 21], and
KucoFS [2]) resort to the DAX method, which bypasses the
DRAM cache and performs I/Os on PM directly.
However, DAX is still suboptimal for heterogeneous mem-
ory/f_ile systems. First, the DAX method potentially loses the
performance bene /f_it of data locality provided by the DRAM
cache and incurs mandatory data copy across DRAM and
PM. Even worse, the performance upper bound of the DAX
is limited by the PM hardware which is inevitably slower
than DRAM. As we show in §2, the performance of DAX-
based systems is inferior to that of cache-based systems in
scenarios with high concurrency and strong data locality,
even though the VFS page cache framework introduces high
software overheads. Last but not least, instant persistence is
the best scene of DAX; but it is overkill in many real-world
scenarios [30].
Moreover, new characteristics in emerging hardware and
customer demand motivate us to revisit the value of DRAM
cache in heterogeneous memory architecture. 1)The per-
formance gap between PM and DRAM cannot be ignored,
and multiple PMs will have di ﬀerent performances in the
future (latency ranges from 170ns to 3000ns [ 14,15]).2)Data
locality exists in various real scenarios, and keeping the hot
data in DRAM can undoubtedly improve I/O performance.
3)Instant persistence in DAX is overkill in many real-world
scenarios [30].
While the DRAM cache still plays an important role in the
future heterogeneous memory architecture, simply reusing
the current implementation, such as the VFS page cache,
is insu ﬃcient. According to our quantitative analysis (§2),
38
DIMES ’23, October 23, 2023, Koblenz, Germany Yubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei Hu
we conclude two challenges of building an e ﬃcient cache
framework on heterogeneous memory:
1)Reduce the data transfer overhead between application
buﬀer and cache. Transferring data between the application
and cache is the most critical fast-path operation, but ex-
isting cache frameworks use memory copy that introduces
substantial performance overhead. Our experiments show
that data copying occupies up to 84% of the overhead in the
/f_ile system with the VFS page cache.
2)Reduce the impact of the “cache tax”. In addition to
data transfer, existing cache frameworks spend lots of ef-
fort to synchronize ( /f_lushing dirty data) and migrate data
(evicting data into/out of cache) across DRAM cache and PM.
Currently, these operations are implemented in a synchro-
nous and serial way and signi /f_icantly increase performance
penalty (more than 30%). For instance, upon a cache miss,
current systems block I/O operation and fetch data from
lower-level storage synchronously.
This paper proposes FLAC (FLAtCache), a novel cache
framework for heterogeneous memory. The key idea of FLAC
is to integrate the cache with the virtual memory manage-
ment subsystem. FLAC provides a single-level view of het-
erogeneous memory and enables a transparent and e ﬃcient
DRAM cache in the data I/O path. FLAC leverages two novel
techniques to deal with the two challenges outlined above:
1) Zero-Copy Caching. FLAC proposes the heterogeneous
page table that uni /f_ies heterogeneous memory into a single
level. Virtual pages within FLAC can be dynamically mapped
to physical pages on DRAM or PM according to their states
(i.e.cached or evicted). We then design the page attaching
mechanism, a set of tightly coupled management operations
on the heterogeneous page table, which optimize the data
transfer between applications and cache in a zero-copy man-
ner. The core idea of page attaching is to map pages between
source and destination addresses with enforced copy on write
(COW). As a result, data read/write to/from FLAC is executed
by page attaching to realize e ﬃcient and safe data transfer.
While page remapping optimizations are also used in some
systems to reduce the overhead of data copy [ 7,10,24,25], us-
ing a similar idea in the /f_ile system cache faces some unique
challenges that will be discussed in §3.1.
2) Parallel-Optimized Cache Management. The cache
management mechanism of FLAC must ensure a low “cache
tax” impact. Leveraging the multi-version feature that is
brought by the zero-copy caching, FLAC can fully exploit
the parallelism of data synchronization and migration with
critical I/O paths. FLAC proposes a 2-Phase /f_lushing mech-
anism that allows the expensive persistence phase in dirty
data synchronization to be lock-free, and proposes asyn-
chronous cache miss handling to amortize the overhead of
loading data to cache in the background.
Furthermore, we leverage FLAC to implement a prototype
of/f_ile system read andwrite operations. The evaluation
shows that FLAC provides a maximum performance increase
 0 0.7 1.4 2.1 2.8 3.5
EXT4-nfEXT4-fEXT4-daxNOVAExecution Time (s)(a) Random Write 0 0.7 1.4 2.1 2.8 3.5
EXT4EXT4-daxNOVAEXT4-cc(b) Random ReadCopyOthersFigure 1. Traditional Cache vs. DAX (Lower is better). “-f/-
nf”: with/without background /f_lushing; “-cc”: cold cache.
of more than an order of magnitude over existing systems.
With a command line application ( tar),FLAC outperforms
NOVA and EXT4 by up to 77.4% and 89.3%, respectively.
2 Cache? or DAX?
With the emergence of new interconnection technologies
(e.g., CXL [ 4], GenZ [ 5]) and persistent storage medias ( e.g.,
3DXPoint [ 12], CXL-based PM/SSD [ 15]), the storage ar-
chitecture evolves from memory-block to all-memory. A
typical heterogeneous memory architecture consists of a
fast, volatile, small capacity layer (DRAM), and a slow, non-
volatile, large capacity layer (PM). Di ﬀerent types of memo-
ries present heterogeneity in multiple aspects. The latency
of DRAM is about 80ns to 140ns, while the latency of low
tier memory ranges from 170ns to 3000ns [ 14,15]. The PM
layer also has lower bandwidth and concurrency than the
DRAM layer [9, 17].
We deeply analyze the overhead of three typical /f_ile sys-
tems with cache (EXT4) and DAX (EXT4-DAX, NOVA) and
discuss the way to e ﬃciently utilize heterogeneous memory.
We use a single test thread to randomly write /read a 10GB
/f_ile with 2MB I/O (the testbed is introduced in §5) and come
up with three observations from the experiments.
Observation 1 : Existing DAX and cache frameworks are sub-
optimal, but DRAM cache still has great value for heteroge-
neous memory /f_ile systems.
The VFS page cache is a typical cache framework that
is designed to bridge the performance gap between DRAM
and block devices. However, the VFS page cache has a heavy
software stack, which makes it unsuitable for the heteroge-
neous memory structure. Therefore, many heterogeneous
memory /f_ile systems proposed in the past decade resort to
the DAX method, i.e., bypassing the DRAM cache in the data
I/O path. However, we think DRAM cache still has a lot of
value in heterogeneous memory /f_ile systems. First, PMs with
diﬀerent performances will emerge in the future, and the
performance gap between PM and DRAM cannot be ignored.
Figure 1 and 3 show that VFS page cache still has better per-
formance than DAX in some cases ( e.g., read and concurrent
write). Second, taking advantage of data locality is still the
main method of performance optimization. Third, POSIX is
still a mainstream semantics and it can tolerate cached I/Os,
which makes instant persistence in DAX an overkill in many
real-world scenarios [30].
39Cache or Direct Access? DIMES ’23, October 23, 2023, Koblenz, Germany
P0v1 P1v1 P0v0 P1v0P2v0 P3v0
Persistent 
PTEsLog P0v0 P1v0 P2v0
DRAM PMVirtual 
Pages
Physical 
PagesWrite Buf in APP0 
(Userspace )Read Buf in APP1
(Userspace )FLAC Space 
(File System Data Area inKernel)
Zero -Copy Write Zero -Copy Read
2-Phase flushing, 
does no block writeAsync cache 
miss handlingHeterogeneous Page Table
Figure 2. Architecture of FLAC . File data management is run
on top of FLAC . The /f_ile read/write operation is converted to
the zero-copy transfer API of FLAC . The pages are /f_lushed
to/loaded from the PM by the parallel-optimized mechanism.
Observation 2 : Data transfer overhead between the /f_ile system
and application bu ﬀer is signi /f_icant but often overlooked, and
it is one of the keys to unlocking the potential of the cache in
heterogeneous memory.
File data I/Os ( read /write ) need to transfer data between
the application bu ﬀer and the storage system ( i.e.cache or
persistent data area in DAX). Memory copy is the main-
stream method to transfer data and it takes up more than 23%
and96% of the total overhead in cache-based and DAX-based
/f_ile systems, respectively (Figure 1). Since the latency of PM
is much smaller than block devices, the performance bottle-
neck of data copy between cache and application is more
obvious. This observation leads us to consider a zero-copy
approach to transfer data between the cache and applica-
tion bu ﬀer. Thanks to the byte-addressability, it is feasible to
build a single-level address space of heterogeneous memory
to achieve zero-copy.
Observation 3 : “Cache Tax” in traditional cache frameworks
is heavy, and it mainly includes the overhead of data synchro-
nization and migration.
Caching increases storage levels and brings extra data
management overhead. Figure 1 shows that the “cache tax”
(denoted as “other”) takes up to 77% of the execution time
in EXT4. Furthermore, the experiments reveal the composi-
tion of the “cache tax”. The background dirty /f_lushing (data
synchronization) and cache miss (data migration) lead to
37%and65% performance declines, respectively. In general,
the “cache tax” is di ﬃcult to eliminate, but we can reduce its
impact on the critical I/O paths by improving the parallelism
between them and front-end I/Os.
3 Flat Cache
We propose FLAC ,aFLAtCache framework integrated with
the virtual memory subsystem to deeply explore the potential
of cache in heterogeneous memory. FLAC is designed for
any heterogeneous memory architecture, while persistent
memory can be existing ( e.g., 3DXPoint) or future ( e.g., CXL-
based PM/SSD) devices.Table 1. Main APIs of FLAC (for/f_ile system developer)
API Main Para. Description
init_flac pm_pathCreate/Recover
theFLAC space
zcopy_from_flac
zcopy_to_flacfrom_addr
to_addr
sizeTransfer data between
application buﬀer
and FLAC with the
zero-copy approach
pflush_addp/f_lush_handle
addr
sizeAttach (map) pages to a
/f_lushing buﬀer and
add to the handle
pflush_commit p/f_lush_handleAtomically /f_lush
dirty pages to PM
pfreeaddr
sizeAtomically reclaim
PMpages
As shown in Figure 2, FLAC maintains a range of con-
tiguous virtual memory addresses, called FLAC space, which
is as large as the available PM space used to store /f_ile data.
FLAC space is indexed by the heterogeneous page table and
it makes the physical location of the page transparent and
exposes a single-level memory space to /f_ile system develop-
ers. Pages are cached in DRAM when they are accessed (the
cache size can be adjusted). Data are transferred between the
application and FLAC space with the zero-copy approach
(§3.1) and synchronized/migrated between DRAM and PM
with the parallel-optimized mechanism (§3.2).
FLAC is a development framework for heterogeneous
memory architecture that allows the /f_ile system developers
to customize the data management ( e.g., read/write logic) on
FLAC space. The other modules of the /f_ile system ( e.g., meta-
data management) are independent of FLAC and they can be
/f_lexibly implemented. Table 1 shows the APIs of FLAC . The
/f_ile system developers initialize FLAC by calling init_flac ,
which creates FLAC space and binds the PM to it. The /f_ile sys-
tem internally uses zcopy_to/from_flac to transfer data
and support read andwrite operations. Data accesses on
FLAC space are transparent to applications, so applications
use standard read andwrite operations to access /f_iles. The
/f_ile system developers are asked to explicitly /f_lush dirty data
from DRAM to PM, which gives them the /f_lexibility to cus-
tomize /f_lushing policies. A pair of APIs, pflush_add and
pflush_commit , provide a high concurrency and atomic
manner to /f_lush data. At the same time, FLAC space can be
atomically reclaimed by calling pfree .
3.1 Zero-Copy Caching
Heterogeneous page table. As Figure 2 shows, FLAC uses
the heterogeneous page table, a customized sub-level table
(e.g., one or multiple PUDs) of the kernel page table, to main-
tainFLAC space. The heterogeneity of the page table has two
meanings. 1)Page table entries (PTEs) belonging to the page
table are replicated in PM for fault recovery. 2)The address
indexed in the page table is dynamically mapped to DRAM
40DIMES ’23, October 23, 2023, Koblenz, Germany Yubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei Hu
or PM as the page is cached or evicted, and a bit in the PTE is
used to indicate the location of the page. The heterogeneous
page table uni /f_ies the page indexes of cache and persistent
storage and simpli /f_ies cache access and management.
Page attaching. The core technique of optimizing the data
transfer in FLAC is a new mechanism for virtual memory
management – page attaching. Given a piece of data, the
page attaching maps its source address (in either DRAM
cache or PM) to the destination address in the application
buﬀer without copying the data. Page attaching allows users
to set permissions ( e.g., read-only) on source and destination
addresses after remapping. Read/write operations can then
be implemented using page attach between the cache and
application bu ﬀer without copying. FLAC guarantees secure
data mapping and sharing under concurrent accesses by
enforcing read-only mapping and copy-on-write (COW).
In contrast to existing works that utilize page remapping to
realize zero-copy, as far as we know, FLAC is the /f_irst work to
use attaching to optimize data transfer between application
and/f_ile system cache. Furthermore, there are some unique
challenges in the /f_ile system cache to employ page remapping.
First, the remapping-based data transfer makes the page
have multiple versions, which needs to work with a new
cache management mechanism to ensure data consistency
and high concurrency (detailed in §3.2). Second, FLAC uses
COW page fault to ensure security and isolation when pages
are attached from/to FLAC space, which may bring extra
overhead in some workloads. Third, zero-copy introduces
the limitation of page alignment, which may reduce the
applicability of the /f_ile system implemented based on FLAC .
The second and third challenges are discussed in §6.
3.2 Parallel-Optimized Cache Management
FLAC requires a cache management mechanism for its multi-
version feature, while ensuring low “cache tax” impact. Exist-
ing cache frameworks execute cache /f_lushing and cache miss
handling with large synchronization overhead: Cache /f_lush-
ing locks the dirty pages until they are completely /f_lushed,
which blocks the foreground writes and reduces the perfor-
mance; cache miss handling blocks the foreground I/O until
the pages are loaded to DRAM cache. The multi-version fea-
ture and the heterogeneous page table design of FLAC give
us an opportunity to fully exploit the parallelism of these op-
erations with critical I/O paths. Figure 2 shows the examples
of cache /f_lushing and cache miss handling in FLAC .
2-Phase /f_lushing. FLAC splits the dirty pages /f_lushing into
two phases: collection phase ( pflush_add ) and persistence
phase ( pflush_commit ). The collection phase allocates a
fresh VA area as a temporary /f_lush bu ﬀer and attaches the
dirty pages to it. This phase requires a lock to prevent concur-
rent writes from modifying the mapping. Then, the persis-
tence phase stores the dirty pages to PM, which is lock-free
since there are no concurrent accesses to the temporarybuﬀer. Because the page mapping in the collection phase
is much faster than cross-memory copy, FLAC signi /f_icantly
reduces the blocking time on foreground writes. In addition,
the persistence phase is atomic – the consistency of pages
and persistent PTEs are protected by log-structured /f_lushing
and logging, respectively.
Asynchronous cache miss handling. Cache miss has less
impact on write operation because it does not require pages
to be loaded into the cache (except in the case of page mis-
alignment). Bene /f_iting from the heterogeneous page table,
FLAC can directly attach the PM pages to the read bu ﬀer
(and returns immediately) and handle the cache miss asyn-
chronously. A background thread in FLAC is responsible for
loading the cache missed pages to DRAM and remapping
page tables pointing to those PM pages to the cached pages
on DRAM. As a result, the overhead of handling cache misses
can be amortized in the background.
4 Case Study: File Read/Write on FLAC
We introduce a simple library /f_ile system based on FLAC to
show its usage and bene /f_its. As FLAC focuses on optimizing
data I/O, the prototype only implements read/write and a
few necessary metadata operations ( e.g., open and close).
Architecture. File data are stored in the FLAC space to
get bene /f_its from the /f_lat cache design, while metadata are
stored in the hash table on the normal shared memory space.
The inode table is stored on both the DRAM and PM. The
prototype borrows the page index design from ctFS [ 21],i.e.,
it allocates a segment of consecutive VAs (virtual memory
address) on FLAC space for each /f_ile, and the start VA and
size of the /f_ile are recorded in the corresponding metadata.
Read and Write. The read/write interface is similar
to that in the traditional POSIX /f_ile. The main parameters
include /f_ile handle, /f_ile oﬀset, access length, and read/write
buﬀer provided by the application. Currently, our prototype
assumes that the /f_ile oﬀset and read/write bu ﬀer are 4KB
page aligned (the applicability is discussed in §6). After open-
ing the /f_ile,read/write gets the start VA on FLAC space
from the /f_ile’s metadata and performs zcopy_from_flac
(read) / zcopy_to_flac (write) to transfer data. In addition,
the/f_ile system launches a background thread to periodically
/f_lush the dirty pages to PM by using the 2-Phase /f_lushing
mechanism (by calling pflush_add andpflush_commit ).
Compared to Related Works. FLAC allows /f_ile systems
based on it to bene /f_it from the DRAM cache while reducing
the e ﬀects of “cache tax” as much as possible We compare
FLAC to a wide range of existing works (shown in Table 2).
vs. Cache-based File Systems/mmap. There are many
/f_ile systems designed based on VFS. Although VFS page
cache can improve the performance in some scenarios in
heterogeneous memory /f_ile systems, these systems su ﬀer
from heavy “cache tax” and fail to optimize the application-
FS data transfer. In addition, some existing works optimize
41Cache or Direct Access? DIMES ’23, October 23, 2023, Koblenz, Germany
Table 2. Comparison with Related Works
Type Typical SystemData
CacheLow/Non Cache
Tax ImpactApp-Storage
Zero-CopyApp-Storage
Decouple
Cache-based FS VFS page cache FSes (ETX4, SPFS [30], etc.) /enc-34 /enc-37 /enc-37 /enc-34
Cache-based mmap mmap in VFS page cache FSes (EXT4, etc.) /enc-34 /enc-37 /enc-34 /enc-37
DAX-based FSNOVA [31], Strata [19], SplitFS [17], WineFS [16], ctFS [21],
KucoFS [2], PMFS [8], libnvmmio [3], EXT4-DAX [6], XFS-DAX, HTMFS [32]/enc-37 /enc-34 /enc-37 /enc-34
DAX-based RuntimeTwizzler [1], Mnemosyne [28], PMDK [13]
zIO [27], SubZero [18]/enc-37 /enc-34 /enc-34 /enc-37
Flat Cache FLAC-based FS /enc-34 /enc-34 /enc-34 /enc-34
DRAM page cache [ 22,23] for PM, but they are built on top
of the virtual memory subsystem and therefore fail to exploit
the full potential of cache. Although some cache-based /f_ile
systems also provide the mmap method to avoid the data
transfer overhead, it makes application design and storage
backend to be coupled, which is complementary to the /f_ile
semantics.
vs. DAX-based File Systems. DAX-based /f_ile systems by-
pass the DRAM cache in data I/O, making them su ﬀer from
high application-storage transfer overhead. Also, the latency
and concurrency of PM hardware greatly limit their per-
formance. In particular, some DAX-based /f_ile systems also
use remapping: SplitFS [ 17] proposes relink, an operation
to atomically move a contiguous extent from one /f_ile to an-
other, which is used to accelerate appends and atomic data
operations; ctFS [ 21] proposes pswap to swap the page map-
ping of two same-sized contiguous virtual addresses, which
is used to reduce the overhead of maintaining /f_ile data in
contiguous virtual addresses. However, neither SplitFS nor
ctFS uses remapping to optimize data copying between appli-
cations and /f_ile systems, and FLAC optimizes this part with
the zero-copy caching technique.
vs. DAX-based Runtime. This type of work usually pro-
vides a memory management library or programming frame-
work for applications. Although the overhead of data transfer
between the application and storage system can be avoided,
they require the application to be co-designed with the stor-
age backend ( e.g., use customized interfaces or object ab-
straction). Some of these works provide zero-copy PM I/O
libraries [ 18,27]. However, they require applications to allo-
cate read/write bu ﬀers on PM to avoid data copy, and thus
force to ship the data processing from DRAM to PM, which
is not friendly for some cases [ 29]. DAX-based runtime fo-
cuses on programming directly on PM and can be seen as
complementary to the /f_ile system.
5 Preliminary Results
We compare the read/write performance in FLAC with
cache-based ( FLAC and EXT4) and DAX-based (EXT4-DAX
and NOVA) /f_ile systems. FLAC and EXT4 are run on a hot
cache. The period of background /f_lushing is 10ms in FLAC
while it is 100ms in EXT4. The experiments are run on aserver with two Intel Xeon Platinum 8380 CPU @ 2.30GHz,
256GB RAM, and 1TB (128GB ×8) Intel 3DXPoint DCPMM.
Porting FLAC to more complicated applications and com-
paring with more recent /f_ile systems, such as ctFS [ 21], are
ongoing work.
Microbenchmark. The benchmark uses 2MB I/O to con-
currently and randomly write/read 64GB data on 64 non-
empty /f_iles (1GB per /f_ile), and no data contention in the
experiments. As Figure 3 (a) and (b) show, FLAC outper-
forms other tested systems by more than 25.9 times and 13.7
times in write and read, respectively. The zero-copy design
ofFLAC contributes to the main performance gain. In addi-
tion, data copy during background /f_lushing does not block
foreground writes, which signi /f_icantly reduces the impact
of background /f_lushing on performance in write-intensive
scenarios.
Real-World Application. We port tar(v1.34) to FLAC ,a
commonly used archiving application. Its main process reads
the input /f_ile, archives it, and writes the data to an output
/f_ile. The tarcontains little computation and represents an
I/O-intensive case. Figure 3 (c) plots the execution time of
archiving a /f_ile with increasing /f_ile size. On average, FLAC
improves NOVA, EXT4, and EXT4-DAX by 48.5%, 54%, and
41.2%, respectively.
6 Discussion and Future Work
Although FLAC promises attractive performance improve-
ment in data I/Os, it still leaves some limitations and open
challenges for our feature work.
1) Reduce COW page fault overhead. Data transfer be-
tween FLAC space and application bu ﬀer is implemented by
page attaching. After attaching, the source and destination
memory are set to read-only for security, which makes the
/f_irststore instruction to the source (write case) and desti-
nation (read case) memory after attaching to trigger a COW
page fault. Our analysis shows that TLB /f_lushing and data
copy are two of the main overheads in the COW page fault.
We have two ideas to reduce the COW page fault overhead:
The /f_irst idea is to /f_lush TLB in batch (the default is once
per page). The second idea is to provide a new interface that
allows the application to detach the original page mappings,
thus completely avoiding COW page fault.
42DIMES ’23, October 23, 2023, Koblenz, Germany Yubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei Hu
 130 200 270Execution Time (100 ms)(a) Random Write  0 3.5 7124816 Number of Threads 20 210 400Execution Time (100 ms)(b) Random Read 0 3.5 7124816 Number of Threads 0 20 40 60 80 1004K64K1M4M16M64MExecution Time (ms)File Size(c) tarEXT4-DAXNOVAEXT4FLAC
Figure 3. Micr obenchmark and Application Performance (Lower is better).
2) Improve the applicability. Zero-copy (page attach-
ing) can be performed only when the application bu ﬀer and
the target memory in FLAC are aligned. Currently, FLAC
asks applications to perform /f_ile accesses following this rule.
For example, one way to adapt an application to FLAC is
to ensure that the read/write bu ﬀer and target /f_ile oﬀset
are 4KB page aligned. To improve the applicability of FLAC ,
a possible solution is to provide a customized bu ﬀer man-
agement mechanism for applications. It allocates a bu ﬀer
larger (and aligned) than the required the /f_ile I/O size and
always perform /f_ile access as page aligned. In this way, the
buﬀer may contain more data than the application needs, so
the mechanism maintains a sliding window to represent the
valid data in the bu ﬀer, and the application calls an explicit
interface to move the window after each /f_ile I/O.
3)FLAC -optimized cache policy. FLAC permits more
powerful cache policies for heterogeneous memory. First,
because FLAC is embedded in the VM subsystem, it can be
aware of more memory access behavior about the applica-
tions ( e.g., allocation/free, reference count), which makes
it possible for FLAC to make better caching decisions. Sec-
ond, the lower layer of heterogeneous memory is fast and
byte-addressable, thus FLAC can investigate more trade-o ﬀs
between data locality and miss ratio. In future work, we aim
to collaborate these new insights brought by FLAC with tra-
ditional hotness-based methods to design an e ﬃcient cache
policy.
4) Ensure security. For data security, FLAC is imple-
mented in the kernel, and userspace applications can use
it only through syscalls. Pages are always mapped to the
application as read-only, which ensures that local operations
of the application do not a ﬀect the data in the cache and
other applications as they are handled by COW page fault.
Implementing a storage system on top of FLAC brings se-
curity considerations for metadata, which can be solved by
using the userspace security mechanisms ( e.g., MPK [ 11,26])
or putting metadata management in the kernel.
5) Ensure crash consistency. FLAC ensures that data
modi /f_ication operations ( pflush_commit andpfree ) are
atomic. However, along with /f_lushing the data, the /f_ile sys-
tem upon FLAC may need to update the related FS-levelmetadata ( e.g., page index) on PM. We plan to design a FS-
FLAC collaboration logging mechanism, which ensures that
data /f_lushing and FS-level metadata updates are in a trans-
action. The basic idea is to allow the /f_ile system to provide
the updated FS-level metadata to FLAC , and they are logged
with updated FLAC -level metadata in the same entry during
data modi /f_ication operations. The /f_ile system is also required
to overload the FS-level recovery function that FLAC calls
to commit the FS-level metadata log during recovery.
6) Space overhead. Compared to traditional page cache,
the multi-version design of FLAC does not incur additional
space overhead. In FLAC , the new version of the page is
created in the application runtime by COW page fault, so the
new version does not take up space in the page cache before it
is overwritten to FLAC . After overwriting, the virtual address
in the FLAC space is mapped to the new version, and the
old version is reclaimed. Furthermore, the zero-copy caching
design naturally brings the deduplication bene /f_it in some
cases ( e.g., the application reuses the write bu ﬀer and only
a small number of pages have been modi /f_ied). We plan to
leverage this advantage to improve the space e ﬃciency of
the DRAM cache and PM.
7 Conclusion
Heterogeneous memory requires innovations of e ﬀective
software architecture to maximize its potential of various
advantages. We analyze the shortcomings of existing cache-
based and DAX-based /f_ile systems, and conclude that DRAM
cache still has great potential in fast all-memory architec-
tures. We propose FLAC ,a/f_lat cache framework for hetero-
geneous memory that embeds the cache into virtual memory
management. FLAC unlocks the potential of cache through
two new techniques: zero-copy caching and parallel-optimized
cache management. We implement a /f_ile system prototype
based on FLAC and show that FLAC has signi /f_icantly better
performance than existing cache and DAX solutions.
Acknowledgments
We thank the anonymous reviewers for their constructive
comments and feedback. We also thank our colleagues in
the Huawei OS Kernel Lab for their help.
43Cache or Direct Access? DIMES ’23, October 23, 2023, Koblenz, Germany
References
[1]Daniel Bittman, Peter Alvaro, Pankaj Mehra, Darrell D. E. Long, and
Ethan L. Miller. 2020. Twizzler: A Data-Centric OS for Non-Volatile
Memory. In Proceedings of the USENIX Annual Technical Conference
(ATC’20) .
[2]Youmin Chen, Youyou Lu, Bohong Zhu, Andrea C. Arpaci-Dusseau,
Remzi H. Arpaci-Dusseau, and Jiwu Shu. 2021. Scalable Persistent
Memory File System with Kernel-Userspace Collaboration. In Proceed-
ings of USENIX Conference on File and Storage Technologies (FAST’21) .
[3]Jungsik Choi, Jaewan Hong, Youngjin Kwon, and Hwansoo Han. 2020.
Libnvmmio: Reconstructing Software IO Path with Failure-Atomic
Memory-Mapped Interface. In Proceedings of the USENIX Annual Tech-
nical Conference (ATC’20) .
[4]CXL Consortium. 2023. Compute Express Link Speci /f_ication Revision
3.0.h/t_tps://www.computeexpresslink.org/download-the-specification
[5]Gen-Z Consortium. 2022. Gen-Z Final Speci /f_ications. h/t_tps:
//genzconsortium.org/specifications/
[6]Johnathan Corbet. 2020. EXT4-DAX. h/t_tps://lwn.net/Articles/717953
[7]Peter Druschel and Larry L. Peterson. 1993. Fbufs: A High-Bandwidth
Cross-Domain Transfer Facility. In Proceedings of the ACM Symposium
on Operating Systems Principles (SOSP’93) .
[8]Subramanya R. Dulloor, Sanjay Kumar, Anil Keshavamurthy, Philip
Lantz, Dheeraj Reddy, Rajesh Sankaran, and Je ﬀJackson. 2014. Sys-
tem Software for Persistent Memory. In Proceedings of the Eleventh
European Conference on Computer Systems (EuroSys’14) .
[9]Subramanya R. Dulloor, Amitabha Roy, Zheguang Zhao, Narayanan
Sundaram, Nadathur Satish, Rajesh Sankaran, Je ﬀJackson, and Karsten
Schwan. 2016. Data Tiering in Heterogeneous Memory Systems. In Pro-
ceedings of the European Conference on Computer Systems (EuroSys’16) .
[10] Sangjin Han, Scott Marshall, Byung-Gon Chun, and Sylvia Ratnasamy.
2012. MegaPipe: A New Programming Interface for Scalable Network
I/O. In Proceedings of the USENIX Conference on Operating Systems
Design and Implementation (OSDI’12) .
[11] Mohammad Hedayati, Spyridoula Gravani, Ethan Johnson, John
Criswell, Michael L. Scott, Kai Shen, and Mike Marty. 2019. Hodor:
Intra-Process Isolation for High-Throughput Data Plane Libraries. In
Proceedings of the USENIX Annual Technical Conference (ATC’19) .
[12] Intel. 2022. 3D XPoint Breakthrough Non-Volatile Memory.
h/t_tps://www.intel.com/content/www/us/en/architecture-and-
technology/intel-micron-3d-xpoint-webcast.html
[13] Intel. 2022. Persistent Memory Development Kit. h/t_tps://pmem.io/
pmdk
[14] Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amir-
saman Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R.
Dulloor, Jishen Zhao, and Steven Swanson. 2019. Basic Performance
Measurements of the Intel Optane DC Persistent Memory Module.
CoRR (2019). h/t_tps://doi.org/arXiv:1903.05714
[15] Myoungsoo Jung. 2023. Hello bytes, bye blocks: PCIe storage meets
compute express link for memory expansion (CXL-SSD). In Proceedings
of Conference on Hot Topics in Storage and File Systems (HotStorage’23) .
[16] Rohan Kadekodi, Saurabh Kadekodi, Soujanya Ponnapalli, Harshad
Shirwadkar, Gregory R. Ganger, Aasheesh Kolli, and Vijay Chi-
dambaram. 2021. WineFS: A Hugepage-Aware File System for Persis-
tent Memory That Ages Gracefully. In Proceedings of the ACM Sympo-
sium on Operating Systems Principles (SOSP’21) .
[17] Rohan Kadekodi, Se Kwon Lee, Sanidhya Kashyap, Taesoo Kim,
Aasheesh Kolli, and Vijay Chidambaram. 2019. SplitFS: Reducing
Software Overhead in File Systems for Persistent Memory. In Proceed-
ings of the ACM Symposium on Operating Systems Principles (SOSP’19) .
[18] Juno Kim, Yun Joon Soh, Joseph Izraelevitz, Jishen Zhao, and Steven
Swanson. 2020. SubZero: Zero-copy IO for Persistent Main Mem-
ory File Systems. In Proceedings of Asia-Paci /f_ic Workshop on Systems
(APSys’20) .[19] Youngjin Kwon, Henrique Fingler, Tyler Hunt, Simon Peter, Emmett
Witchel, and Thomas Anderson. 2017. Strata: A cross media /f_ile system.
InProceedings of the ACM Symposium on Operating Systems Principles
(SOSP’17) .
[20] Benjamin C. Lee, Ping Zhou, Jun Yang, Youtao Zhang, Bo Zhao, Engin
Ipek, Onur Mutlu, and Doug Burger. 2010. Phase-Change Technology
and the Future of Main Memory. IEEE Micro 30, 1 (2010), 143–143.
[21] Ruibin Li, Xiang Ren, Xu Zhao, Siwei He, Michael Stumm, and Ding
Yuan. 2022. ctFS: Replacing File Indexing with Hardware Memory
Translation through Contiguous File Allocation for Persistent Memory.
InProceedings of USENIX Conference on File and Storage Technologies
(FAST’22) .
[22] Yubo Liu, Hongbo Li, Yutong Lu, Zhiguang Chen, Nong Xiao, and Ming
Zhao. 2020. HasFS: optimizing /f_ile system consistency mechanism on
NVM-based hybrid storage architecture. Cluster Computing 23 (2020),
2510–2515.
[23] Jiaxin Ou, Jiwu Shu, and Youyou Lu. 2016. A High Performance File
System for Non-Volatile Main Memory. In Proceedings of the Eleventh
European Conference on Computer Systems (EuroSys’16) .
[24] Vivek S. Pai, Peter Druschel, and Willy Zwaenepoel. 1999. IO-Lite:
A Uni /f_ied I/O Bu ﬀering and Caching System. In Proceedings of the
USENIX Conference on Operating Systems Design and Implementation
(OSDI’99) .
[25] Yuxin Ren, Gabriel Parmer, Teo Georgiev, and Gedare Bloom. 2016.
CBufs: E ﬃcient, System-Wide Memory Management and Sharing. In
Proceedings of the ACM SIGPLAN International Symposium on Memory
Management (ISMM’16) .
[26] Vasily A. Sartakov, Lluís Vilanova, and Peter Pietzuch. 2021. CubicleOS:
A Library OS with Software Componentisation for Practical Isolation.
InProceedings of the International Conference on Architectural Support
for Programming Languages and Operating Systems (ASPLOS’21) .
[27] Timothy Stamler, Deukyeon Hwang, Amanda Raybuck, Wei Zhang,
and Simon Peter. 2022. zIO: Accelerating IO-Intensive Applications
with Transparent Zero-Copy IO. In Proceedings of the USENIX Confer-
ence on Operating Systems Design and Implementation (OSDI’22) .
[28] Haris Volos, Andres Jaan Tack, and Michael M. Swift. 2011.
Mnemosyne: Lightweight Persistent Memory. In Proceedings of the
International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS’11) .
[29] Yongfeng Wang, Yinjin Fu, Yubo Liu, Zhiguang Chen, and Nong Xiao.
2022. Characterizing and Optimizing Hybrid DRAM-PM Main Mem-
ory System with Application Awareness. In Proceedings of Design,
Automation & Test in Europe Conference & Exhibition (DATE’22) .
[30] Hobin Woo, Daegyu Han, Seungjoon Ha, Sam H. Noh, and Beomseok
Nam. 2023. On Stacking a Persistent Memory File System on Legacy
File Systems. In Proceedings of USENIX Conference on File and Storage
Technologies (FAST’23) .
[31] Jian Xu and Steven Swanson. 2016. NOVA: A Log-Structured File Sys-
tem for Hybrid Volatile/Non-Volatile Main Memories. In Proceedings
of USENIX Conference on File and Storage Technologies (FAST’16) .
[32] Jifei Yi, Mingkai Dong, Fangnuo Wu, and Haibo Chen. 2022. HTMFS:
Strong Consistency Comes for Free with Hardware Transactional
Memory in Persistent Memory File Systems. In Proceedings of USENIX
Conference on File and Storage Technologies (FAST’22) .
44Memory Disaggregation: Advances and Open Challenges
Hasan Al Maruf, Mosharaf Chowdhury
SymbioticLab, University of Michigan
Abstract
Compute and memory are tightly coupled within each server
in traditional datacenters. Large-scale datacenter operators
have identi ￿ed this coupling as a root cause behind ￿eet-
wide resource underutilization and increasing Total Cost of
Ownership (TCO). With the advent of ultra-fast networks
and cache-coherent interfaces, memory disaggregation has
emerged as a potential solution, whereby applications can
leverage available memory even outside server boundaries.
This paper summarizes the growing research landscape
of memory disaggregation from a software perspective and
introduces the challenges toward making it practical un-
der current and future hardware trends. We also re ￿ect on
our seven-year journey in the SymbioticLab to build a com-
prehensive disaggregated memory system over ultra-fast
networks. We conclude with some open challenges toward
building next-generation memory disaggregation systems
leveraging emerging cache-coherent interconnects.
1Introduction
Modern datacenter applications – low-latency online ser-
vices, big data analytics, and AI/ML workloads alike – are of-
ten memory-intensive. As the number of users increases and
we collect more data in cloud datacenters, the overall mem-
ory demand of these applications continue to rise. Despite
their performance bene ￿ts, memory-intensive applications
experience disproportionate performance loss whenever their
working sets do not completely ￿t in the available memory.
For instance, our measurements across a range of memory-
intensive applications show that if half their working sets do
not￿t in memory, performance can drop by 8⇥to25⇥[22].
Application developers often sidestep such disasters by
over-allocating memory, but pervasive over-allocation in-
evitably leads to datacenter-scale memory underutilization.
Indeed, memory utilization at many hyperscalers hovers
around 40%–60% [ 1,22,25,40]. Service providers running on
public clouds, such as Snow ￿ake, report 70%–80% underuti-
lized memory on average [ 48]. Since DRAM is a signi ￿cant
driver of infrastructure cost and power consumption [ 33],
excessive underutilization leads to high TCO.
At the same time, increasing the e ￿ective memory capac-
ity and bandwidth of each server to accommodate ever-larger
working sets is challenging as well. In fact, memory band-
width is a bigger bottleneck than memory capacity today as
the former increases at a slower rate. For example, to increase
memory bandwidth by 3.6⇥in their datacenters, Meta hadto increase capacity by 16⇥[33]. To provide su ￿cient mem-
ory capacity and/or bandwidth, computing and networking
resources become stranded in traditional server platforms,
which eventually causes ￿eet-wide resource underutilization
and increases TCO.
Memory disaggregation addresses memory-related rightsiz-
ing problems at both software and hardware levels. Applica-
tions are able to allocate memory as they need without being
constrained by server boundaries. Servers are not forced to
add more computing and networking resources when they
only need additional memory capacity or bandwidth. By ex-
posing all unused memory across all the servers as a memory
pool to all memory-intensive applications, memory disag-
gregation can improve both application-level performance
and overall memory utilization. Multiple hardware vendors
and hyperscalers have projected [ 9,10,28,33] up to 25%
TCO savings without a ￿ecting application performance via
(rack-scale) memory disaggregation.
While the idea of leveraging remote machines’ memory
is decades old [ 15,18,20,29,31,35], only during the past
few years, the latency and bandwidth gaps between memory
and communication technologies have come close enough
to make it practical. The ￿rst disaggregated memory1solu-
tions (In ￿niswap [ 22] and the rest) leveraged RDMA over
In￿niBand or Ethernet, but they are an order-of-magnitude
slower than local memory. To bridge this performance gap
and to address practical issues like performance isolation,
resilience, scalability, etc., we have built a comprehensive set
of software solutions. More recently, with the rise of cache-
coherent Compute Express Link (CXL) [ 3] interconnects and
hardware protocols, the gap is decreasing even more. We are
at the cusp of taking a leap toward next-generation software-
hardware co-designed disaggregated memory systems.
This short paper is equal parts a quick tutorial, a retro-
spective on the In ￿niswap project summarizing seven years’
worth of research, and a non-exhaustive list of future predic-
tions based on what we have learned so far.
2Memory Disaggregation
Simply put, memory disaggregation exposes memory capac-
ity available in remote locations as a pool of memory and
shares it across multiple servers over the network. It decou-
ples the available compute and memory resources, enabling
independent resource allocation in the cluster. A server’s
1Remote memory and far memory are often used interchangeably with the
term disaggregated memory.arXiv:2305.03943v1  [cs.DC]  6 May 2023Compute BladesMemory BladesCPUCacheCPUCacheCPUCacheNetworkDRAMPMEMDRAMDRAMDRAMDRAM(a) Physically DisaggregatedMonolithic ServerCPUMemoryNICDiskOSMonolithic ServerCPUMemoryNICDiskOSNetwork(b) Logically Disaggregated
Figure 1: Physical vs. logical disaggregation architectures.
local and remote memory together constitute its total physi-
cal memory. An application’s locality of memory reference
allows the server to exploit its fast local memory to maintain
high performance, while remote memory provides expanded
capacity with an increased access latency that is still orders-
of-magnitude faster than accessing persistent storage (e.g.,
HDD, SSD). The OS and/or application runtime provides the
necessary abstractions to expose all the available memory in
the cluster, hiding the complexity of setting up and access-
ing remote memory (e.g., connection setup, memory access
semantics, network packet scheduling, etc.) while providing
resilience, isolation, security, etc. guarantees.
2.1 Architectures
Memory disaggregation systems have two primary cluster
memory architectures.
Physical Disaggregation. In a physically-disaggregated
architecture, compute and memory nodes are detached from
each other where a cluster of compute blades are connected
to one or more memory blades through network (e.g., PCIe
bridge) [ 30] (Figure 1a). A memory node can be a traditional
monolithic server with low compute resource and large mem-
ory capacity, or it can be network-attached DRAM. For better
performance, the compute nodes are usually equipped with
a small amount of memory for caching purposes.
Logical Disaggregation. In a logically-disaggregated ar-
chitecture, traditional monolithic servers hosting both com-
pute and memory resources are connected to each other
through the network (e.g., In ￿niband, RoCEv2) (Figure 1b).
This is a popular approach for building a disaggregated mem-
ory system because one does not need to change existing
hardware architecture; simply incorporating appropriate
software to provide a remote memory interface is su ￿cient.
In such a setup, usually, each of the monolithic servers hasTable 1: Selected memory disaggregation proposals.
Abstraction SystemHardware
TransparentOS
TransparentApplication
Transparent
Virtual
Memory
Management
(VMM)Global Memory [19] Yes No Yes
Memory Blade [30] No No Yes
In￿niswap [22] Yes Yes Yes
Leap [32] Yes No Yes
LegoOS [44] Yes No Yes
zSwap [25] Yes No Yes
Kona [14] Yes No Yes
Fastswap [12] Yes No Yes
Hydra [27] Yes Yes Yes
Virtual File
System (VFS)Memory Pager [31] Yes Yes No
Remote Regions [11] Yes Yes No
Custom
APIFaRM [17] Yes Yes No
FaSST [24] Yes Yes No
Memtrade [34] Yes Yes No
Programming
RuntimeAIFM [42] Yes Yes No
Semeru [49] Yes Yes No
their own OS. In some cases, the OS itself can be disaggre-
gated across multiple hosts [ 44]. Memory local to a host is
usually prioritized for running local jobs. Unutilized memory
on remote machines can be pooled and exposed to the cluster
as remote [14, 22, 27, 32, 34, 42, 44].
Hybrid Approach. Cache-coherent interconnects like
CXL provides the opportunity to build a composable hetero-
geneous memory systems that combine logical and physi-
cal disaggregation approaches. Multiple monolithic servers,
compute devices, memory nodes, or network specialized de-
vices can be connected through fabric or switches where soft-
ware stacks can provide the cache-line granular or traditional
virtual memory-based disaggregated memory abstraction.
2.2 Abstractions and Interfaces
Interfaces to access disaggregated memory can either be
transparent to the application or need minor to complete
re-write of applications (Table 1). The former has broader
applicability, while the latter might have better performance.
Application-Transparent Interface. Access to remote
disaggregated memory without signi ￿cant application
rewrites typically relies on two primary mechanisms: disag-
gregated Virtual File System (VFS) [ 11], that exposes remote
memory as ￿les and disaggregated Virtual Memory Man-
ager (VMM) for remote memory paging [ 22,27,32,44]. In
both cases, data is communicated in small chunks or pages
(typically, 4KB). In case of remote memory as ￿les, pages
go through the ￿le system before they are written to/read
from the remote memory. For remote memory paging and
distributed OS, page faults cause the VMM to write pages to
and read them from the remote memory. Remote memory
paging is more suitable for traditional applications because
it does not require software or hardware modi ￿cations.
Non-Transparent Interface. Another approach is to di-
rectly expose remote memory through custom API (KV-
store, remote memory-aware library or system calls) and
2CacheMain MemoryCXL-Memory(DDR, LPDDR, NVM, ...)Network-Attached MemorySSDHDDRegister0.2ns1-40ns80-140ns170-400ns2-4μs10-40μs3-10msFigure 2: Latency pro ￿le of di ￿erent memory technologies.
modify the applications incorporating these speci ￿c APIs
[17,24,34,41,42,49]. All the memory (de)allocation, trans-
actions, synchronizations, etc. operations are handled by
the underlying implementations of these APIs. Performance
optimizations like caching, local-vs-remote data placement,
prefetching, etc. are often handled by the application.
2.3 Challenges in Practical Memory Disaggregation
Simply relying on fast networks or interconnects is not su ￿-
cient to practical memory disaggregation. A comprehensive
solution must address challenges in multiple dimensions:
•High Performance. A disaggregated memory system in-
volves the network in its remote memory path, which is
at least an order-of-magnitude slower than memory chan-
nels attached to CPU and DRAM (80–140 nanoseconds vs.
microseconds; see Figure 2). Hardware-induced remote
memory latency is signi ￿cant and impacts application
performance [ 22,32,33]. Depending on the abstraction,
software stacks can also introduce signi ￿cant overheads.
For example, remote memory paging over existing VMM
can add tens of microseconds latency for a 4KB page [ 32].
•Performance Isolation. When multiple applications
with di ￿erent performance requirements (e.g., latency-
vs. bandwidth-sensitive workloads) compete for disaggre-
gated memory, depending on where the applications are
running and where the remote memory is located, they
may be contending for resources inside the server, on the
NIC, and in the network on the hardware side and variety
of resources in the application runtimes and OSes. This
is further exacerbated by the presence of multiple tiers of
memory with di ￿erent latency-bandwidth characteristics.
•Memory Heterogeneity. Memory hierarchy within a
server is already heterogeneous (Figure 2). Disaggregated
memory – both network-attached and emerging CXL
memory [ 21,28,33] – further increases heterogeneity
in terms of latency-bandwidth characteristics. In such a
setup, simply allocating memory to applications is not
enough. Instead, decisions like how much memory to al-
locate in which tier at what time is critical as well.•Resilience to Expanded Failure Domains. Applica-
tions relying on remote memory become susceptible to
new failure scenarios such as independent and correlated
failures of remote machines, evictions from and corrup-
tions of remote memory, and network partitions. They also
su￿er from stragglers or late-arriving remote responses
due to network congestion and background tra ￿c[16].
These uncertainties can lead to catastrophic failures and
service-level objective (SLO) violations.
•E￿ciency and Scalability. Disaggregated memory sys-
tems are inherently distributed. As the number of memory
servers, the total amount of disaggregated memory, and
the number of applications increase, the complexity of
￿nding unallocated remote memory in a large cluster, allo-
cating them to applications without violating application-
speci ￿c SLOs, and corresponding meta-data overhead of
memory management increase as well. Finding e ￿cient
matching at scale is necessary for high overall utilization.
•Security. Although security of disaggregated memory is
often sidestepped within the con ￿nes of a private datacen-
ter, it is a major challenge for memory disaggregation in
public clouds. Since data residing in remote memory may
be read by entities without proper access, or corrupted
from accidents or malicious behavior, the con ￿dentiality
and integrity of remote memory must be protected. Addi-
tional concerns include side channel and remote rowham-
mer attacks over the network [ 45,46], distributed coor-
dinated attacks, lack of data con ￿dentiality and integrity
and client accountability during CPU bypass operations
(e.g., when using RDMA for memory disaggregation).
3In￿niswap: A Retrospective
To the best of our knowledge, In ￿niswap is the ￿rst memory
disaggregation system with a comprehensive and cohesive
set of solutions for all the aforementioned challenges. It ad-
dresses host-level, network-level, and end-to-end aspects of
practical memory disaggregation over RDMA. At a high level,
In￿niswap provides a paging-based remote memory abstrac-
tion that can accommodate any application without changes,
while providing a high-performance yet resilient, isolated,
and secure data path to remote disaggregated memory.
Bootstrapping. Our journey started in 2016, when we
simply focused on building an application-transparent in-
terface to remote memory that are distributed across many
servers. In ￿niswap [ 22] transparently exposed remote disag-
gregated memory through paging without any modi ￿cations
to applications, hardware, or OSes of individual servers. It em-
ployed a block device with traditional I/O interface to VMM.
The block device divided its whole address space into smaller
slabs and transparently mapped them across many servers’
3remote memory. In ￿niswap captured 4KB page faults in run-
time and redirected them to remote memory using RDMA.
From the very beginning, we wanted to design a system
that would scale without losing e ￿ciency down the line.
To this end, we designed decentralized algorithms to iden-
tify free memory, to distribute memory slabs, and to evict
slabs for memory reclamation. This removed the overhead of
centralized meta-data management without losing e ￿ciency.
Improving Performance. In￿niswap’s block layer-
based paging caused high latency overhead during remote
memory accesses. This happens because Linux VMM is not
optimized for microsecond-scale operations. We gave up one
degree of freedom and designed Leap [ 32] in 2018 – we opti-
mized the OS for remote memory data path by identifying
and removing non-critical operations while paging.
Even with the leanest data path, a reactive page fetch-
ing system must su ￿er microsecond-scale network latency
on the critical path. Leap introduced a remote memory
prefetcher to proactively bring in the correct pages into a
local cache to provide sub-microsecond latency (comparable
to that of a local page access) on cache hits.
Providing Resilience. In￿niswap originally relied on lo-
cal disks to tolerate remote failures, which resulted in slow
failure recovery. Maintaining multiple in-memory replicas
was not an option either as it e ￿ectively halved the total
capacity. We started exploring erasure coding as a memory-
e￿cient alternative. Speci ￿cally, we divided each page into
:splits to generate Aencoded parity splits and spread the
(:+A)splits to (:+A)failure domains – any :out of (:+A)
splits would then su ￿ce to decode the original data. How-
ever, erasure coding was traditionally applied to large ob-
jects [ 38]. By 2019/20, we built Hydra [ 27] whose carefully
designed data path could perform online erasure coding
within a single-digit microsecond tail latency. Hydra also
introduced CodingSets, a new data placement scheme that
balanced availability and load balancing, while reducing the
probability of data loss by an order of magnitude even under
large correlated failures.
Multi-Tenancy Issues. We observed early on (circa
2017) that accessing remote memory over a shared network
su￿ers from contention in the NIC and inside the network
[54]. While our optimized data paths in Leap and Hydra
could address some of the challenges inside the host, they
did not extend to resource contentions in the RDMA NIC
(RNIC). We designed Justitia [ 56] in 2020 to improve the
network bottleneck in RNICs by transparently monitoring
the latency pro ￿les of each application and providing per-
formance isolation. More recently, we have looked into im-
proving Quality-of-Service (QoS) inside the network as well
[55].CPU0CPU1Interconnect32 GB/s perlinkDRAMDRAM38.4 GB/s per channel~100 ns~180 ns(a) Without CXLCPU0CXL64 GB/s per x16 linkDRAMDRAM~100 ns~170-250 ns38.4 GB/s per channel(b) With CXL on PCIe 5.0
Figure 3: A CXL system compared to a dual-socket server.
Expanding to Public Clouds. While In ￿niswap and re-
lated projects were designed for cooperative private data-
centers, memory disaggregation in public clouds faces addi-
tional concerns. In 2021, we ￿nished designing Memtrade
[34] to harvest all the idle memory within virtual machines
(VMs) – be it unallocated, or allocated to an application but
infrequently utilized, and exposed them to a disaggregated
memory marketplace. Memtrade allows producer VMs to
lease their idle application memory to remote consumer VMs
for a limited period of time while ensuring con ￿dentiality
and integrity. It employs a broker to match producers with
consumers while satisfying performance constraints.
Detours Along the Way. Throughout this journey, we
collaborated on side quests like designing a decentralized
resource management algorithm using RDMA primitives
[51], meta-data management inside the network using pro-
grammable switches [ 53],￿ne-grained compute disaggrega-
tion [ 52] etc. Some of our forays into designing hardware
support were nipped in the bud, often because we could
not￿nd the right partners. In hindsight, perhaps we were
fortunate given how quickly the industry converged on CXL.
Summing it Up. In￿niswap along with all its extensions
can provide near-memory performance for most memory-
intensive applications even when 75% and sometimes more of
their working sets reside in remote memory in an application-
and hardware-transparent manner, in the presence of failures,
load imbalance, and multiple tenants. After seven years, we
declared victory on this chapter in 2022.
4Hardware Trend: Cache-Coherent
Interconnects
Although networking technologies like In ￿niBand and Eth-
ernet continue to improve, their latency remain considerably
4Device–Samsung’s1stgeneration CXL Memory ExpanderCPU–Intel Sapphire Rapids w/ CXL 1.1CPU–AMD Genoa w/ CXL 1.1CPU–NVIDIA Grace w/ CXL 2.0CPU–AmpereOne-2 w/ CXL 2.0 on PCIe 5.0CPU–Intel Diamond Rapids w/ CXL3.0 on PCIe 6.0Device–1stgen memory pooling controllersCPU–AmpereOne-3 w/ CXL2.0 on PCIe 6.020221stHalf20222ndHalf202320242026Figure 4: CXL roadmap paves the way for memory pooling and disaggregation in next-generation datacenter design.
high for providing a cache-coherent memory address space
across disaggregated memory devices. CXL (Compute Ex-
press Link) [ 3] is a new processor-to-peripheral/accelerator
cache-coherent interconnect protocol that builds on and
extends the existing PCIe protocol by allowing coherent
communication between the connected devices.2It provides
byte-addressable memory in the same physical address space
and allows transparent memory allocation using standard
memory allocation APIs. It also allows cache-line granularity
access to the connected devices and underlying hardware
maintains cache-coherency and consistency. With PCIe 5.0,
CPU-to-CXL interconnect bandwidth is similar to the cross-
socket interconnects (Figure 3) on a dual-socket machine [ 57].
CXL-Memory access latency is also similar to the NUMA ac-
cess latency. CXL adds around 50-100 nanoseconds of extra
latency over normal DRAM access.
CXL Roadmap. Today, CXL-enabled CPUs and memory
devices support CXL 1.0/1.1 (Figure 4) that enables a point-to-
point link between CPUs and accelerator memory or between
CPUs and memory extenders. CXL 2.0 spec enables one-hop
switching that allows multiple accelerators without ( Type-
1 device ) or with memory ( Type-2 device ) to be con ￿gured
to a single host and have their caches be coherent to the
CPUs. It also allows memory pooling across multiple hosts
using memory expanding devices ( Type-3 device ). A CXL
switch has a fabric manager (it can be on-board or external)
that is in charge of the device address-space management.
Devices can be hot-plugged to the switch. A virtual CXL
switch partitions the CXL-Memory and isolate the resources
between multiple hosts. It provides telemetry for load on each
connected devices for load balancing and QoS management.
CXL 3.0 adds multi-hop hierarchical switching – one can
have any complex types of network through cascading and
fan-out. This expands the number of connected devices and
the complexity of the fabric to include non-tree topologies,
like Spine/Leaf, mesh- and ring-based architectures. CXL 3.0
supports PCIe 6.0 (64 GT/s i.e., up to 256 GB/s of throughput
for a x16 duplex link) and expand the horizon of very com-
plex and composable rack-scale server design with varied
2Prior industry standards in this space such as CCIX [ 2], OpenCAPI [ 8],
Gen-Z [ 5] etc. have all come together under the banner of CXL consortium.
While there are some related research proposals (e.g., [ 26]), CXL is the de
facto industry standard at the time of writing this paper.GFAMGFAMGFAMNICNICCXL SwitchCXL SwitchCXL SwitchCPUMemoryCPUMemoryCPUAcceleratorCXL SwitchCXL SwitchCXL Switch
Figure 5: CXL 3.0 enables a rack-scale server design with
complex networking and composable memory hierarchy.
memory technologies (Figure 5). A new Port-Based Routing
(PBR) feature provides a scalable addressing mechanism that
supports up to 4,096 nodes. Each node can be any of the
existing three types of devices or the new Global Fabric At-
tached Memory (GFAM) device that supports di ￿erent types
of memory (i.e., Persistent Memory, Flash, DRAM, other fu-
ture memory types, etc.) together in a single device. Besides
memory pooling, CXL 3.0 enables memory sharing across
multiple hosts on multiple end devices. Connected devices
(i.e., accelerators, memory expanders, NICs, etc.) can do peer-
to-peer communicate bypassing the host CPUs.
In essence, CXL 3.0 enables large networks of memory
devices. This will proliferate software-hardware co-designed
memory disaggregation solutions that not only simplify and
better implement previous-generation disaggregation solu-
tions (e.g., In ￿niswap) but also open up new possibilities.
5Disaggregation Over Intra-Server CXL
With the emergence of new hardware technologies comes the
opportunity to rethink and revisit past design decisions, and
CXL is no di ￿erent. Earlier software solutions for memory
disaggregation over RDMA are not optimized enough in CXL-
based because of its much lower latency bound, especially
for intra-server CXL (CXL 1.0/1.1) with 100s of nanoseconds
latency. Recent works in leveraging CXL 1.0/1.1 within a
server have focused on (tiered) memory pooling [ 28,33] be-
cause a signi ￿cant portion of datacenter application working
sets can be o ￿oaded to a slower-tier memory without ham-
pering performance [ 25,33,34]. We have recently worked
on two fundamental challenges in this context.
Memory Usage Characterization. Datacenter applica-
tions have diverse memory access latency and bandwidth re-
quirements. Sensitivity toward di ￿erent memory page types
5can also vary across applications. Understanding and charac-
terizing such behaviors is critical to designing heterogeneous
tiered-memory systems. Chameleon [ 33] is a lightweight
user-space memory access behavior characterization tool
that can readily be deployed in production without disrupt-
ing running application(s) or modifying the OS. It utilizes the
Precise Event-Based Sampling (PEBS) mechanism of mod-
ern CPU’s Performance Monitoring Unit (PMU) to collect
hardware-level performance events related to memory ac-
cesses. It then generates a heat-map of memory usage for
di￿erent page types and provides insights into an applica-
tion’s expected performance with multiple temperature tiers.
Memory Management. Given applications’ page char-
acterizations, TPP [ 33] provides an OS-level transparent page
placement mechanism, to e ￿ciently place pages in a tiered-
memory system. TPP has three components: (a)a lightweight
reclamation mechanism to demote colder pages to the slow
tier; (b)decoupling the allocation and reclamation logic for
multi-NUMA systems to maintain a headroom of free pages
on the fast tier; and (c)a reactive page promotion mecha-
nism that e ￿ciently identi ￿es hot pages trapped in the slow
memory tier and promote them to the fast memory tier to
improve performance. It also introduces support for page
type-aware allocation across the memory tiers.
6CXL-Disaggregated Memory at Rack-Scale
and Beyond: Open Challenges
Although higher than intra-server CXL latency, rack-scale
CXL systems with a CXL switch (CXL 2.0) will experience
much lower latency than RDMA-based memory disaggre-
gation. With a handful of hops in CXL 3.0 setups, latency
will eventually reach a couple microseconds similar to that
found in today’s RDMA-based disaggregated memory sys-
tems. For next-generation memory disaggregation systems
that operate between these two extremes, i.e., rack-scale and
a little beyond, many open challenges exist. We may even
have to revisit some of our past design decisions (§2). Here
we present a non-exhaustive list of challenges informed by
our experience.
6.1 Abstractions
Memory Access Granularity. CXL enables cache-line
granular memory access over the connected devices, whereas
existing OS VMM modules are designed for page-granular
(usually, 4KB or higher) memory access. Throughout their
lifetimes, applications often write a small part of each page;
typically only 1-8 cache-lines out of 64 [ 14]. Page-granular
access causes large dirty data ampli ￿cation and bandwidth
overuse. In contrast, ￿ne-grained memory access over a large
memory pool causes high meta-data management overhead.
Based on an application’s memory access patterns, remotememory abstractions should support transparent and dy-
namic adjustments to memory access granularity.
Memory-QoS Interface. Traditional solutions for mem-
ory page management focus on tracking (a subset of) pages
and counting accesses to determine the heat of the page and
then moving pages around. While this is enough to provide
a two-level, hot-vs-cold QoS, it cannot capture the entire
spectrum of page temperature. Potential solutions include
assigning a QoS level to (1) an entire application; (2) individ-
ual data structures; (3) individual mmap() calls; or even (4)
individual memory accesses. Each of these approaches have
their pros and cons. At one extreme, assigning a QoS level
to an entire application maybe simple, but it cannot capture
time-varying page temperature of large, long-running ap-
plications. At the other end, assigning QoS levels to individ-
ual memory accesses requires recompilation of all existing
applications as well as cumbersome manual assignments,
which can lead to erroneous QoS assignments. A combina-
tion of aforementioned approaches may reduce developer’s
overhead while providing su ￿cient ￿exibility to perform
spatiotemporal memory QoS management.
6.2 Management and Runtime
Memory Address Space Management. From CXL 2.0
onward, devices can be hot-plugged to the CXL switches.
Device-attached memory is mapped to the system’s coherent
address space and accessible to host using standard write-
back semantics. Memory located on a CXL device can either
be mapped as Host-managed Device Memory (HDM) or Pri-
vate Device Memory (PDM). To update the memory address
space for connected devices to di ￿erent host devices, a sys-
tem reset is needed; tra ￿c towards the device needs to stop
to alter device address mapping during this reset period.
An alternate solution to avoid this system reset is to map
the whole physical address space to each host when a CXL-
device is added to the system. The VMM or fabric manager
in the CXL switch will be responsible to maintain isolation
during address-space management. How to split the whole
address-space in to sizable memory blocks for the e ￿cient
physical-to-virtual address translation of a large memory
network is an interesting challenge [26, 53].
Uni￿ed Runtime for Compute Disaggregation. CXL
Type-2 devices (accelerator with memory) maintains cache
coherency with the CPU. CPU and Type-2 devices can inter-
changeably use each other’s memory and both get bene ￿ted.
For example, applications that run on CPUs can bene ￿t as
they can now access very high bandwidth GPU memory.
Similarly, for GPU users, it is bene ￿cial for capacity expan-
sion even though the memory bandwidth to and from CPU
memory will be lower. In such a setup, remote memory ab-
stractions should track the availability of compute cores and
6e￿ciently perform near-memory computation to improve
the overall system throughput.
Future datacenters will likely be equipped with numerous
domain-speci ￿c compute resources/accelerators. In such a
system, one can borrow the idle cores of one compute re-
source and perform extra computation to increase the overall
system throughput. A uni ￿ed runtime to support malleable
processes that can be immediately decomposed into smaller
pieces and o ￿oaded to any available compute nodes can
improve both application and cluster throughput [41, 52].
6.3 Allocation Policies
Memory Allocation in Heterogenous NUMA Clus-
ter.For better performance, hottest pages need to be on the
fastest memory tier. However, due to memory capacity con-
straints across di ￿erent tiers, it may not always be possible
to utilize the fastest or performant memory tier. Determining
what fraction of memory is needed at a particular memory
tier to maintain the desired performance of an application at
di￿erent points of its life cycle is challenging. This is even
more di ￿cult when multiple applications coexist. E ￿cient
promotion or demotion of pages of di ￿erent temperatures
across memory tiers at rack scale is necessary. One can con-
sider augmenting TPP by incorporating a lightweight but
e￿ective algorithm to select the migration target considering
node distances from the CPU, load on CPU-memory bus,
current load on di ￿erent memory tiers, network state, and
the QoS requirements of the migration-candidate pages.
Allocation Policy for Memory Bandwidth Expan-
sion. For memory bandwidth-bound applications, CPU-to-
DRAM bandwidth often becomes the bottleneck and in-
creases the average memory access latency. CXL’s additional
memory bandwidth can help by spreading memory across
the top-tier and remote nodes. Instead of only placing cold
pages into CXL-Memory, which has low bandwidth con-
sumption, an ideal solution should place the right amount of
bandwidth-heavy, latency-insensitive pages to CXL-Memory.
The methodology to identify the ideal fraction of such work-
ing sets may even require hardware support.
Memory Sharing and Consistency. CXL 3.0 allows
memory sharing across multiple devices. Through an en-
hanced coherency semantics, multiple hosts can have a co-
herent copy of a shared segment, with back invalidation
for synchronization. Memory sharing improves application-
level performance by reducing unnecessary data movement
and improves memory utilization. Sharing a large memory
address space, however, results in signi ￿cant overhead and
complexity in the system that plagued classic distributed
shared memory (DSM) proposals [ 36]. Furthermore, sharing
memory across multiple devices increases the security threat
in the presence of any malicious application run on the samehardware space. We believe that disaggregated memory sys-
tems should cautiously approach memory sharing and avoid
it unless it is absolutely necessary for speci ￿c scenarios.
6.4 Rack-Level Objectives
Rack-Scale Memory Temperature. To obtain insights
into an application’s expected performance with multiple
temperature tiers, it is necessary to understand the heat
map of memory usage for that application. Existing hot page
identi ￿cation mechanisms (including Chameleon) are limited
to a single host OS or user-space mechanism. They either use
access bit-based mechanism [ 4,6,47], special CPU feature-
based (e.g., Intel PEBS) tools [ 39,43,50], or OS features [ 7,33]
to determine the page temperature within a single server.
So far, there is no distributed mechanism to determine the
cluster-wide relative page temperature. Combining the data
of all the OS or user-space tools and coordinating between
them to ￿nd rack-level hot pages is an important problem.
CXL fabric manager is perhaps the place where one can
get a cluster-wide view of hardware counters for each CXL
device’s load, hit, and access-related information. One can
envision extending Chameleon for rack-scale environments
to provide observability into each application’s per-device
memory temperature.
Hardware-Software Co-Design for a Better Ecosys-
tem. Hardware features can further enhance performance
of disaggregation systems in rack-scale setups. A memory-
side cache and its associated prefetcher on the CXL ASIC
or switch might help reduce the e ￿ective latency of CXL-
Memory. Hardware support for data movement between
memory tiers can help reduce page migration overheads in
an aggressively provisioned system with very small amount
of local memory and high amount of CXL-Memory. Addition-
ally, the fabric manager of a CXL switch should implement
policies like fair queuing, congestion control, load balancing
etc. for better network management. Incorporating Leap’s
prefetcher and Hydra’s erasure-coded resilience ideas into
CXL switch designs can enhance system-wide performance.
Energy- and Carbon-Aware Memory Disaggrega-
tion. Datacenters represent a large and growing source of
energy consumption and carbon emissions [ 13]. Some esti-
mates place datacenters to be responsible for 1-2% of aggre-
gate worldwide electricity consumption [ 23,37]. To reduce
the TCO and carbon footprint, and enhance hardware life
expectancy, datacenter rack maintain a physical energy bud-
get or power cap. Rack-scale memory allocation, demotion,
and promotion policies can be augmented by incorporating
energy-awareness in their decision-making process. In gen-
eral, we can introduce energy-awareness in the software
stack that manage compute, memory, and network resources
in a disaggregated cluster.
77Conclusion
We started the In ￿niswap project in 2016 with the conviction
that memory disaggregation is inevitable, armed only with a
few data points that hinted it might be within reach. As we
conclude this paper in 2023, we have successfully built a com-
prehensive software-based disaggregated memory solution
over ultra-fast RDMA networks that can provide a seamless
experience for most memory-intensive applications. With
diverse cache-coherent interconnects ￿nally converging un-
der the CXL banner, the entire industry (and ourselves) are
at the cusp of taking a leap toward next-generation software-
hardware co-designed disaggregated systems. Join us. Mem-
ory disaggregation is here to stay.
Acknowledgements
Juncheng Gu, Youngmoon Lee, and Yiwen Zhang co-led dif-
ferent aspects of the In ￿niswap project alongside the authors.
We thank Yiwen Zhang for his feedback on this paper. Spe-
cial thanks to our many collaborators, contributors, users,
and cloud resource providers (namely, CloudLab, Chameleon
Cloud, and UM ConFlux) for making In ￿niswap successful.
Our expeditions into next-generation memory disaggrega-
tion solutions have greatly bene ￿ted from our collaborations
with Meta. Our research was supported in part by National
Science Foundation grants (CCF-1629397, CNS-1845853, and
CNS-2104243) and generous gifts from VMware and Meta.
References
[1]Alibaba Cluster Trace 2018. https://github .com/alibaba/clusterdata/
blob/master/cluster-trace-v2018/trace_2018 .md.
[2]CCIX. https://www .ccixconsortium .com/.
[3]Compute Express Link (CXL). https://www .computeexpresslink .org/.
[4]DAMON: Data Access MONitoring Framework for
Fun and Memory Management Optimizations. https:
//www .linuxplumbersconf .org/event/7/contributions/659/
attachments/503/1195/damon_ksummit_2020 .pdf.
[5]Gen-Z. https://genzconsortium .org/.
[6]Idle page tracking-based working set estimation. https://lwn .net/
Articles/460762/.
[7]NUMA Balancing (AutoNUMA). https://mirrors .edge .kernel .org/
pub/linux/kernel/people/andrea/autonuma/autonuma _bench-
20120530 .pdf.
[8]OpenCAPI. https://opencapi .org/.
[9]Rack-scale computing at Yahoo! http://www .intel.com/content/dam/
www/public/us/en/documents/presentation/idf15-yahoo-rack-scale-
computing-presentation .pdf.
[10] Tencent explores datacenter resource-pooling using Intel rack scale
architecture (Intel RSA). http://www .intel .com/content/dam/www/
public/us/en/documents/white-papers/rsa-tencent-paper .pdf.
[11] M. K. Aguilera, N. Amit, I. Calciu, X. Deguillard, J. Gandhi, S. No-
vakovi ć, A. Ramanathan, P. Subrahmanyam, L. Suresh, K. Tati,
R. Venkatasubramanian, and M. Wei. Remote regions: a simple ab-
straction for remote memory. In USENIX ATC , 2018.
[12] E. Amaro, C. Branner-Augmon, Z. Luo, A. Ousterhout, M. K. Aguilera,
A. Panda, S. Ratnasamy, and S. Shenker. Can far memory improve job
throughput? In EuroSys , 2020.[13] T. Anderson, A. Belay, M. Chowdhury, A. Cidon, and I. Zhang. Tree-
house: A case for carbon-aware datacenter software. In HotCarbon ,
2022.
[14] I. Calciu, M. T. Imran, I. Puddu, S. Kashyap, H. A. Maruf, O. Mutlu, and
A. Kolli. Rethinking software runtimes for disaggregated memory. In
ASPLOS , 2021.
[15] H. Chen, Y. Luo, X. Wang, B. Zhang, Y. Sun, and Z. Wang. A transparent
remote paging model for virtual machines. In International Workshop
on Virtualization Technology , 2008.
[16] J. Dean and L. A. Barroso. The tail at scale. Communications of the
ACM , 56(2):74–80, 2013.
[17] A. Dragojevi ć, D. Narayanan, O. Hodson, and M. Castro. FaRM: Fast
remote memory. In NSDI , 2014.
[18] S. Dwarkadas, N. Hardavellas, L. Kontothanassis, R. Nikhil, and R. Stets.
Cashmere-VLM: Remote memory paging for software distributed
shared memory. In IPPS/SPDP , 1999.
[19] M. J. Feeley, W. E. Morgan, E. Pighin, A. R. Karlin, H. M. Levy, and C. A.
Thekkath. Implementing global memory management in a workstation
cluster. In ACM SIGOPS Operating Systems Review , volume 29, pages
201–212. ACM, 1995.
[20] E. W. Felten and J. Zahorjan. Issues in the implementation of a remote
memory paging system. Technical Report 91-03-09, University of
Washington, Mar 1991.
[21] D. Gouk, S. Lee, M. Kwon, and M. Jung. Direct access, High-
Performance memory disaggregation with DirectCXL. In USENIX
ATC, 2022.
[22] J. Gu, Y. Lee, Y. Zhang, M. Chowdhury, and K. G. Shin. E ￿cient
memory disaggregation with In ￿niswap. In NSDI , 2017.
[23] N. Jones. How to stop data centres from gobbling up the world’s
electricity. Nature , 561:163–166, 2018.
[24] A. Kalia, M. Kaminsky, and D. G. Andersen. FaSST: Fast, scalable
and simple distributed transactions with two-sided (RDMA) datagram
RPCs. In OSDI , 2016.
[25] A. Lagar-Cavilla, J. Ahn, S. Souhlal, N. Agarwal, R. Burny, S. Butt,
J. Chang, A. Chaugule, N. Deng, J. Shahid, G. Thelen, K. A. Yurt-
sever, Y. Zhao, and P. Ranganathan. Software-de ￿ned far memory in
warehouse-scale computers. In ASPLOS , 2019.
[26] S.-s. Lee, Y. Yu, Y. Tang, A. Khandelwal, L. Zhong, and A. Bhattachar-
jee. MIND: In-network memory management for disaggregated data
centers. In SOSP , 2021.
[27] Y. Lee, H. A. Maruf, M. Chowdhury, A. Cidon, and K. G. Shin. Hydra :
Resilient and highly available remote memory. In FAST , 2022.
[28] H. Li, D. S. Berger, S. Novakovic, L. Hsu, D. Ernst, P. Zardoshti, M. Shah,
S. Rajadnya, S. Lee, I. Agarwal, M. D. Hill, M. Fontoura, and R. Bianchini.
Pond: CXL-based memory pooling systems for cloud platforms. In
ASPLOS , 2023.
[29] S. Liang, R. Noronha, and D. K. Panda. Swapping to remote memory
over In ￿niBand: An approach using a high performance network block
device. In IEEE International Conference on Cluster Computing , 2005.
[30] K. Lim, J. Chang, T. Mudge, P. Ranganathan, S. K. Reinhardt, and T. F.
Wenisch. Disaggregated memory for expansion and sharing in blade
servers. SIGARCH , 2009.
[31] E. P. Markatos and G. Dramitinos. Implementation of a reliable remote
memory pager. In USENIX ATC , 1996.
[32] H. A. Maruf and M. Chowdhury. E ￿ectively prefetching remote mem-
ory with Leap. In USENIX ATC , 2020.
[33] H. A. Maruf, H. Wang, A. Dhanotia, J. Weiner, N. Agarwal, P. Bhat-
tacharya, C. Petersen, M. Chowdhury, S. Kanaujia, and P. Chauhan.
TPP: Transparent page placement for CXL-enabled tiered-memory. In
ASPLOS , 2023.
[34] H. A. Maruf, Y. Zhong, H. Wong, M. Chowdhury, A. Cidon, and C. Wald-
spurger. Memtrade: A disaggregated-memory marketplace for public
8clouds. In SIGMETRICS , 2023.
[35] T. Newhall, S. Finney, K. Ganchev, and M. Spiegel. Nswap: A network
swapping module for Linux clusters. In Euro-Par , 2003.
[36] B. Nitzberg and V. Lo. Distributed shared memory: A survey of issues
and algorithms. Computer , 24(8):52–60, 1991.
[37] F. Pearce. Energy hogs: Can world’s huge data centers be made more
e￿cient? Yale Environment , 2018.
[38] K. Rashmi, M. Chowdhury, J. Kosaian, I. Stoica, and K. Ramchandran.
EC-Cache: Load-balanced, low-latency cluster caching with online
erasure coding. In OSDI , 2016.
[39] A. Raybuck, T. Stamler, W. Zhang, M. Erez, and S. Peter. HeMem:
Scalable tiered memory management for big data applications and real
NVM. In SOSP , 2021.
[40] C. Reiss, A. Tumanov, G. R. Ganger, R. H. Katz, and M. A. Kozuch.
Heterogeneity and dynamicity of clouds at scale: Google trace analysis.
InSoCC , 2012.
[41] Z. Ruan, S. J. Park, M. K. Aguilera, A. Belay, and M. Schwarzkopf.
Nu: Achieving microsecond-scale resource fungibility with logical
processes. In NSDI , 2023.
[42] Z. Ruan, M. Schwarzkopf, M. K. Aguilera, and A. Belay. AIFM: High-
performance, application-integrated far memory. In OSDI , 2020.
[43] H. Servat, A. J. Peña, G. Llort, E. Mercadal, H.-C. Hoppe, and J. Labarta.
Automating the application data placement in hybrid memory systems.
InIEEE International Conference on Cluster Computing , 2017.
[44] Y. Shan, Y. Huang, Y. Chen, and Y. Zhang. LegoOS: A disseminated,
distributed OS for hardware resource disaggregation. In OSDI , 2018.
[45] A. Tatar, R. K. Konoth, E. Athanasopoulos, C. Giu ￿rida, H. Bos, and
K. Razavi. Throwhammer: Rowhammer attacks over the network and
defenses. In ATC, 2018.
[46] S.-Y. Tsai, M. Payer, and Y. Zhang. Pythia: Remote oracles for the
masses. In USENIX Security , 2019.[47] Vladimir Davydov. Idle Memory Tracking. https://lwn .net/Articles/
639341/.
[48] M. Vuppalapati, J. Miron, R. Agarwal, D. Truong, A. Motivala, and
T. Cruanes. Building an elastic query engine on disaggregated storage.
InNSDI , 2020.
[49] C. Wang, H. Ma, S. Liu, Y. Li, Z. Ruan, K. Nguyen, M. D. Bond, R. Ne-
travali, M. Kim, and G. H. Xu. Semeru: A Memory-Disaggregated
managed runtime. In OSDI , 2020.
[50] K. Wu, Y. Huang, and D. Li. Unimem: Runtime data managementon
non-volatile memory-based heterogeneous main memory. In SC, 2017.
[51] D. Y. Yoon, M. Chowdhury, and B. Mozafari. Distributed lock manage-
ment with RDMA: Decentralization without starvation. In SIGMOD ,
2018.
[52] J. You, J. Wu, X. Jin, and M. Chowdhury. Ship compute or ship data?
why not both? In NSDI , 2021.
[53] Z. Yu, Y. Zhang, V. Braverman, M. Chowdhury, and X. Jin. NetLock:
Fast, centralized lock management using programmable switches. In
SIGCOMM , 2020.
[54] Y. Zhang, J. Gu, Y. Lee, M. Chowdhury, and K. G. Shin. Performance
isolation anomalies in RDMA. In ACM SIGCOMM KBNets , 2017.
[55] Y. Zhang, G. Kumar, N. Dukkipati, X. Wu, P. Jha, M. Chowdhury, and
A. Vahdat. Aequitas: Admission control for performance-critical RPCs
in datacenters. In ACM SIGCOMM , 2022.
[56] Y. Zhang, Y. Tan, B. Stephens, and M. Chowdhury. Justitia: Software
Multi-Tenancy in hardware Kernel-Bypass networks. In NSDI , 2022.
[57] W. Zhao and J. Ning. Project Tioga Pass Rev 0.30 : Facebook Server Intel
Motherboard V4.0 Spec. https://www .opencompute .org/documents/
facebook-server-intel-motherboard-v40-spec.
9Pond: CXL-Based Memory Pooling Systems for Cloud Platforms
Huaicheng Li†, Daniel S. Berger⇤‡, Stanko Novakovic⇤, Lisa Hsu⇤, Dan Ernst⇤,
Pantea Zardoshti⇤, Monish Shah⇤, Samir Rajadnya⇤, Scott Lee⇤, Ishwar Agarwal⇤,
Mark D. Hill⇤ , Marcus Fontoura⇤, Ricardo Bianchini⇤
†Virginia Tech and CMU⇤Microsoft Azure‡University of Washington University of Wisconsin-Madison
Abstract
Public cloud providers seek to meet stringent performance
requirements and low hardware cost. A key driver of per-
formance and cost is main memory. Memory pooling
promises to improve DRAM utilization and thereby re-
duce costs. However, pooling is challenging under cloud
performance requirements. This paper proposes Pond,
the ﬁrst memory pooling system that both meets cloud
performance goals and signiﬁcantly reduces DRAM cost.
Pond builds on the Compute Express Link (CXL) standard
for load/store access to pool memory and two key insights.
First, our analysis of cloud production traces shows that
pooling across 8-16 sockets is enough to achieve most
of the beneﬁts. This enables a small-pool design with
low access latency. Second, it is possible to create ma-
chine learning models that can accurately predict how
much local and pool memory to allocate to a virtual ma-
chine (VM) to resemble same-NUMA-node memory per-
formance. Our evaluation with 158 workloads shows
that Pond reduces DRAM costs by 7% with performance
within 1-5% of same-NUMA-node VM allocations.
1.Introduction
Motivation. Many public cloud customers deploy their
workloads in the form of virtual machines (VMs), for
which they get virtualized compute with performance
approaching that of a dedicated cloud, but without having
to manage their own on-premises datacenter. This creates
a major challenge for public cloud providers: achieving
excellent performance for opaque VMs ( i.e., providers do
not know and should not inspect what is running inside
the VMs) at a competitive hardware cost.
A key driver of both performance and cost is main
memory. The gold standard for memory performance is
for accesses to be served by the same NUMA node as
the cores that issue them, leading to latencies in tens of
nanoseconds. A common approach is to preallocate all
VM memory on the same NUMA node as the VM’s cores.
Preallocating and statically pinning memory also facilitate
the use of virtualization accelerators [ 1–6], which are
enabled by default, for example, on AWS and Azure [ 7,8].
At the same time, DRAM has become a major portion of
hardware cost due to its poor scaling properties with only
nascent alternatives [ 9–15]. For example, DRAM can be
50% of server cost [ 16].Through analysis of production traces from Azure, we
identify memory stranding as a dominant source of mem-
ory waste and a potential source of massive cost savings.
Stranding happens when all cores of a server are rented
(i.e., allocated to customer VMs) but unallocated memory
capacity remains and cannot be rented. We ﬁnd that up to
25% of DRAM becomes stranded as more cores become
allocated to VMs.
Limitations of the state of the art. Despite this signiﬁ-
cant amount of stranding, reducing DRAM usage in the
public cloud is challenging due to its stringent perfor-
mance requirements. For example, existing techniques
for process-level memory compression [ 17,18] require
page fault handling, which adds microseconds of latency,
and moving away from statically preallocated memory.
Pooling memory via memory disaggregation is a
promising approach because stranded memory can be re-
turned to the disaggregated pool and used by other servers.
Unfortunately, existing pooling systems also have mi-
crosecond access latencies and require page faults [ 1,19–
24] or changes to the VM guest [ 17,21–23,25–38].
Our work. This work describes Pond, the ﬁrst system to
achieve both same-NUMA-node memory performance
and competitive cost for public cloud platforms. To
achieve this, Pond combines hardware and systems tech-
niques. It relies on the Compute Express Link (CXL)
interconnect standard [ 39], which enables cacheable
load/store ( ld/st ) accesses to pooled memory on In-
tel, AMD, and ARM processors [ 40–42] at nanosecond-
scale latencies. CXL access via loads/stores is a game
changer as it allows memory to remain statically preal-
located while physically being located in a shared pool.
However, even with loads/stores, CXL accesses still face
higher latencies than same-NUMA-node accesses. Pond
introduces systems support for CXL-based pooling that
dramatically reduces the impact of this higher latency.
Pond is feasible because of four key insights. First, by
analyzing traces from 100 production clusters at Azure,
we ﬁnd that pool sizes between 8-16 sockets lead to sufﬁ-
cient DRAM savings. The pool size deﬁnes the number
of CPU sockets able to use pool memory. Further, analy-
sis of CXL topologies lead us to estimate that CXL will
add 70-90ns to access latencies over same-NUMA-node
DRAM with a pool size of 8-16 sockets, and add more
than 180ns for rack-scale pooling. We conclude that
1arXiv:2203.00241v4  [cs.OS]  21 Oct 2022Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
grouping 8 dual-socket (or 16 single-socket) servers is
enough to achieve most of the beneﬁts of pooling.
Second, by emulating either 64ns or 140ns of memory
access overheads, we ﬁnd that 43% and 37% of 158 work-
loads are within 5% of the performance on same-NUMA-
node DRAM when entirely allocated in pool memory.
However, more than 21% of workloads suffer a perfor-
mance loss above 25%. This emphasizes the need for
small pools and shows the challenge with achieving same-
NUMA-node performance. This characterization also
allows us to train a machine learning (ML) model that can
identify a subset of insensitive workloads ahead of time
to be allocated on the Pond memory pool.
Third, we observe through measurements at Azure that
⇠50% of all VMs touch less than 50% of their rented
memory. Conceptually, allocating untouched memory
from the pool should not have any performance impact
even for latency-sensitive VMs. We ﬁnd that — while
this concept does not hold for the uniform address spaces
assumed in prior work [ 1,19–24] — it does hold if we
expose pool memory to a VM’s guest OS as a zero-core
virtual NUMA ( zNUMA ) node, i.e., a node with memory
but no cores, like Linux’s CPU-less NUMA [ 43]. Our
experiments show zNUMA effectively biases memory
allocations away from the zNUMA node. Thus, a VM
with a zNUMA sized to match its untouched memory will
indeed not see any performance impact.
Fourth, Pond can allocate CXL memory with same-
NUMA-node performance using correct predictions of a)
whether a VM will be latency-sensitive and b)a VM’s
amount of untouched memory. For incorrect predictions,
Pond introduces a novel monitoring system that detects
poor memory performance and triggers a mitigation that
migrates the VM to use only same-NUMA-node memory.
Further, we ﬁnd that all inputs to train and run Pond’s ML
models can be obtained from existing hardware telemetry
with no measurable overhead.
Artifacts. CXL is still a year from broad deployment.
Meanwhile, deploying Pond requires extensive testing
within Azure’s system software and distributed software
stack. We implement Pond on top of an emulation layer
that is deployed on production servers. This allows us to
prove the key concepts behind Pond by exercising the VM
allocation workﬂow, zNUMA, and by measuring guest
performance. Additionally, we support the four insights
from above by reporting from extensive experiments and
measurements in Azure’s datacenters. We evaluate the
effectiveness of pooling using simulations based on VM
traces from 100 production clusters.
Contributions. Our main contributions are:
•The ﬁrst public characterization of memory stranding
and untouched memory at a large public cloud provider.
•The ﬁrst analysis of the effectiveness and latency of
MemoryDeviceRequest (Req)Data Response (DRS) Cache Miss
CXLPort
MemoryDeviceCache Write Back
CXLPortRequest with Data (RwD)No Data Response (NDR)
CXL &PCIe PHY
Arb/Mux
Transaction &Link LayersCPUFabric
PCIeWires
4ns2ns19nsRound-trip latency measuredon Intel Sapphire Rapids 
Intel/AMD/ARMCore/LLC/Fabric
Intel/AMD/ARMCore/LLC/FabricFigure 1: CXL Request Flow (§ 2).CPU cache misses and
write-backs to addresses mapped to CXL devices are translated
to requests on a CXL port by the HDM decoder. Intel measures
the round-trip port latency to be 25ns.
different CXL memory pool sizes.
•Pond, the ﬁrst CXL-based full-stack memory pool that
is practical and performant for cloud deployment.
•An accurate prediction model for latency and resource
management at datacenter scale. These models enable
a conﬁgurable performance slowdown of 1-5%.
•An extensive evaluation that validates Pond’s design in-
cluding the performance of zNUMA and our prediction
models in a production setting. Our analysis shows that
we can reduce DRAM needs by 7% with a Pond pool
spanning 16 sockets, which corresponds to hundreds of
millions of dollars for a large cloud provider.
2.Background
Hypervisor memory management. Public cloud work-
loads are virtualized [ 44]. To maximize performance and
minimize overheads, hypervisors perform minimal mem-
ory management and rely on virtualization accelerators
to improve I/O performance [ 1,45–47]. Examples of
common accelerators are direct I/O device assignment
(DDA) [ 1,45] and Single Root I/O Virtualization (SR-
IOV) [ 46,47]. Accelerated networking is enabled by
default on AWS and Azure [ 7,8]. As pointed out in prior
work, virtualization acceleration requires statically preal-
locating (or “pinning”) a VM’s entire address space [ 1–6].
Memory stranding. Cloud VMs demand a vector of re-
sources ( e.g., CPUs, memory, etc.) [ 48–51]. Scheduling
VMs thus leads to a multi-dimensional bin-packing prob-
lem [ 49,52–54] which is complicated by constraints such
as spreading VMs across multiple failure domains. Con-
sequently, it is difﬁcult to provision servers that closely
match the resource demands of the incoming VM mix.
When the DRAM-to-core ratio of VM arrivals and the
server resources do not match, tight packing becomes
more difﬁcult. We deﬁne a resource as stranded when
it is technically available to be rented to a customer, but
is practically unavailable as some other resource has ex-
hausted. The typical scenario for memory stranding is
that all cores have been rented, but there is still memory
available in the server.
Reducing stranding. Multiple techniques can re-
2Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
010203040
60 7080 90Scheduled CPU Cores [%]Stranded Memory [%]5th Percentile95th Percentile
Outliers[a]   Stranding vs.      CPU utilizations
01234567Racks01 5 3 045 60 75Time [Days][b] Stranding over time 
Figure 2: Memory stranding. (a) Stranding increases signiﬁ-
cantly as more CPU cores are scheduled; (b) Stranding changes
dynamically over time.
duce memory stranding. For example, oversubscribing
cores [ 55,56] enables more memory to be rented. How-
ever, oversubscription only applies to a subset of VMs
for performance reasons. Our measurements at Azure
(§3.1) include clusters that enable oversubscription and
still show signiﬁcant memory stranding.
The approach we target is to disaggregate a portion
of memory into a pool that is accessible by multiple
hosts [ 31,57,58]. This breaks the ﬁxed hardware conﬁg-
uration of servers. By dynamically reassigning memory
to different hosts at different times, we can shift memory
resources to where they are needed, instead of relying
on each individual server to be conﬁgured for all cases
pessimistically. Thus, we can provision servers close to
the average DRAM-to-core ratios and tackle deviations
via the memory pool.
Pooling via CXL. CXL contains multiple protocols in-
cluding ld/st memory semantics (CXL.mem) and I/O
semantics (CXL.io). CXL.mem maps device memory to
the system address space. Last-level cache (LLC) misses
to CXL memory addresses translate into requests on a
CXL port whose reponses bring the missing cachelines
(Figure 1). Similarly, LLC write-backs translate into CXL
data writes. Neither action involves page faults or DMAs.
CXL memory is virtualized using hypervisor page tables
and the memory-management unit and is thus compati-
ble with virtualization acceleration. The CXL.io protocol
facilitates device discovery and conﬁguration. CXL 1.1
targets directly-attached devices, 2.0 [ 59,60] adds switch-
based pooling, and 3.0 [ 61,62] standardizes switch-less
pooling (§ 4) and higher bandwidth.
CXL.mem uses PCIe’s eletrical interface with custom
link and transaction layers for low latency. With PCIe 5.0,
859095100
28 1 6 3 264Pool Size [CPU Sockets]Required Overall DRAM [%]10%30%50%Percentage of pool memory    assigned to each VMFigure 3: Impact of pool size (§ 3.1).Small pools of 32
sockets are sufﬁcient to signiﬁcantly reduce memory needs.
the bandwidth of a birectional ⇥8-CXL port at a typical
2:1 read:write-ratio matches a DDR5-4800 channel. CXL
request latencies are largely determined by the CXL port.
Intel measures round-trip CXL port traversals at 25ns [ 63]
which, when combined with expected controller-side la-
tencies, leads to an end-to-end overhead of 70ns for CXL
reads, compared to NUMA-local DRAM reads. While
FPGA-based prototypes report higher latency [ 64,65], In-
tel’s measurements match industry-expectations for ASIC-
based memory controllers [ 62–64].
3.Memory Stranding & Workload Sensitiv-
ity to Memory Latency
3.1.Stranding at Azure
This section quantiﬁes the severity of memory stranding
and untouched memory at Azure using production data.
Dataset. We measure stranding in 100 cloud clusters
over a 75-day period. These clusters host mainstream
ﬁrst-party and third-party VM workloads. They are rep-
resentative of the majority of the server ﬂeet. We select
clusters with similar deployment years, but spanning all
major regions on the planet. A trace from each cluster
contains millions of per-VM arrival/departure events, with
the time, duration, resource demands, and server-id.
Memory stranding. Figure 2a shows the daily average
amount of stranded DRAM across clusters, bucketed by
the percentage of scheduled CPU cores. In clusters where
75% of CPU cores are scheduled for VMs, 6% of memory
is stranded. This grows to over 10% when ⇠85% of
CPU cores are allocated to VMs. This makes sense since
stranding is an artifact of highly utilized nodes, which
correlates with highly utilized clusters. Outliers are shown
by the error bars, representing 5thand 95thpercentiles.
At 95th, stranding reaches 25% during high utilization
periods. Individual outliers even reach 30% stranding.
Figure 2b shows stranding over time across 8 racks.
A workload change (around day 36) suddenly increased
stranding signiﬁcantly. Furthermore, stranding can affect
many racks concurrently ( e.g., racks 2, 4–7) and it is
3Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
020406080100
P1→P13 YCSB A→FML/Web, etc. bc, bfs, cc,pr, sssp, tcQueries 1→22 501.perlbench_r→657.xz_sfacesim, vips, fft, etc.ProprietaryRedisVoltDBSpark GAPBSTPC-H SPEC CPU 2017PARSECSPLASH2xSlowdown (%)Local:   78ns, remote: 142ns (182%)Local: 115ns, remote: 255ns (222%)Slowdown: performance under all remotememory relative to all local memoryNot run on redconﬁguration,insufﬁcient DRAMper NUMA node
Figure 4: Performance slowdowns when memory latency increases by 182-222% (§ 3.3).Workloads have different sensitivity
to additional memory latency (as in CXL). X-axis shows 158 representative workloads; Y is the normalized performance slowdown,
i.e., performance under higher (remote) latency relative to all local memory. “Proprietary” denotes production workloads at Azure.
0.2.4.6.81
05255075100Threeoutliersunder255nswith>100%slowdowns(max124%)Slowdown(%)Remote:142ns(182%)Remote:255ns(222%)CDFofslowdownsunderCXL
Figure 5: CDF of slowdowns (§ 3.3).Higher remote latency
(red) only slightly affects the head of the distribution (work-
loads with less than 5% slowdown). The body and tail of the
distribution see signiﬁcantly higher slowdowns.
generally hard to predict which clusters/racks will have
stranded memory.
NUMA spanning. Many VMs are small and can ﬁt on a
single socket. On two-socket systems, the hypervisor at
Azure seeks to schedule such that VMs ﬁt entirely (cores
and memory) on a single NUMA node. In rare cases, we
seeNUMA spanning where a VM has all of its cores on
one socket and a small amount of memory from another
socket. We ﬁnd that spanning occurs for about 2-3% of
VMs and fewer than 1% of memory pages, on average.
Savings from pooling. Azure currently does not pool
memory. However, by analyzing its VM-to-server traces,
we can estimate the amount of DRAM that could be saved
via pooling. Figure 3presents average reductions from
pooling DRAM when VMs are scheduled with a ﬁxed
percentage of either 10%, 30%, or 50% of pool DRAM.
The pool size refers to the number of sockets that can
access the same DRAM pool. As the pool size increases,
the ﬁgure shows that required overall DRAM decreases.
However, this effect diminishes for larger pools. For ex-
ample, with a ﬁxed 50% pool DRAM, a pool with 32
sockets saves 12% of DRAM while a pool with 64 sock-
ets saves 13% of DRAM. Note that allocating a ﬁxed 50%
of memory to pool DRAM leads to signiﬁcant perfor-
mance loss compared to socket-local DRAM (§ 6). Pond
overcomes this challenge with multiple techniques (§ 4).Summary and implications. From this analysis, we
draw a few important observations and implications for
Pond:
•We observe 3-27% of stranded memory in production
at the 95thpercentile, with some outliers at 36%.
•Almost all VMs ﬁt into one NUMA node.
•Pooling memory across 16-32 sockets can reduce clus-
ter memory demand by 10%. This suggests that mem-
ory pooling can produce signiﬁcant cost reductions but
assumes that a high percentage of DRAM can be allo-
cated on memory pools. When implementing DRAM
pools with cross-NUMA latencies, providers must care-
fully mitigate potential performance impacts.
3.2.VM Memory Usage at Azure
We use Pond’s telemetry on opaque VMs (§ 4.2) to char-
acterize the percentage of untouched memory across our
cloud clusters. Generally, we ﬁnd that while VM memory
usage varies across clusters, all clusters have a signiﬁcant
fraction of VMs with untouched memory. Overall, the
50thpercentile is 50% untouched memory.
Summary and implications. From this analysis, we
draw key observations and implications for Pond:
•VM memory usage varies widely.
•In the cluster with the least amount of untouched mem-
ory, still over 50% of VMs have more than 20% un-
touched memory. Thus, there is plenty of untouched
memory that can be disaggregated at no performance
penalty.
•The challenges are (1) predicting how much untouched
memory a VM is likely to have and (2) conﬁning the
VM’s accesses to local memory. Pond addresses both.
3.3.Workload Sensitivity to Memory Latency
To characterize the performance impact of CXL latency
for typical workloads in Azure’s datacenters, we evalu-
ate 158 workloads under two scenarios of emulated CXL
access latencies: 182% and 222% increase in memory
4Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
...EMC128 PCIe 5.0 lanes(16 hosts w/ x8 CXL links)12 channelsDDR516-socket PondIODCCDCCDCCDCCDCCDCCDCCDCCDCCDCCDCCDCCDComparison: AMD Genoa   128PCIe 5.0 lanes12     channelsDDR5  397mm2CPUCPUCPU...EMC64 PCIe 5.0 lanes(8 hosts w/ x8 CXL links)6 channelsDDR58-socket PondCPUCPUCPU...
EMC(8 hosts w/ x8 CXL links)12 channelsDDR532/64-socket PondCPUCPUCPU...SwitchCPUCPUCPU...Switch...96 PCIe 5.0 lanes(4 EMCs w/x8 CXL links)(8 switches w/x8 CXL links)...64 PCIe 5.0 lanes≈1/2 Genoa IOD≈ Genoa IOD
Figure 6: External memory controller (EMC) (§ 4.1).The
EMC is multi-headed which allows connecting multiple CXL
hosts and DDR5 DIMMs. A 16-socket Pond requires 128 PCIe
5.0 lanes and 12 DDR5 channels, which is comparable to the IO-
die (IOD) on AMD Genoa [ 42,66]. Larger Pond conﬁgurations
combine a switch with the multi-headed EMC.
latency, respectively. We then compare the workload
performance to NUMA-local memory placement. Experi-
mental details are in § 6.1. Figures 4and5show workload
slowdowns relative to NUMA-local performance for both
scenarios.
Under a 182% increase in memory latency, we ﬁnd that
26% of the 158 workloads experience less than 1% slow-
down under CXL. An additional 17% of workloads see
less than 5% slowdowns. At the same time, some work-
loads are severely affected with 21% of the workloads
facing >25% slowdowns.
Different workload classes are affected differently,
e.g., GAPBS (graph processing) workloads generally see
higher slowdowns. However, the variability within each
workload class is typically much higher than across work-
load classes. For example, within GAPBS even the same
graph kernel reacts very differently to CXL latency, based
on different graph datasets. Overall, every workload class
has at least one workload with less than 5% slowdown
and one workload with more than 25% slowdown (except
SPLASH2x).
Azure’s proprietary workloads are less impacted than
the overall workload set. Of the 13 production workloads,
6 do not see noticeable impact ( <1%); 2 see ⇠5% slow-
down; and the remaining half are impacted by 10–28%.
This is in part because these production workloads are
NUMA-aware and often include data placement optimiza-
tions.
Under a 222% increase in memory latency, we ﬁnd that
23% of the 158 workloads experience less than 1% slow-
down under CXL. An additional 14% of workloads see
less than 5% slowdowns. More than 37% of workloads
face >25% slowdowns. Generally, we ﬁnd that higher
latency magniﬁes the effects seen under lower latency:
workloads performing well under 182% latency also tend
to perform well under 222% latency; workloads severely
affected by 182% are even more affected by 222%.
Summary and implications. While the performance of
CXL PortFlight timeRetimerAddress mapping, permission (ACL)Network-on-chip (NOC)Switch arbitration (ARB)Core/LLC/FabricMemory Controller (MC) MC & DRAM25ns5ns20ns5ns10ns10ns40ns45ns
CPUCPU
EMC
CXLPort25ns
CXLPort
ACLNOC25ns15ns
Core/LLC/Fabric
MC &DRAM45ns40ns5ns
EMC
CXLPort25ns
CXLPort25ns15ns5+20+5ns
EMC
Switch
CXLPort
CXLPort25ns
CXLPort25ns15nsRe-timer
Core/LLC/Fabric40ns
Core/LLC/Fabric40ns
Core/LLC/Fabric40ns
MC &DRAM45ns
MC &DRAM45ns
MC &DRAM45nsLocal DRAM (85ns)8-socket Pond (155ns, 182%)16-socket Pond (180ns, 212%)32/64-socket Pond (>270ns, 318%)EMCPMCPUCPUEMCCPUCPUCPUCPUCPUCPU8-socketPondCPUCPUCPUCPUCPUCPUEMCPMEMC16-socket PondCPUCPUCPUCPUCPUCPUCPUCPURe-timerRe-timerRe-timerRe-timerRe-timerRe-timerRe-timerRe-timer
CXLPort25ns25ns20ns
ARBNOC
<500mm>500mm
5+20+5nsRe-timer5+20+5nsRe-timerLatency assumptions
ACLNOCACLNOCFigure 7: Pool size and latency tradeoffs (§ 4.1).Small Pond
pools of 8-16 sockets add only 75-90ns relative to NUMA-local
DRAM. Latency increases for larger pools that require retimers
and a switch.
some workloads is insensitive to disaggregated memory
latency, some are heavily impacted. This motivates our
design decision to include socket-local DRAM alongside
pool DRAM to mitigate CXL latency impact for those
latency-sensitive workloads. Memory pooling solutions
can be effective if they’re are effective at identiﬁying
sensitive workloads.
4.Pond Design
Our measurements and observations at Azure (§ 2–3) lead
us to deﬁne the following design goals.
G1Performance comparable to NUMA-local DRAM
G2Compatibility with virtualization accelerators
G3Compatibility with opaque VMs and unchanged
guest OSes/applications
G4Low host resource overhead
To quantify (G1), we deﬁne a performance degrada-
tion margin (PDM) for a given workload as the allowable
slowdown relative to running the workload entirely on
NUMA-local DRAM. Pond seeks to achieve a conﬁg-
urable PDM,e.g., 1%, for a conﬁgurable tail-percentage
(TP) of VMs, e.g., 98% (§ 3.1). To achieve this high per-
formance, Pond uses a small but fast CXL pool (§ 4.1).
As Pond’s memory savings come from pooling instead of
oversubscription, Pond must minimize pool fragmenta-
tion and wastage in its system software layer (§ 4.2). To
achieve (G2), Pond preallocates local and pool memory
at VM start. Pond decides this allocation in its allocation,
performance monitoring, and mitigation pipeline (§ 4.3).
This pipeline uses novel prediction models to achieve the
PDM(§4.4). Finally, Pond overcomes VM-opaqueness
(G3) and host-overheads (G4) using lightweight hardware
counter telemetry.
5Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
0100200300181 6 3 264Pool Size [Sockets]Access Latency [ns]Switches onlyPond with multi-headed design1 SwitchNUMA-local latency baseline2 SwitchesNo switches1 Switch-36%
Figure 8: Pool access latency comparison (§ 4.1).Pond
reduces latencies by 1/3 compared to switch-only designs.
4.1.Hardware Layer
Hosts within a Pond pool have separate cache coherency
domains and run separate hypervisors. Pond uses an
ownership model where pool memory is explicitly moved
among hosts. A new external memory controller (EMC)
ASIC implements the pool using multiple DDR5 channels
accessed through a collection of CXL ports running at
PCIe 5 speeds.
EMC memory management. The EMC offers mul-
tiple CXL ports and appears to each host as a single
logical memory device [ 59,60]. In CXL 3.0 [ 61,62],
this conﬁguration is standardized as multi-headed device
(MHD) [ 62, §2.5]. The EMC exposes its entire capacity
on each port ( e.g., to hosts) via a Host-managed Device
Memory (HDM) decoder. Hosts program each EMC’s
address range but treat them initially as ofﬂine. Pond
dynamically assigns memory at the granularity of 1GB
memory slices. Each slice is assigned to at most one
host at a given time and hosts are explicitly notiﬁed about
changes (§ 4.2). Tracking 1024 slices (1TB) and 64 hosts
(6 bits) requires 768B of EMC state. The EMC imple-
ments dynamic slice assignment by checking permission
of each memory access, i.e., whether requestor and owner
of the cacheline’s slice match. Disallowed accesses result
in fatal memory errors.
EMC ASIC design. The EMC offers multiple ⇥8-CXL
ports, which communicate with DDR5 memory con-
trollers (MC) via an on-chip network (NOC). The MCs
must offer the same reliability, availability, and service-
ability capabilities [ 67,68] as server-grade memory con-
trollers including memory error correction, management,
and isolation. A key design parameter of Pond’s EMC is
the pool size, which deﬁnes the number of CPU sockets
able to use pool memory. We ﬁrst observe that the EMC’s
IO, (De)Serializer, and MC requirements resemble AMD
Genoa’s 397mm2IO-die (IOD) [ 42,66]. Figure 6shows
that EMC requirements for a 16-socket Pond parallel the
IOD’s requirements, with a small 8-socket Pond paral-
leling half an IOD. Thus, up to 16-sockets can directly
connect to an EMC. Pool sizes of 32-64 would combine
CXL switches with Pond’s multi-headed EMC. The opti-
Host 1(H1)Host 2(H2)EMC 1Hosts map local andEMC memory at boot 
PoolManager(PM)Usedby VM1Usedby VM2VM2leaves
VMschedulerTimet=3t=2t=1t=0H1H1H1H1H1H1H2Releasecapacity(H1,y)xyxyxyNew VM(1GB onpool)Addcapacity(H2,y)H1t=4
Usedby VM3xyLocalDRAMEMCmemorystarts inofﬂinestatexyAdd capacityevents leadthe host OSto online theassociated1GB sliceFigure 9: Pool management example (§ 4.2).Pond assigns
pool memory to at most one host at a time. This example shows
Pond’s asynchronous memory release strategy which engages
when a VM departs ( t=1 and t=2). During VM scheduling,
memory is added to the corresponding host before the VM starts
(t=3 and t=4).
mal design point balances the potential pool savings for
larger pool sizes (§ 6) with the added cost of larger EMCs
and switches.
EMC Latency. While latency is affected by propaga-
tion delays, it is dominated by CXL port latency, and
any use of CXL retimers and CXL switches. Port laten-
cies are discussed in § 2and [ 63]. Retimers are devices
used to maintain CXL/PCIe signal integrity over longer
distances and add about 10ns of latency in each direc-
tion [ 69,70]. In datacenter conditions, signal integrity
simulations [ 71] indicate that CXL could require retimers
above 500mm. Switches add at least 70ns of latency due
to ports/arbitration/NOC with estimates above 100ns [ 72].
Figure 7breaks down Pond’s latency for different pool
sizes. Figure 8compares Pond’s latency to a design that
relies only on switches instead of a multi-headed EMC.
We ﬁnd that Pond reduces latencies by 1/3 with 8-and
16-socket pools adding only 70-90ns relative to NUMA-
local DRAM. In practice, we expect Pond to be deployed
primarily with small 8/16-socket pools, given the latency
and cost overheads, and diminishing returns of larger
pools (§ 3). Modern CPUs can connect to multiple EMCs
which allows scaling to meet bandwidth and capacity
goals for different clusters.
4.2.System Software Layer
Pond’s system software involves multiple components.
Pool memory ownership. Pool management involves
assigning Pond’s memory slices to hosts and reclaiming
them for the pool (Figure 9). It involves 1) implementing
the control paths for pool-level memory assignment and
2) preventing pool memory fragmentation.
6Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
Figure 10: zNUMA (§ 4.2).zNUMA seen from a Linux VM.
Hosts discover local and pool capacity through CXL de-
vice discovery and map them to their address space. Once
mapped, the pool address range is marked hot-pluggable
and “not enabled.” Slice assignment is controlled at run-
time via a Pool Manager (PM) that is colocated on the
same blade as the EMCs (Figure 7). In Pond’s current
design, the PM is connected to EMCs and CPU sockets
via a low-power management bus ( e.g.,[73]). To allocate
pool memory, the Pool Manager triggers two types of in-
terrupts at the EMC and host driver. Add_capacity(host,
slice) interrupts the host driver which reads the address
range to be hot-plugged. The driver then communicates
with the OS memory manager to bring the memory on-
line. The EMC adds the host id to its permission table at
the slice offset. Release_capacity(host, slice) works
similarly by ofﬂining the slice on the host and resetting
the slice’s permission table entry on the EMC. An al-
ternative to this design would be inband-communication
using the Dynamic Capacity Device (DCD) feature in
CXL 3.0 [ 62, §9.13]. This change would maintain the
same functionality for Pond.
Pond must avoid fragmenting its online pool memory
as the contiguous 1GB address range must be free before
it can be ofﬂined for reassignment to another host. Pool
memory is allocated to VMs in 1GB-aligned increments
(§4.3). While this prevents fragmentation due to VM
starts and completions, our experience has shown that host
agents and drivers can allocate pool memory and cause
fragmentation. Pond thus uses a special-purpose memory
partition that is only available to the hypervisor. Host
agents and drivers allocate memory in host-local memory
partition, which effectively contains fragmentation.
With these optimizations, ofﬂining 1GB slices empir-
ically takes 10-100 milliseconds/GB. Onlining memory
is near instantaneous with microseconds/GB. These ob-
servations are reﬂected in Pond’s asynchronous release
strategy (§ 4.3).
Failure management. Hosts only interleave across local
memory. This minimizes the EMCs’ blast radius and
facilitate memory hot-plugging. EMC failures affect only
VMs with memory on that EMC, while VMs with memory
on other EMCs continue normally. CPU/host failures are
isolated and associated pool memory is reallocated to
other hosts. Pool Manager failures prevent reallocating
pool memory but do not affect the datapath.
Exposing pool memory to VMs. VMs that use both
VMschedulerMLSystemPoolManagerA1A2A5VM1Server BladesVM2HypervisorMemory PoolsEMCQoSMonitorMitigationManagerVM requestA3A4B1B2B3WorkloadpredictionFigure 11: Pond control plane workﬂow (§ 4.3). A) The
VM scheduler uses ML-based predictions that identify latency-
sensitive VMs and their likely amount of untouched memory to
decide on VM placement (see Figure 13).B)The monitoring
pipeline reconﬁgures VMs if quality-of-service (QoS) not met.
NUMA-local and pool memory see pool memory as a
zNUMA node. The hypervisor creates a zNUMA node by
adding a memory block ( node_memblk ) without an entry
in the node_cpuid in the SLIT/SRAT tables [ 74]. We
later show the guest-OS preferentially allocates memory
from the local NUMA node before going to zNUMA (§ 6).
Thus, if zNUMA is sized to the amount of untouched
memory, it is never going to be used. Figure 10shows the
view of a Linux VM which includes the correct latency in
the NUMA distance matrix ( numa_slit ). This facilitates
guest-OS NUMA-aware memory management [ 75,76]
for the rare case that the zNUMA is used (§ 4.4).
Reconﬁguration of memory allocation. To remain com-
patible with ( G2), local and pool memory mapping gen-
erally remain static during a VM’s lifetime. There are
two exceptions that are implemented today. When live-
migrating a VM or when remapping a page with a memory
fault, the hypervisor temporarily disables virtualization ac-
celeration and the VM falls back to a slower I/O path [ 77].
Both events are quick and transient and typically only
happen once during a VM’s lifetime. We implement a
third variant which allows Pond a one-time correction to
a suboptimal memory allocation. Speciﬁcally, if the host
has local memory available, Pond disables the accelera-
tor, copies all of the VM’s memory to local memory, and
enables the accelerator again. This takes about 50ms for
every GB of pool memory that Pond allocated to the VM.
Telemetry for opaque VMs. Pond requires two types of
telemetry for VMs. First, we use the core-performance-
measurement-unit (PMU) to gather hardware counters
related to memory performance. Speciﬁcally, we use the
top-down-method for analysis (TMA) [ 78,79]. TMA
characterizes how the core pipeline slots are used. For
example, we use the “memory-bound” metric, which is
deﬁned as pipeline stalls due to memory loads and stores.
Figure 12lists these metrics. While TMA was developed
for Intel, its relevant parts are available on AMD and
ARM as well [ 80]. We modify Azure’s production hy-
pervisor to associate these metrics with individual VMs
(§5) and record VM counter samples in a distributed
7Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
CPUTMA pipeline slot for: backend-bound,memory-bound,store-bound,DRAM-latency-boundOther counters: LLC MPI,memory bandwidth,memory parallelismOfﬂinetestrunsA/B testson internalworkloadsRelative slowdown if on pool memoryModel trainingMetricsFeaturesLabelsLatency insensitive?CorePMU
Figure 12: Metrics and training of the latency insensi-
tive model (§ 4.2).This model uses metrics from the core’s
performance-measurement-unit (PMU). It is trained with labels
gathered from ofﬂine runs and internal workloads.
database. All our core-PMU-metrics use simple coun-
ters and induce negligible overhead (unlike event-based
sampling [ 81,82]).
Second, we use hypervisor telemetry to track a VM’s
untouched pages. We use an existing hypervisor counter
that tracks guest-committed memory, which overestimates
used memory. This counter is available for 98% of Azure
VMs. We also scan access bits in the hypervisor page ta-
ble (§ 5). Since we only seek untouched pages, frequently
access bits reset is not required. This minimizes overhead.
4.3.Distributed Control Plane Layer
Figure 11shows the two tasks performed by Pond’s con-
trol plane: (A) predictions to allocate memory during VM
scheduling and (B) QoS monitoring and resolution.
Predictions and VM scheduling (A). Pond uses ML-
based prediction models (§ 4.4) to decide how much pool
memory to allocate for a VM during scheduling. After
a VM request arrives (A1), the scheduler queries the dis-
tributed ML serving system (A2) for a prediction on how
much local memory to allocate for the VM. The scheduler
then informs the Pool Manager about the target host and
associated pool memory needs (A3). The Pool Manager
triggers a memory onlining workﬂow using the conﬁgu-
ration bus to the EMCs and host (A4). Memory onlining
is fast enough to not block a VM’s start time (§ 4.2). The
scheduler informs the hypervisor to start the VM on a
zNUMA node matching the onlined memory amount.
Memory ofﬂining is slow and cannot happen on the
critical path of VM starts (§ 4.2). Pond resolves this by
always keeping a buffer of unallocated pool memory. This
buffer is replenished when VMs terminate and hosts asyn-
chronously release associated slices.
QoS monitoring (B). Pond continuously inspects the per-
formance of all running VMs via its QoS monitor. The
monitor queries hypervisor and hardware performance
counters (B1) and uses an ML model of latency sensi-
tivity (§ 4.4) to decide whether the VM’s performance
impact exceeds the PDM. In this case, the monitor asks
its mitigation manager (B2) to trigger a memory recon-
Hypervisoraccess bitsCorePMUCPUWorkload history?DecisionLatency insensitive?Untouched memory?Prediction model
Entirely pool DRAMPool DRAM=untouchedEntirely local DRAMActionYesNoYesNoYesNo
Latency insensitive?NoYesReconﬁguration mitigation(A) VM scheduling(B) QoS monitoringOverpredicted untouched?Continue monitoringNoYesVM type, OS, Region,Percentiles of memoryusage in previous VMby same Customer,Workload name.VM Metadata
CorePMUCPUFigure 13: Pond prediction models (§ 4.4). Pond’s two
prediction models (dark grey) rely on telemetry (blue boxes) that
is available for all VM types, including third-party opaque VMs.
ﬁguration (§ 4.2) through the hypervisor (B3). After this
reconﬁguration, the VM uses only local memory.
4.4.Prediction Models
Pond’s VM scheduling (A) and QoS monitoring (B) algo-
rithms rely on two prediction models (in Figure 13).
Predictions for VM scheduling (A). For scheduling, we
ﬁrst check if we can correlate a workload history with the
VM requested. This works by checking if there have been
previous VMs with the same metadata as the request VM,
e.g., the customer-id, VM type, and location. This is based
on the observation that VMs from the same customer tend
to exhibit similar behavior [ 48].
If we have prior workload history, we make a predic-
tion on whether this VM is likely to be memory latency
insensitive, i.e., its performance would be within the PDM
while using only pool memory. (Model details appear
below.) Latency-insensitive VMs are allocated entirely
on pool DRAM.
If the VM has no workload history or is predicted to
be latency-sensitive, we predict untouched memory ( UM)
over its lifetime. Interestingly, UMpredictions with only
generic VM metadata such as customer history, VM type,
guest OS, and location are accurate (§ 6). VMs without
untouched memory ( UM=0) are allocated entirely with
local DRAM. VMs with a UM>0are allocated with a
rounded-down GB-aligned percentage of pool memory
and a corresponding zNUMA node; the remaining mem-
ory is allocated on local DRAM.
If we underpredict UM, the VM will not touch the slower
pool memory as the guest OS prioritizes allocating local
DRAM. If we overpredict UM, we rely on the QoS monitor
for mitigation. Importantly, Pond always keeps a VM’s
memory mapped in hypervisor page tables at all times.
This means that even if our predictions happen to be
incorrect, performance does not fall off a cliff.
8Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
VM: memory, cores, OS;Location: region,  availability zone;Customer history fromprevious VMs:  0/25/50/75/100   percentile untouched;Workload nameGuest-committedmemory counterAccess bits neverset since VM startUntouched memory percentageModel trainingMetadataFeaturesLabelsUntouched memory?Hypervisorpage tablesAddress   A M
Figure 14: Training of the untouched memory model (§ 4.4).
This model uses VM metadata as features and labels of un-
touched memory gathered from hypervisor telemetry.
QoS monitoring (B). For zNUMA VMs, Pond monitors
if it overpredicted the amount of untouched memory dur-
ing scheduling. For pool-backed VMs and zNUMA VMs
with less untouched memory than predicted, we use the
sensitivity model to determine whether the VM workload
is suffering excessive performance loss. If not, the QoS
monitor initiates a live VM migration to a conﬁguration
allocated entirely on local DRAM.
Model details. Pond’s two ML prediction models con-
sume telemetry that is available for opaque VMs from
Pond’s system software layer (§ 4.2). Figure 12shows
features, labels, and the training procedure for the latency
insensitivity model. The model uses supervised learning
(§5) with core-PMU metrics as features and the slowdown
of pool memory relative to NUMA-local memory as la-
bels. Pond gets samples of slowdowns from ofﬂine test
runs and A/B tests of internal workloads which make their
performance numbers available. These feature-label-pairs
are used to retrain the model daily. As the core-PMU is
lightweight (§ 5), Pond continuously measures core-PMU
metrics at VM runtime. This enable the QoS monitor to
react quickly and enables retaining a history of VMs that
have been latency sensitive.
Figure 14shows the inputs and training procedure for
the untouched-memory model. The model uses super-
vised learning (details in § 5) with VM metadata as fea-
tures and the minimum untouched memory over each
VM’s lifetime as labels. Its most important feature is
a range of percentiles ( e.g., 80th–99th) of the recorded
untouched memory by a customer’s VMs in the last week.
Parameterization of prediction models. Pond’s latency
insensitivity model is parameterized to stay below a target
rate of false positives (FP),i.e., workloads it incorrectly
speciﬁes as latency insensitive but which are actually
sensitive to memory latency. This parameter enforces a
tradeoff as the percentage of workloads that are labeled as
latency insensitive ( LI) is a function of FP. For example,
a rate of 0.1% FPmay force the model to 5% of LI.
Similarly, Pond’s untouched memory model is parame-
terized to stay below a target rate of overpredictions (OP),
i.e., workloads that touch more memory than predictedand thus would use memory pages on the zNUMA node.
This parameter enforces a tradeoff as the percentage of
untouched memory ( UM) is a function of OP. For example,
a rate of 0.1% OPmay force the model to 3% of UM.
With two models and their respective parameters, Pond
needs to decide how to balance FPandOPbetween the
two models. This balance is done by solving an optimiza-
tion problem based on the given performance degradation
margin ( PDM) and the target percentage of VMs that meet
this margin ( TP). Speciﬁcally, Pond seeks to maximize
the average amount of memory that is allocated on the
CXL pool, which is deﬁned by LIandUM, while keep-
ing the percentage of false positives ( FP) and untouched
overpredictions ( OP) below the TP.
maximize (LIPDM)+(UM)
subject to (FPPDM)+(OP)(100 TP) (1)
Note that TPessentially deﬁnes how often the QoS moni-
tor has to engage and initiate memory reconﬁgurations.
Besides PDMandTP, Pond has no other parameters as
it automatically solves the optimization problem from
Eq.(1). The models rely on their respective framework’s
default hyperparameters (§ 5).
5.Implementation
We implement and evaluate Pond on production servers
that emulate pool latency. Pond artifacts are open-sourced
athttps://github.com/vtess/Pond .
System software. This implementation comprises three
parts. First, we emulate a single-socket system with a
CXL pool on a two-socket server by disabling all cores
in one socket, while keeping its memory accessible from
the other socket. This memory mimics the pool.
Second, we change Azure’s hypervisor to instantiate
arbitrary zNUMA topologies. We extend the API between
the control plane and the host to pass the desired zNUMA
topology to the hypervisor.
Third, we implement support in Azure’s hypervisor for
the telemetry required for training Pond’s models. We
extend each virtual core’s metadata with a copy of its core-
PMU state and transfer this state when it gets scheduled
on different physical cores. Pond samples core-PMU
counters once per second, which takes 1ms. We enable
access bit scanning in hypervisor page tables. We scan
and reset access bits every 30 minutes, which takes 10s.
Distributed control plane. We train our prediction mod-
els by aggregating daily telemetry into a central database.
The latency insensitivity model uses a simple random
forest (RandomForest) from Scikit-learn [ 83] to classify
whether a workload exceeds the PDM. The model uses a
set of 200 hardware counters as supported by current Intel
processors. The untouched memory model uses a gradient
9Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
boosted regression model (GBM) from LightGBM [ 84]
and makes a quantile regression prediction with a conﬁg-
urable target percentile. After exporting to ONNX [ 85],
the prototype adds the prediction (the size of zNUMA)
on the VM request path using a custom inference serv-
ing system similar to [ 86–88]. Azure’s VM scheduler
incorporates zNUMA requests and pool memory as an
additional dimension into its bin packing, similar to other
cluster schedulers [ 49,89–93].
6.Evaluation
Our evaluation addresses the performance of zNUMA
VMs (§ 6.2,§6.3), the accuracy of Pond’s prediction mod-
els (§ 6.4), and Pond’s end-to-end DRAM savings (§ 6.5).
6.1.Experimental Setup
We evaluate the performance of our prototype using
158 cloud workloads. Speciﬁcally, our workloads
span in-memory databases and KV-stores (Redis [ 94],
VoltDB [ 95], and TPC-H on MySQL [ 96]), data and
graph processing (Spark [ 97] and GAPBS [ 98]), HPC
(SPLASH2x [ 99]), CPU and shared-memory benchmarks
(SPEC CPU [ 100] and PARSEC [ 101]), and a range
of Azure’s internal workloads (Proprietary). Figure 4
overviews these workloads. We quantify DRAM savings
with simulations.
Prototype setup. We run experiments on production
servers at Azure and similarly-conﬁgured lab servers. The
production servers use either two Intel Skylake 8157M
sockets with each 384GB of DDR4, or two AMD EPYC
7452 sockets with each 512GB of DDR4. On Intel, we
measure 78ns NUMA-local latency and 80GB/s band-
width and 142ns remote latency and 30GB/s bandwidth
(3/4 of a CXL ⇥8 link). On AMD, we measure 115ns
NUMA-local latency and 255ns remote latency. Our
BIOS disables hyper-threading, turbo-boost, and C-states.
We use performance results of VMs entirely backed
by NUMA-local DRAM as our baseline . We present
zNUMA performance as normalized slowdowns, i.e., the
ratio to the baseline. Performance metrics are workload
speciﬁc, e.g., job runtime, throughput and tail latency, etc.
Each experiment involves running the application with
one of 7 zNUMA sizes (as percentages of the workload’s
memory footprint in Figure 16). With at least three rep-
etitions of each run and 158 workloads, our evaluation
spans more than 3,500 experiments and 10,000 machine
hours. Most experiments used lab servers; we spot check
outliers on production servers.
Simulations. Our simulations are based on traces of pro-
duction VM requests and their placement on servers. The
traces are from randomly selected 100 clusters across 34
datacenters globally over 75 days.
Workloads Trafﬁc to zNUMA
Video 0.25%
Database 0.06%
KV store 0.11%
Analytics 0.38%
Figure 15: Effectiveness of zNUMA (§ 6.2).Latency sensi-
tive workloads get a local vNUMA node large enough to cover
the workload’s footprint. zNUMA nodes holds the VM’s remain-
ing memory on Pond CXL pool. Access bit scans, e.g., for Video
(right), show that this conﬁguration indeed minimizes trafﬁc to
the zNUMA node.
The simulator implements different memory allocation
policies and tracks each server and each pool’s memory
capacity at second accuracy. Generally, the simulator
schedules VMs on the same nodes as in the trace and
changes their memory allocation to match the policy. For
rare cases where a VM does not ﬁt on a server, e.g., due to
insufﬁcient pool memory, the simulator moves the VMs
to another server.
Model evaluation. We evaluate our model with produc-
tion resource logs. About 80% of VMs have sufﬁcient
history to make a sensitivity prediction. Our deployment
does not report each workload’s perceived performance
(opaque VMs). We thus evaluate latency sensitivity model
based on our 158 workloads.
6.2.zNUMA VMs on Production Nodes
We perform a small-scale experiment on Azure produc-
tion nodes to validate zNUMA VMs. The experiment
evaluates four internal workloads: an audio/video confer-
encing application, a database service, a key-value store,
and a business analytics service. To see the effectiveness
of zNUMA, we assume a correct prediction of untouched
memory, i.e., the local footprint ﬁts into the VM’s local
vNUMA node. Figure 15shows access bit scans over 48
hours from the video workload and a table that shows the
trafﬁc to the zNUMA node for the four workloads.
Finding 1. We ﬁnd that zNUMA nodes are effective at
containing the memory access to the local vNUMA node.
A small fraction of accesses goes to the zNUMA node.
We suspect that this is in part due to the guest OS memory
manager’s metadata that is explicitly allocated on each
10Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
{
0204060
00 % 1 0 % 2 0 % 4 0 %60% 75% 100%Pool Memory [%]Slowdown [%]CorrectUntouchedPredictionAll local memoryOverprediction of untouched memory 
Figure 16: Slowdown under different pool allocations
(§6.3).Performance for no pool memory and a correctly-sized
zNUMA is comparable. Small slowdowns arise from run-to-run
variation. Slowdown become noticable as soon as the workload
spills into the zNUMA and steadily increases until the whole
workload is allocated on pool memory (100% spilled).
vNUMA node. We ﬁnd that the video workload sends
fewer than 0.25% of memory accesses to the zNUMA
node. Similarly, the other three workloads send 0.06-
0.38% of memory access to the zNUMA node. Accesses
within the local vNUMA node are spread out
Implications. With a negligible fraction of memory ac-
cesses on zNUMA, we expect negligible performance
impact given a correct prediction of untouched memory.
6.3.zNUMA VMs in the Lab
We scale up our evaluation to 158 workloads in a lab set-
ting. Since we fully control these workloads, we can now
also explicitly measure their performance. We rerun each
workload on all-local memory, a correctly sized zNUMA
(0% spilled), differently-sized zNUMA nodes sized be-
tween 10-100% of the workload’s footprint. Figure 16
shows a violin plot of associated slowdowns. This setup
covers both normal behavior (all-local and 0% spill) and
misprediction behavior for latency sensitive workloads.
Thus, this is effectively a sensitivity study.
Finding 2. With a correct prediction of untouched mem-
ory, workload slowdowns have a similar distribution to
all-local memory.
Implications. This performance result is expected since
the zNUMA node is rarely accessed (§ 6.2). Our evalua-
tion can thus assume no performance impact under correct
predictions of untouched memory (§ 6.5).
Finding 3. For overpredictions of untouched memory
(and correspondingly undersized local vNUMA nodes),
the workload spills into zNUMA. Many workloads see
an immediate impact on slowdown. Slowdowns further
increase if more workload memory spills into zNUMA.
Some workloads are slowed down by as much as 30-35%
with 20-75% of workload memory spilled and up to 50%
if entirely allocated on pool memory. We use access bit
scans to verify that these workloads indeed actively access
0102001 0 2 030 40 5060Workloads Insensitive to CXL Latency [%]False Positives:Slowdown > PDM [%]Memory-BoundDRAM-BoundRandomForestFigure 17: Latency insensitivity model (§ 6.4).As we in-
crease how many workloads are marked as insensitive ( LI), the
rate of false positives ( FP) increases. Pond’s RandomForest
slightly outperforms a heuristic based only on the DRAM-bound
TMA performance counter.
their entire working set.
Implications. Allocating a ﬁxed percentage of pool
DRAM to VMs would lead to signiﬁcant performance
slowdowns. There are only two strategies to reduce this
impact: 1) identify which workloads will see slowdowns
and 2) allocate untouched memory on the pool. Pond
employs both strategies.
6.4.Performance of Prediction Models
We evaluate Pond’s prediction models (§ 4.4) and its com-
bined prediction model based on Eq.( 1).
6.4.1. Predicting Latency Sensitivity Pond seeks to pre-
dict whether a VM is latency insensitive, i.e., whether
running the workload on pool memory would stay within
the performance degradation margin ( PDM). We tested the
model for PDMbetween 1-10% and on both 182% and
222% latency increases, but report details only for 5%
and 182%. Other PDMvalues lead to qualitatively simi-
lar results. The 222% model is 16% less effective given
the same false positive rate target. We compare thresh-
olds on memory and DRAM boundedness [ 78,79] to our
RandomForest (§ 5).
Figure 17shows the model’s false positive rate as a
function of the percentage of workloads labeled as latency
insensitive, similar to a precision-recall curve [ 102]. Error
bars show 99% conﬁdence from a 100-fold validation
based on randomly splitting into equal-sized training and
testing datasets.
Finding 4. While DRAM boundedness is correlated with
slowdown, we ﬁnd examples where high slowdown oc-
curs even for a small percentage of DRAM boundedness.
For example, multiple workloads exceed 20% slowdown
with just two percent of DRAM boundedness.
Implication. This shows the general hardness of predict-
ing whether workloads exceed the PDM. Heuristic as well
as predictors will make statistical errors.
Finding 5. We ﬁnd that “DRAM bound” signiﬁcantly
outperforms “Memory bound” (Figure 17). Our Random-
11Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
0510152025
010 20 3040 50Average Untouched Memory [% GB-Hours]Overpredictions [% VMs]Fixed Amount / VMGBM Model 1GB-aligned
Figure 18: Untouched memory model (§ 6.4).As we in-
crease untouched memory ( UM), our GBM has a signiﬁcantly
lower rate of overpredictions ( OP) that a strawman model.
Forest performs slightly better than “DRAM bound”.
Implication. Our RandomForest can place 30% of work-
loads on the pool with only 2% of false positives.
6.4.2. Predicting Untouched Memory Pond predicts
the amount of untouched memory over a VM’s future life-
time (§ 4.4). We evaluate this model using metadata and
resource usage logs from 100 clusters over 75 days. The
model is trained nightly and evaluated on the subsequent
day. Figure 18compares our GBM model to the heuristic
that assumes a ﬁxed fraction of memory as untouched
across all VMs. The ﬁgure shows the overprediction rate
as a function of the average amount of untouched memory.
Figure 19shows a production version of the untouched
memory model during the ﬁrst 110 days of 2022.
Finding 6. We ﬁnd that the GBM model is 5⇥more
accurate than the static policy, e.g., when labeling 20% of
memory as untouched, GBM overpredicts only 2.5% of
VMs while the static policy overpredicts 12%.
Implication. Our prediction model identiﬁes 25% of un-
touched memory while only overpredicting 4% of VMs.
Finding 7. The production version of our model performs
similarly to the simulated model. Distributional shifts lead
to some variability over time.
Implication. We ﬁnd that accurately predicting un-
touched memory is practical and a realistic assumption.
6.4.3. Combined Prediction Models We characterize
Pond’s combined models (Eq. (1)) using “scheduling mis-
predictions”, i.e., the fraction of VMs that will exceed the
PDM. This incorporates the overpredictions of untouched
memory, how much the model overpredicted, and the
probability of this overprediction leading to a workload
exceeding the PDM. Further, Pond uses its QoS monitor
to mitigate up to 1% of mispredictions. Figure 20shows
scheduling mispredictions as a function of the average
amount of cluster DRAM that is allocated on its pools for
182% and 222% memory latency increases, respectively.
Finding 8. Pond’s combined model outperforms its in-
dividual models by ﬁnding their optimal combination.
01020304050
02 550 75100Day in 2022Average UntouchedMemory [%]0510Overpredictions[% of VMs]Figure 19: Untouched memory ML model performance in
production (§ 6.4).Our production model targets 4% over-
predictions ( OP). It average untouched memory percentage is
similar to the simulated model (Figure 18).
Implication. With a 2% scheduling misprediction target,
Pond can schedule 44% and 35% of DRAM on pools with
182% and 222% memory latency increases, respectively.
6.5.End-to-end Reduction in Stranding
We characterize Pond’s end-to-end performance while
constraining its rate of scheduling mispredictions. Fig-
ure21shows the reduction in aggregate cluster memory
as a function of pool size for Pond under 182% and 222%
memory latency increase, respectively, and a strawman
static allocation policy. We evaluate multiple scenarios;
the ﬁgure shows PDM=5% and TP=98%. In this scenario,
the strawman statically allocates each VM with 15% of
pool DRAM. About 10% of VMs would touch the pool
DRAM (Figure 18). Of those touching pool DRAM, we’d
expect that about1
4would see a slowdown exceeding a
PDM=5% (Figure 16). So, the strawman would have about
2.5% of scheduling mispredictions.
Finding 9. At a pool size of 16 sockets, Pond reduces
overall DRAM requirements by 9% and 7% under 182%
and 222% latency increases, respectively. Static reduces
DRAM by 3%. When varying PDMbetween 1 and 10%
andTPbetween 90 and 99.9% we ﬁnd the relative savings
of the three systems to be qualitatively similar.
Implication. Pond can safely reduce cost. A QoS mon-
itor that mitigates more than 1% of mispredictions, can
achieve more aggressive performance targets ( PDM).
Finding 10. Throughout the simulations, Pond’s pool
memory ofﬂining speeds remain below 1GB/s and 10GB/s
for 99.99% and 99.999% of VM starts, respectively.
Implication. Pond is practical and achieve design goals.
7.Discussion
Robustness of ML. Similar to other oversubscribed re-
sources (CPU [ 55] and disks [ 103]), customers may
12Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
012345
02 040 60Average Pool DRAM [%]Slowdown > PDM [%]At 182% (142ns)At 222% (255ns)Pond with CXL latency emulated
Figure 20: Combined model (§ 6.4).Pond’s overall tradeoff
between average allocation of pool memory and mispredictions
after solving Eq. (1).
overuse resources to get local memory. When multiplex-
ing resource for millions of customers, any individual
customer’s behavior will have a small impact. Providers
can also provide small discounts when resources are not
fully utilized.
Alternatives to static memory preallocation. Pond is
designed for compatibility static memory as potential
workarounds are not yet practical. The PCIe Address
Translation Service (ATS/PRI) [ 104] enables compatibil-
ity with page faults. Unfortunately, ATS/PRI-devices are
not yet broadly available [ 1]. Virtual IOMMUs [ 2,5,6]
allow ﬁne-grained pinning but require guest OS changes
and introduce overhead.
8.Related Work
Hardware-level disaggregation: Hardware-based disag-
gregation designs [ 19,58,105–109] are not easily de-
ployable as they do not rely on commodity hardware.
For instance, ThymesisFlow [ 58] and Clio [ 105] propose
FPGA-based rack-scale memory disaggregation designs
on top of OpenCAPI [ 110] and RDMA. Their hardware
layer shares goals with Pond. Their software goals dif-
fer fundamentally, e.g., ThymesisFlow advocates appli-
cation changes for performance, while Pond focuses on
platform-level ML-driven pool memory management that
is transparent to users.
Hypervisor/OS level disaggregation: Hypervisor/OS
level approaches [ 17,21,22,31–38] rely on page faults
and access monitoring to maintain the working set in lo-
cal DRAM. Such OS-based approaches bring signiﬁcant
overhead, jitter, and are incompatible with virtualization
acceleration ( e.g., DDA).
Runtime/application level disaggregation: Runtime-
based disaggregation designs [ 23,25,26] propose cus-
tomized APIs for remote memory access. While effective,
this approach requires developers to explicitly use these
mechanisms at the application level.
Memory tiering: Prior works have considered the
broader impact of extended memory hierarchies and how
859095100
28 1 6 3 264Pool Scope [CPU Sockets]Required Overall DRAM [%]Fixed 15% percentage of VM memory with CXL latency at 182%Pond with CXL latency at 222%Pond with CXL latency at 182%Figure 21: Memory savings under performance constraints
(§6.5).Simulated end-to-end evaluation of memory savings
achieved by Pond under PDM=5% and scheduling mispredic-
tions TP=98%.
to handle them [ 17,111–114]. For example, Google
achieves 6 µs latency via proactive hot/cold page detec-
tion and compression [ 17,115]. Nimble [ 75] optimizes
Linux’s page tracking mechanism to tier pages for in-
creased migration bandwidth. Pond takes a different ML-
based approach looking at memory pooling design at the
platform-level and is orthogonal to these works.
ML for systems: ML is increasingly applied to tackle
systems problems, such as cloud efﬁciency [ 48,55], mem-
ory/storage optimizations [ 116,117], microservices [ 118],
caching/prefetching policies [ 119,120]. We uniquely ap-
ply ML methods for untouched memory prediction to
support pooled memory provisioning to VMs without
jeopardizing QoS.
Coherent memory and NUMA optimizations: Tradi-
tional cache coherent NUMA architectures [ 121] use
specialized interconnects to implement a shared address
space. There are also system-level optimizations for
NUMA, such as NUMA-aware data placement [ 122] and
proactive page migration [ 76]. NUMA scheduling poli-
cies [ 123–125] balance compute and memory across
NUMA nodes. Pond’s ownership overcomes the need for
coherence across the memory pool. zNUMA’s zero-core
nature requires rethinking of existing optimizations which
are largely optimized for symmetric NUMA systems.
9.Conclusion
DRAM costs are an increasing cost factor for cloud
providers. This paper is motivated by the observation
of stranded and untouched memory across 100 produc-
tion cloud clusters. We proposed Pond, the ﬁrst full-
stack memory pool that satisﬁes the requirements of cloud
providers. Pond comprises contributions at the hardware,
system software, and distributed system layers. Our re-
sults showed that Pond can reduce the amount of needed
DRAM by 7% with a pool size of 16 sockets and assum-
ing CXL increases latency by 222%. This translates into
an overall reduction of 3.5% in cloud server cost.
13Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
References
[1]Ilya Lesokhin, Haggai Eran, Shachar Raindel, Guy Shapiro, Sagi
Grimberg, Liran Liss, Muli Ben-Yehuda, Nadav Amit, and Dan
Tsafrir. Page Fault Support for Network Controllers. In Proceed-
ings of the 22nd ACM International Conference on Architectural
Support for Programming Languages and Operating Systems
(ASPLOS) , 2017.
[2]Kun Tian, Yu Zhang, Luwei Kang, Yan Zhao, and Yaozu Dong.
coIOMMU: A Virtual IOMMU with Cooperative DMA Buffer
Tracking for Efﬁcient Memory Management in Direct I/O. In
Proceedings of the 2020 USENIX Annual Technical Conference
(ATC) , 2020.
[3]Ben-Ami Yassour, Muli Ben-Yehuda, and Orit Wasserman. On
the DMA Mapping Problem in Direct Device Assignment. In
Proceedings of the 3rd ACM International Conference on Systems
and Storage (SYSTOR) , 2010.
[4]Paul Willmann, Scott Rixner, and Alan L. Cox. Protection Strate-
gies for Direct Access to Virtualized I/O Devices. In Proceedings
of the USENIX Annual Technical Conference (ATC) , 2008.
[5]Nadav Amit, Muli Ben-Yehuda, IBM Research, Dan Tsafrir, and
Assaf Schuster. vIOMMU: Efﬁcient IOMMU Emulation. In
Proceedings of the 2011 USENIX Annual Technical Conference
(ATC) , 2011.
[6]Muli Ben-Yehuda, Michael D. Day, Zvi Dubitzky, Michael Fac-
tor, Nadav Har’El, Abel Gordon, Anthony Liguori, Orit Wasser-
man, and Ben-Ami Yassour. The Turtles Project: Design and
Implementation of Nested Virtualization. In Proceedings of the
9th USENIX Symposium on Operating Systems Design and Im-
plementation (OSDI) , 2010.
[7]AWS: Enhanced networking support. https://docs.aws.a
mazon.com/AWSEC2/latest/UserGuide/enhanced-net
working.html#supported_instances .
[8]Azure Accelerated Networking: Supported VM instances. http
s://docs.microsoft.com/en-us/azure/virtual-net
work/accelerated-networking-overview#supported
-vm-instances .
[9]Onur Mutlu. Memory Scaling: A Systems Architecture Perspec-
tive. In IEEE International Memory Workshop (IMW) , 2013.
[10] Prashant Nair, Dae-Hyun Kim, and Moinuddin K. Qureshi.
ArchShield: Architectural Framework for Assisting DRAM
Scaling by Tolerating High Error Rates. In Proceedings of the
40th Annual International Symposium on Computer Architecture
(ISCA) , 2013.
[11] Onur Mutlu. Main Memory Scaling: Challenges and Solution
directions. In More than Moore Technologies for Next Generation
Computer Design . Springer, 2015.
[12] Sung-Kye Park. Technology Scaling Challenge and Future
Prospects of DRAM and NAND Flash Memory. In IEEE In-
ternational Memory Workshop (IMW) , 2015.
[13] Shigeru Shiratake. Scaling and Performance Challenges of Fu-
ture DRAM. In IEEE International Memory Workshop (IMW) ,
2020.
[14] The Next New Memories. https://semiengineering.com/
the-next-new-memories/ , 2019.
[15] Micron Ends 3D XPoint Memory. https://www.forbes.c
om/sites/tomcoughlin/2021/03/16/micron-ends-3d
-xpoint-memory/ , 2021.
[16] CXL And Gen-Z Iron Out A Coherent Interconnect Strategy.
https://www.nextplatform.com/2020/04/03/cxl-and-
gen-z-iron-out-a-coherent-interconnect-strateg
y/, 2020.
[17] Andres Lagar-Cavilla, Junwhan Ahn, Suleiman Souhlal, Neha
Agarwal, Radoslaw Burny, Shakeel Butt, Jichuan Chang, Ashwin
Chaugule, Nan Deng, Junaid Shahid, Greg Thelen, Kamil Adam
Yurtsever, Yu Zhao, and Parthasarathy Ranganathan. Software-Deﬁned Far Memory in Warehouse-Scale Computers. In Proceed-
ings of the 24th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems
(ASPLOS) , 2019.
[18] Johannes Weiner, Niket Agarwal, Dan Schatzberg, Leon Yang,
Hao Wang, Blaise Sanouillet, Bikash Sharma, Tejun Heo,
Mayank Jain, Chunqiang Tang, and Dimitrios Skarlatos. TMO:
Transparent Memory Ofﬂoading in Datacenters. In Proceed-
ings of the 27th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems
(ASPLOS) , 2022.
[19] Kevin T. Lim, Jichuan Chang, Trevor N. Mudge, Parthasarathy
Ranganathan, Steven K. Reinhardt, and Thomas F. Wenisch. Dis-
aggregated Memory for Expansion and Sharing in Blade Servers.
InProceedings of the 36th Annual International Symposium on
Computer Architecture (ISCA) , 2009.
[20] Kevin Lim, Yoshio Turner, Jichuan Chang, J Renato Santos, and
Parthasarathy Ranganathan. Disaggregated Memory Beneﬁts for
Server Consolidation. HP Laboratories , 2011.
[21] Wenqi Cao and Ling Liu. Hierarchical Orchestration of Disag-
gregated Memory. IEEE Transactions on Computers (TC) , 69(6),
June 2020.
[22] Kevin Lim, Yoshio Turner, Jose Renato Santos, Alvin AuY-
oung, Jichuan Chang, Parthasarathy Ranganathan, and Thomas F.
Wenisch. System-level Implications of Disaggregated Memory.
InProceedings of the 18th International Symposium on High
Performance Computer Architecture (HPCA-18) , 2012.
[23] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K. Aguilera, and
Adam Belay. AIFM: High-Performance, Application-Integrated
Far Memory. In Proceedings of the 14th USENIX Symposium on
Operating Systems Design and Implementation (OSDI) , 2020.
[24] Sebastian Angel, Mihir Nanavati, and Siddhartha Sen. Disaggre-
gation and the Application. In The 12th USENIX Workshop on
Hot Topics in Cloud Computing (HotCloud) , 2020.
[25] Chenxi Wang, Haoran Ma, Shi Liu, Yuanqi Li, Zhenyuan Ruan,
Khanh Nguyen, Michael D. Bond, Ravi Netravali, Miryung Kim,
and Guoqing Harry Xu. Semeru: A Memory-Disaggregated Man-
aged Runtime. In Proceedings of the 14th USENIX Symposium
on Operating Systems Design and Implementation (OSDI) , 2020.
[26] Irina Calciu, M. Talha Imran, Ivan Puddu, Sanidhya Kashyap,
Hasan Al Maruf, Onur Mutlu, and Aasheesh Kolli. Rethinking
Software Runtimes for Disaggregated Memory. In Proceed-
ings of the 26th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems
(ASPLOS) , 2021.
[27] Aleksandar Dragojevic, Dushyanth Narayanan, Miguel Castro,
and Orion Hodson. FaRM: Fast Remote Memory. In Proceedings
of the 11th USENIX Symposium on Networked Systems Design
and Implementation (NSDI) , 2014.
[28] Marcos K. Aguilera, Nadav Amit, Irina Calciu, Xavier Deguil-
lard, Jayneel Gandhi, Stanko Novakovic, Arun Ramanathan,
Pratap Subrahmanyam, Lalith Suresh, Kiran Tati, Rajesh
Venkatasubramanian, and Michael Wei. Remote Regions: a
Simple Abstraction for Remote Memory. In Proceedings of the
2018 USENIX Annual Technical Conference (ATC) , 2018.
[29] Shin-Yeh Tsai, Yizhou Shan, and Yiying Zhang. Disaggregating
Persistent Memory and Controlling Them Remotely: An Explo-
ration of Passive Disaggregated Key-Value Stores. In Proceed-
ings of the 2020 USENIX Annual Technical Conference (ATC) ,
2020.
[30] Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Kon-
stantin Taranov, Dejan Milojicic, and Gustavo Alonso. Farview:
Disaggregated Memory with Operator Ofﬂoading for Database
Engines. arXiv:2106.07102 , 2021.
[31] Peter X. Gao, Akshay Narayan, Sagar Karandikar, Joao Car-
reira, Sangjin Han, Rachit Agarwal, Sylvia Ratnasamy, and Scott
1Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
Shenker. Network Requirements for Resource Disaggregation.
InProceedings of the 12th USENIX Symposium on Operating
Systems Design and Implementation (OSDI) , 2016.
[32] Juncheng Gu, Youngmoon Lee, Yiwen Zhang, Mosharaf Chowd-
hury, and Kang G. Shin. Efﬁcient Memory Disaggregation with
Inﬁniswap. In Proceedings of the 14th USENIX Symposium on
Networked Systems Design and Implementation (NSDI) , 2017.
[33] Yizhou Shan, Yutong Huang, Yilun Chen, and Yiying Zhang.
LegoOS: A Disseminated, Distributed OS for Hardware Resource
Disaggregation. In Proceedings of the 13th USENIX Symposium
on Operating Systems Design and Implementation (OSDI) , 2018.
[34] Kwangwon Koh, Kangho Kim, Seunghyub Jeon, and Jaehyuk
Huh. Disaggregated Cloud Memory with Elastic Block Manage-
ment. IEEE Transactions on Computers (TC) , 68(1), 2019.
[35] Emmanuel Amaro, Christopher Branner-Augmon, Zhihong Luo,
Amy Ousterhout, Marcos K. Aguilera, Aurojit Panda, Sylvia
Ratnasamy, and Scott Shenker. Can Far Memory Improve Job
Throughput? In Proceedings of the 2020 EuroSys Conference
(EuroSys) , 2020.
[36] Hasan Al Maruf and Mosharaf Chowdhury. Effectively Prefetch-
ing Remote Memory with Leap. In Proceedings of the 2020
USENIX Annual Technical Conference (ATC) , 2020.
[37] Blake Caldwell, Sepideh Goodarzy, Sangtae Ha, Richard Han,
Eric Keller, Eric Rozner, and Youngbin Im. FluidMem: Full,
Flexible, and Fast Memory Disaggregation for the Cloud. In
International Conference on Distributed Computing Systems
(ICDCS) , 2020.
[38] Youngmoon Lee, Hassan Al Maruf, Mosharaf Chowdhury, and
Kang G. Shin. Mitigating the Performance-Efﬁciency Tradeoff
in Resilient Memory Disaggregation. arXiv:1910.09727 , 2019.
[39] Compute Express Link Speciﬁcation. Available at https://ww
w.computeexpresslink.org , 2020.
[40] Sapphire Rapids Uncovered: 56 Cores, 64GB HBM2E, Multi-
Chip Design. https://www.tomshardware.com/news/in
tel-sapphire-rapids-xeon-scalable-specificatio
ns-and-features , 2021.
[41] CXL Consortium Member Spotlight: Arm. https://www.co
mputeexpresslink.org/post/cxl-consortium-member-
spotlight-arm , 2021.
[42] AMD Unveils Workload-Tailored Innovations and Products at
The Accelerated Data Center Premiere. https://www.amd.co
m/en/press-releases/2021-11-08-amd-unveils-wor
kload-tailored-innovations-and-products-the-ac
celerated , 2021.
[43] Christopher Lameter. Flavors of Memory supported by Linux,
Their Use and Beneﬁt. https://events19.linuxfounda
tion.org/wp-content/uploads/2017/11/The-Flavor
s-of-Memory-Supported-by-Linux-their-Use-and-B
enefit-Christoph-Lameter-Jump-Trading-LLC.pdf ,
2019.
[44] Alexandru Agache, Marc Brooker, Andreea Florescu, Alexandra
Iordache, Anthony Liguori, Rolf Neugebauer, Phil Piwonka, and
Diana-Maria Popa. Firecracker: Lightweight Virtualization for
Serverless Applications. In Proceedings of the 17th USENIX
Symposium on Networked Systems Design and Implementation
(NSDI) , 2020.
[45] Intel Virtualization Technology for Directed I/O. https://so
ftware.intel.com/content/dam/develop/external/
us/en/documents/vt-directed-io-spec.pdf , 2020.
[46] Huaicheng Li, Mingzhe Hao, Stanko Novakovic, Vaibhav Gogte,
Sriram Govindan, Dan R. K. Ports, Irene Zhang, Ricardo Bian-
chini, Haryadi S. Gunawi, and Anirudh Badam. LeapIO: Efﬁcient
and Portable Virtual NVMe Storage on ARM SoCs. In Proceed-
ings of the 25th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems
(ASPLOS) , 2020.[47] Single-Root Input/Output Virtualization. httpp://www.pcis
ig.com/specifications/iov/single_root , 2019.
[48] Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich,
Marcus Fontoura, and Ricardo Bianchini. Resource Central:
Understanding and Predicting Workloads for Improved Resource
Management in Large Cloud Platforms. In Proceedings of the
26th ACM Symposium on Operating Systems Principles (SOSP) ,
2017.
[49] Ori Hadary, Luke Marshall, Ishai Menache, Abhisek Pan, Esa-
ias E Greeff, David Dion, Star Dorminey, Shailesh Joshi, Yang
Chen, Mark Russinovich, and Thomas Moscibroda. Protean: VM
Allocation Service at Scale. In Proceedings of the 14th USENIX
Symposium on Operating Systems Design and Implementation
(OSDI) , 2020.
[50] Abhishek Verma, Madhukar Korupolu, and John Wilkes. Evaluat-
ing Job Packing in Warehouse-scale Computing. In International
Conference on Cluster Computing (Cluster) , 2014.
[51] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Op-
penheimer, Eric Tune, and John Wilkes. Large-scale cluster
management at Google with Borg. In Proceedings of the 2015
EuroSys Conference (EuroSys) , 2015.
[52] Rina Panigrahy, Kunal Talwar, Lincoln Uyeda, and Udi Wieder.
Heuristics for Vector Bin Packing. https://www.microsof
t.com/en-us/research/publication/heuristics-for-
vector-bin-packing/ , 2011.
[53] Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula,
Sriram Rao, and Aditya Akella. Multi-Resource Packing for
Cluster Schedulers. In Proceedings of the ACM Special Interest
Group on Data Communication (SIGCOMM) , 2014.
[54] Yossi Azar, Ilan Reuven Cohen, Seny Kamara, and Bruce Shep-
herd. Tight Bounds for Online Vector Bin Packing. In Pro-
ceedings of the 45th ACM symposium on Theory of Computing
(STOC) , 2013.
[55] Yawen Wang, Kapil Arya, Marios Kogias, Manohar Vanga,
Aditya Bhandari, Neeraja J. Yadwadkar, Siddhartha Sen, Sameh
Elnikety, Christos Kozyrakis, and Ricardo Bianchini. SmartHar-
vest: Harvesting Idle CPUs Safely and Efﬁciently in the Cloud.
InProceedings of the 2021 EuroSys Conference (EuroSys) , 2021.
[56] Pradeep Ambati, Íñigo Goiri, Felipe Vieira Frujeri, Alper Gun,
Ke Wang, Brian Dolan, Brian Corell, Sekhar Pasupuleti, Thomas
Moscibroda, Sameh Elnikety, Marcus Fontoura, and Ricardo
Bianchini. Providing SLOs for Resource-Harvesting VMs in
Cloud Platforms. In Proceedings of the 14th USENIX Symposium
on Operating Systems Design and Implementation (OSDI) , 2020.
[57] Marcos K. Aguilera, Nadav Amit, Irina Calciu, Xavier Deguil-
lard, Jayneel Gandhi, Pratap Subrahmanyam, Lalith Suresh, Ki-
ran Tati, Rajesh Venkatasubramanian, and Michael Wei. Remote
Memory in the Age of Fast Networks. In Proceedings of the 8th
ACM Symposium on Cloud Computing (SoCC) , 2017.
[58] Christian Pinto, Dimitris Syrivelis, Michele Gazzetti, Panos
Koutsovasilis, Andrea Reale, Kostas Katrinis, and Peter Hofs-
tee. ThymesisFlow: A Software-Deﬁned, HW/SW Co-Designed
Interconnect Stack for Rack-Scale Memory Disaggregation. In
53rd Annual IEEE/ACM International Symposium on Microar-
chitecture (MICRO-53) , 2020.
[59] CXL 2.0 Speciﬁcation. https://www.computeexpresslink
.org/download-the-specification , 2020.
[60] Compute Express Link 2.0 White Paper. https://b373ea
f2-67af-4a29-b28c-3aae9e644f30.filesusr.com/ug
d/0c1418_14c5283e7f3e40f9b2955c7d0f60bebe.pdf ,
2021.
[61] Debendra Das Sharma. CXL 3.0: New Features for Increased
Scale and Optimized Resource Utilization. In Flash Memory
Summit , 2022.
[62] CXL 3.0 Speciﬁcation. https://www.computeexpresslink
.org/download-the-specification , 2022.
2Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
[63] Debendra Das Sharma. Compute Express Link: An Open
Industry-standard Interconnect Enabling Heterogenous Data-
centric Computing. In Proceedings of the 29th IEEE Hot In-
terconnects symposium (HotI29) , 2022.
[64] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes
Weiner, Niket Agarwal, Pallab Bhattacharya, Chris Petersen,
Mosharaf Chowdhury, Shobhit Kanaujia, and Prakash Chauhan.
TPP: Transparent Page Placement for CXL-Enabled Tiered Mem-
ory.arXiv:2206.02878 , 2022.
[65] Donghyun Gouk, Sangwon Lee, Miryeong Kwon, , and Myoung-
soo Jung. Direct Access, High-Performance Memory Disaggre-
gation with DirectCXL. In Proceedings of the 2022 USENIX
Annual Technical Conference (ATC) , 2022.
[66] AMD EPYC Genoa and SP5 Platform Leaked. https://docs
.aws.amazon.com/AWSEC2/latest/UserGuide/disk-p
erformance.html , 2021.
[67] Reliability, Availability, and Serviceability (RAS) Integration
and Validation Guide for the Intel Xeon Processor E7 Family.
https://www.intel.com/content/dam/develop/exte
rnal/us/en/documents/emca2-integration-validat
ion-guide-556978.pdf , 2015.
[68] AMD EPYC brings new RAS capability. https://www.amd.
com/system/files/2017-06/AMD-EPYC-Brings-New-R
AS-Capability.pdf , 2017.
[69] CXL Use-cases Driving the Need For Low Latency Performance
Retimers. https://www.microchip.com/en-us/about/bl
og/learning-center/cxl--use-cases-driving-the-
need-for-low-latency-performance-reti , 2021.
[70] Enabling PCIe 5.0 System Level Testing and Low Latency Mode
for CXL. https://www.asteralabs.com/videos/aries-
smart-retimer-for-pcie-gen-5-and-cxl/ , 2021.
[71] Elene Chobanyan, Casey Morrison, and Pegah Alavi. End-to-
End System-Level Simulations with Retimers for PCIe Gen5 &
CXL. DesignCon, slides available at https://www.asterala
bs.com/wp-content/themes/astera-labs/images/re
timer-cxl.pdf , 2020.
[72] Timothy Prickett Morgan. Pci-express 5.0: The unintended but
formidable datacenter interconnect. DesignCon, slides available
athttps://www.nextplatform.com/2021/02/03/pci-ex
press-5-0-the-unintended-but-formidable-datace
nter-interconnect/ , 2021.
[73] MIPI I3C Bus Sensor Speciﬁcation. https://www.mipi.org
/specifications/i3c-sensor-specification , 2021.
[74] UEFI. Advanced Conﬁguration and Power Interface Speciﬁca-
tion, 2021.
[75] Zi Yan, Daniel Lustig, David Nellans, and Abhishek Bhattachar-
jee. Nimble Page Management for Tiered Memory Systems. In
Proceedings of the 24th ACM International Conference on Ar-
chitectural Support for Programming Languages and Operating
Systems (ASPLOS) , 2019.
[76] Huang Ying. AutoNUMA: Optimize Memory Placement for
Memory Tiering System. https://lwn.net/Articles/835
402/ , 2019.
[77] Adam Ruprecht, Danny Jones, Dmitry Shiraev, Greg Harmon,
Maya Spivak, Michael Krebs, Miche Baker-Harvey, and Tyler
Sanderson. Vm live migration at scale. ACM SIGPLAN Notices ,
53(3), March 2018.
[78] Ahmad Yasin. A Top-Down Method for Performance Analysis
and Counters Architecture. In IEEE International Symposium on
Performance Analysis of Systems and Software (ISPASS) , 2014.
[79] Top-down Microarchitecture Analysis Method. https://so
ftware.intel.com/content/www/us/en/develop/doc
umentation/vtune-cookbook/top/methodologies/to
p-down-microarchitecture-analysis-method.html ,
2021.
[80] Mateusz Jarus and Ariel Oleksiak. Top-Down CharacterizationApproximation Based on Performance Counters Architecture for
AMD Processors. Simulation Modelling Practice and Theory ,
2016.
[81] Soramichi Akiyama and Takahiro Hirofuchi. Quantitative Evalu-
ation of Intel PEBS Overhead for Online System-noise Analysis.
InProceedings of the 7th International Workshop on Runtime
and Operating Systems for Supercomputers (ROSS) , 2017.
[82] Amanda Raybuck, Tim Stamler, Wei Zhang, Mattan Erez, and
Simon Peter. HeMem: Scalable Tiered Memory Management
for Big Data Applications and Real NVM. In Proceedings of the
28th ACM Symposium on Operating Systems Principles (SOSP) ,
2021.
[83] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent
Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter
Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine Learning in Python. Journal of Machine Learning
Research (JMLR) , 12, 2011.
[84] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen,
Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A Highly
Efﬁcient Gradient Boosting Decision Tree. Advances in Neural
Information Processing Systems (NIPS) , 2017.
[85] ONNX. Open Neural Network Exchange: the Open Standard for
Machine Learning Interoperability. https://onnx.ai/ , 2021.
[86] Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harm-
sen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and
Jordan Soyke. Tensorﬂow-serving: Flexible, High-performance
ML Serving. In Proceedings of the 31st Conference on Neural
Information Processing Systems (NIPS) , 2017.
[87] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin,
Joseph E. Gonzalez, and Ion Stoica. Clipper: A Low-Latency
Online Prediction Serving System. In Proceedings of the 14th
USENIX Symposium on Networked Systems Design and Imple-
mentation (NSDI) , 2017.
[88] Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich,
Marcus Fontoura, and Ricardo Bianchini. Resource Central:
Understanding and Predicting Workloads for Improved Resource
Management in Large Cloud Platforms. In Proceedings of the
26th ACM Symposium on Operating Systems Principles (SOSP) ,
2017.
[89] Brendan Burns, Brian Grant, David Oppenheimer, Eric Brewer,
and John Wilkes. Borg, Omega, and Kubernetes. Communica-
tions of the ACM , 59(5), 2016.
[90] Christina Delimitrou and Christos Kozyrakis. Paragon: Qos-
aware scheduling for heterogeneous datacenters. ACM SIGPLAN
Notices , 48(4), 2013.
[91] Christina Delimitrou, Daniel Sanchez, and Christos Kozyrakis.
Tarcil: Reconciling Scheduling Speed and Quality in Large
Shared Clusters. In Proceedings of the 6th ACM Symposium
on Cloud Computing (SoCC) , 2015.
[92] Malte Schwarzkopf, Andy Konwinski, Michael Abdel-Malek,
and John Wilkes. Omega: ﬂexible, scalable schedulers for large
compute clusters. In Proceedings of the 2013 EuroSys Confer-
ence (EuroSys) , 2013.
[93] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Op-
penheimer, Eric Tune, and John Wilkes. Large-scale Cluster
Management at Google with Borg. In Proceedings of the 2015
EuroSys Conference (EuroSys) , 2015.
[94] Redis. https://redis.io , 2021.
[95] VoltDB. https://www.voltdb.com , 2021.
[96] TPC-H Benchmark. http://www.tpc.org/tpch , 2021.
[97] HiBench: The Bigdata Micro Benchmark Suite. https://gi
thub.com/Intel-bigdata/HiBench , 2021.
[98] Scott Beamer, Krste Asanovi ´c, and David Patterson. The GAP
Benchmark Suite. arXiv:1508.03619 , 2015.
[99] Xusheng Zhan, Yungang Bao, Christian Bienia, and Kai Li. PAR-
SEC3.0: A Multicore Benchmark Suite with Network Stacks
3Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’23) (Preprint)
and SPLASH-2X. ACM SIGARCH Computer Architecture News
(CAN) , 44(5), 2017.
[100] SPEC CPU 2017. https://www.spec.org/cpu2017 , 2021.
[101] Christian Bienia, Sanjeev Kumar, Jaswinder Pal Singh, and Kai
Li. The PARSEC Benchmark Suite: Characterization and Archi-
tectural Implications. In IEEE International Conference on Par-
allel Architectures and Compilation Techniques (PACT) , 2008.
[102] Michael Buckland and Fredric Gey. The Relationship Between
Recall and Precision. Journal of the American Society for Infor-
mation Science , 45(1), 1994.
[103] AWS: optimize disk performance for instance store volumes.
https://docs.aws.amazon.com/AWSEC2/latest/User
Guide/disk-performance.html .
[104] PCI Express Address Translation Services. https://compos
ter.com.ua/documents/ats_r1.1_26Jan09.pdf , 2009.
[105] Zhiyuan Guo, Yizhou Shan, Xuhao Luo, Yutong Huang, and
Yiying Zhang. Clio: A Hardware-Software Co-Designed Dis-
aggregated Memory System. In Proceedings of the 27th ACM
International Conference on Architectural Support for Program-
ming Languages and Operating Systems (ASPLOS) , 2022.
[106] Vlad Nitu, Boris Teabe, Alain Tchana, Canturk Isci, and Daniel
Hagimont. Welcome to Zombieland: Practical and Energy-
efﬁcient Memory Disaggregation in a Datacenter. In Proceedings
of the 2018 EuroSys Conference (EuroSys) , 2018.
[107] Irina Calciu, Ivan Puddu, Aasheesh Kolli, Andreas Nowatzyk,
Jayneel Gandhi, Onur Mutlu, and Pratap Subrahmanyam. Project
PBerry: FPGA Acceleration for Remote Memory. In Proceed-
ings of the 17th Workshop on Hot Topics in Operating Systems
(HotOS XVII) , 2019.
[108] K. Katrinis, D. Syrivelis, D. Pnevmatikatos, G. Zervas,
D. Theodoropoulos, I. Koutsopoulos, K. Hasharoni, D. Raho,
C. Pinto, F. Espina, S. Lopez-Buedo, Q. Chen, M. Nemirovsky,
D. Roca, H. Klos, and T. Berends. Rack-scale Disaggregated
Cloud Data Centers: The dReDBox Project Vision. In Design
Automation and Test in Europe (DATE) , 2016.
[109] Jorge Gonzalez, Alexander Gazman, Maarten Hattink, Mauricio
G. Palma, Meisam Bahadori, Ruth Rubio-Noriega, Lois Orosa,
Madeleine Glick, Onur Mutlu, Keren Bergman, and Rodolfo
Azevedo. Optically Connected Memory for Disaggregated Data
Centers. In IEEE 32nd International Symposium on Computer
Architecture and High Performance Computing (SBAC-PAD) ,
2020.
[110] OpenCAPI Consortium. https://opencapi.org/ , 2021.
[111] Sudarsun Kannan, Ada Gavrilovska, Vishal Gupta, and Karsten
Schwan. HeteroOS: OS Design for Heterogeneous Memory
Management in Datacenters. In Proceedings of the 44th An-
nual International Symposium on Computer Architecture (ISCA) ,
2017.
[112] Neha Agarwal and Thomas F. Wenisch. Thermostat: Application-
transparent Page Management for Two-tiered Main Memory. In
Proceedings of the 22nd ACM International Conference on Ar-
chitectural Support for Programming Languages and Operating
Systems (ASPLOS) , 2017.
[113] Ahmed Abulila, Vikram Sharma Mailthody, Zaid Qureshi, Jian
Huang, Nam Sung Kim, Jinjun Xiong, and Wen mei Hwu. Flat-
Flash: Exploiting the Byte-Accessibility of SSDs within a Uni-
ﬁed Memory-Storage Hierarchy. In Proceedings of the 24th ACM
International Conference on Architectural Support for Program-
ming Languages and Operating Systems (ASPLOS) , 2019.
[114] Jonghyeon Kim, Wonkyo Choe, and Jeongseob Ahn. Exploring
the Design Space of Page Management for Multi-Tiered Memory
Systems. In Proceedings of the 2021 USENIX Annual Technical
Conference (ATC) , 2021.
[115] Linux Memory Management Documentation - zswap. https:
//www.kernel.org/doc/html/latest/vm/zswap.html ,
2020.[116] Giulio Zhou and Martin Maas. Learning on Distributed Traces
for Data Center Storage Systems. In Proceedings of the 4th
Conference on Machine Learning and Systems (MLSys) , 2021.
[117] Martin Maas, David G. Andersen, Michael Isard, Moham-
mad Mahdi Javanmard, Kathryn S. McKinley, and Colin Raffel.
Learning-based Memory Allocation for C++ Server Workloads.
InProceedings of the 25th ACM International Conference on Ar-
chitectural Support for Programming Languages and Operating
Systems (ASPLOS) , 2020.
[118] Yanqi Zhang, Weizhe Hua, Zhuangzhuang Zhou, G. Edward Suh,
and Christina Delimitrou. Sinan: ML-Based and QoS-Aware Re-
source Management for Cloud Microservices. In Proceedings of
the 26th ACM International Conference on Architectural Support
for Programming Languages and Operating Systems (ASPLOS) ,
2021.
[119] Zhan Shi, Akanksha Jain, Kevin Swersky, Milad Hashemi,
Parthasarathy Ranganathan, and Calvin Lin. A Hierarchical
Neural Model of Data Prefetching. In Proceedings of the 26th
ACM International Conference on Architectural Support for Pro-
gramming Languages and Operating Systems (ASPLOS) , 2021.
[120] Zhan Shi, Xiangru Huang, Akanksha Jain, and Calvin Lin. Ap-
plying Deep Learning to the Cache Replacement Problem. In
52nd Annual IEEE/ACM International Symposium on Microar-
chitecture (MICRO-52) , 2019.
[121] James Laudon and Daniel Lenoski. The SGI Origin: A ccNUMA
Highly Scalable Server. In Proceedings of the 24th Annual Inter-
national Symposium on Computer Architecture (ISCA) , 1997.
[122] Baptiste Lepers, Vivien Quéma, and Alexandra Fedorova. Thread
and Memory Placement on NUMA Systems: Asymmetry Mat-
ters. In Proceedings of the 2015 USENIX Annual Technical
Conference (ATC) , 2015.
[123] Jonathan Corbet. Autonuma: the other approach to numa schedul-
ing.https://lwn.net/Articles/488709/ , 2012.
[124] Dulloor Subramanya Rao and Karsten Schwan. vNUMA-mgr:
Managing VM memory on NUMA platforms. In IEEE Inter-
national Conference on High Performance Computing (HiPC) ,
2010.
[125] Ming Liu and Tao Li. Optimizing Virtual Machine Consoli-
dation Performance on NUMA Server Architecture for Cloud
Workloads. In Proceedings of the 41th Annual International
Symposium on Computer Architecture (ISCA) , 2014.
4Memory disaggregation:
why now and what are the challenges
Marcos K. Aguilera1, Emmanuel Amaro1, Nadav Amit1, Erika Hunhoff2, Anil Yelam3, Gerd Zellweger1
1VMware Research2University of Colorado, Boulder3UC San Diego
Abstract
Hardware disaggregation has emerged as one of the most
fundamental shifts in how we build computer systems
over the past decades. While disaggregation has been
successful for several types of resources (storage, power,
and others), memory disaggregation has yet to happen.
We make the case that the time for memory disaggre-
gation has arrived. We look at past successful disaggre-
gation stories and learn that their success depended on
two requirements: addressing a burning issue and being
technically feasible. We examine memory disaggrega-
tion through this lens and ﬁnd that both requirements
are ﬁnally met. Once available, memory disaggregation
will require software support to be used effectively. We
discuss some of the challenges of designing an operating
system that can utilize disaggregated memory for itself
and its applications.
1 Introduction
Hardware disaggregation , or simply disaggregation ,
means separating hardware resources (e.g., disks, GPU,
memory) that have been traditionally combined in a sin-
gle server enclosure. Disaggregation has physical and log-
ical implications. Physically, the disaggregated resource
is placed in a separate box or chassis, increasing its dis-
tance from other resources. Logically, the disaggregated
resource becomes less coupled with the rest of the sys-
tem.
Figure 1illustrates memory disaggregation: the mem-
ory moves from the conﬁnes of a host into a memory pool,
which can then be accessed by multiple servers. This pro-
vides two beneﬁts: servers can access large amounts of
memory in the pool (more than a server can have locally)
and servers can use the disaggregated memory to efﬁ-
ciently share data. Note one need not disaggregate allserversCPUmemoryCPUmemoryCPUmemoryCPUmemoryCPUmemoryCPUmemCPUmemCPUmemCPUmemCPUmemdisaggmemoryserversmemorypoolFigure 1: With traditional servers (left), we have CPU,
memory, disks, and network adapters on a single enclo-
sure. With memory disaggregation (right), we move some
of that memory to a pool. Servers still have some local
memory but they can also access the memory in the pool
to obtain additional memory or share data.
memory: some of it remains local memory in servers.
Memory disaggregation originated from ideas in the
1990s [ 11], and Intel proposed a commercial architecture
in 2013 [ 16]. Nevertheless, memory disaggregation has
yet to happen in production systems.
To understand why, we look at history to study other
forms of successful disaggregation: mainframe comput-
ers, storage, power and cooling, and GPUs (Section 2).
From these efforts, we learn that disaggregation succeeds
based on two requirements: the need to solve a burning
issue and the availability of a feasible technology.
We argue that these two requirements are now met for
memory disaggregation as big data increasingly moves
to memory, and memory becomes an expensive resource;
meanwhile, the emergence of Compute eXpress Link
(CXL) provides technical feasibility (Section 3).38
Once available, disaggregated memory can signiﬁ-
cantly beneﬁt data-intensive systems, such as machine
learning data pipelines, distributed computing frame-
works, and database systems. These systems can lever-
age the two main capabilities of disaggregated memory
(larger memories and data sharing across servers), but
they will require appropriate operating system support.
We discuss the challenges of building an operating sys-
tem that can leverage disaggregated memory to beneﬁt
both applications and the operating system itself (Sec-
tion4).
2 Disaggregation: the past
Disaggregation brings different beneﬁts and trade-offs
that depend on the disaggregated resource. In the rest of
this section, we discuss past successful disaggregation
efforts and the lessons we can learn from them.
2.1 From mainframes to clusters
The transition from large mainframe computers to com-
puter clusters in the 1970s and 1980s can be seen as an
extreme form of disaggregation where many resources
were simultaneously disaggregated: a monolithic box
was separated into multiple minicomputers that worked
together. Initially, these minicomputers were tightly cou-
pled to provide the illusion of a single system (e.g., in a
V AXcluster, one could uniformly handle processes, users,
and disks from any computer in the cluster). Over time,
minicomputers became less and less coupled with each
other, losing their single-system transparency in favor
of larger systems deployed across broader geographic
locations (e.g., Unix clusters). This trend eventually led
to systems that connect minicomputers in different or-
ganizations (often universities or national laboratories)
over long-distance lines using modems at ﬁrst, or ded-
icated lines later, to exchange email and access usenet
newsgroups.
Here, disaggregation was motivated initially by cost
and scale (mainframes were expensive) and later by ac-
cessibility (users around the country wanted access to
computers, but mainframes were only available in a few
locations, while the smaller computers could be spread
around). Ultimately, mainframe disaggregation led to the
creation of the Internet.
2.2 Storage disaggregation
Storage disaggregation followed a gradual path starting
in the early 1990s. It started with the realization thatif a computer crashes, all data in its locally attached
disks become inaccessible. To address this problem and
improve data availability, the industry introduced dual-
ported SCSI disks, which could be connected to two com-
puters simultaneously. This idea evolved into storage area
networks (SANs), where disk arrays are attached to a net-
work fabric connected to many computers which can
access any disk. The network fabric was initially dedi-
cated for storage (Fibre Channel) but later proposals used
general-purpose networks (iSCSI, NVMe over Fabrics).
Although storage disaggregation was initially moti-
vated by improved data availability, it later brought other
beneﬁts: manageability (it is easier to manage a central
pool of storage than storage spread over all computers),
reliability (the central storage pool became sophisticated
fault-tolerant disk arrays), and disaster tolerance (disk
arrays could copy data to other disk arrays across wide
area networks).
2.3 Power and cooling disaggregation
Power supplies and fans are clunky components that tend
to fail often. Thus, for reliability, servers tend to have
two sets of power supplies and fans, but this is a signiﬁ-
cant waste of space and money: a rack with 40 servers
would require 80 power supplies and fans. It makes much
more sense to disaggregate the power supplies and fans
so that many servers can share a few. Blade servers ac-
complished this in the 2000s, where a single chassis with
its own power supplies and fans housed multiple servers.
This idea has motivated the rack designs in the Open
Compute Project [ 25] and Open19 [ 26], which are used
in today’s hyperscalers: each rack has power supplies
and fans, and rack servers take DC power as input.
Power and cooling disaggregation was initially moti-
vated by density and cost in blade servers, and it eventu-
ally led to new data center designs.
2.4 GPU disaggregation
Graphics Processing Units, or GPUs, started as graphics
cards that produced a computer’s video output. Later,
GPUs gained general-purpose acceleration support for
Single Instruction Multiple Data computations (e.g., vec-
tor and matrix operations), often used in machine learning
algorithms. Modern GPUs are extremely powerful and
have a much higher degree of parallelism than proces-
sors by several orders of magnitude. This power comes
at a high cost in price and power consumption. If many
servers need access to GPUs, it is expensive to provi-
sion each server with its own GPU. If those GPUs are
used at different times, their utilization will be low. With39GPU disaggregation, the GPUs in a pool are accessible
to many servers, so one can provision a smaller number
of GPUs and increase their utilization. This idea was ﬁrst
implemented in software in the mid-2010s by redirecting
CUDA calls to a remote server housing the GPU (e.g.,
Bitfusion [ 6]). Later, composable infrastructure systems
gained GPU disaggregation support (e.g., HPE Synergy,
Dell PowerEdge MX, Supermicro SuperCloud Composer,
Lenovo ThinkAgile CP).
GPU disaggregation was motivated by utilization and
cost, and we believe the future will bring other beneﬁts
as the idea gains further adoption.
2.5 Lessons
Looking at the past success of hardware disaggregation,
we identify some common patterns. We observe that
there are two requirements for disaggregation of a re-
source to succeed. First, it needs to address a burning
issue; that is, it needs a powerful, compelling motivation:
cost and scalability for mainframe disaggregation, data
availability for storage disaggregation, density and cost
for power and cooling disaggregation, and utilization and
cost for GPU disaggregation. A burning issue is required
because disaggregation needs signiﬁcant and simultane-
ous investment from many parties: chip makers, device
makers, systems integrators, and sometimes operating
system and application vendors. These parties must over-
come signiﬁcant inertia to build solutions that work well
together, and this is only possible with a strong use case.
The second requirement to disaggregate a resource is a
feasible technology , which often requires addressing non-
trivial technical problems at many levels of the system
stack: cluster systems, remote storage systems, blades,
composable hardware, etc.
Another lesson we learn from the past is that, once
disaggregation happens, it brings a much broader impact
than originally anticipated. Mainframe disaggregation
led to a revolution in distributed systems; storage disag-
gregation created an entire industry of storage appliances;
power and cooling disaggregation led to new data center
designs; and GPU disaggregation is still ongoing so the
jury is still out, but it will possibly create new computing
paradigms for processing of data pipelines.
3 Memory disaggregation: why now?
Memory disaggregation originated with paging to re-
mote memory in the early 1990s [ 11]. The network had
become fast enough that it was more efﬁcient to page
to remote memory using the network than to a localdisk. These initial efforts provided a rudimentary form of
memory disaggregation: a processor accessing disaggre-
gated memory incurs a page fault, bringing the data to
local memory before the processor can resume execution.
Full-ﬂedged memory disaggregation was ﬁrst proposed
in [21], where a processor can issue loads and stores di-
rectly to disaggregated memory, without incurring page
faults, thus improving performance.
While memory disaggregation is an old idea, it has
not yet been commercially successful. As we mentioned
before, Intel proposed a Rack Scale Architecture in
2013 [ 16] that included memory disaggregation. How-
ever, the proposal did not succeed because it was based
on the technology of silicon photonics [ 32], which has
not yet prevailed.
That begs the question: why is the right time for mem-
ory disaggregation now? We next explain why we believe
that is the case, by arguing that memory now satisﬁes
the two requirements for successful disaggregation, as
described in Section 2.5: addressing a burning issue and
the existence of feasible technology.
Burning issue. Memory is increasingly becoming a
problematic resource in computing systems, due to four
reasons. First, the need for memory is surging as big
data moves to memory for faster processing in a broad
range of systems and applications, such as in-memory
databases, data analytics, streaming analytics, and ma-
chine learning. Second, the memory capacity of indi-
vidual servers is constrained due to physical limitations
in the density of memory DIMMs and the number of
DIMMs that can be connected to a processor. Third,
over time, applications need more memory but upgrading
memory is notoriously challenging due to the stringent
memory population rules for servers [ 1]—for example,
all memory controllers should have the same number and
sizes of DIMMs, and all memory channels should have
the same capacity. Failure to follow these rules results in
poor memory bandwidth. Fourth, the cost of memory is
growing due to the proliferation of new use cases: smart
vehicles, 5G phones, gaming consoles, and data centers.
Meanwhile, supply remains restricted by an oligopoly
of only three memory companies (Samsung, SK Hynix,
Micron). As a result, memory can dominate the cost of
the system. In fact, cloud operators report that memory
can constitute 50% of server cost [ 20] and 37% of total
cost of ownership [ 23].
Feasible technology. The technology that enables dis-
aggregated memory has signiﬁcantly advanced. Commer-
cially, Remote Direct Memory Access (RDMA) has be-40come commodity with technologies such as RDMA over
Converged Ethernet (RoCE), which provides RDMA on
Ethernet networks. Furthermore, standards for disaggre-
gated memory are emerging, initially through the GenZ
Consortium [ 12] and more recently through Compute
eXpress Link (CXL [ 9]). CXL allows devices to pro-
vide cache-coherent memory on the PCIe bus; version
1 of CXL enables local memory expansion cards while
subsequent versions will enable memory disaggregation
via cards connected to memory pools. Research proto-
types that leverage these technologies, including The
Machine [ 18,34] based on Gen-Z, and DirectCXL [ 13]
and Pond [ 20] based on CXL, demonstrate the feasibil-
ity of disaggregated memory. Moreover, CXL version 1
is already supported by the latest server processors by
Intel and AMD, while device vendors gradually intro-
duce local memory expansion cards. We anticipate CXL
memory disaggregation to come soon after (say, in 3–5
years) following a similar trajectory as storage disaggre-
gation: beginning with multi-ported memory connected
to a few hosts (say, 2–8), followed by CXL switches on
dedicated memory fabrics connecting memory to a rack
of servers, and ultimately reaching convergence of the
network and memory fabric (e.g., CXL over Ethernet or
IP). A noteworthy feature of CXL is that it will allow the
memory pool to use different memory technology from
ones locally attached to servers. Consequently, as servers
evolve to use the newer DDR5 memory, the pool can be
provisioned with older DDR3 and DDR4 memory that is
incompatible or too slow be used in the server.
4An operating system for disaggregated
memory
We are developing a new operating system (OS) that of-
fers ﬁrst-class support for disaggregated memory. Our
goal is to allow the OS and applications to take full advan-
tage of the capabilities of disaggregated memory: greater
memory capacity and the ability to share data efﬁciently
across servers. We refer to the union of the servers and
the disaggregated memory as the cluster . This new OS
differs from traditional distributed OSes in two key ways.
First, we want the OS to conveniently expose disaggre-
gated memory to applications so they can beneﬁt from it.
Second, the OS itself can use disaggregated memory to
improve its design—for example, by sharing kernel data
structures across servers.
Disaggregated memory allows the OS to provide the il-
lusion of a single system across the cluster to applications.
In particular, the OS will allow processes to have threads
running on different servers, and these threads can ac-cess a uniﬁed ﬁle system. In addition, as processes can
span multiple servers, we need a communication stack
that can provide a uniﬁed view across the cluster, so an
external client can reach application processes regardless
of which servers are running them.
In terms of disaggregated memory management, the
OS will offer both transparent and targeted memory al-
locations. A transparent allocation can be served from
either local or disaggregated memory (i.e., the process
does not care), while a targeted allocation chooses be-
tween local memory or the memory pool. Transparent
allocations allow disaggregated memory to be used as an
extension of local memory. In contrast, a targeted allo-
cation provides faster memory (if allocating locally) or
shareable memory (if allocating from the pool).
Providing ﬁrst-class support for shareable disaggre-
gated memory will beneﬁt data-intensive distributed ap-
plications, which often incur signiﬁcant overheads when
sharing data because they must rely on traditional net-
work stacks (e.g., RPC, TCP) and pay the costs of serial-
ization, deserialization, and copying [ 17]. Concretely, we
target data processing systems (e.g., machine learning
pipelines [ 15]), distributed computing frameworks (e.g.,
Spark [ 37], Ray [ 35]), cluster schedulers (e.g., Kuber-
netes), and some traditional applications such as database
systems (SQL and NoSQL), web servers, and ﬁle servers.
For example, a distributed computing framework can
optimize object movement between tasks by using disag-
gregated memory; a database system doing distributed
query processing may create shared intermediate data and
materialized views in disaggregated memory; a cluster
scheduler also needs to share executables, job inputs, and
job outputs; a web server or ﬁle server can share caches;
and all of these systems can beneﬁt from a larger memory
capacity that disaggregated memory can provide.
4.1 Challenges
Building an OS for disaggregated memory raises a num-
ber of challenges.
Memory allocation. When a process allocates mem-
ory, the OS must decide from where. If the application
does not care if the memory is local or disaggregated,
the OS must pick a type based on memory availability
and locality. This presents a memory tiering problem,
where two types of memory offer different trade-offs:
local memory is faster and private, while disaggregated
memory is larger and shared. Even if the application
makes a targeted request for local or disaggregated mem-
ory, the OS needs appropriate mechanisms to organize the41memory. Traditional schemes such as buddy allocators
and slab allocators need to be revisited since allocation
requests will now be distributed across servers and must
scale to large memory sizes [ 22] and a large number of
processes, while keeping fragmentation under control. A
solution is to handle all allocations centrally at one of
the servers; another solution is to allocate memory in
a distributed fashion, which requires efﬁcient coordina-
tion across servers. These solutions trade off simplicity,
global optimality, allocation latency, and fragmentation.
Memory migration presents a related problem: the OS
can move physical memory (while keeping virtual ad-
dresses) to remove fragmentation and improve locality
(e.g., a process has freed some local memory so that data
in disaggregated memory can be migrated locally for
faster access). Memory migration requires policies of
when and what to migrate, analogous to non-uniform
memory access (NUMA) migration policies.
Scheduling. Where should processes and their threads
execute? This decision depends on several factors. First,
the servers eligible to schedule a thread depend on their
resource availability (CPU, local memory, etc.). Second,
processes that share buffers in disaggregated memory
can beneﬁt from cache locality if they are scheduled
on the same server or a small number of servers; this
afﬁnity needs to be considered. Third, the OS should
provide a balanced consumption of per-server CPU and
memory; to do so, the scheduler and memory allocator
must collaborate. Fourth, the memory pool may have
memory regions with different distances to a server, and
it may have regions that are only accessible to some of
the servers due to the topology of the memory fabric; this
is another case where the scheduler and allocator must
coordinate. A scheduler design that weighs these factors
can be centralized (as we can centralize the allocator), but
a distributed design can scale better and provide lower
latency.
Addressing. Disaggregated memory will have a pool
memory controller that connects the pool memory to the
servers. Servers will map virtual addresses to physical
addresses, and the pool controller will map physical ad-
dresses to pool addresses. Translating addresses in the
pool controller allows the system to easily move mem-
ory within the pool (without it, all servers using a shared
buffer would have to ﬁx their mappings when memory
is moved), which in turn is useful for maintenance (e.g.,
adding or removing DIMMs to the pool) and optimization
(e.g., for pools that have regions with different memory
types). This creates many questions: what should thegranularity of these mappings be, what is the underlying
mechanism used to maintain these mappings, and who
will maintain the mappings. Answering these questions
will require a co-design of the pool controller (which per-
forms the actual translations) and the OS (which provides
the functional requirements). Another related challenge
is how to correctly share pointers to shared buffers across
servers. A simple solution is to map the buffers to the
same virtual address at all servers, but this solution may
not scale well as it requires memory allocations to return
unique virtual addresses across processes in the cluster
(if virtual addresses are not unique, buffers cannot be
shared between processes).
OS state. Operating systems keep many types of in-
ternal state (e.g., process tables, device state, network
connections), which raises three challenges for a disag-
gregated memory OS. First, we must identify the state
needed locally at each server and the state that must be
shared across servers. For example, some memory allo-
cation metadata should be shared because we would like
different processes to communicate using disaggregated
memory; similarly, some OS scheduler state should be
shared to support efﬁcient process synchronization mech-
anisms (e.g., wait queues for locks being held). On the
other hand, device state is local because we do not wish
to expose devices across servers. An exception for this
is the storage and network devices, which we address
below.
The second challenge is efﬁciently coordinating ac-
cess by different servers. Although this problem exists in
traditional operating systems, it is more severe when us-
ing disaggregated memory since the coordination cost is
higher (disaggregated memory is slower than local mem-
ory), and more parties can coordinate (thousands of cores
across all CPUs in all servers). In theory, RCU, wait-free
data structures, or ﬁne-grained locking mechanisms can
be used to tackle this challenge. However, with much
higher memory latencies and number of cores, even the
most sophisticated approaches eventually succumb under
contention. Therefore, we need radically new approaches
to manage OS state that scales well for reads and writes
across machines. Partitioning (for writes) and replica-
tion (for reads) are often employed strategies to solve
these challenges. Replication trades off extra memory
consumption for more performance so ideally it can be
applied in a dynamic way, using only memory the OS
can currently spare.
The third challenge is that the larger capacity of disag-
gregated memory will result in much larger kernel data
structures, to the point that they will require their own42memory management mechanisms (e.g., memory migra-
tion). These mechanisms are normally used on user mem-
ory and they can be hard to apply to kernel memory (e.g.,
migration of kernel memory can cause deadlocks as ker-
nel pages get write-protected and subsequently cause a
page fault).
File system. To provide a single ﬁle system view across
all servers, a simple solution is to mount a network ﬁle
system (e.g., NFS, CIFS) on all servers. However, this
approach misses an opportunity to leverage disaggre-
gated memory. File systems can use the larger capacity
of disaggregated memory for caching (buffer cache, page
cache, inode cache, etc.), and they can use the sharing
ability of disaggregated memory to share these caches
across servers to improve cache utilization and reduce
disk IO—if a server reads a ﬁle, another server can ﬁnd its
contents in the disaggregated memory cache. Designing
a disaggregated memory ﬁle system raises the question
of how to keep the various in-memory ﬁle system data
structures, how to synchronize access to these structures,
and how to schedule IO on disk. The result will be a type
of distributed ﬁle system that is uniquely designed for
disaggregated memory.
External communication. In addition to local network
endpoints for each server, we believe the OS should pro-
vide network endpoints for the entire cluster. That is,
the cluster should have an external IP address (or a few
IP addresses) where communication with that IP on a
given port is transparently mapped to a server in the clus-
ter. According to application needs, the target server can
be ﬁxed or it can be chosen from a number of servers
that balance load among themselves. This functionality is
useful for implementing scalable and highly available ser-
vices, such as web servers, ﬁle servers, etc. To implement
this feature, the OS leverages disaggregated memory to
maintain shared network state (e.g., which processes are
bound to which ports) and an efﬁcient way to coordinate
shared access.
Failures. Disaggregated memory can experience par-
tial or even total failures as the pool crashes or loses
connectivity to the servers. The OS needs to handle such
failures gracefully by containing the problem to the parts
of the system that used the failed memory. It may be nec-
essary to kill processes that lost data, but not necessarily
take down the entire OS. Moreover, the OS will support
the notion of optional memory , which is memory that is
not vital for the execution of the process, such as caches
or reconstructible data. If a failure affects only optionalmemory of a process, the process receives an exception
and continues running. Finally, the OS needs to cleanly
decouple the data structures that it keeps in disaggregated
memory to minimize the blast radius. In some cases, it
could make sense to keep critical kernel data structures
entirely in local memory.
Security. We need to ﬁnd a suitable trust model for the
cluster. In a typical OS, the kernel is trusted and processes
are isolated from each other. Our OS can offer defense
in depth by providing isolation of access to the (shared)
disaggregated memory; in particular, there should be iso-
lation between processes in different servers and perhaps
even between kernels in different servers. A related issue
is to provide address space isolation in disaggregated
memory for locations that are not being shared across
servers. Here, again, is an opportunity for a co-design
of the disaggregated memory controller and OS, where
the controller provides isolation mechanisms (e.g., ac-
cess control for servers) while the OS sets the policy.
Enclaves play an important role as they can provide a
thin trusted computing base across servers, which can
be used to control the hardware mechanisms. Disaggre-
gated memory encryption is another interesting capability
and even more relevant than encryption of local mem-
ory, as we expect hot-swap of disaggregated memory
similar to hot-swap of disks, where it is desirable to pro-
tect data of memory being taken out (DRAM chips keep
data residues for some time after they are powered down,
which can lead to known attacks).
5 Other related work
LegoOS [ 31] is an OS designed for hardware disaggre-
gation of all resources (not just memory) based on the
notion of loosely coupled monitors. Our goal is different,
as we are focused on disaggregated memory, speciﬁcally
on how to build an OS that allows applications to best
leverage shareable disaggregated memory. The notion
of a single system image across a cluster is provided by
cluster OSes (e.g., VMS [ 27] for V AXclusters); we be-
lieve it is worth revisiting these design with disaggregated
memory and modern hardware in mind.
Firebox [ 3] and dReDBox [ 5] provide hardware plat-
forms that disaggregate multiple resources, which fur-
ther motivates OSs designed for disaggregation. Work on
HPE’s The Machine included not just the hardware for
persistent disaggregated memory, but also software sup-
port [ 18] and OS challenges [ 10], which overlap some of
the challenges we describe but with a different perspec-
tive (e.g., due to the fact that disaggregated memory is43persistent or due to the speciﬁcs of their hardware design).
Unfortunately, a detailed write-up of their OS design is
not yet available.
Memory vendors are providing toolkits (e.g., [ 33]) to
utilize CXL memory in existing OSes. This effort is ade-
quate for CXL memory expansion cards—which provide
additional local memory—but we believe shareable dis-
aggregated memory will require a new OS or signiﬁcant
OS modiﬁcations.
Remote memory access using RDMA has become
widely available with RoCE [ 29] and have enabled sys-
tems that provide disaggregated memory through pag-
ing [1,14], user libraries [ 30,38], or by leveraging cache
coherence mechanisms [ 8]. These systems provide one
beneﬁt of disaggregated memory (larger memory capac-
ity) but does not allow processes in different servers to
share memory. Some of these systems use software-based
solutions that are likely to underperform the hardware-
based disaggregation that is enabled by CXL.
Recent memory tiering systems have focused on mech-
anisms akin to NUMA migration with the goal of main-
taining hot subsets of memory in fast DRAM while mi-
grating cold subsets to slower locally-attached memory
(e.g., NVM) [ 23,28,36]. Therefore, these systems neither
present disaggregated memory to applications, nor do
they consider sharing of such memory across processes
in multiple servers.
Disaggregated memory can be seen as a form of dis-
tributed shared memory [ 2,4,7,19,24], which has been
extensively studied decades ago and recently. There is an
important lesson to learn from this body of work: cache
coherence is extremely hard to scale. We thus believe
cache-coherent disaggregated memory systems will be
limited to a rack or a few racks of servers.
6 Conclusion
Memory disaggregation will ﬁnally become a reality,
enabled by the emergence of CXL and its adoption by
suppliers of memory, chipsets, controllers, servers, op-
erating systems, and software. This industry alignment
is driven by the memory problems faced by distributed
data-intensive applications. Once available, memory dis-
aggregation will introduce many OS challenges that must
be addressed to best use this technology for memory
expansion and memory sharing across servers.
References
[1]Emmanuel Amaro, Christopher Branner-Augmon,
Zhihong Luo, Amy Ousterhout, Marcos K Aguil-era, Aurojit Panda, Sylvia Ratnasamy, and Scott
Shenker. Can far memory improve job through-
put? In European Conference on Computer Sys-
tems, pages 1–16, April 2020.
[2]Cristiana Amza, Alan L. Cox, Shandya Dwarkadas,
Pete Keleher, Honghui Lu, Ramakrishnan Raja-
mony, Weimin Yu, and Willy Zwaenepoel. Tread-
Marks: Shared memory computing on networks of
workstations. IEEE Computer , 29(2):18–28, Febru-
ary 1996.
[3]Krste Asanovi ´c. FireBox: A hardware building
block for 2020 Warehouse-Scale computers. In
USENIX Conference on File and Storage Technolo-
gies, February 2014. Keynote talk.
[4]J. K. Bennett, J. B. Carter, and W. Zwaenepoel.
Munin: Distributed shared memory based on type-
speciﬁc memory coherence. In ACM Symposium on
Principles and Practice of Parallel Programming ,
pages 168–176, March 1990.
[5]Maciej Bielski, Ilias Syrigos, Kostas Katrinis, Dim-
itris Syrivelis, Andrea Reale, Dimitris Theodor-
opoulos, Nikolaos Alachiotis, Dionisios N. Pnev-
matikatos, Evert H. Pap, Georgios Zervas, Vaib-
hawa Mishra, Arsalan Saljoghei, Alvise Rigo,
Jose Fernando Zazo, Sergio Lopez-Buedo, Martí
Torrents, Ferad Zyulkyarov, Michael Enrico, and
Oscar Gonzalez de Dios. dReDBox: Materializ-
ing a full-stack rack-scale system prototype of a
next-generation disaggregated datacenter. In De-
sign, Automation & Test in Europe Conference &
Exhibition , pages 1093–1098, March 2018.
[6]VMware Bitfusion. https://core.vmware.com/
bitfusion .
[7]Qingchao Cai, Wentian Guo, Hao Zhang, Divyakant
Agrawal, Gang Chen, Beng Chin Ooi, Kian-Lee
Tan, Yong Meng Teo, and Sheng Wang. Efﬁcient
distributed memory management with RDMA and
caching. Proceedings of the VLDB Endowment ,
11(11):1604–1617, July 2018.
[8]Irina Calciu, M. Talha Imran, Ivan Puddu, Sanid-
hya Kashyap, Hasan Al Maruf, Onur Mutlu, and
Aasheesh Kolli. Rethinking software runtimes for
disaggregated memory. In ACM International Con-
ference on Architectural Support for Programming
Languages and Operating Systems , April 2021.
[9]Compute eXpress Link. https://www.
computeexpresslink.org .44[10] Paolo Faraboschi, Kimberly Keeton, Tim Marsland,
and Dejan Milojicic. Beyond processor-centric op-
erating systems. In Workshop on Hot Topics in
Operating Systems , May 2015.
[11] E. Felten and J. Zahorjan. Issues in the implementa-
tion of a remote memory paging system. Technical
Report CSE TR 91-03-09, University of Washing-
ton, March 1991.
[12] Gen-Z consortium. https://en.wikipedia.
org/wiki/Gen-Z_(consortium) .
[13] Donghyun Gouk, Sangwon Lee, Miryeong Kwon,
and Myoungsoo Jung. Direct access, high-
performance memory disaggregation with Di-
rectCXL. In USENIX Annual Technical Conference ,
pages 287–294, June 2022.
[14] Juncheng Gu, Youngmoon Lee, Yiwen Zhang,
Mosharaf Chowdhury, and Kang G Shin. Efﬁcient
memory disaggregation with Inﬁniswap. In Sympo-
sium on Networked Systems Design and Implemen-
tation , pages 649–667, March 2017.
[15] Hannes Hapke and Catherine Nelson. Building
Machine Learning Pipelines . O’Reilly Media, Inc,
July 2020.
[16] Intel rack scale architecture. https:
//www-conf.slac.stanford.edu/
xldb2016/talks/published/Tues_6_
Mohan-Kumar-Rack-Scale-XLDB-Updated.
pdf.
[17] Svilen Kanev, Juan Pablo Darago, Kim Hazel-
wood, Parthasarathy Ranganathan, Tipp Moseley,
Gu-Yeon Wei, and David Brooks. Proﬁling a
warehouse-scale computer. In International Sym-
posium on Computer Architecture , pages 158–169,
June 2015.
[18] Kimberly Keeton. Memory driven computing. In
USENIX Conference on File and Storage Technolo-
gies, February 2017. Keynote presentation.
[19] Seung-seob Lee, Yanpeng Yu, Yupeng Tang,
Anurag Khandelwal, Lin Zhong, and Abhishek
Bhattacharjee. MIND: In-network memory man-
agement for disaggregated data centers. In ACM
Symposium on Operating Systems Principles , pages
488–504, October 2021.
[20] Huaicheng Li, Daniel S. Berger, Stanko Novakovic,
Lisa Hsu, Dan Ernst, Pantea Zardoshti, MonishShah, Samir Rajadnya, Scott Lee, Ishwar Agarwal,
Mark D. Hill, Marcus Fontoura, and Ricardo Bian-
chini. Pond: CXL-based memory pooling systems
for cloud platforms. In ACM International Con-
ference on Architectural Support for Programming
Languages and Operating Systems , March 2023.
[21] Kevin Lim, Jichuan Chang, Trevor Mudge,
Parthasarathy Ranganathan, Steven K Reinhardt,
and Thomas F Wenisch. Disaggregated memory
for expansion and sharing in blade servers.
ACM SIGARCH Computer Architecture News ,
37(3):267–278, June 2009.
[22] Mark Mansi and Michael M. Swift. /0sim: Prepar-
ing system software for a world with terabyte-
scale memories. In ACM International Confer-
ence on Architectural Support for Programming
Languages and Operating Systems , page 267–282,
March 2020.
[23] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Jo-
hannes Weiner, Niket Agarwal, Pallab Bhattacharya,
Chris Petersen, Mosharaf Chowdhury, Shobhit
Kanaujia, and Prakash Chauhan. TPP: Transparent
page placement for CXL-enabled tiered-memory.
InACM International Conference on Architectural
Support for Programming Languages and Operat-
ing Systems , page 742–755, March 2023.
[24] Jacob Nelson, Brandon Holt, Brandon Myers, Pre-
ston Briggs, Luis Ceze, Simon Kahan, and Mark
Oskin. Latency-tolerant software distributed shared
memory. In USENIX Annual Technical Conference ,
pages 291–305, July 2015.
[25] Open compute project. https://www.
opencompute.org .
[26] Open19. https://www.open19.org .
[27] OpenVMS. https://en.wikipedia.org/wiki/
OpenVMS .
[28] Amanda Raybuck, Tim Stamler, Wei Zhang, Mat-
tan Erez, and Simon Peter. HeMem: Scalable tiered
memory management for big data applications and
real NVM. In ACM Symposium on Operating Sys-
tems Principles , pages 392–407, October 2021.
[29] RDMA over Converged Ethernet. https:
//en.wikipedia.org/wiki/RDMA_over_
Converged_Ethernet .45[30] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K
Aguilera, and Adam Belay. AIFM: High-
performance, application-integrated far memory. In
Symposium on Operating Systems Design and Im-
plementation , pages 315–332, November 2020.
[31] Yizhou Shan, Yutong Huang, Yilun Chen, and Yiy-
ing Zhang. LegoOS: A disseminated, distributed
os for hardware resource disaggregation. In Sympo-
sium on Operating Systems Design and Implemen-
tation , pages 69–87, October 2018.
[32] Silicon photonics. https://en.wikipedia.org/
wiki/Silicon_photonics .
[33] Scalable memory development kit. https://
github.com/OpenMPDK/SMDK .
[34] Paul Teich. HPE powers up The Ma-
chine architecture, January 2017. https:
//www.nextplatform.com/2017/01/09/
hpe-powers-machine-architecture .
[35] Stephanie Wang, Eric Liang, Edward Oakes, Ben-
jamin Hindman, Frank Sifei Luan, Audrey Cheng,
and Ion Stoica. Ownership: A distributed futures
system for ﬁne-grained tasks. In Symposium on Net-
worked Systems Design and Implementation , pages
671–686, April 2021.
[36] Zi Yan, Daniel Lustig, David Nellans, and Abhishek
Bhattacharjee. Nimble page management for tiered
memory systems. In ACM International Confer-
ence on Architectural Support for Programming
Languages and Operating Systems , pages 331–345,
April 2019.
[37] Matei Zaharia, Mosharaf Chowdhury, Michael J.
Franklin, Scott Shenker, and Ion Stoica. Spark:
Cluster computing with working sets. In Workshop
on Hot Topics in Cloud Computing , June 2010.
[38] Yang Zhou, Hassan MG Wassel, Sihang Liu, Jiaqi
Gao, James Mickens, Minlan Yu, Chris Kennelly,
Paul Turner, David E Culler, Henry M Levy, et al.
Carbink: Fault-tolerant far memory. In Symposium
on Operating Systems Design and Implementation ,
pages 55–71, July 2022.
46Persistent Memory Research in the Post-Optane Era
Peter Desnoyers
Northeastern University
p.desnoyers@northeastern.eduIan Adams
Intel Corporation
ian.f.adams@intel.comTyler Estro
Stony Brook University
testro@cs.stonybrook.edu
Anshul Gandhi
Stony Brook University
anshul@cs.stonybrook.eduGeo￿Kuenning
Harvey Mudd College
geo￿@cs.hmc.eduMike Mesnier
Intel Corporation
michael.mesnier@intel.com
Carl Waldspurger
Carl Waldspurger Consulting
carl@waldspurger.orgAvani Wildani
Emory University and Cloud ￿are
agadani@gmail.comErez Zadok
Stony Brook University
ezk@fsl.cs.sunysb.edu
ABSTRACT
After over a decade of researcher anticipation for the arrival
of persistent memory (PMem), the ￿rst shipments of 3D
XPoint-based Intel Optane Memory in 2019 were quickly
followed by its cancellation in 2022. Was this another case of
an idea quickly fading from future to past tense, relegating
work in this area to the graveyard of failed technologies?
The recently introduced Compute Express Link (CXL) may
o￿er a path forward, with its persistent memory pro ￿le o￿er-
ing a universal PMem attachment point. Yet new technolo-
gies for memory-speed persistence seem years o ￿, and may
never become competitive with evolving DRAM and ￿ash
speeds. Without persistent memory itself, is future PMem
research doomed? We o ￿er two arguments for why reports
of the death of PMem research are greatly exaggerated.
First, the bulk of persistent-memory research has not in
fact addressed memory persistence, but rather in-memory
crash consistency, which was never an issue in prior sys-
tems where CPUs could not observe post-crash memory
states. CXL memory pooling allows multiple hosts to share
a single memory, all in di ￿erent failure domains, raising
crash-consistency issues even with volatile memory.
Second, we believe CXL necessitates a “disaggregation” of
PMem research. Most work to date assumed a single tech-
nology and set of features, i.e., speed, byte addressability,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for pro ￿t or commercial advantage and that
copies bear this notice and the full citation on the ￿rst page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speci ￿c
permission and/or a fee. Request permissions from permissions@acm.org.
DIMES ’23, October 23, 2023, Koblenz, Germany
©2023 Copyright held by the owner/author(s). Publication rights licensed
to ACM.
ACM ISBN 979-8-4007-0300-3/23/10. . . $15.00
https://doi.org/10.1145/3609308.3625268and CPU load/store access. With an open interface allowing
new topologies and diverse PMem technologies, we argue
for the need to examine these features individually and in
combination.
While one form of PMem may have been canceled, we
argue that the research problems it raised not only remain
relevant but have expanded in a CXL-based future.
CCS CONCEPTS
•Information systems !Storage class memory .
KEYWORDS
Persistent memory, PMem, 3D XPoint, Optane, CXL.
ACM Reference Format:
Peter Desnoyers, Ian Adams, Tyler Estro, Anshul Gandhi, Geo ￿
Kuenning, Mike Mesnier, Carl Waldspurger, Avani Wildani, and Erez
Zadok. 2023. Persistent Memory Research in the Post-Optane Era.
In1st Workshop on Disruptive Memory Systems (DIMES ’23), October
23, 2023, Koblenz, Germany. ACM, New York, NY, USA, 8 pages.
https://doi.org/10.1145/3609308.3625268
1 INTRODUCTION
As CPU processing speeds and core counts continue to grow,
so too do the I/O speeds needed to feed data to ever-faster
CPUs, with some workloads ( e.g., indexes, Bloom ￿lters)
being particularly sensitive to I/O latency. Yet as storage
latencies drop into the 10s of microseconds, improvements
in device speed begin to be overshadowed by software de-
lays and overheads in the OS storage stack. While various
strategies have been used to reduce these overheads [ 54],
persistent memory allows them to be bypassed entirely for
most accesses.
In recent years, the availability of persistent memory
(PMem) has spurred a ￿urry of research [ 5,12,18,22,
25,26,28,29,37,47,53]. PMem’s unique properties en-
couraged research in the storage community and beyond:
23
DIMES ’23, October 23, 2023, Koblenz, Germany P. Desnoyers et al.
algorithms [ 6,10,12], compilers [ 23,32,33], data struc-
tures [ 18,28,29,37],￿le systems [ 25,31,48], key-value
stores [ 5,26,55], operating systems [ 3,27,40,50], and even
non-systems areas have been a ￿ected. Industry e ￿orts pro-
duced the Storage Networking Industry Association (SNIA)
programming model [46] and the PMDK [21] library.
When Intel canceled its 3D XPoint-based Optane product
line [ 20], researchers were suddenly left wondering whether
persistent-memory technologies had any future. Yet behind
the headlines, both Micron [ 36] and Intel [ 19] embraced
the industry Compute Express Link (CXL) [ 8] standard as
their future direction for persistent and hierarchical memory.
Others have also begun to discuss the lessons learned and
outline future prospects for PMem [2, 15, 24, 45].
Persistent memory has in e ￿ect taken one step backwards,
losing a storage technology, and one tentative step forwards,
gaining an alternate, arguably superior interface. This new
interface is not only vendor-independent but multipurpose,
with use cases ( e.g., cache-coherent GPU-to-host access, CXL
memory pooling) that are likely to ensure its viability inde-
pendent of market demands for persistent memory. Before
CXL, only CPU vendors could consider integrating persistent
memory into a system; with CXL, even academic researchers
can design and deploy FPGA-based PMem prototypes. But
should they?
Answering this question requires examining the de ￿ning
characteristics of PMem in more detail: (a) persistence, (b)
byte addressability, and (c) direct access via CPU load/store
instructions.
Byte addressability reduces the cost of small accesses;
load/store access dramatically accelerates some I/O tasks,
providing direct user-space access without kernel interven-
tion. In addition to those features, Optane provided near-
DRAM speed and better-than-DRAM cost per bit.
Given multiple potential persistent technologies and meth-
ods of access, we believe it is important to consider PMem’s
features—including persistence itself—individually as well
as in various combinations. Is load/store access important,
or would user-space byte-granular access via an RDMA-like
mechanism provide similar performance? Which Optane
performance improvements require near-DRAM speed, vs.
those that are enabled by merely better-than-NVMe per-
formance? What about the non-persistent case with CXL
memory pooling, and does multi-host access across multi-
ple failure domains pose the same challenges as single-host
PMem, or new ones? Finally, how important is price, and in
particular would PMem be viable if it were no cheaper than
DRAM?1
1We note that “cheaper than DRAM” is a vague target, as high-density
DIMMs carry a cost premium of up to 10 ⇥over lower densities.2 WHAT IS PERSISTENT MEMORY?
Bypersistent Memory orPMem we refer to media with
byte-addressable access ( e.g., via hardware access at cache-
line granularity)via CPU load/store instructions, with coher-
ent caching, but with the persistence properties of storage.
PMem supports direct memory access (DMA) by other de-
vices, and is fast enough to warrant waiting for a load in-
struction rather than context-switching to another thread as
is done with slower storage ( e.g., NAND Flash) [ 42]. Software
support ( e.g., via libraries conforming to the SNIA NVM Pro-
gramming Model [ 46]) allows PMem implementations using
natively persistent media ( e.g., 3D XPoint) or natively volatile
(e.g., DRAM) devices with hardware support for persistence
in the event of power loss.
Additional higher-level functions supported by the SNIA
model include: (a) PMem-aware ￿le systems— e.g.,ext4 with
the DAX option—which provide naming, access control, and
the ability to map persistent data into the virtual address
space. (b) Library APIs that allow applications to discover
whether store instructions are considered persistent as soon
as they are visible to other threads, or if ￿ush operations are
required to guarantee that stores have been committed. (c)
Software mechanisms to detect failures unique to PMem, e.g.,
an incomplete ￿ush on fail execution after a power failure.
A Brief timeline of PMem products. Battery-backed RAM
has a long history of use for RAID stripe bu ￿ers [16], and
before that magnetic core memory was persistent across
power loss2[39]. However persistent memory as we know
it can be traced to shortly after 2000—both conceptual work
onstorage-class memory [11] and products in the form of
NVDIMM-N [49], DRAM DIMMs with energy storage and
￿ash backup that allow memory contents to last across power
loss. NVDIMMs used standard memory sockets, but required
platform support for power-loss noti ￿cation. They were
shipped by several companies for nearly a decade [ 49], but
because they were much more expensive than conventional
DRAM, they were rarely if ever deployed as entire storage
systems.
Later in that decade, emerging technologies such as Phase
Change Memory [ 11] resulted in sustained research interest
in persistent memory, accelerated by Intel and Micron’s an-
nouncement of 3D XPoint memory and Intel’s Optane plans.
In 2019, Intel began shipping Optane memory devices, us-
ing the DDR-T variant of standard memory sockets. Optane
had much higher capacity and lower cost per gigabyte than
NVDIMM-Ns, since it leveraged the native persistence of
3D XPoint. However, it had lower performance—around 3 ⇥
the latency of DRAM, with bandwidth somewhat lower for
read and much lower for write [ 22]. Since Optane greatly
2Due to cost, this persistence was rarely used for anything except boot
loaders.
24Persistent Memory Research in the Post-Optane Era DIMES ’23, October 23, 2023, Koblenz, Germany
PMem
UserKernelStandardFile API
PMem driverApplicationFile System
    Application
Application
StandardRaw DeviceAccess
Load /StoreStandardFile API
PMem-AwareFile SystemMMUMappings
DAXPMem driver
Block I/O
Figure 1: Block and PMem data paths. Direct access
(DAX, upper right) incurs no software overhead.
outperformed NAND Flash, its primary use case was as a
persistent write cache for very large data structures such as
in-memory databases. Due to its high capacity and (arguably)
lower cost per gigabyte, Optane was also considered as a
potential second tier of volatile memory, cached by DRAM.
Micron stopped production of 3D XPoint in 2021, and
in 2022 Intel discontinued their Optane product line. As
of this writing no other high-capacity PMem products are
available commercially, and no future 3D XPoint products
are expected. The number of companies shipping NVDIMM-
Ns has declined recently, although they are still available in
capacities of around 16–32GB.
PMem bene ￿ts.Figure 1 illustrates the di ￿erence between
the common Block I/O data path and the PMem data path. The
rightmost application in the ￿gure uses standard ￿le APIs to
open and memory-map a PMem ￿le; all PMem I/O is then able
to use the standard load/store model. This is made possible
by the DAX (Direct Access) feature in speci ￿c￿le systems,
allowing mmap to directly map underlying address-space-
resident memory, along with hardware persistence support,
e.g., enabled by appropriate PMDK library operations.
These accesses are far more e ￿cient than access via the
block I/O data path. In the PMem case individual instruc-
tions retrieve data from cache, while the memory controller
issues a single read to the memory device for each cache
line accessed. In contrast, block access typically requires
user/kernel transitions for each access, multiple PCIe trans-
actions for data and descriptor transfers and doorbell register
writes, and a signi ￿cant in-kernel software path3.
The performance di ￿erence is even larger for small ac-
cesses, as block I/Os are typically rounded up to a 4 KB block
size, while PMem is accessed at cache-line granularity. Data
structures can be mapped into application memory as shown
3User-space access through SPDK [ 54] can reduce the software overhead of
this process, but the PCIe overhead remains.by the rightmost arrow in the ￿gure, and then accessed di-
rectly, without needing to copy data into DRAM. This ability
to access persistent data in place is one of the major bene ￿ts
of PMem [44].
PMem challenges. Systems supporting PMem have two
levels of store persistence, as per the SNIA Programming
model. The most common level, avoiding the need for more
expensive platform logic, requires applications to ￿ush stores
explicitly to ensure persistence. While storage has always
worked this way, programmers are not used to having to
￿ush memory stores; this introduces new software complex-
ities. The problem is exacerbated by existing code that as-
sumes block writes are atomic, which allows atomic updates
of large data structures. Libraries like PMDK [ 44] normally
handle some of the complex logic around ￿ushing and trans-
actions, but signi ￿cant work may be needed to adapt existing
software [34].
The second level of persistence is provided by platforms
that automatically ￿ush all CPU caches to PMem on power
loss or system crash. This relieves the software from that
responsibility. But since this feature is not guaranteed to exist
on every platform with PMem, the software must typically
handle both cases, so no complexity is avoided.
The lack of native language support for PMem is also
problematic, requiring libraries to use non-idiomatic con-
structs like preprocessor macros to support PMem, adding
to debugging complexity. Although it is possible that id-
iomatic, usable PMem extensions to high-level languages
will emerge in the future, such improvements typically ar-
rive only slowly. The fact that software must be modi ￿ed to
use PMem at all is itself a problem, since software changes
are expensive. To mitigate this, a number of ways to leverage
PMem transparently have emerged. Ideas such as Whole
System Persistence [ 38] and Whole Process Persistence [ 17]
can leverage the bene ￿ts of PMem’s in-place access without
application modi ￿cation. In many cases language support for
transparent use of PMem may be di ￿cult—existing code of-
ten assumes that data structures are assembled in ephemeral
bu￿ers, never visible outside a limited range of code; the lack
of bu ￿ering in PMem accesses may necessitate signi ￿cant
changes in strategy.
Finally, a consistent pain point for PMem has been that
it isdirectly attached to a single host; if the host goes down,
access to that persistent data is lost. This di ￿ers from other
storage systems that can be attached on the network and
made accessible from multiple hosts ( e.g., NAS, SAN). Several
solutions to replicate PMem in software have been imple-
mented [13, 14, 52], but they increase complexity further.
25DIMES ’23, October 23, 2023, Koblenz, Germany P. Desnoyers et al.
3 CXL: A NEW PMEM INTERFACE
CPU changes were needed to e ￿ciently support new PMem
products. Modi ￿cations to the DDR protocol supported
variable timing [ 1] and power-loss noti ￿cation. For perfor-
mance [ 7,9], new instructions and memory controller de-
signs [43] were needed to quickly and reliably persist data.
In 2019, the ￿rst version of the Compute Express Link (CXL)
speci ￿cation was released by a consortium of over 250 com-
panies. As of November 2020, version 2.0 of the CXL speci-
￿cation contains ￿rst-class support for PMem, rather than
adding it as an afterthought as was done for DDR protocols.
CXL 1.1 and 2.0 run over PCIe 5, while CXL 3.0 uses PCIe
6, introducing three new protocols:
•CXL.io : PCIe functionality, including device enumer-
ation and PCIe-style data transfers.
•CXL.cache : allows device caches as part of the CPU
cache-coherency domain.
•CXL.mem : allows hosts to access device-attached
memory with cache-coherent loads and stores.
A CXL Type 3 Memory Device , built using the CXL.io and
CXL.mem protocols, allows OSes to have a single, generic
driver supporting both volatile memory and PMem, even
on the same device [ 8,24]. Moreover, while previous PMem
devices required explicit CPU support, CXL allows indepen-
dent vendors and even researchers to build a wide variety of
PMem devices. CXL incorporates the lessons learned from
prior PMem products, and in many cases allows binary com-
patibility for applications developed for NVDIMM [ 49] and
Optane devices.
With CXL, memory pooling , supported by the Multi-
Headed Device (MHD) model in CXL 3.0, allows multiple
hosts to access memory presented by a single device. This
provides the ability to disaggregate both volatile and persis-
tent memory, and to dynamically assign it to di ￿erent hosts
over time [ 30], as shown in Figure 2, providing a separate
“memory appliance” with its own power, failure domain, and
reliability characteristics. As an example, Pond [ 30] uses a
custom controller to provide single-host cache coherence,
coupled with dynamic access control assigning each memory
region to a single host at a time.
Such memory pooling o ￿ers an opportunity for appli-
cation-transparent data replication across failure-domain
boundaries. This in turn addresses a key limitation of prior
PMem con ￿gurations, where such replication required ex-
plicit application support, typically requiring slower soft-
ware intervention rather than being implemented in hard-
ware.
Pond and similar approaches allow non-concurrent shar-
ing of memory where, for example, a new host may take
Memory Media
Host 0DisaggregatedMemory PoolHost N
…CXLCXLMemory Media
Host 0DisaggregatedMemory PoolHost N
…CXLCXLFigure 2: Basic CXL memory pooling.
ownership of a memory region after a crash; additional fea-
tures allow concurrent sharing of memory regions by multi-
ple hosts, with either non-coherent access (requiring explicit
￿ush operations) or optionally with full cache coherency
across hosts.
Memory pooling and sharing also introduce new security
concerns. The CXL speci ￿cation supports low-level encryp-
tion for memory and interconnect links [ 8, Section 11.0];
further research is needed in this area.
4 RESEARCH GOING FORWARD
Although other persistent memory technologies predated
it, 3D XPoint was perhaps the ￿rst solid-state technology
to o￿er both cost and performance midway between con-
temporary main memory and block storage technologies—
the cheaper-than-DRAM, faster-than-NAND ￿ash window.
Since this “storage-class memory” window is a moving target,
the emergence of a new and competitive persistent-memory
technology is heavily dependent on progress at both ends
of this window—progress driven by enormous investments
based on the size of these markets.
As a result, it is entirely possible that we will not see a
solid-state memory technology arise that directly replaces 3D
XPoint. Yet we argue that in the CXL era, persistent-memory
research remains just as relevant, for two reasons:
•Hybrid persistent memory [ 41]. Even in the absence of
new technologies, hybrid strategies combining DRAM,
￿ash, and energy storage will enable future CXL-
attached persistent-memory systems at varying price
and performance points.
•Multi-host consistency. PMem raised the new (at
the time) problem of crash consistency for memory;
in previous systems memory contents were lost on
power failure, and the CPU could never observe crash-
inconsistent memory states. CXL memory pooling al-
lows memory to be observed from multiple indepen-
dent failure domains, leading to similar challenges
26Persistent Memory Research in the Post-Optane Era DIMES ’23, October 23, 2023, Koblenz, Germany
#Example Storage / Memory Technologies Coherent Byte-addressable Persistent
1Volatile RAM disk 8 8 8
2Conventional HDD, SSD 8 8 4
3Incoherent load/store to PCIe address space 8 4 8
4Byte-addressable I/O device ( e.g., object storage) 8 4 4
5Memory pooling 4 4 8
6Conventional PMem use cases, including NVDIMM 4 4 4
Table 1: A taxonomy of storage technologies that may support coherency, byte-addressability, and persistence.
even in the absence of persistence, while doing so un-
der a range of topologies and speeds.
Changes brought about by CXL. Historically ( i.e., before
PMem) memory researchers have not had to worry about
issues like data persistence, durability, and availability; these
were issues speci ￿c to storage systems. PMem changed this
and opened up a decade’s worth of research. A key resulting
artifact is PMDK [ 21]—a suite of libraries providing a single
consistency model across a range of hardware persistence
features. Storage researchers working at the device or block
level watched with interest as memory researchers tackled
key storage issues like transactions and atomic writes.
Looking up from the block layer, PMem changed very
little. Researchers quickly dealt with the low-hanging fruit
(e.g., block-mode abstractions to PMem [ 4]). Otherwise, there
were few opportunities at the storage ( i.e., block) layer.
But CXL will change this in two ways: (1) by bringing
memory abstractions to a standardized I/O interconnect, and
(2) by making persistence optional (as discussed earlier, CXL
works with both volatile and non-volatile memory). This
means that memory and storage researchers will need to
coordinate, especially if the goal is an optimized solution
that spans all hardware and software layers.
For example, although a CXL device could be exposed
as a hybrid device with a completely separate memory API
(CXL.mem and/or CXL.cache) and storage API (CXL.io), de-
signing such a solution is a missed opportunity. Rather, the
memory “half” of a device should leverage the storage half
for bulk data, and the storage half should leverage the mem-
ory half for coherence and byte addressability. One example
is a computational SSD that modi ￿es data in host memory,
without resorting to bulk DMA operations. Alternatively,
consider a GPU or an FPGA using CXL.cache to gain coher-
ent access to host memory. If that same data is destined for
block storage, we do not want to send it to the PCI layer a
second time; the data may already be partially present in the
device, just in a memory form. Hence, CXL introduces the
need for the memory and storage halves to coordinate, and
therein lies the potential for new research.New research opportunities. We introduce new opportuni-
ties brought about by CXL across three dimensions: persis-
tence, byte addressability, and coherence. We consider six
of the eight possible combinations: three map to existing
memory or storage technologies and three are entirely new,
representing research opportunities going forward.
For the taxonomy in Table 1, we de ￿nepersistent as being
able to survive a cold reboot or loss of power, coherent4to
mean that read operations (across CPUs or hosts as appro-
priate) will transparently see the result of write operations
from other CPUs or hosts, and byte-addressable as allow-
ing accesses smaller than a single sector (512 bytes). It is
worth noting that byte addressability does not require a co-
herent memory interface. Indeed, object storage protocols
already allow for byte-granular access [ 35] on the PCIe bus
using versions of standard I/O commands; we therefore treat
coherency and byte-addressability independently.
A number of rows represent conventional storage tech-
nologies. Rows 1 and 2 represent RAM disks and conven-
tional block devices such as NVMe drives. Access is at a
block granularity, and cached data ( i.e., kernel bu ￿er caches)
is managed “manually”. Row 6, in turn, corresponds to ex-
isting PMem architectures, combining persistence, cache
coherency, and byte addressability.
Other combinations are less common. In row 3, read and
write operations can be performed at byte granularity, but
without coherence or persistence. PCIe address space pro-
vides these semantics, with operations performed via load
and store instructions. Although the NVMe speci ￿cation
de￿nes an optional PCIe address space allowing such di-
rect access, it is not supported by any commonly available
devices. Alternatively, In ￿niBand RDMA verbs provide an
I/O-operation-based mechanism that is byte-addressable but
o￿ers noncoherent access to (remote) volatile memory.
In row 4, byte addressability and persistence are combined
with non-coherent access, e.g., via I/O commands rather than
4We note that coherence in the non-byte-addressable model is not novel, as
it is the traditional access model for block devices.
27DIMES ’23, October 23, 2023, Koblenz, Germany P. Desnoyers et al.
CPU load/store operations. This model is used by object stor-
age devices that provide byte-aligned read and write opera-
tions, although it could also be applied to ￿at address spaces.
At present there are no commercially available modern object
storage devices; the ￿at-address-space model corresponds to
RDMA access to remote persistent memory.
Combinations with cache-coherent access at block granu-
larity seem either impossible or impractical, and are omitted
from Table 1.
Finally, row 5—cache-coherent byte-addressable access to
volatile storage—corresponds to CXL memory pooling with
volatile RAM.
Research questions. In a post-Optane landscape with
CXL attached volatile and non-volatile devices, we see a
range of problems which remain to be addressed.
Latency and memory access: Optane memory is no
slower than cross-NUMA-node access to DRAM, while poten-
tial future technologies may have signi ￿cantly higher worst-
case latency. At what point are architectural changes in the
CPU or memory controller needed to address non-uniform
access times? Is there a point where software-controlled
access commands become more e ￿cient than handling op-
erations with wildly di ￿erent latencies within the hardware
pipeline?
Performance factors: Optane memory provides both
byte addressability and low latency— 10⇥less than the fastest
(Optane) NVMe devices, and 100⇥less than typical ones.
Optane-based applications and systems have been shown
to provide signi ￿cantly higher performance than NVMe-
based ones, but how much of this improvement is due to
byte addressability, and how much due to performance? Fu-
ture PMem technologies may be slower than Optane, and
the answer to this question is important for assessing their
potential.
Memory pooling and crash consistency: Will the ap-
proaches used to provide crash consistency with a single
host attached to a single persistent memory be appropriate
for multiple attached hosts across multiple failure domains?
Application intent: Operating systems go to great
lengths to infer application intent, allowing, e.g., I/O prefetch-
ing and migration of data to lower-performance memory
tiers. This is more di ￿cult with PMem, where accesses are
performed by hardware rather than software, and may be
especially important for hybrid PMem systems.
Byte-granular I/O devices: High-performance PMem-
based systems often achieve some of their gains by perform-
ing small atomic updates to stored data structures, e.g., by
atomically swapping pointers [ 51]. Extensions to the NVMe
protocol might allow such accesses to be performed on ex-
ternal storage, without coherent load/store access from the
CPU. Is direct load/store access even necessary to achievethe bene ￿ts of byte-granular access, or can I/O protocols
evolve to incorporate this model?
Collectively, these CXL-enabled opportunities motivate
more distributed storage systems research, including job de-
composition, scheduling, safely sharing data, and program-
ming and managing storage devices that speak both byte and
block protocols. It remains to be seen whether this takes the
form of computational memory, computational storage, or
some hybrid. Indeed, CXL will blur the lines between mem-
ory and storage, allowing us to rethink and expand the role
of a “device.” Devices will become computing peers, bringing
a wide and exciting array of possibilities.
5 CONCLUSION
We posit that the current lack of commercial PMem avail-
ability does not detract from its importance and promise
as a core storage technology, both in academia and indus-
try. The Compute Express Link (CXL) interconnect carries
forward the lessons from previous PMem implementations
and lowers the barrier for developing new PMem products.
The wide adoption of the CXL standard allays vendor lock-in
concerns, and is a core reason that we believe PMem is worth
continued research e ￿ort. In particular, CXL enables one to
consider each PMem attribute separately or in combination:
byte addressability, persistence, and direct access via CPU
load/store instructions. Finally, new CXL features such as
memory pooling and sharing are seeing considerable interest
as rich areas for future PMem research and development.
ACKNOWLEDGMENTS
We thank Andrew Rudo ￿for his extensive contributions
to this work. We thank the anonymous reviewers for their
constructive feedback. This work was made possible in part
thanks to Dell-EMC, NetApp, Facebook, and IBM support; a
SUNY/IBM Alliance award; and NSF awards CNS-1910327,
CCF-1918225, CNS-1900706, CNS-1951880, CNS-2106263,
CNS-2106434, and CNS-2214980.
REFERENCES
[1]JEDEC Solid State Technology Association. 2021. JEDEC Publishes
DDR4 NVDIMM-P Bus Protocol Standard.
[2]Lawrence Benson, Marcel Weisgut, and Tilmann Rabl. 2023. What
We Can Learn from Persistent Memory for CXL. In 20th Conference
on Database Systems for Business, Technology and Web (BTW) , Bir-
gitta König-Ries, Stefanie Scherzinger, Wolfgang Lehner, and Gottfried
Vossen (Eds.). Gesellschaft für Informatik e.V., Dresden, Germany, 535–
554. https://doi.org/10.18420/BTW2023-48
[3]Miao Cai and Hao Huang. 2021. A survey of operating system support
for persistent memory. Frontiers of Computer Science 15 (2021), 154207.
[4]Feng Chen, Michael Mesnier, and Scott Hahn. 2014. A Protected Block
Device for Persistent Memory. In Proceedings of the 30th Symposium
on Mass Storage Systems and Technologies (MSST) . IEEE, Santa Clara,
CA, 1–12.
28Persistent Memory Research in the Post-Optane Era DIMES ’23, October 23, 2023, Koblenz, Germany
[5]Youmin Chen, Youyou Lu, Fan Yang, Qing Wang, Yang Wang, and
Jiwu Shu. 2020. FlatStore: An E ￿cient Log-Structured Key-Value
Storage Engine for Persistent Memory. In Proceedings of the Twenty-
Fifth International Conference on Architectural Support for Programming
Languages and Operating Systems . ACM, Lausanne, Switzerland, 1077–
1091.
[6]Zhaole Chu, Yongping Luo, and Peiquan Jin. 2021. An E ￿cient Sorting
Algorithm for Non-Volatile Memory. Int. J. Softw. Eng. Knowl. Eng. 31
(2021), 1603–1621.
[7]Joel Coburn, Adrian M. Caul ￿eld, Ameen Akel, Laura M. Grupp, Ra-
jesh K. Gupta, Ranjit Jhala, and Steven Swanson. 2011. NV-Heaps:
Making Persistent Objects Fast and Safe with Next-Generation, Non-
Volatile Memories. In Proceedings of the Sixteenth International Confer-
ence on Architectural Support for Programming Languages and Operat-
ing Systems (ASPLOS) . ACM, Newport Beach, CA, 105–118.
[8]Compute Express Link. 2022. Compute Express Link (CXL) Speci ￿ca-
tion. Available from http://www.computeexpresslink.org .
[9]Jeremy Condit, Edmund B. Nightingale, Christopher Frost, Engin Ipek,
Benjamin Lee, Doug Burger, and Derrick Coetzee. 2009. Better I/O
through Byte-Addressable, Persistent Memory. In Proceedings of the
ACM SIGOPS 22nd Symposium on Operating Systems Principles . ACM,
Big Sky, Montana, USA, 133–146. https://doi.org/10.1145/1629575.
1629589
[10] Laxman Dhulipala, Charles McGu ￿ey, Hong Kyu Kang, Yan Gu, Guy E.
Blelloch, Phillip B. Gibbons, and Julian Shun. 2019. Sage: Parallel
Semi-Asymmetric Graph Algorithms for NVRAMs. Proc. VLDB Endow.
13 (2019), 1598–1613.
[11] R. F. Freitas and W. W. Wilcke. 2008. Storage-Class Memory: The Next
Storage System Technology. IBM Journal of Research and Development
52, 4/5 (July 2008), 439–447. https://doi.org/10.1147/rd.524.0439
[12] G. Gill, Roshan Dathathri, Loc Hoang, Ramesh V. Peri, and Keshav
Pingali. 2019. Single Machine Graph Analytics on Massive Datasets
using Intel Optane DC Persistent Memory. Proceedings of the VLDB
Endowment 13 (2019), 1304 – 1318.
[13] Tomasz Gromadzki and Jan Marian Michalski. 2019. Persistent
Memory Replication Over Traditional RDMA Part 4: Persistent
Memory Development Kit (PMDK)-Based PMEM Replication.
https://www.intel.com/content/www/us/en/developer/articles/
technical/persistent-memory-replication-over-traditional-
rdma-part-4-persistent-memory-development.html .
[14] Shashank Gugnani, Scott Guthridge, Frank Schmuck, Owen Anderson,
Deepavali Bhagwat, and Xiaoyi Lu. 2022. Arcadia: A Fast and Reliable
Persistent Memory Replicated Log. arXiv:cs.DC/2206.12495
[15] Jim Handy and Tom Coughlin. 2023. Optane’s Dead: Now What?
Computer 56, 3 (2023), 125–130. https://doi.org/10.1109/MC.2023.
3235096
[16] Dave Hitz, James Lau, and Michael Malcolm. 1994. File System Design
for an NFS File Server Appliance. In Proceedings of the USENIX Winter
1994 Technical Conference (ATC) . USENIX Association, San Francisco,
California, 19–19.
[17] George Hodgkins, Yi Xu, Steven Swanson, and Joseph Izraele-
vitz. 2023. Zhuque: Failure Isn’t an Option, It’s an Excep-
tion. http://nvmw.ucsd.edu/nvmw2023-program/nvmw2023-
paper16-presentation_slides.pdf 14th Non-Volatile Memories Work-
shop.
[18] Deukyeon Hwang, Wook-Hee Kim, Youjip Won, and Beomseok Nam.
2018. Endurable Transient Inconsistency in Byte-Addressable Persis-
tent B+-Tree. In USENIX Conference on File and Storage Technologies .
USENIX Association, Oakland, CA, 187–200.
[19] Intel Corporation. 2022. Intel Optane persistent memory
and Intel®Xeon®scalable processors o ￿er a practical migra-
tion path to memory expansion, tiering, and pooling withCompute Express Link (CXLTM)-attached memory devices.
https://www.intel.com/content/dam/www/central-libraries/
us/en/documents/2022-11/optane-pmem-to-cxl-tech-brief.pdf
[20] Intel Corporation. 2022. Intel Reports Second-Quarter 2022 Financial
Results. https://www.intc.com/news-events/press-releases/detail/
1563/intel-reports-second-quarter-2022- ￿nancial-results .
[21] Intel Corporation. 2023. Persistent Memory Development Kit (PMDK).
pmem.io .
[22] Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amir-
saman Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R.
Dulloor, Jishen Zhao, and Steven Swanson. 2019. Basic Performance
Measurements of the Intel Optane DC Persistent Memory Module.
https://doi.org/10.48550/ARXIV.1903.05714
[23] Jungi Jeong and Changhee Jung. 2021. PMEM-Spec: Persistent Memory
Speculation (Strict Persistency Can Trump Relaxed Persistency). In
Proceedings of the 26th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems (ASPLOS) .
ACM, virtual, 517–529.
[24] Myoungsoo Jung. 2022. Hello Bytes, Bye Blocks: PCIe Storage Meets
Compute Express Link for Memory Expansion (CXL-SSD). In Proceed-
ings of the 14th ACM Workshop on Hot Topics in Storage and File Systems
(HotStorage) . ACM, Virtual Event, 45–51. https://doi.org/10.1145/
3538643.3539745
[25] Rohan Kadekodi, Se Kwon Lee, Sanidhya Kashyap, Taesoo Kim,
Aasheesh Kolli, and Vijay Chidambaram. 2019. SplitFS: Reducing
Software Overhead in File Systems for Persistent Memory. In Pro-
ceedings of the 27th ACM Symposium on Operating Systems Principles
(SOSP) . ACM, Huntsville, Ontario, Canada, 494–508.
[26] Olzhas Kaiyrakhmet, Song Yeon Lee, Beomseok Nam, Sam H. Noh, and
Young ri Choi. 2019. SLM-DB: Single-Level Key-Value Store with Per-
sistent Memory. In USENIX Conference on File and Storage Technologies .
USENIX Association, Boston, MA, 191–205.
[27] Rajat Kateja, Andrew Pavlo, and Greg Ganger. 2020. Vilamb: Low Over-
head Asynchronous Redundancy for Direct Access NVM. , 17 pages.
https://arxiv.org/abs/2004.09619
[28] Se Kwon Lee, Jayashree Mohan, Sanidhya Kashyap, Taesoo Kim, and
Vijay Chidambaram. 2019. RECIPE: Converting Concurrent DRAM
Indexes to Persistent-Memory Indexes. In Proceedings of the 27th ACM
Symposium on Operating Systems Principles (SOSP) . ACM, Huntsville,
Ontario, Canada, 462–477.
[29] Lucas Lersch, Xiangpeng Hao, Ismail Oukid, Tianzheng Wang, and
Thomas Willhalm. 2019. Evaluating Persistent Memory Range Indexes.
Proc. VLDB Endow. 13 (2019), 574–587.
[30] Huaicheng Li, Daniel S. Berger, Stanko Novakovic, Lisa R. Hsu, Dan
Ernst, Pantea Zardoshti, Monish Shah, Samir Rajadnya, Scott Lee,
Ishwar Agarwal, Mark D. Hill, Marcus Fontoura, and Ricardo Bianchini.
2022. Pond: CXL-Based Memory Pooling Systems for Cloud Platforms.
InProceedings of the 28th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems (ASPLOS) ,
Vol. 2. ACM, Lausanne, Switzerland, 574–587.
[31] Jen-Kuang Liu and Sheng-De Wang. 2022. CFFS: A Persistent Mem-
ory File System for Contiguous File Allocation With Fine-Grained
Metadata. IEEE Access 10 (2022), 91678–91698.
[32] Qingrui Liu, Joseph Izraelevitz, Se Kwon Lee, Michael L. Scott, Sam H.
Noh, and Changhee Jung. 2018. iDO: Compiler-Directed Failure Atom-
icity for Nonvolatile Memory. In 51st Annual IEEE/ACM International
Symposium on Microarchitecture (MICRO) . IEEE, Fukuoka, Japan, 258–
270.
[33] Sara Mahdizadeh-Shahri, Seyed Armin Vakil-Ghahani, and Aasheesh
Kolli. 2020. (Almost) Fence-less Persist Ordering. In 53rd Annual
IEEE/ACM International Symposium on Microarchitecture (MICRO) .
ACM, Virtual, 539–554.
29DIMES ’23, October 23, 2023, Koblenz, Germany P. Desnoyers et al.
[34] Virendra J. Marathe, Margo Seltzer, Steve Byan, and Tim Harris. 2017.
Persistent Memcached: Bringing Legacy Code to Byte-Addressable
Persistent Memory. In 9th USENIX Workshop on Hot Topics in Storage
and File Systems (HotStorage 17) . USENIX Association, Santa Clara,
CA. https://www.usenix.org/conference/hotstorage17/program/
presentation/marathe
[35] Michael P. Mesnier, Gregory R. Ganger, and Erik Riedel. 2003. Object-
based Storage. IEEE Communications 44, 8 (August 2003), 84–90.
[36] Micron. 2021. Micron Updates Data Center Portfolio Strategy to
Address Growing Opportunity for Memory and Storage Hierarchy
Innovation. https://investors.micron.com/news-releases/news-
release-details/micron-updates-data-center-portfolio-strategy-
address-growing
[37] Moohyeon Nam, Hokeun Cha, Young ri Choi, Sam H. Noh, and
Beomseok Nam. 2019. Write-Optimized Dynamic Hashing for Persis-
tent Memory. In USENIX Conference on File and Storage Technologies .
USENIX Association, Boston, MA, 31–44.
[38] Dushyanth Narayanan and Orion Hodson. 2012. Whole-System Per-
sistence. SIGARCH Comput. Archit. News 40, 1 (mar 2012), 401–410.
https://doi.org/10.1145/2189750.2151018
[39] Emerson W. Pugh, Lyle R. Johnson, and John H. Palmer. 2003. IBM’s
360 and early 370 systems . MIT Press, Cambridge, Massachusetts.
[40] Han Jie Qiu, Sihang Liu, Xinyang Song, Samira Khan, and Gennady
Pekhimenko. 2022. Pavise: Integrating Fault Tolerance Support for
Persistent Memory Applications. In Proceedings of the International
Conference on Parallel Architectures and Compilation Techniques . ACM,
Chicago, IL, 109–123.
[41] The Register. 2022. Last week Intel killed Optane. Today, Kioxia and
Everspin announced comparable tech. https://www.theregister.
com/2022/08/02/kioxia_everspin_persistent_memory/
[42] Andy Rudo ￿. 2017. Persistent Memory Programming. USENIX ;login:
42, 2 (July 2017), 34–40.
[43] Andy M. Rudo ￿. 2016. Deprecating the PCOMMIT Instruc-
tion. https://www.intel.com/content/www/us/en/developer/
articles/technical/deprecate-pcommit-instruction.html .
[44] Steve Scargall. 2020. Programming Persistent Memory: A Comprehensive
Guide for Developers . Apress, New York, New York. 5–7 pages. https:
//doi.org/10.1007/978-1-4842-4932-1
[45] Xinyang (Kevin) Song, Sihang Liu, and Gennady Pekhimenko. 2022.
Persistent Memory —- A New Hope. https://www.sigarch.org/
persistent-memory-a-new-hope/
[46] Storage Networking Industry Association. 2017. NVM Programming
Model (NPM). https://www.snia.org/sites/default/ ￿les/technical-
work/npm/release/SNIA-NVM-Programming-Model-v1.2.pdf
[47] Alexander van Renen, Lukas Vogel, Viktor Leis, Thomas Neumann,
and Alfons Kemper. 2019. Persistent Memory I/O Primitives. In Pro-
ceedings of the 15th International Workshop on Data Management on
New Hardware (DaMoN) . ACM, Amsterdam, Netherlands, 1–7.
[48] Jingyu Wang, Shengan Zheng, Ziyi Lin, Yuting Chen, and Linpeng
Huang. 2022. Zebra: An E ￿cient, RDMA-Enabled Distributed Per-
sistent Memory File System. In International Conference on Database
Systems for Advanced Applications . ACM, Virtual, 341–349.
[49] Wikipedia. 2023. NVDIMM — Wikipedia, The Free Encyclo-
pedia. http://en.wikipedia.org/w/index.php?title=NVDIMM&
oldid=1141063008 . [Online; accessed 27-March-2023].
[50] Jian Xu, Juno Kim, Amir Saman Memaripour, and Steven Swanson.
2019. Finding and Fixing Performance Pathologies in Persistent Mem-
ory Software Stacks. In Proceedings of the Twenty-Fourth International
Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS) . ACM, Providence, RI, 427–439.
[51] Jian Xu and Steven Swanson. 2016. NOVA: A Log-structured File
System for Hybrid Volatile/Non-volatile Main Memories. In Proceedingsof the 14th Usenix Conference on File and Storage Technologies . USENIX
Association, Santa Clara, CA, 323–338.
[52] Jian Xu, Lu Zhang, Amirsaman Memaripour, Akshatha Gangadharaiah,
Amit Borase, Tamires Brito Da Silva, Steven Swanson, and Andy Rudo ￿.
2017. NOVA-Fortis: A Fault-Tolerant Non-Volatile Main Memory File
System. In Proceedings of the 26th Symposium on Operating Systems
Principles . ACM, Shanghai China, 478–496. https://doi.org/10.1145/
3132747.3132761
[53] Jian Yang, Juno Kim, Morteza Hoseinzadeh, Joseph Izraelevitz, and
Steven Swanson. 2020. An Empirical Guide to the Behavior and Use of
Scalable Persistent Memory. In USENIX Conference on File and Storage
Technologies (FAST) . USENIX Association, Santa Clara, CA, 169–182.
[54] Ziye Yang, James R. Harris, Benjamin Walker, Daniel Verkamp, Chang-
peng Liu, Cunyin Chang, Gang Cao, Jonathan Stern, Vishal Verma,
and Luse E. Paul. 2017. SPDK: A Development Kit to Build High Per-
formance Storage Applications. In 2017 IEEE International Conference
on Cloud Computing Technology and Science (CloudCom) . IEEE, Hong
Kong, 154–161. https://doi.org/10.1109/CloudCom.2017.14
[55] Wenhui Zhang, Xingsheng Zhao, Song Jiang, and Hong Jiang. 2021.
ChameleonDB: A Key-value Store for Optane Persistent Memory. In
Proceedings of the Sixteenth European Conference on Computer Systems
(Eurosys) . ACM, Edinburgh, Scotland, 194–209.
30This paper is included in the Proceedings of the  
2022 USENIX Annual Technical Conference.
July 11–13, 2022 • Carlsbad, CA, USA
978-1-939133-29-8
Open access to the Proceedings of the 
2022 USENIX Annual Technical Conference 
is sponsored byDirect Access, High-Performance Memory 
Disaggregation with DirectCXL
Donghyun Gouk, Sangwon Lee, Miryeong Kwon, and Myoungsoo Jung, Computer 
Architecture and Memory Systems Laboratory, Korea Advanced Institute of Science 
and Technology (KAIST)
https://www.usenix.org/conference/atc22/presentation/goukDirect Access, High-Performance Memory Disaggregation with D IRECT CXL
Donghyun Gouk, Sangwon Lee, Miryeong Kwon, Myoungsoo Jung
Computer Architecture and Memory Systems Laboratory,
Korea Advanced Institute of Science and Technology (KAIST)
http://camelab.org
Abstract
New cache coherent interconnects such as CXL have recently
attracted great attention thanks to their excellent hardware
heterogeneity management and resource disaggregation capa-
bilities. Even though there is yet no real product or platform
integrating CXL into memory disaggregation, it is expected
to make memory resources practically and efﬁciently disag-
gregated much better than ever before.
In this paper, we propose directly accessible memory dis-
aggregation, DIRECT CXL that straight connects a host pro-
cessor complex and remote memory resources over CXL’s
memory protocol ( CXL.mem ). To this end, we explore a practi-
cal design for CXL-based memory disaggregation and make
it real. As there is no operating system that supports CXL,
we also offer CXL software runtime that allows users to uti-
lize the underlying disaggregated memory resources via sheer
load/store instructions. Since DIRECT CXL does not require
any data copies between the host memory and remote memory,
it can expose the true performance of remote-side disaggre-
gated memory resources to the users.
1 Introduction
Memory disaggregation has attracted great attention thanks
to its high memory utilization, transparent elasticity, and re-
source management efﬁciency [ 1–3]. Many studies have ex-
plored various software and hardware approaches to realize
memory disaggregation and put signiﬁcant efforts into making
it practical in large-scale systems [ 4–16].
We can broadly classify the existing memory disaggrega-
tion runtimes into two different approaches based on how they
manage data between a host and memory server(s): i) page-
based and ii) object-based. The page-based approach [ 4–10]
utilizes virtual memory techniques to use disaggregated mem-
ory without a code change. It swaps page cache data resid-
ing on the host’s local DRAMs from/to the remote mem-
ory systems over a network in cases of a page fault. On the
other hand, the object-based approach handles disaggregated
memory from a remote using their own database such as a
key-value store instead of leveraging the virtual memory sys-
tems [ 11–16]. This approach can address the challenges im-
posed by address translation (e.g., page faults, context switch-
ing, and write ampliﬁcation), but it requires signiﬁcant source-
level modiﬁcations and interface changes.While there are many variants, all the existing approaches
need to move data from the remote memory to the host mem-
ory over remote direct memory access ( RDMA )[4,5,11–13,
15,16] (or similar ﬁne-grain network interfaces [ 7,9,10,17]).
In addition, they even require managing locally cached data
in either the host or memory nodes. Unfortunately, the data
movement and its accompanying operations (e.g., page cache
management) introduce redundant memory copies and soft-
ware fabric intervention, which makes the latency of disaggre-
gated memory longer than that of local DRAM accesses by
multiple orders of magnitude. In this work, we advocate com-
pute express link (CXL [ 18]), which is a new concept of open
industry standard interconnects offering high-performance
connectivity among multiple host processors, hardware accel-
erators, and I/O devices [ 19]. CXL is originally designed to
achieve the excellency of heterogeneity management across
different processor complexes, but both industry and academia
anticipate its cache coherence ability can help improve mem-
ory utilization and alleviate memory over-provisioning with
low latency [ 20–22]. Even though CXL exhibits a great po-
tential to realize memory disaggregation with low monetary
cost and high performance, it has not been yet made for pro-
duction, and there is no platform to integrate memory into a
memory pooling network.
We demonstrate DIRECT CXL , direct accessible disaggre-
gated memory that connects host processor complex and
remote memory resources over CXL’s memory protocol
(CXL.mem ). To this end, we explore a practical design for
CXL-based memory disaggregation and make it real. Speciﬁ-
cally, we ﬁrst show how to disaggregate memory over CXL
and integrate the disaggregated memory into processor-side
system memory. This includes implementing CXL controller
that employs multiple DRAM modules on a remote side. We
then prototype a set of network infrastructure components
such as a CXL switch in order to make the disaggregated
memory connected to the host in a scalable manner. As there
is no operating system that support CXL, we also offer CXL
software runtime that allows users to utilize the underlying
disaggregated memory resources through sheer load/store in-
structions. DIRECT CXL does not require any data copies
between the host memory and remote memory, and therefore,
it can expose the true performance of remote-side disaggre-
gated memory resources to the users.
In this work, we prototype DIRECT CXL using many cus-
USENIX Association 2022 USENIX Annual Technical Conference    287tomized memory add-in-cards, 16 nmFPGA-based processor
nodes, a switch, and a PCIe backplane. On the other hand, DI-
RECT CXL software runtime is implemented based on Linux
5.13. To the best of our knowledge, this is the ﬁrst work that
brings CXL 2.0 into a real system and analyzes the perfor-
mance characteristics of CXL-enabled disaggregated memory
design. The results of our real system evaluation show that
the disaggregated memory resources of DIRECT CXL can ex-
hibit DRAM-like performance when the workload can enjoy
the host processor’s cache. When the load/store instructions
go through the CXL network and are served from the disag-
gregated memory, D IRECT CXL’s latency is shorter than the
best latency of RDMA by 6.2 ⇥, on average. For real-world
applications, DIRECT CXL exhibits 3 ⇥better performance
than RDMA-based memory disaggregation, on average.
2 Memory Disaggregation and Related Work
2.1 Remote Direct Memory Access
The basic idea of memory disaggregation is to connect a host
with one or more memory nodes, such that it does not restrict
a given job execution because of limited local memory space.
For the backend network control, most disaggregation work
employ remote direct memory access (RDMA) [ 4,5,11–13,
15,16] or similar customized DMA protocols [ 7,9,10]. Figure
1shows how RDMA-style data transfers (one-sided RDMA)
work. For both the host and memory node sides, RDMA needs
hardware support such as RDMA NIC ( RNIC [23]), which
is designed toward removing the intervention of the network
software stack as much as possible. To move data between
them, processes on each side ﬁrst require deﬁning one or
more memory regions ( MRs) and letting the MR(s) to the
underlying RNIC. During this time, the RNIC driver checks
all physical addresses associated with the MR’s pages and
registers them to RNIC’s memory translation table ( MTT ).
Since those two RNICs also exchange their MR’s virtual
address at the initialization, the host can simply send the
memory node’s destination virtual address with data for a
write. The remote node then translates the address by referring
to its MTT and copies the incoming data to the target location
of MR. Reads over RDMA can also be performed in a similar
manner. Note that, in addition to the memory copy operations
(for DMA), each side’s application needs to prepare or retrieve
the data into/from MRs for the data transfers, introducing
additional data copies within their local DRAM [ 24].
2.2 Swap: Page-based Memory Pool
Page-based memory disaggregation [ 4–10] achieves memory
elasticity by relying on virtual memory systems. Speciﬁcally,
this approach intercepts paging requests when there is a page
fault, and then it swaps the data to a remote memory node in-
stead of the underlying storage. To this end, a disaggregation
driver underneath the host’s kernel swap daemon ( kswapd )
converts the incoming block address to the memory node’s/g44/g381/g400/g410/g90/g69/g47/g18/g68/g286/g373/g381/g396/g455/g3/g374/g381/g282/g286/g17/g437/g296/g296/g286/g396/g4/g393/g393/g68/g286/g373/g381/g396/g455/g3/g396/g286/g336/g349/g381/g374/g90/g437/g374/g410/g349/g373/g286/g68/g100/g100/g90/g69/g47/g18/g3/g282/g396/g349/g448/g286/g396/g115/g4/g894/g373/g286/g373/g895/g47/g374/g349/g410/g47/g374/g349/g410/g18/g381/g393/g455/g24/g68/g4/g115/g4/g882/g410/g381/g882/g87/g4/g920/g3/g24/g68/g4/g100/g396/g258/g374/g400/g296/g286/g396/g3/g1092/g115/g4/g853/g3/g367/g286/g374/g336/g410/g346/g853/g3/g282/g258/g410/g258/g1093/g90/g69/g47/g18/g17/g437/g296/g296/g286/g396/g4/g393/g393/g68/g286/g373/g381/g396/g455/g3/g396/g286/g336/g349/g381/g374/g90/g437/g374/g410/g349/g373/g286/g68/g100/g100/g90/g69/g47/g18/g3/g282/g396/g349/g448/g286/g396/g115/g4/g894/g346/g381/g400/g410/g895/g47/g374/g349/g410/g18/g381/g393/g455Figure 1: Data movement over RDMA.
virtual address. It then copies the target page to RNIC’s MR
and issues the corresponding RDMA request to the mem-
ory node. Since all operations for memory disaggregation is
managed under kswapd , it is easy-to-adopt and transparent
to all user applications. However, page-based systems suffer
from performance degradation due to the overhead of page
fault handling, I/O ampliﬁcations, and context switching when
there are excessive requests for the remote memory [ 16].
Note that there are several studies that migrate locally
cached data in a ﬁner granular manner [ 4–7] or reduce the
page fault overhead by ofﬂoading memory management (in-
cluding page cache coherence) to the network [ 8] or memory
nodes [ 9,10]. However, all these approaches use RDMA (or a
similar network protocol), which is essential to cache the data
and pay the cost of memory operations for network handling.
2.3 KVS: Object-based Memory Pool
In contrast, object-based memory disaggregation systems
[11–16] directly intervene in RDMA data transfers using their
own database such as key-value store (KVS). Object-based
systems create two MRs for both host and memory node sides,
each dealing with buffer data and submission/completion
queues (SQ/CQ). Generally, they employ a KV hash-table
whose entries point to corresponding (remote) memory ob-
jects. Whenever there is a request of Put(orGet) from an
application, the systems place the corresponding value into
the host’s buffer MR and submit it by writing the remote
side of SQ MR over RDMA. Since the memory node keeps
polling SQ MR, it can recognize the request. The memory
node then reads the host’s buffer MR, copies the value to
its buffer MR over RDMA, and completes the request by
writing the host’s CQ MR. As it does not lean on virtual mem-
ory systems, object-based systems can address the overhead
imposed by page swap. However, the performance of object-
based systems varies based on the semantics of applications
compared to page-based systems; kswapd fully utilizes local
page caches, but KVS does not for remote accesses. In addi-
tion, this approach is unfortunately limited because it requires
signiﬁcant source-level modiﬁcations for legacy applications.
3 Direct Accessible Memory Aggregation
While caching pages and network-based data exchange are
essential in the current technologies, they can unfortunately
signiﬁcantly deteriorate the performance of memory disaggre-
gation. DIRECT CXL instead directly connects remote mem-
ory resources to the host’s computing complex and allows
users to access them through sheer load/store instructions.
288    2022 USENIX Annual Technical Conference USENIX Association/g28/g374/g282/g393/g381/g349/g374/g410/g18/g396/g381/g400/g400/g271/g258/g396/g44/g381/g400/g410/g3/g4/g44/g381/g400/g410/g3/g18/g87/g104/g90/g87/g62/g381/g272/g258/g367/g3/g24/g90/g4/g68/g18/g87/g104/g24/g90/g4/g68/g18/g121/g62/g856/g373/g286/g373/g18/g121/g62/g3/g400/g449/g349/g410/g272/g346/g44/g381/g400/g410/g3/g17/g44/g24/g68/g28/g87/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g44/g24/g68/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g90/g87/g104/g94/g87/g44/g24/g68/g28/g87/g90/g87/g104/g94/g87/g44/g24/g68/g28/g87/g18/g121/g62/g3/g448/g349/g396/g410/g437/g258/g367/g346/g349/g286/g396/g258/g396/g272/g346/g455/g24/g94/g87/g3/g1004/g24/g94/g87/g3/g1005/g38/g68/g104/g94/g87/g3/g1004/g24/g94/g87/g1004/g1005/g104/g94/g87/g1004/g3/g3/g3/g1005/g75/g121/g75/g75/g44/g24/g68/g17/g4/g90/g94/g455/g400/g410/g286/g373/g373/g286/g373/g856
/g87/g18/g47/g286/g3/g272/g381/g374/g296/g349/g336/g3/g400/g393/g258/g272/g286/g258/g282/g282/g396/g286/g400/g400/g3/g400/g393/g258/g272/g286/g62/g381/g258/g282/g876/g94/g410/g381/g396/g286
/g24/g286/g448/g349/g272/g286/g3/g373/g286/g373/g381/g396/g455/g18/g121/g62/g3/g296/g367/g349/g410/g87/g18/g47/g286/g3/g894/g38/g367/g286/g454/g17/g437/g400/g895/g17/g4/g90/g3/g400/g349/g460/g286/g17/g4/g90/g3/g271/g258/g400/g286/g44/g24/g68/g3/g400/g349/g460/g286/g18/g121/g62/g856/g373/g286/g373/g104/g94/g87/g3/g1005/g24/g94/g87/g24/g94/g87/g90/g381/g381/g410/g3/g87/g381/g396/g410/g28/g374/g437/g373/g286/g396/g258/g410/g349/g381/g374/g1005/g90/g286/g400/g286/g396/g448/g286/g282/g3/g296/g381/g396/g3/g18/g121/g62/g44/g24/g68/g3/g271/g258/g400/g286/g882/g4/g282/g282/g396/g286/g400/g400/g3/g410/g396/g258/g374/g400/g367/g258/g410/g349/g381/g374/g17/g4/g90/g876/g44/g24/g68/g3/g271/g258/g400/g286/g1007/g17/g4/g90/g876/g44/g24/g68/g3/g400/g349/g460/g286/g1006/g44/g381/g400/g410/g3/g18/g87/g104/g68/g286/g373/g856/g3/g396/g286/g395/g856/g68/g286/g373/g856/g3/g396/g286/g395/g856
/g104/g400/g286/g396/g94/g286/g336/g373/g286/g374/g410/g44/g24/g68/g60/g286/g396/g374/g286/g367/g75/g296/g296/g400/g286/g410/g1085/g400/g349/g460/g286/g373/g134/g135/g152/g373/g133/g154/g142/g350/g144/g149/g613/g18/g121/g62/g282/g286/g448/g349/g272/g286/g143/g143/g131/g146/g75/g296/g296/g400/g286/g410/g139/g145/g133/g150/g142/g24/g349/g396/g286/g272/g410/g18/g121/g62/g3/g396/g437/g374/g410/g349/g373/g286/g373/g134/g135/g152/g373/g134/g139/g148/g135/g133/g150/g133/g154/g142/g1006/g1007/g1008/g44/g24/g68/g3/g400/g286/g336/g373/g286/g374/g410/g3/g410/g258/g271/g367/g286/g94/g286/g336/g381/g296/g296/g400/g286/g410/g400/g349/g460/g286/g951/g396/g286/g296/g4/g393/g393/g367/g349/g272/g258/g410/g349/g381/g374/g1005Figure 2: DIRECT CXL ’s connection method.(a) CXL virtual hierarchy. (b) CXL switch.
Figure 3: D IRECT CXL’s network and switch.Figure 4: DIRECT CXL software
runtime.
3.1 Connecting Host and Memory over CXL
CXL devices and controllers. In practice, existing memory
disaggregation techniques still require computing resources
at the remote memory node side. This is because all DRAM
modules and their interfaces are designed as passive peripher-
als, which require the control computing resources. CXL.mem
in contrast allows the host computing resources directly ac-
cess the underlying memory through PCIe buses ( FlexBus ); it
works similar to local DRAM, connected to their system buses.
Thus, we design and implement CXL devices as pure passive
modules, each being able to have many DRAM DIMMs with
its own hardware controllers. Our CXL device employs mul-
tiple DRAM controllers, connecting DRAM DIMMs over the
conventional DDR interfaces. Its CXL controller then exposes
the internal DRAM modules to FlexBus through many PCIe
lanes. In the current architecture, the device’s CXL controller
parses incoming PCIe-based CXL packets, called CXL ﬂits ,
converts their information (address and length) to DRAM
requests, and serves them from the underlying DRAMs using
the DRAM controllers.
Integrating devices into system memory. Figure 2shows
how CXL devices’ internal DRAMs are mapped (exposed)
to a host’s memory space over CXL. The host CPU’s sys-
tem bus contains one or more CXL root ports ( RPs), which
connect one or more CXL devices as endpoint ( EP) devices.
Our host-side kernel driver ﬁrst enumerates CXL devices by
querying the size of their base address register (BAR) and
their internal memory, called host-managed device memory
(HDM ), through PCIe transactions. Based on the retrieved
sizes, the kernel driver maps BAR and HDM in the host’s
reserved system memory space and lets the underlying CXL
devices know where their BAR and HDM (base addresses)
are mapped in the host’s system memory. When the host CPU
accesses an HDM system memory through load/store instruc-
tion, the request is delivered to the corresponding RP, and the
RP converts the requests to a CXL ﬂit. Since HDM is mapped
to a different location of the system memory, the memory
address space of HDM is different from that of EP’s internal
DRAMs. Thus, the CXL controller translates the incoming ad-
dresses by simply deducting HDM’s base address from them
and issues the translated request to the underlying DRAM
controllers. The results are returned to the host via a CXL
switch and FlexBus. Note that, since HDM accesses have no
software intervention or memory data copies, DIRECT CXL
can expose the CXL device’s memory resources to the host
with low access latency.Designing CXL network switch. Figure 3aillustrates how
DIRECT CXL can disaggregate memory resources from a host
using one or more and CXL devices, and Figure 3bshows
our CXL switch organization therein. The host’s CXL RP is
connected to upstream port (USP) of either a CXL switch
or the CXL device directly. The CXL switch’s downstream
port(DSP) also connects either another CXL switch’s USP or
the CXL device. Note that our CXL switch employs multiple
USPs and DSPs. By setting an internal routing table, our CXL
switch’s fabric manager (FM) reconﬁgures the switch’s cross-
bar to connect each USP to a different DSP, which creates a
virtual hierarchy from a root (host) to a terminal (CXL de-
vice). Since a CXL device can employ one or more controllers
and many DRAMs, it can also deﬁne multiple logical devices,
each exposing its own HDM to a host. Thus, different hosts
can be connected to a CXL switch and a CXL device. Note
that each CXL virtual hierarchy only offers the path from one
to another to ensure that no host is sharing an HDM.
3.2 Software Runtime for DirectCXL
In contrast to RDMA, once a virtual hierarchy is established
between a host and CXL device(s), applications running on
the host can directly access the CXL device by referring to
HDM’s memory space. However, it requires software run-
time/driver to manage the underlying CXL devices and ex-
pose their HDM in the application’s memory space. We thus
support DIRECT CXL runtime that simply splits the address
space of HDM into multiple segments, called cxl-namespace .
DIRECT CXL runtime then allows the applications to access
each CXL-namespace as memory-mapped ﬁles ( mmap ).
Figure 4shows the software stack of our runtime and how
the application can use the disaggregated memory through
cxl-namespaces. When a CXL device is detected (at a PCIe
enumeration time), DIRECT CXL driver creates an entry de-
vice (e.g., /dev/directcxl ) to allow users to manage a
cxl-namespace via ioctl . If users ask a cxl-namespace to
/dev/directcxl , the driver checks a (physically) contiguous
address space on an HDM by referring to its HDM segment
table whose entry includes a segment’s offset, size, and refer-
ence count (recording how many cxl-namespaces that indicate
this segment). Since multiple processes can access this table,
its header also keeps necessary information such as spinlock,
read/write locks, and a summary of table entries (e.g., valid
entry numbers). Once DIRECT CXL driver allocates a seg-
ment based on the user request, it creates a device for mmap
(e.g., /dev/cxl-ns0 ) and updates the segment table. The user
USENIX Association 2022 USENIX Annual Technical Conference    289/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286
/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g400/g18/g121/g62/g3/g94/g449/g349/g410/g272/g346/g18/g121/g62/g3/g346/g381/g400/g410/g3/g393/g396/g381/g272/g286/g400/g400/g381/g396/g400/g24/g349/g400/g258/g336/g336/g396/g286/g336/g258/g410/g286/g282/g3/g373/g286/g373/g381/g396/g455/g87/g18/g47/g286/g3/g271/g258/g272/g364/g393/g367/g258/g374/g286/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g18/g121/g62/g3/g400/g449/g349/g410/g272/g346/g18/g121/g62/g3/g346/g381/g400/g410/g18/g121/g62/g3/g296/g367/g349/g410
/g24/g90/g4/g68/g24/g90/g4/g68/g24/g90/g4/g68/g24/g90/g4/g68/g18/g381/g374/g410/g396/g381/g367/g367/g286/g396/g18/g121/g62/g3/g28/g87/g18/g121/g62/g3/g90/g87/g18/g87/g104/g62/g62/g18/g62/g381/g272/g258/g367/g3/g24/g90/g4/g68/g18/g121/g62/g856/g373/g286/g373/g69/g855/g68/g374/g286/g410/g449/g381/g396/g364/g18/g121/g62/g3/g346/g381/g400/g410(a) Network topology. (b) Implementation.
Figure 5: CXL-enabled cluster.
application can then map the cxl-namespace to its process
virtual memory space using mmap with vm_area_struct .
Note that DIRECT CXL software runtime is designed for
direct access of CXL devices, which is a similar concept to the
memory-mapped ﬁle management of persistent memory de-
velopment toolkit (PMDK [ 25]). However, it is much simpler
and more ﬂexible for namespace management than PMDK.
For example, PMDK’s namespace is very much the same idea
as NVMe namespace, managed by ﬁle systems or DAX with
a ﬁxed size [ 26]. In contrast, our cxl-namespace is more sim-
ilar to the conventional memory segment, which is directly
exposed to the application without a ﬁle system employment.
3.3 Prototype Implementation
Figure 5aillustrates our design of a CXL network topology
to disaggregate memory resources, and the corresponding im-
plementation in a real system is shown in Figure 5b. There
arennumbers of compute hosts connected to mnumber of
CXL devices through a CXL switch; in our prototype, nand
mare four, but those numbers can scale by having more CXL
switches. Speciﬁcally, each CXL device prototype is built on
our customized add-in-card (AIC) CXL memory blade that
employs 16 nmFPGA and 8 different DDR4 DRAM modules
(64GB). In the FPGA, we fabricate a CXL controller and eight
DRAM controllers, each managing the CXL endpoint and
internal DRAM channels. As yet there is no processor archi-
tecture supporting CXL, we also build our own in-house host
processor using RISC-V ISAs, which employs four out-of-
order cores whose last-level cache ( LLC) implements CXL RP.
Each CXL-enabled host processor is implemented in a high-
performance datacenter accelerator card, taking a role of a
host, which can individually run Linux 5.13 and DIRECT CXL
software runtime. We expose four CXL devices (32 DRAM
modules) to the four hosts through our PCIe backplane. We
extended the backplane with one more accelerator card that
implements DIRECT CXL ’s CXL switch. This switch imple-
ments FM that can create multiple virtual hierarchies, each
connecting a host and a CXL device in a ﬂexible manner.
To the best of our knowledge, there are no commercialized
CXL 2.0 IPs for the processor side’s CXL engines and CXL
switch. Thus, we built all DIRECT CXL IPs from the ground.
The host-side processors require advanced conﬁguration and
power interface (ACPI [ 27]) for CXL 2.0 enumeration (e.g.,
RP location and RP’s reserved address space). Since RISC-V
does not support ACPI yet, we enable the CXL enumeration
by adding such information into the device tree [ 28]. Speciﬁ-
cally, we update an MMIO register designated as a property ofthe tree’s node to let the processor know where CXL RP exists.
On the other hand, we add a new ﬁeld ( cxl-reserved-area )
in the node to indicate where an HDM can be mapped. Our
in-house softcore processors work at 100MHz while CXL
and PCIe IPs (RP, EP, and Switch) operate at 250MHz.
4 Evaluation
Testbed prototypes for memory disaggregation. In addition
to the CXL environment that we implemented in Section 3.3
(DirectCXL ), we set up the same conﬁguration with it for our
RDMA-enabled hardware system ( RDMA ). For RDMA , we use
Mellanox ConnectX-3 VPI InﬁniBand RNIC (56Gbps, [ 29])
instead of our CXL switch as RDMA network interface card
(RNIC). In addition, we port Mellanox OpenFabric Enterprise
Distribution (OFED) v4.9 [ 30] as an RDMA driver to enable
RNIC in our evaluation testbed. Lastly, we port FastSwap [ 1]
and HERD [ 12] into RISC-V Linux 5.13.19 computing envi-
ronment atop RDMA , each realizing page-based disaggregation
(Swap ) and object-based disaggregation ( KVS).
For better comparison, we also conﬁgure the host proces-
sors to use only their local DRAM ( Local ) by disabling all
the CXL memory nodes. Note that we used the same testbed
hardware mentioned above for both CXL experiments and
non-CXL experiments but differently conﬁgured the testbed
for each reference. For example, our testbed’s FPGA chips
for the host (in-house) processors and CXL devices use all
the same architecture/technology and product line-up.
Benchmark and workloads. Since there is no microbench-
mark that we can compare different memory pooling tech-
nologies ( RDMA vs.DirectCXL ), we also build an in-house
memory benchmark for in-depth analysis of those two tech-
nologies (Section 4.1). For RDMA , this benchmark allocates a
large size of the memory pool at the remote side in advance.
This benchmark allows a host processor to send random mem-
ory requests to a remote node with varying lengths; the re-
mote node serves the requests using the pre-allocated memory
pool. For DirectCXL andLocal , the benchmark maps cxl
namespace oranonymous mmap to user spaces, respectively.
The benchmark then generates a group of RISC-V memory
instructions, which can cover a given address length in a
random pattern and directly issues them without software
intervention. For the real workloads, we use Facebook’s deep
learning recommendation model (DLRM [ 31]), an in-memory
database used for the HERD evaluation (MemDB [ 12]), and
four graph analysis workloads (MIS [ 32], BFS [ 33], CC [ 34],
and BC [ 35]) coming from Ligra [ 36]. All their tables and data
structures are stored in the remote node, while each host’s lo-
cal memory handles the execution code and static data. Table
1summarizes the per-node memory usage and total data sizes
for each workload that we tested.
4.1 In-depth Analysis of RDMA and CXL
In this subsection, we compare the performance of RDMA
and CXL technologies when the host and memory nodes are
290    2022 USENIX Annual Technical Conference USENIX Association1K4K16K64K256K1M4M16M64M256M1G1101001k10k03 0 0 2 4 0 0 2 7 0 0Latency (cycles)PCIe MemoryNetwork CPU cache RDMADirectCXLDMAx8.3 fasterDMA641282565121K2K4K04k8k12k16kBreakdown(cycles)Payload (bytes)Library Copy Memory Network641282565121K2K4K01k2kBreakdown(cycles)Payload (bytes)MemoryPCIe CPU Cache Latency (cycles)Working set sizeLocal RDMA DirectCXLL1D (4)L2 (24)Local (60)CXL (328)RDMA (2027~2042 cycles)x5.5x510.5x34Figure 6: RDMA vs. CXL.(a) RDMA breakdown. (b) CXL breakdown.
Figure 7: Sensitivity tests. Figure 8: Memory hierarchy performance.
conﬁgured through a 1:1 connection. Figure 6shows latency
breakdown of RDMA andDirectCXL when reading 64 bytes of
data. One can observe from the ﬁgure that RDMA requires two
DMA operations, which doubles the PCIe transfer and mem-
ory access latency. In addition, the communication overhead
of InﬁniBand ( Network ) takes 78.7% (2129 cycles) of the
total latency (2705 cycles). In contrast, DirectCXL only takes
328 cycles for memory load request, which is 8.3 ⇥faster than
RDMA . There are two reasons behind this performance differ-
ence. First, DirectCXL straight connects the compute nodes
and memory nodes using PCIe while RDMA requires proto-
col/interface changes between InﬁniBand and PCIe. Second,
DirectCXL can translate memory load/store request from
LLC into the CXL ﬂits whereas RDMA must use DMA to
read/write data from/to memory.
Sensitivity tests. Figure 7adecomposes RDMA latency into es-
sential hardware ( Memory andNetwork ), software ( Library ),
and data transfer latencies ( Copy ). In this evaluation, we in-
strument two user-level InﬁniBand libraries, libibverbs and
libmlx4 to measure the software side latency. Library is
the primary performance bottleneck in RDMA when the size of
payloads is smaller than 1KB (4158 cycles, on average). As
the payloads increase, Copy gets longer and reaches 28.9% of
total execution time. This is because users must copy all their
data into RNIC’s MR, which takes extra overhead in RDMA . On
the other hand, Memory andNetwork shows a performance
trend similar to RDMA analyzed in Figure 6. Note that the actual
times of Network (Figure 7a) do not decrease as the payload
increases; while Memory increases to handle large size of data,
RNIC can simultaneously transmit the data to the underlying
network. These overlapped cycles are counted by Memory in
our analysis. As shown in Figure 7b, the breakdown analysis
forDirectCXL shows a completely different story; there is
neither software nor data copy overhead. As the payloads
increase, the dominant component of DirectCXL ’s latency is
LLC ( CPU Cache ). This is because LLC can handle 16 con-
current misses through miss status holding registers (MSHR)
in our custom CPU. Thus, many memory requests (64B) com-
posing a large payload data can be stalled at CPU, which
takes 67% of the total latency to handle 4KB payloads. PCIe
shown in Figure 7adoes not decrease as the payloads increase
because of a similar reason of RDMA ’sNetwork . However, it
Per-node usage Total
usageData stored in
remote memory Local Remote
DLRM [ 31]Less than
100MB17GB 68GB Embedding tables.
MemDB [ 12] 4GB 16GB Key-value pairs and tree structure.
Ligra [ 36] 7GB 28GB Deserialized graph structure.
Table 1: Memory usage characteristic of each workload.is not as much as what Network did as only 16 concurrent
misses can be overlapped. ote that PCIe shown in Figures 6
and7bincludes the latency of CXL IPs (RP, EP, and Switch),
which is different from the pure cycles of PCIe physical bus.
The pure cycles of PCIe physical bus (FlexBus) account for
28% of DirectCXL latency. The detailed latency decomposi-
tion will be analyzed in Section 4.2.
Memory hierarchy performance. Figure 8shows latency
cycles of different components in the system’s memory hier-
archy. While Local andDirectCXL exhibits CPU cache by
lowering the memory access latency to 4 cycles, RDMA has neg-
ligible impacts on CPU cache as their network overhead is
much higher than that of Local . The best-case performance of
RDMA was 2027 cycles, which is 6.2 ⇥and 510.5 ⇥slower than
that of DirectCXL and L1 cache, respectively. DirectCXL
requires 328 cycles whereas Local requires only 60 cycles in
the case of L2 misses. Note that the performance bottleneck
ofDirectCXL isPCIe including CXL IPs (77.8% of the total
latency). This can be accelerated by increasing the working
frequency, which will be discussed shortly.
4.2 Latency Distribution and Scaling Study
Latency distribution. In addition to the latency trend (av-
erage) we reported above, we also analyze complete latency
behaviors of Local ,RDMA , and DirectCXL . Figure 9shows
the latency CDF of memory accesses (64B) for the different
pooling methods. RDMA shows the performance curve, which
ranges from 1790 cycles to 4006 cycles. The reason why there
is a difference between the minimum and maximum latency
ofRDMA is RNIC’s MTT memory buffer and CPU caches for
data transfers. While RDMA cannot take the beneﬁts from di-
rect load/store instruction with CPU caches, its data transfers
themselves utilize CPU caches. Nevertheless, RDMA cannot
avoid the network accesses for remote memory accesses, mak-
ing its latency worse than Local by 36.8 ⇥, on average. In
contrast, the latency behaviors of DirectCXL are similar to
Local . Even though the latency of DirectCXL (reported in
Figures 6and7b) is the average value, its best performance
is the same as Local (4⇠24 cycles). This is because, as we
showed in the previous section, DirectCXL can take the ben-
eﬁts of CPU caches directly. The tail latency is 2.8 ⇥worse
than Local , but its latency curve is similar to that of Local .
This is because both DirectCXL andLocal use the same
DRAM (and there is no network access overhead).
Speed scaling estimation. The cycle numbers that we re-
ported here are measured at each host’s CPU using register-
level instrumentation. We believe it is sufﬁcient and better
USENIX Association 2022 USENIX Annual Technical Conference    29102001800200022000255075100CDF (%)Latency (cycles)Local RDMA DirectCXL L1DL2LocalCXL4006Figure 9: Memory-level
latency CDF (64B).Measurement
clock domain !DIRECT CXL PCIe 5.0 x8 (Estimated)
CPU (100MHz) CPU (1.2GHz) Time delay
L1/L2 cache 30 30 25 ns
CXL IPs (2.0)* 165 287 239 nsPCIeFlexBus 91 69 57 ns
DRAM controller 42 126 105 ns
Total 328 512 426 ns
*Including RP, EP, and Switch Unit: cycles
Table 2: Latency breakdown and
estimated 64B load latency.DLRMMemDB0.00.51.0Norm. Exec. TimeSwap KVS DirectCXL MISBFSCCBC0.00.51.0
SwapKVSCXL0.00.51.0Norm.Exec. TimeRDMA Software Workload DLRMSwapKVSCXLMemDBSwapCXLMISSwapCXLBFSSwapCXLCCSwapCXLBC(a) Execution Time. (b) Execution breakdown.
Figure 10: Real workload performance.
than a cross-time-domain analysis to decompose the system
latency. Nevertheless, we estimate a time delay in cases where
the target system accelerates the frequency of its processor
complex and CXL IPs (RP, EP, and Switch) by 1.2GHz and
1GHz, respectively. Table 2decomposes DirectCXL ’s latency
of a 64B memory load and compares it with the estimated time
delay. The cycle counts of L1/L2 cache misses are not differ-
ent as they work in all the same clock domain of CPU. While
other components (FlexBus, CXL IPs, and DRAM controller)
speed up by 4 ⇥(250MHz !1GHz), the number of cycles
increases since CPU gets faster by 12 ⇥. Note that, as the
version of PCIe is changed and the number of lanes for PCIe
increases by double, FlexBus’s cycles decrease. The table in-
cludes the time delays corresponding to the estimated system
from the CPU’s viewpoint. While the time delay of FlexBus is
pretty good ( ⇠60ns), the corresponding CXL IPs have room
to improve further with a higher working frequency.
4.3 Performance of Real Workloads
Figure 10ashows the execution latency of Swap ,KVS, and
DirectCXL when running DLRM, MemDB, and four work-
loads from Ligra. For better understanding, all the results in
this subsection are normalized to those of Swap . For Ligra, we
only compare DirectCXL with Swap because Ligra’s graph
processing engines (handling in-/out-edges and vertices) is
not compatible with a key-value structure. KVScan reduce the
latency of Swap as it addresses the overhead imposed by page-
based I/O granularity to access the remote memory. However,
it has two major issues behind KVS. First, it requires signif-
icant modiﬁcation of the application’s source codes, which
is often unable to service (e.g., MIS, BFS, CC, BC). Sec-
ond, KVSrequires heavy computation such as hashing at the
memory node, which increases monetary costs. In contrast,
DirectCXL without having a source modiﬁcation and remote-
side resource exhibits 3 ⇥and 2.2 ⇥better performance than
Swap and even KVS, respectively.
To better understand this performance improvement of
DirectCXL , we also decompose the execution times into
RDMA , network library intervention ( Software ), and appli-
cation execution itself ( Workload ) latencies, and the results
are shown in Figure 10b. This ﬁgure demonstrates where
Swap degrades the overall performance from its execution;
51.8% of the execution time is consumed by kernel swap
daemon ( kswapd ) and FastSwap driver, on average. This is
because Swap just expands memory with the local and remote
based on LRU, which makes its page exchange frequent. Thereason why KVSshows performance better than Swap in the
cases of DLRM and MemDB is mainly related to workload
characteristics and its service optimization. For DLRM, KVS
loads the exact size of embeddings rather than a page, which
reduces Swap ’s data transfer overhead as high as 6.9 ⇥. While
KVS shows the low overhead in our evaluation, RDMA and
Software can linearly increase as the number of inferences
increases; in our case, we only used 13.5MB (0.0008%) of
embeddings for single inference. For MemDB, as KVSstores
all key-value pairs into local DRAM, it only accesses remote-
side DRAM to inquiry values. However, it spends 55.3% and
24.9% of the execution time for RDMA andSoftware to han-
dle the remote DRAMs, respectively. In contrast, DirectCXL
removes such hardware and software overhead, which ex-
hibits much better performance than Swap andKVS. Note that
MemDB contains 2M key-value pairs whose value size is
2KB, and its host queries 8M Getrequests by randomly gen-
erating their keys. This workload characteristic roughly makes
DirectCXL ’s memory accesses be faced with a cache miss
for every four queries. Note that Workload ofDirectCXL is
longer than that of KVS, because DirectCXL places all hash
table and tree for key-value pairs whereas KVShas it in local
DRAM. Lastly, all the four graph workloads show similar
trends; Swap is always slower than DirectCXL . They require
multiple graph traverses, which frequently generate random
memory access patterns. As Swap requires exchanging 4KB
pages to read 8B pointers for graph traversing, it shows 2.2 ⇥
worse performance than DirectCXL .
5 Conclusion
In this paper, we propose DIRECT CXL that connects host
processor complex and remote memory resources over CXL’s
memory protocol ( CXL.mem ). The results of our real system
evaluation show that the disaggregated memory resources
ofDIRECT CXL can exhibit DRAM-like performance when
the workload can enjoy the host-processor’s cache. For real-
world applications, it exhibits 3 ⇥better performance than
RDMA-based memory disaggregation, on average.
6 Future Work and Acknowledgement
The authors are extending the kernel for efﬁcient CXL mem-
ory management and consider having an SoC silicon as fu-
ture work of DirectCXL . This work is protected by one or
more patents. The authors would like to thank the anonymous
reviewers for their comments, and Myoungsoo Jung is the
corresponding author ( mj@camelab.org ).
292    2022 USENIX Annual Technical Conference USENIX AssociationReferences
[1]Emmanuel Amaro, Christopher Branner-Augmon, Zhi-
hong Luo, Amy Ousterhout, Marcos K Aguilera, Aurojit
Panda, Sylvia Ratnasamy, and Scott Shenker. Can far
memory improve job throughput? In Proceedings of the
Fifteenth European Conference on Computer Systems ,
pages 1–16, 2020.
[2]Ling Liu, Wenqi Cao, Semih Sahin, Qi Zhang, Juhyun
Bae, and Yanzhao Wu. Memory disaggregation: Re-
search problems and opportunities. In 2019 IEEE 39th
International Conference on Distributed Computing Sys-
tems (ICDCS) , pages 1664–1673. IEEE, 2019.
[3]Kevin Lim, Yoshio Turner, Jose Renato Santos, Alvin
AuYoung, Jichuan Chang, Parthasarathy Ranganathan,
and Thomas F Wenisch. System-level implications of
disaggregated memory. In IEEE International Sympo-
sium on High-Performance Comp Architecture , pages
1–12. IEEE, 2012.
[4]Juncheng Gu, Youngmoon Lee, Yiwen Zhang, Mosharaf
Chowdhury, and Kang G Shin. Efﬁcient memory dis-
aggregation with inﬁniswap. In 14th USENIX Sympo-
sium on Networked Systems Design and Implementation
(NSDI 17) , pages 649–667, 2017.
[5]Marcos K Aguilera, Nadav Amit, Irina Calciu, Xavier
Deguillard, Jayneel Gandhi, Stanko Novakovic, Arun
Ramanathan, Pratap Subrahmanyam, Lalith Suresh, Ki-
ran Tati, et al. Remote regions: a simple abstraction for
remote memory. In 2018 USENIX Annual Technical
Conference (USENIX ATC 18) , pages 775–787, 2018.
[6]Chenxi Wang, Haoran Ma, Shi Liu, Yuanqi Li, Zhenyuan
Ruan, Khanh Nguyen, Michael D Bond, Ravi Netravali,
Miryung Kim, and Guoqing Harry Xu. Semeru: A
memory-disaggregated managed runtime. In 14th
USENIX Symposium on Operating Systems Design and
Implementation (OSDI 20) , pages 261–280, 2020.
[7]Christian Pinto, Dimitris Syrivelis, Michele Gazzetti,
Panos Koutsovasilis, Andrea Reale, Kostas Katrinis,
and H Peter Hofstee. Thymesisﬂow: a software-
deﬁned, hw/sw co-designed interconnect stack for rack-
scale memory disaggregation. In 2020 53rd Annual
IEEE/ACM International Symposium on Microarchitec-
ture (MICRO) , pages 868–880. IEEE, 2020.
[8]Seung-seob Lee, Yanpeng Yu, Yupeng Tang, Anurag
Khandelwal, Lin Zhong, and Abhishek Bhattacharjee.
Mind: In-network memory management for disaggre-
gated data centers. In Proceedings of the ACM SIGOPS
28th Symposium on Operating Systems Principles , pages
488–504, 2021.[9]Zhiyuan Guo, Yizhou Shan, Xuhao Luo, Yutong Huang,
and Yiying Zhang. Clio: A hardware-software co-
designed disaggregated memory system. In Proceedings
of the 27th ACM International Conference on Architec-
tural Support for Programming Languages and Operat-
ing Systems , pages 417–433, 2022.
[10] Irina Calciu, M Talha Imran, Ivan Puddu, Sanidhya
Kashyap, Hasan Al Maruf, Onur Mutlu, and Aasheesh
Kolli. Rethinking software runtimes for disaggregated
memory. In Proceedings of the 26th ACM International
Conference on Architectural Support for Programming
Languages and Operating Systems , pages 79–92, 2021.
[11] Shin-Yeh Tsai, Yizhou Shan, and Yiying Zhang. Dis-
aggregating persistent memory and controlling them
remotely: An exploration of passive disaggregated key-
value stores. In 2020 USENIX Annual Technical Con-
ference (USENIX ATC 20) , pages 33–48, 2020.
[12] Anuj Kalia, Michael Kaminsky, and David G Andersen.
Using rdma efﬁciently for key-value services. In Pro-
ceedings of the 2014 ACM Conference on SIGCOMM ,
pages 295–306, 2014.
[13] Aleksandar Dragojevi ´c, Dushyanth Narayanan, Miguel
Castro, and Orion Hodson. Farm: Fast remote mem-
ory. In 11th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 14) , pages 401–414,
2014.
[14] Aleksandar Dragojevi ´c, Dushyanth Narayanan, Ed-
mund B Nightingale, Matthew Renzelmann, Alex
Shamis, Anirudh Badam, and Miguel Castro. No com-
promises: Distributed transactions with consistency,
availability, and performance. In Proceedings of the
25th symposium on operating systems principles , pages
54–70, 2015.
[15] Jacob Nelson, Brandon Holt, Brandon Myers, Preston
Briggs, Luis Ceze, Simon Kahan, and Mark Oskin.
Latency-tolerant software distributed shared memory. In
2015 USENIX Annual Technical Conference (USENIX
ATC 15) , pages 291–305, 2015.
[16] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K Aguil-
era, and Adam Belay. Aifm: High-performance,
application-integrated far memory. In 14th USENIX
Symposium on Operating Systems Design and Imple-
mentation (OSDI 20) , pages 315–332, 2020.
[17] Gen-Z Consortium. Gen-Z Final Speciﬁcations. https:
//genzconsortium.org/specifications/ .
[18] CXL Consortium. Compute Express Link Speciﬁcation
Revision 2.0. https://www.computeexpresslink.
org/download-the-specification .
USENIX Association 2022 USENIX Annual Technical Conference    293[19] CXL Consortium. Compute Express
Link™ 2.0 White Paper. https://www.
computeexpresslink.org/_files/ugd/0c1418_
14c5283e7f3e40f9b2955c7d0f60bebe.pdf .
[20] Navin Shenoy. A Milestone in Moving Data. https:
//newsroom.intel.com/editorials/milestone-
moving-data .
[21] Debendra Das Sharma. CXL: Coherency, Memory,
and I/O Semantics on PCIe Infrastructure. https:
//www.electronicdesign.com/technologies/
embedded-revolution/article/21162617/cxl-
coherency-memory-and-io-semantics-on-pcie-
infrastructure .
[22] Patrick Kennedy. Compute Express Link
or CXL What it is and Examples. https:
//www.servethehome.com/compute-express-
link-or-cxl-what-it-is-and-examples/ .
[23] Hari Subramoni, Ping Lai, Miao Luo, and Dha-
baleswar K Panda. Rdma over ethernet—a preliminary
study. In 2009 IEEE International Conference on Clus-
ter Computing and Workshops , pages 1–9. IEEE, 2009.
[24] Philip Werner Frey and Gustavo Alonso. Minimizing the
hidden cost of rdma. In 2009 29th IEEE International
Conference on Distributed Computing Systems , pages
553–560. IEEE, 2009.
[25] Intel. Persistent Memory Developer Kit Version v1.11.0.
https://pmem.io/ .
[26] Intel. NVDIMM Namespace Speciﬁcation.
https://pmem.io/documents/NVDIMM_Namespace_
Spec.pdf .
[27] UEFI Forum, Inc. Advanced Conﬁguration and Power
Interface (ACPI) Speciﬁcation Version 6.4. https://
uefi.org/specs/ACPI/6.4/ , 2021.
[28] Linaro. The devicetree speciﬁcation. https://www.
devicetree.org/ .[29] Mellanox. Mellanox ConnectX-3 FDR (56Gbps) Inﬁni-
Band VPI. https://www.mellanox.com/related-
docs/prod_adapter_cards/PB_ConnectX3_VPI_
Card_Dell.pdf .
[30] Xilinx. Mellanox OpenFabrics Enterprise Distri-
bution. https://www.mellanox.com/products/
infiniband-drivers/linux/mlnx_ofed .
[31] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael
Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo
Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,
Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey
Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman
Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko,
Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vi-
jay Rao, Bill Jia, Liang Xiong, and Misha Smelyan-
skiy. Deep learning recommendation model for per-
sonalization and recommendation systems. CoRR ,
abs/1906.00091, 2019.
[32] Michael Luby. A simple parallel algorithm for the max-
imal independent set problem. SIAM journal on com-
puting , 15(4):1036–1053, 1986.
[33] Alan Bundy and Lincoln Wallen. Breadth-ﬁrst search.
InCatalogue of artiﬁcial intelligence tools , pages 13–13.
Springer, 1984.
[34] Fan Chung and Linyuan Lu. Connected components in
random graphs with given expected degree sequences.
Annals of combinatorics , 6(2):125–145, 2002.
[35] Ulrik Brandes. A faster algorithm for betweenness cen-
trality. Journal of mathematical sociology , 25(2):163–
177, 2001.
[36] Julian Shun and Guy E Blelloch. Ligra: a lightweight
graph processing framework for shared memory. In
Proceedings of the 18th ACM SIGPLAN symposium on
Principles and practice of parallel programming , pages
135–146, 2013.
294    2022 USENIX Annual Technical Conference USENIX AssociationElastic Use of Far Memory for In-Memory Database Management
Systems
Donghun Lee
Minseon Ahn
Jungmin Kim
dong.hun.lee@sap.com
minseon.ahn@sap.com
jimmy.kim@sap.com
SAP Labs Korea
Seoul, South KoreaDaniel Booss
Daniel Ritter
Oliver Rebholz
daniel.booss@sap.com
daniel.ritter@sap.com
oliver.rebholz@sap.com
SAP SE
Walldorf, Germany
Thomas Willhalm
thomas.willhalm@intel.com
Intel Deutschland GmbH
Feldkirchen, GermanySuprasad Mutalik Desai
Navneet Singh
suprasad.desai@intel.com
navneet.singh@intel.com
Intel Technology India Pvt. Ltd.
Bengaluru, India
ABSTRACT
The separation and independent scalability of compute and mem-
ory is one of the crucial aspects for modern in-memory database
systems (IMDBMSs) in the cloud. The new, cache-coherent memory
interconnect Compute Express Link (CXL) promises elastic mem-
ory capacity through memory pooling. In this work, we adapt the
well-known IMDBMS, SAP HANA, for memory pools by features
of table data placement and operational heap memory allocation
on far memory, and study the impact of the limited bandwidth and
higher latency of CXL. Our results show negligible performance
degradation for TPC-C. For the analytical workloads of TPC-H,
a notable impact on query processing is observed due to the lim-
ited bandwidth and long latency of our early CXL implementation.
However, our emulation shows it would be acceptably smaller with
the improved CXL memory devices.
CCS CONCEPTS
•Hardware !Emerging interfaces ;•Information systems
!Database management system engines .
KEYWORDS
CXL, Far memory, Memory pool, In-Memory Database, DBMS,
Database Management Systems
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro ￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speci ￿c permission
and/or a fee. Request permissions from permissions@acm.org.
DaMoN ’23, June 18–23, 2023, Seattle, WA, USA
©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0191-7/23/06. . . $15.00
https://doi.org/10.1145/3592980.3595311ACM Reference Format:
Donghun Lee, Minseon Ahn, Jungmin Kim, Daniel Booss, Daniel Ritter,
Oliver Rebholz, Thomas Willhalm, Suprasad Mutalik Desai, and Navneet
Singh. 2023. Elastic Use of Far Memory for In-Memory Database Manage-
ment Systems. In 19th International Workshop on Data Management on New
Hardware (DaMoN ’23), June 18–23, 2023, Seattle, WA, USA. ACM, New York,
NY, USA, 9 pages. https://doi.org/10.1145/3592980.3595311
1 INTRODUCTION
The cloud computing industry is constantly evolving, and the needs
of the elasticity in computing power and memory capacity are be-
coming increasingly important. The new memory interconnect
Compute Express Link (CXL) [ 7] promises large, cache-coherent
memory capacity and independent, elastic scalability. CXL enables
dynamic memory expansion, a disaggregated memory system, and
a memory pool, all of which could help to meet the growing de-
mand for the ￿exible system architecture of cloud-based in-memory
database management systems (IMDBMSs) and reduce their total
cost ownership (TCO). CXL-attached memory capacity can be more
elastically scaled, based on the actual demand, which makes it easier
to manage and optimize the resource utilization.
In our previous work [ 2], we laid out the vision of a CXL memory
device for elastic memory expansion of IMDBMSs, and gave initial
performance results for standard database benchmarks. We showed
that we can easily expand the memory space with nearly no or
small amount of performance impact. Additionally in [ 11], we sug-
gested the classi ￿cation of the di ￿erent memory distances, based on
their actual physical distance but taking the transport layer into ac-
count, and introduced general CXL use cases for the disaggregated
memory system (e. g., shared memory pooling, multi-socket). Due
to various latencies and di ￿erent raw materials of CXL memory
devices, there would be diverse con ￿gurations of tiered memory
systems for IMDBMSs. As in [ 1,3,11], we use the term far memory
for memory devices connected via network – without mediation
by a local processor and better availability due to separate fault
35
DaMoN ’23, June 18–23, 2023, Sea ￿le, WA, USA Donghun Lee, et al.
domains – like CXL 1.1 or CXL-switch based on CXL 2.0, which
could consist of DRAM or PMEM to support direct load or store
commands within a reasonable duration.
In this work, we introduce and describe use cases for enterprise-
scale IMDBMS that leverage a memory pool, namely use idle com-
pute,near-zero downtime upgrade ,query burst ,distributed compute ,
andfailover . We extend the well-known SAP HANA IMDBMS by
two main features that enable dynamic memory allocation using
far memory for those use cases: (i) moving the main storage of any
table data to far memory and (ii) allocating the heap memory of SAP
HANA execution engine (HEX) [ 25] to far memory. As far memory
usually has a higher latency and a limited bandwidth compared to
local memory, understanding the performance impact of the two
features (cf. (i), (ii)) on an IMDBMS is crucial. Hence, we evaluate
their performance impact on far memory using TPC-C and TPC-H.
The main contributions of this work are:
•Speci ￿cation of CXL use cases for enterprise-scale IMDBMSs,
•Adaptation of a SAP HANA with far memory by features of
(i) table data placement and (ii) operational heap memory
allocation, and
•Extensive evaluation of the adapted IMDBMS for transac-
tional and analytical workloads.
Our experimental analysis shows that transactional workloads like
TPC-C have nearly no performance degradation, despite longer
latencies of far memory. Analytical workloads like TPC-H su ￿er
from performance degradation due to the limited bandwidth and
long latency of our early CXL implementation, but it would be
acceptable with the improved CXL memory devices.
The remainder of this paper is organized as follows: Section 2 in-
troduces CXL and explains the elasticity requirement in cloud-based
IMDBMSs. In Section 3, we introduce use cases of far memory for
IMDBMSs. The implementation details of far memory are presented
in Section 4. In Section 5, we conduct the performance evaluation
and discuss important insights. Section 6 discusses related works
and Section 7 concludes the paper.
2 BACKGROUND
In this section, we introduce the CXL memory interconnect and
describe requirements on the elasticity of cloud IMDBMSs.
2.1 Compute Express Link
Compute Express Link (CXL) is an open standard to support cache-
coherent interconnect between a variety of devices [ 7]. After the
introduction of CXL in 2019, the standard has evolved and contin-
ues to be enhanced. CXL 1.1 de ￿nes the protocol for three major
device types: accelerators with cache-only (type 1), cache with at-
tached memory (type 2), and memory expansion (type 3). CXL 2.0
expands the speci ￿cation – among other capabilities – to memory
pools using CXL switches on a device level. CXL 3.0 introduces
fabric capabilities and management, improved memory sharing and
pooling with dynamic capacity capability, enhanced coherency, and
peer-to-peer communication.
Since the market of CXL devices is emerging, several vendors
have announced products using CXL. For example, Samsung [ 18]and SK Hynix [ 19] introduce CXL DDR5 modules, AsteraLabs [ 5] an-
nounced a CXL memory accelerator, and Montage technology [ 28]
will o ￿er a CXL memory expander controller.
2.2 Elasticity of Cloud IMDBMSs
One of the key requirements of modern IMDBMSs in the cloud
is “elasticity”. Current public cloud infrastructures are o ￿ering
virtually unlimited computing and storage resources on demand.
With the emergence of disaggregated memory technology such as
CXL, Gen-Z [ 8], OpenCAPI [ 9] (latter two subsumed by CXL), and
CCIX [ 6], elastic memory capacity in cloud infrastructure will be
available in the near future.
However, traditional database architectures are designed for
￿xed amounts of resources, which would make it di ￿cult for them
to leverage such elasticity o ￿ered in public cloud infrastructures.
To ful ￿ll the requirements of elastic computing capabilities in cloud
environments, SAP HANA provides independently, scalable work-
ers, called Elastic Compute Node (ECN). ECNs are similar to SAP
HANA scale-out instances1, except that ECNs have an ephemeral
persistence, which does not need to be persisted via NSE [ 25], and is
excluded from services like HANA backup and recovery. Therefore,
ECNs do not store normal database tables, while temporary tables or
replica tables are populated to ECN instances. With temporary per-
sistence, ECN instances can be easily added and removed depending
on the incoming workload state or the customer’s demands.
Since CXL version 2.0, the memory pooling capability is enabled.
This feature goes beyond simple memory expansion and allows
for the dynamic scaling of memory capacity for multiple systems,
supporting a more ￿exible architecture for systems that use it and
resulting in the reduced TCO. Memory pooling is particularly use-
ful for IMDBMSs, where large amounts of memory are needed
to achieve optimal performance. Thus, our use cases will center
around memory pooling for IMDBs and we need to evaluate their
impacts on performance for various workloads.
2.3 Data Storage in SAP HANA
SAP HANA is one of the leading Hybrid Transaction / Analytical
Processing (HTAP)2supporting OLTP and OLAP workloads in a
single system, which simpli ￿es the overall system architecture with
low TCO [ 22–24]. It uses a compressed, columnar storage layout
for fast-read accesses and a low memory footprint. The columnar
data is stored in the read-optimized main storage and maintains
a separate delta storage for optimized writes [ 10,26]. The delta
storage is periodically merged with the main storage [17].
3 FAR MEMORY USE CASES FOR SAP HANA
Memory devices are some of the most expensive parts in the mod-
ern computer architectures. Separating memory from compute and
memory pooling are promising trends to reduce the memory con-
sumption of an IMDBMS like SAP HANA. That is achieved by
allocating only required memory at a particular time and enabling
1SAP HANA: https://help.sap.com/docs/SAP_HANA_PLATFORM/
6b94445c94ae495c83a19646e7c3fd56/a165e192ba374c2a8b17566f89fe8419.html
2Translytical data platforms: https://news.sap.com/2022/12/translytical-data-
platforms-forrester-wave-sap-a-leader/
36Elastic Use of Far Memory for In-Memory Database Management Systems DaMoN ’23, June 18–23, 2023, Sea ￿le, WA, USA
dynamic memory scaling. Subsequently, we present use cases lever-
aging memory pooling for IMDBs, before we describe how far
memory can be practically used in SAP HANA.
3.1 Use Cases
We argue that the following far memory use cases, leveraging CXL
memory pooling, are bene ￿cial for IMDBs.
3.1.1 Use idle compute. For workloads running on a SAP HANA
process that utilizes only a portion of the available compute ca-
pability but does not have enough memory to run another SAP
HANA process, hosts with free CPU resources can execute another
SAP HANA process using the memory from the memory pool. The
memory of the process is bound to the memory exposed by the
memory pool, providing simpli ￿ed life cycle management, since all
memory allocated in both local and far by the process gets freed
when it terminates. In a variant of this case, only the main storage
of a SAP HANA process could be put into far memory, which keeps
columnar tables on the pool using the same techniques used for
PMem [ 4]. This case could be most promising for SAP HANA ECNs
to leverage idle compute capacity.
3.1.2 Near-zero downtime upgrade. SAP provides near-zero down-
time upgrades of enterprise software systems such as SAP S/4HANA.
For the upgrade, a “bridging” database subsystem is created as a
part of the existing system where a view is generated for each SAP-
speci ￿c or customer-speci ￿c table in the bridge’s schema. Business
users are transparently transferred and reconnected to the bridge
subsystem without any interruption while upgrading the original
subsystem. Meanwhile, the bridge subsystem imitates the origi-
nal system and contains all data of the production system that
users need to continue their work. Even though not the entire
database is cloned but only selected tables based on the changes
to be performed by the maintenance event, we need additional
memory capacity for the bridge subsystem. Therefore, a memory
pool could be utilized, supporting a tighter sizing on the existing
system, regardless of the additional memory requirements during
the upgrade.
3.1.3 ￿ery Burst. Occasionally, more memory is needed during
phases with more memory-demanding queries. For example, there
are increasing memory demands for big queries during the ￿scal
closing process at the end of every month, quarter, or year. In ad-
dition to the memory space for main storage and delta storage,
SAP HANA needs temporal, operational memory space for keep-
ing intermediate results and data structures for query processing.
SAP HANA’s execution engine (HEX) [ 25] has a clean memory
management design and allows for specifying whether memory
allocations are done in local or far memory. Using this feature, we
allocate the operational memory in a memory pool. It contributes
to tight memory sizing and elastic memory capability so that we
can allocate more memory dynamically according to the increased
memory demands.
3.1.4 Distributed compute. When additional compute nodes are
added for SAP HANA’s system replication, it may be advantageous
to move table data into a memory pool to avoid unnecessary data
replication. This allows data to be read from multiple SAP HANAhosts, with synchronized, on-demand updates of the main or delta
storage. This usage has the potential to reduce the overall memory
costs and enable e ￿cient memory usage in SAP HANA. However,
this requires CXL 3.0, since it needs synchronization among multi-
ple hosts. As an interim solution, a dedicated, independent memory
space for multiple SAP HANA instances in a memory pool could
be utilized, which is supported by CXL 2.0. For that, we gather all
the table data for each SAP HANA instance in a memory pool and
allocate only the operational memory in the local memory in each
host. This approach contributes to better global memory utilization
and lower TCO.
3.1.5 Failover. During the planned or unplanned downtime of a
SAP HANA instance, a takeover will be performed by another SAP
HANA instance. If the main storage of columnar tables is allocated
in the memory pool, the SAP HANA processes, which want to
take over, can attach to that memory pool and read the tables after
a failover without loading the data into memory. This scenario
will signi ￿cantly reduce the required failover time. The ownership
change of the memory space in a memory pool among multiple
hosts, however, requires CXL 3.0.
3.2 Far Memory Usages in SAP HANA
Similar to [ 11], we use the term “far memory” as the memory
device connected via CXL 1.1 or CXL-switch based on CXL 2.0. To
support the use cases of the CXL memory pool mentioned in the
previous section, SAP HANA provides some features to enable (i)
dynamic data movement and (ii) memory allocation to far memory.
Subsequently, we discuss more details of the two features supported
by SAP HANA.
3.2.1 Moving main storage. CXL memory pooling can be used by
moving the main storage to far memory. We use SAP HANA’s per-
sistent memory feature [ 4] to put the main storage of the columnar
tables on far memory with fairly clear semantics on the lifecycle of
the main storage. The far memory could be con ￿gured as fsdax or
tempfs. Any speci ￿ed table data can be moved to far memory. This
feature can be used in the scenarios addressed in Sects. 3.1.1, 3.1.2,
3.1.4 and 3.1.5.
3.2.2 Allocating HEX heap memory. CXL memory pool can be used
by allocating HEX heap memory in the far memory. As mentioned
in Section 3.1.3, the new SAP HANA execution engine (HEX) [ 25]
can allocate the heap memory required to process a given query
to near or far memory according to the con ￿gured memory only
NUMA node information in SAP HANA. Among the several mem-
ory allocators within SAP HANA, the HEX heap memory is the
most dominant part of dynamic memory allocation during its query
processing. This feature can be used in the scenarios addressed in
Sects. 3.1.1 and 3.1.3.
Usually, far memory has a relatively longer latency than local
memory – probably about three to four times longer than a single
NUMA hop in modern computer architecture. Therefore, it is impor-
tant to understand the performance characteristics when we apply
the far memory usages in SAP HANA. The accesses to the main
storage are usually sequential and mostly done by data-intensive op-
erations where L2 prefetching scheme may hide the longer latency
of the far memory. Rather, the accesses to the HEX heap memory
37DaMoN ’23, June 18–23, 2023, Sea ￿le, WA, USA Donghun Lee, et al.
Figure 1: System setup and far memory CXL prototype
Figure 2: Implementation of the CXL memory pool
are mainly random and frequently done during compute-intensive
operations. We will show the performance e ￿ect of the two usages
against the two di ￿erent workloads (OLTP and OLAP) in Sect. 5.
4 CXL PROTOTYPE IMPLEMENTATION
In this section, we describe the CXL prototype that we integrated
into SAP HANA as far memory, give more details on its internal
implementation and discuss limitations.
4.1 CXL Prototype Overview
Figure 1 shows the system setup used to study the performance
impact on IMDBMSs. The experimental memory pool is realized
using an Intel ®Agilex ®AGI027 FPGA card which supports PCIe
Gen5 x16 CXL connectivity. This FPGA card is connected to the
second socket by directly inserting it in a PCIe slot at Socket 1 with16 PCIe lanes and connected to Socket 0 through two MCIO 8x
cables.
4.2 Memory Pool Implementation Details
Figure 2 grants a more detailed view into the implementation of our
CXL memory pool on the FPGA card. The R-Tile Intel FPGA IP for
CXL implements the CXL link and transaction layer management
functions needed to build FPGA-based CXL 1.1/2.0 compliant Type
1, Type 2, and Type 3 endpoint designs. The CXL IP solution consists
of a combination of a protocol Soft IP in the FPGA main fabric die
paired with the Hard IP companion R-Tile. R-Tile manages all CXL
link functions. It connects to a CPU host with a PCIe Gen5x16
interface and supports up to a 64GB/s theoretical bandwidth. The
device enumerates as a CXL endpoint.
The Soft IP manages transaction layer functions. For Type 3,
the CXL.mem transaction layer receives CXL.mem requests issued
from a CPU host, and generates host-managed device memory
(HDM) requests to an HDM subsystem. CXL.io transaction layer
receives CXL.io requests, either con ￿guration space requests or
memory space requests issued from a CPU host, and forwards them
to the targeted control and status registers. The User Streaming
Interface allows a user design to implement additional custom
CXL.io features.
The FPGA card hosts 2x8GB on-board DDR4 1333 MHz memory,
which is exposed to the host as memory. It is important to note
that in this experimental setup, the FPGA allows accesses to exactly
the same amount of memory over each of the two CXL links. In
other words, the same far memory can be exposed to two di ￿erent
NUMA nodes of the same size without address overlap between
them. The FPGA does not create a single cache-coherent domain,
and thus cannot support coherency between these two NUMA
nodes assigned to the far memory. Therefore, applications must
manage accesses to the shared memory.
4.3 Known Limitations of Prototype
It is worth noting that the bandwidth is limited due to the current
implementation of our CXL prototype, and not inherent to CXL.
Options to increase the prototype’s bandwidth would include:
•Use a faster speed FPGA which supports DDR4 3200 Mbps
or DDR5 5600 Mbps.
•Increase the number of slices for the CXL IP.
•Increase the number of independent DDR channels within
the device from one to four channels, for example.
Apart from improving the performance of the device itself, multiple
devices can be interleaved to aggregate their bandwidth.
5 PERFORMANCE EVALUATION
In this section, we evaluate SAP HANA on CXL far memory imple-
mentation for common transactional and analytical workloads.
5.1 System Con ￿guration
Our experimental setup is based on Intel 4thgeneration Xeon pro-
cessor code-named Sapphire Rapids . The system is equipped with
two processors with a base frequency of 2.0GHz and 56 cores each
38Elastic Use of Far Memory for In-Memory Database Management Systems DaMoN ’23, June 18–23, 2023, Sea ￿le, WA, USA
(a) I-con ￿g
(b) Y-con ￿g
Figure 3: Far memory setup variants
Figure 4: Far memory con ￿gurations in I-con ￿g
plus Hyper-Threading. Each processor has 128GBin 8 channels,
one16 GB DDR5 4800 MHz DIMM per channel.
To see the performance e ￿ect on various use cases, we use
two di ￿erent far memory setups, I-con ￿g and Y-con ￿g, as shown
in Fig. 3. First, I-con ￿g enumerates the entire far memory as NUMA
node 2 of size 16GBdirectly connected to Socket 0 as shown
in Fig. 3a. We enable the far memory connection only from Socket
0to see the performance impact on far memory use cases. In this
setup, the CXL connection to Socket 1 is disabled and the whole
far memory in FPGA is accessed through NUMA node 2. Since the
far memory is directly connected to the ￿rst socket, CPU a ￿nity of
SAP HANA is set to CPU0 onSocket 0 to avoid any NUMA e ￿ect.To study the performance e ￿ects on two di ￿erent usage types
of far memory, i. e., (i) moving the main storage and (ii) allocating
HEX heap memory, we divide the far memory into two parts, (A)
fsdax for the main storage and (B) memory only NUMA node for
the HEX heap memory. Then, we evaluate four di ￿erent con ￿gura-
tions by combining these two usage types. The ￿rst con ￿guration,
Both DRAM , has both the main storage and the HEX heap memory
in DRAM, which is our baseline in this experiment. The second
con￿guration, Main FAR , puts the main storage in the far memory
and the HEX heap memory remaining in DRAM while the third
con￿guration, HEX FAR , keeps the main storage in DRAM and
puts the HEX heap memory in the far memory. The last con ￿gura-
tion, Both FAR , has both in the far memory. Figure 4 shows the
location of the main storage and the HEX heap memory for each
con￿guration in this far memory setup.
The second far memory setup is Y-con ￿g where the far memory
is connected to both CPUs as shown in Fig. 3b. We enable both far
memory connections to con ￿rm the e ￿ect of mixed workloads from
multiple sockets to a memory pool. We believe it is a general form
of the memory pool with CXL 2.0. To avoid any memory access
violation, we partition the whole far memory into two halves. The
￿rst half is set to NUMA node 2 directly connected to Socket 0 ,
and the second half is set to NUMA node 3 directly connected to
Socket 1 .
5.2 Experiment on I-Con ￿g
Our experiments are conducted using a transactional / OLTP and
an analytical / OLAP benchmark. First, we use TPC-C [ 29] with 100
warehouses for OLTP workloads, with an fsdax size of 12GB, to
support intermediate delta merges, while the memory only NUMA
node is set to 4GBfor the HEX heap memory. Second, we use TPC-
H[30] for evaluating OLAP workloads. Due to the limited capacity
of the far memory, we use TPC-H SF10, which requires 4.7GB
for the main storage. When testing TPC-H, the FPGA memory is
divided into 8GBforfsdax and8GBfor memory only NUMA node
because TPC-H requires more HEX heap memory during query
processing. Subsequently, we report our experimental results for
transactional and analytical query processing on IMDBs on CXL
far memory.
5.2.1 OLTP (OnLine Transaction Processing). We use TPC-C for
OLTP workload evaluation with 100 warehouses. To populate the
transactions in this benchmark test, the number of client threads
per process is set to 28. Figure 5 shows the performance and CXL
tra￿c in all four con ￿gurations of TPC-C. The performance is nor-
malized to the value at the 16-process con ￿guration of the baseline
Both DRAM . As shown in Fig. 5, TPC-C has no signi ￿cant per-
formance degradation even with the smaller available bandwidth
and the longer latency of our far memory implementation. Since
8-process con ￿guration, CPU utilization of SAP HANA shows up
to 80-90% at maximum. The total number of client connections to
SAP HANA is 224 with 8 client processes, which is more than twice
the total number of available logical cores in a single socket. Even
with the 16-process con ￿guration, where SAP HANA shows nearly
full CPU utilization, there is no performance degradation when
using far memory.
39DaMoN ’23, June 18–23, 2023, Sea ￿le, WA, USA Donghun Lee, et al.
2 4 8 160.20.40.60.81.01.2
# Client ProcessesNormalized ThroughputBoth DRAM Main FAR HEX FAR Both FAR
(a) Normalized Throughput2 4 8 160.00.51.01.52.0
# Client ProcessesTra￿c (GB/s)
(b) CXL Total Tra ￿c
2 4 8 160.00.51.01.52.0
# Client ProcessesTra￿c (GB/s)
(c) CXL Write Tra ￿c2 4 8 160.00.51.01.52.0
# Client ProcessesTra￿c (GB/s)
(d) CXL Read Tra ￿c
Figure 5: TPC-C throughput and CXL tra ￿c
1 2 4 80.20.40.60.81.0
# StreamsNormalized ThroughputBoth DRAM Main FAR HEX FAR Both FAR
(a) Normalized Throughput1 2 4 8051015
# StreamsTra￿c (GB/s)
(b) CXL Total Tra ￿c
1 2 4 8051015
# StreamsTra￿c (GB/s)
(c) CXL Write Tra ￿c1 2 4 8051015
# StreamsTra￿c (GB/s)
(d) CXL Read Tra ￿c
Figure 6: TPC-H throughput and CXL tra ￿c
Our analysis of CXL tra ￿c measured with customized Intel ®
Performance Counter Monitor [ 21] shows that there is a meaningful
amount of data access to the far memory, but the total amount of
CXL tra ￿c is much lower than the maximum bandwidth of our farmemory implementation. HEX FAR always has a similar amount of
CXL write tra ￿c to that of CXL read tra ￿c. It is also observed that
Main FAR has CXL write tra ￿c only in the con ￿gurations with 8
or more processes. This is due to a delta merge after collecting a
40Elastic Use of Far Memory for In-Memory Database Management Systems DaMoN ’23, June 18–23, 2023, Sea ￿le, WA, USA
Both DRAM Main FAR HEX FAR Both FAR0.00.51.01.52.01.001.121.261.58Normalized Time
(a) Normalized execution time at 1 streamBoth DRAM Main FAR HEX FAR Both FAR0.00.20.40.60.81.01.00
0.600.53
0.311.000.93 0.960.88Normalized ThroughputMeasured CXL Emulation
(b) Normalized throughput at 8 streams
Figure 7: TPC-H performance comparison
certain amount of deltas in the host DRAM. Since CXL tra ￿c is not
saturated in TPC-C, the amount of CXL tra ￿c inBoth FAR appears
as the sum of those in both Main FAR andHEX FAR . Our analysis
with Intel ®VTune [ 13] shows that the TPC-C workload causes
many lock con ￿icts and a high synchronization overhead within
SAP HANA. We conclude that this synchronization overhead is
hiding the longer latency of far memory. Thus, it is not sensitive to
the bandwidth or latency of the far memory.
5.2.2 OLAP (OnLine Analytical Processing). We use TPC-H with
SF10 for OLAP workload evaluation due to the limited capacity in
the CXL memory device. Figure 6 shows the overall performance
and CXL tra ￿c in all four con ￿gurations of TPC-H. The perfor-
mance is normalized to the value at the 8-stream con ￿guration of
the baseline Both DRAM . We observe that Main FAR has no CXL
write tra ￿c, but it has much more CXL read tra ￿c than HEX FAR .
Thus, it has a larger amount of CXL total tra ￿c than HEX FAR .
However, Main FAR shows less performance degradation than
HEX FAR . Our analysis with Intel ®VTune reports that Main FAR
has higher memory bandwidth bound than HEX FAR . When mov-
ing the main storage, the access pattern is mainly sequential [ 14]
and the prefetching scheme e ￿ciently exploits the bandwidth. Thus,
Main FAR is a￿ected by limited bandwidth of far memory. Prefetch-
ing contributes to the better performance even with the much larger
amount of CXL tra ￿c. When allocating HEX heap memory, the
access pattern is rather random and, thus is a ￿ected by long latency
of far memory. Note that Both FAR has smaller CXL write tra ￿c
than HEX FAR because the CXL total tra ￿c is already saturated.
Figure 7 shows the performance comparison for TPC-H. In the
execution time with a single stream in Fig. 7a, the performance
degradation compared to the baseline Both DRAM is measured
12.0% for Main FAR , 26.1% for HEX FAR , and 57.9% for Both FAR .
In the throughput with 8 streams, it is measured 39.9% in Main FAR ,
47.4% in HEX FAR , and 69.2% in Both FAR as shown in Fig. 7b. The
overall performance in TPC-H is bound by the limited bandwidth
of our far memory prototype. Since most of the internal bandwidth
in our far memory prototype is consumed from the 4-stream con ￿g-
uration, the performance becomes saturated in all con ￿gurations
using the far memory as shown in Fig. 6b. Thus, TPC-H results
show a higher performance degradation because of the bandwidth
limitation in our implementation.To project the performance impact on future CXL devices, we
perform CXL emulation using the memory in the remote NUMA
node (cf. Socket1 in Fig. 1), assuming that the access latency to
the memory in the remote NUMA node through UPI is similar to
the latency of future CXL devices with improved bandwidth. Ac-
cording to the performance test in CXL emulation, performance
degradation can be reduced up to 7.2% in Main FAR , up to 4.1% in
HEX FAR , and up to 11.9% in Both FAR as shown in Fig. 7b. The
results demonstrate that the performance degradation by analyt-
ical workload can be signi ￿cantly reduced when the latency and
bandwidth of far memory are improved. Similar results have been
observed in our previous work [2].
5.3 Experiment on Y-Con ￿g
In this experiment, we use only TPC-H SF10 because OLAP work-
loads are more sensitive to the bandwidth limitation in far memory
con￿gurations. To generate mixed workloads for Y-con ￿g, we in-
stall two separate SAP HANA instances and assign each instance to
a di￿erent socket to allow them to run independently and concur-
rently without any interference except the far memory. The ￿rst
instance running at Socket 0 is set to HEX FAR by allocating the
HEX heap memory to NUMA node 2. The second instance running
atSocket 1 is set to Main FAR by moving the main storage to
fsdax in NUMA node 3.
Figure 8 shows performance throughput of TPC-H in Y-con ￿g
normalized to the previously measured performance value at the
8-stream con ￿guration of the baseline Both DRAM . To see the
performance impact on the bandwidth competition in the far mem-
ory, we measure the performance for each instance in both cases
(1) when they are running separate ly consuming the far memory
bandwidth exclusively and (2) when they are running concur-
rent ly competing each other for the limited bandwidth of the far
memory. This ￿gure shows that HEX Far has less performance
degradation than Main FAR when comparing the performance
from the separate execution to the concurrent execution for each
instance. In the 8-stream con ￿guration, HEX Far has 42.1% per-
formance degradation, while Main FAR has 54.8%. When they are
running concurrently, it is observed that HEX Far always has bet-
ter performance than Main FAR . We believe that the performance
degradation comes from the CXL tra ￿c reduction in the concurrent
execution because Main FAR has a larger amount of CXL tra ￿c
41DaMoN ’23, June 18–23, 2023, Sea ￿le, WA, USA Donghun Lee, et al.
1 2 4 80.00.20.40.6
# StreamsNormalized ThroughputHEX FAR (separate) Main FAR (separate)
HEX FAR (concurrent) Main FAR (concurrent)
Figure 8: TPC-H throughput on Y-con ￿g
than HEX Far . Therefore, it is concluded that the prefetch bene ￿t
inMain FAR is reduced when workloads are competing for the
limited available internal bandwidth in the CXL memory pool.
5.4 Discussion
Our experimental results show that the far memory usages have
di￿erent impacts depending on workloads. TPC-C has negligible
performance degradation in any far memory usage because of the
small amount of data accesses and high synchronization overhead,
which hides longer latency of far memory. Unlike TPC-C, TPC-H
has a certain amount of performance degradation because of the
limited available bandwidth and long latency when accessing the
large amount of data. When the main storage is moved to the far
memory, bandwidth dominantly a ￿ects the performance degrada-
tion because prefetching for the sequential accesses on the main
storage saturates the available bandwidth. When the HEX heap
memory is allocated in the far memory, latency dominantly a ￿ects
the performance degradation because of the random accesses on
the HEX heap memory. Our analysis with CXL emulation showed
that the performance degradation would be acceptably smaller if
the future CXL memory pool has better bandwidth and latency.
From the experimental results on Y-con ￿g, we observed that
prefetch bene ￿t is reduced when the available internal bandwidth
in CXL memory pool is limited. Therefore, we conclude that CXL
memory pool must have enough internal bandwidth to fully exploit
the prefetch bene ￿t of sequential accesses.
6 RELATED WORK
Memory disaggregation has been active in recent years, and several
proposals have been made to address the challenges of managing
and accessing remote memory. To enable high-performance mem-
ory disaggregation in modern data centers, CXL technology has
been widely adopted. Gouk et al. [ 12] have introduced DirectCXL,
a system that allows for direct access to remote memory and uti-
lizes intelligent caching and prefetching techniques to achieve high
performance. Similarly, Maruf et al. [ 16] have presented a memory
placement strategy that utilizes CXL to create a tiered memory
hierarchy that applications can seamlessly access.CXL for memory expansion and acceleration is one of the hot
research topics. Sim et al. [ 27] have proposed a computational CXL-
memory solution that expands memory capacity and improves
performance for memory-intensive applications. Park et al. [ 20]
have developed a CXL memory expander that scales memory per-
formance and capacity in modern data centers.
Furthermore, the performance and e ￿ciency of CXL-enabled
memory pooling systems have been investigated. Li et al. [ 15]
have introduced a CXL-based memory pooling system that can be
employed in cloud platforms to enhance memory utilization and
performance. Additionally, Yang et al. [ 31] have presented a CXL-
enabled hybrid memory pool that combines DRAM and PMEM
resources to improve memory performance.
7 CONCLUSION
The separation of compute and memory, and their composition
through far memory based on CXL enables new use cases for
IMDBMSs, shows a path from current resource over-provisioning,
and reduces their TCO. In this paper, we introduced and discussed
promising far memory use cases for IMDBMSs and two fundamen-
tal adaptations (i. e., table data placement and operational heap
memory allocation on far memory) to SAP HANA, and conducted
a performance evaluation of these two adaptations using the far
memory implemented on an early FPGA-based CXL prototype.
The results of our experimental evaluation show that transac-
tional workloads like TPC-C have nearly no performance degrada-
tion in any far memory usage because of the small amount of data
accesses and high synchronization overhead. However, analytical
workloads, such as TPC-H, causing large amount of data accesses,
have a certain amount of performance degradation because of the
limited available bandwidth and long latency of far memory. As
shown in our CXL emulation results using remote NUMA mem-
ory, we expect an acceptable performance degradation in TPC-H
with improved bandwidth and latency of far memory in the future.
Therefore, we conclude that the far memory can allow elasticity in
IMDBMSs with the improved CXL memory devices.
In future work, we will further develop solutions on how to
avoid performance impacts, especially for data intensive analytic
workloads due to the limited bandwidth and long latency of far
memory. Moreover, we will collect and specify more far memory
use cases for memory pools to fully utilize the elastic memory
capacity for cloud IMDBMSs.
REFERENCES
[1]Marcos K. Aguilera, Kimberly Keeton, Stanko Novakovic, and Sharad Singhal.
2019. Designing Far Memory Data Structures: Think Outside the Box. In HotOS .
ACM, 120–126. https://doi.org/10.1145/3317550.3321433
[2]Minseon Ahn, Andrew Chang, Donghun Lee, Jongmin Gim, Jungmin Kim, Jaemin
Jung, Oliver Rebholz, Vincent Pham, Krishna T. Malladi, and Yang-Seok Ki. 2022.
Enabling CXL Memory Expansion for In-Memory Database Management Systems.
InDaMoN . ACM, 8:1–8:5. https://doi.org/10.1145/3533737.3535090
[3]Emmanuel Amaro, Christopher Branner-Augmon, Zhihong Luo, Amy Ouster-
hout, Marcos K. Aguilera, Aurojit Panda, Sylvia Ratnasamy, and Scott Shenker.
2020. Can far memory improve job throughput?. In EuroSys . ACM, 14:1–14:16.
https://doi.org/10.1145/3342195.3387522
[4]Mihnea Andrei, Christian Lemke, Günter Radestock, et al .2017. SAP HANA
adoption of non-volatile memory. Proceedings of the VLDB Endowment 10, 12
(2017), 1754–1765. https://doi.org/10.14778/3137765.3137780
[5]AsteraLabs. 2022. CXL Memory Accelerators . https://www.asteralabs.com/
products/cxl-memory-platform/
[6]CCIX Consortium. 2017. CCIX . https://www.ccixconsortium.com/
42Elastic Use of Far Memory for In-Memory Database Management Systems DaMoN ’23, June 18–23, 2023, Sea ￿le, WA, USA
[7]Compute Express Link Consortium. 2019. CXL. https://www.computeexpresslink.
org/
[8]Gen-Z Consortium. 2016. Gen-Z . https://genzconsortium.org/
[9]OpenCAPI Consortium. 2014. OpenCAPI . https://opencapi.org/
[10] Franz Faerber, Alfons Kemper, Per-Åke Larson, Justin J. Levandoski, Thomas
Neumann, and Andrew Pavlo. 2017. Main Memory Database Systems. Found.
Trends Databases 8, 1-2 (2017), 1–130. https://doi.org/10.1561/1900000058
[11] Andreas Geyer, Daniel Ritter, Dong Hun Lee, Minseon Ahn, Johannes Pietrzyk,
Alexander Krause, Dirk Habich, and Wolfgang Lehner. 2023. Working with
Disaggregated Systems. What are the Challenges and Opportunities of RDMA
and CXL?. In BTW (LNI, Vol. P-331) . Gesellschaft für Informatik e.V., 751–755.
https://doi.org/10.18420/BTW2023-47
[12] Donghyun Gouk, Sangwon Lee, Miryeong Kwon, and Myoungsoo Jung. 2022.
Direct access, High-Performance memory disaggregation with DirectCXL. In
USENIX ATC . 287–294. https://www.usenix.org/conference/atc22/presentation/
gouk
[13] Intel®. 2021. Intel®VTune ™Pro￿ler. https://www.intel.com/content/www/us/
en/developer/tools/oneapi/vtune-pro ￿ler.html
[14] Robert Lasch, Thomas Legler, Norman May, Bernhard Scheirle, and Kai-Uwe
Sattler. 2022. Cost modelling for optimal data placement in heterogeneous
main memory. Proceedings of the VLDB Endowment 15, 11 (2022), 2867–2880.
https://www.vldb.org/pvldb/vol15/p2867-lasch.pdf
[15] Huaicheng Li, Daniel S Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko
Novakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, et al .2023.
Pond: Cxl-based memory pooling systems for cloud platforms. In ASPLOS . ACM,
574–587. https://doi.org/10.1145/3575693.3578835
[16] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agarwal,
Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit O. Kanaujia,
and Prakash Chauhan. 2022. TPP: Transparent Page Placement for CXL-Enabled
Tiered Memory. CoRR abs/2206.02878 (2022). https://doi.org/10.48550/arXiv.
2206.02878 arXiv:2206.02878
[17] J. McGlone, P. Palazzari, and J. B. Leclere. 2018. Accelerating Key In-memory
Database Functionality with FPGA Technology. In ReConFig . 1–8. https://doi.
org/10.1109/RECONFIG.2018.8641722
[18] Samsung Newsroom. 2022. Samsung Electronics Introduces Industry’s First 512GB
CXL Memory Module . https://news.samsung.com/global/samsung-electronics-
introduces-industrys- ￿rst-512gb-cxl-memory-module
[19] SK Hynix Newsroom. 2022. SK hynix Develops DDR5 DRAM CXLTM Memory
to Expand the CXL Memory Ecosystem . https://news.skhynix.com/sk-hynix-
develops-ddr5-dram-cxltm-memory-to-expand-the-cxl-memory-ecosystem/
[20] S. J. Park, H. Kim, K.-S. Kim, J. So, J. Ahn, W.-J. Lee, D. Kim, Y.-J. Kim, J. Seok, J.-G.
Lee, H.-Y. Ryu, C. Y. Lee, J. Prout, K.-C. Ryoo, S.-J. Han, M.-K. Kook, J. S. Choi,J. Gim, Y. S. Ki, S. Ryu, C. Park, D.-G. Lee, J. Cho, H. Song, and J. Y. Lee. 2022.
Scaling of Memory Performance and Capacity with CXL Memory Expander. In
2022 IEEE HCS . IEEE, 1–27. https://doi.org/10.1109/HCS55958.2022.9895633
[21] Intel®PCM. 2023. Intel®Performance Counter Monitor . https://github.com/intel/
pcm
[22] Hasso Plattner. 2009. A common database approach for OLTP and OLAP using
an in-memory column database. In SIGMOD . ACM, 1–2. https://doi.org/10.1145/
1559845.1559846
[23] Hasso Plattner. 2014. The Impact of Columnar In-memory Databases on Enter-
prise Systems: Implications of Eliminating Transaction-maintained Aggregates.
Proc. VLDB Endow. 7, 13 (Aug. 2014), 1722–1729. https://doi.org/10.14778/2733004.
2733074
[24] Iraklis Psaroudakis, Florian Wolf, Norman May, Thomas Neumann, Alexander
Boehm, Anastasia Ailamaki, and Kai-Uwe Sattler. 2015. Scaling Up Mixed Work-
loads: A Battle of Data Freshness, Flexibility, and Scheduling. Performance Char-
acterization And Benchmarking: Traditional To Big Data 8904 (2015), 16. 97–112.
https://doi.org/10.1007/978-3-319-15350-6_7
[25] Reza Sherkat, Colin Florendo, Mihnea Andrei, Rolando Blanco, Adrian Dragusanu,
Amit Pathak, Pushkar Khadilkar, Neeraj Kulkarni, Christian Lemke, Sebastian
Seifert, Sarika Iyer, Sasikanth Gottapu, Robert Schulze, Chaitanya Gottipati,
Nirvik Basak, Yanhong Wang, Vivek Kandiyanallur, Santosh Pendap, Dheren
Gala, Rajesh Almeida, and Prasanta Ghosh. 2019. Native Store Extension for SAP
HANA. Proc. VLDB Endow. 12, 12 (2019), 2047–2058. https://doi.org/10.14778/
3352063.3352123
[26] Vishal Sikka, Franz Färber, Wolfgang Lehner, Sang Kyun Cha, Thomas Peh,
and Christof Bornhövd. 2012. E ￿cient Transaction Processing in SAP HANA
Database: The End of a Column Store Myth. In SIGMOD . ACM, 731–742. https:
//doi.org/10.1145/2213836.2213946
[27] Joonseop Sim, Soohong Ahn, Taeyoung Ahn, Seungyong Lee, Myunghyun Rhee,
Jooyoung Kim, Kwangsik Shin, Donguk Moon, Euiseok Kim, and Kyoung Park.
2023. Computational CXL-Memory Solution for Accelerating Memory-Intensive
Applications. IEEE Comput. Archit. Lett. 22, 1 (2023), 5–8. https://doi.org/10.1109/
LCA.2022.3226482
[28] Montage Technology. 2022. Montage Technology Delivers the World’s First CXL ™
Memory eXpander Controller . https://www.montage-tech.com/Press_Releases/
20220506
[29] TPC-C. 2023. TPC-C . https://www.tpc.org/tpcc/
[30] TPC-H. 2023. TPC-H . https://www.tpc.org/tpch/
[31] Qirui Yang, Runyu Jin, Bridget Davis, Devasena Inupakutika, and Ming Zhao.
2022. Performance Evaluation on CXL-enabled Hybrid Memory Pool. In NAS.
IEEE, 1–5. https://doi.org/10.1109/NAS55553.2022.9925356
43A Case for CXL-Centric Server Processors
Albert ChoAnish SaxenaMoinuddin Qureshi Alexandros Daglis
Georgia Institute of Technology
Abstract
The memory system is a major performance determinant for
server processors. Ever-growing core counts and datasets de-
mand higher bandwidth and capacity as well as lower latency
from the memory system. To keep up with growing demands,
DDR—the dominant processor interface to memory over the
past two decades—has offered higher bandwidth with every
generation. However, because each parallel DDR interface
requires a large number of on-chip pins, the processor’s mem-
ory bandwidth is ultimately restrained by its pin-count, which
is a scarce resource. With limited bandwidth, multiple memory
requests typically contend for each memory channel, resulting
in signiﬁcant queuing delays that often overshadow DRAM’s
service time and degrade performance.
We present COAXIAL, a server design that overcomes mem-
ory bandwidth limitations by replacing all DDR interfaces to
the processor with the more pin-efﬁcient CXL interface. The
widespread adoption and industrial momentum of CXL makes
such a transition possible, offering 4higher bandwidth per
pin compared to DDR at a modest latency overhead. We
demonstrate that, for a broad range of workloads, CXL’s la-
tency premium is more than offset by its higher bandwidth. As
COAXIALdistributes memory requests across more channels,
it drastically reduces queuing delays and thereby both the
average value and variance of memory access latency. Our
evaluation with a variety of workloads shows that COAXIAL
improves the performance of manycore throughput-oriented
servers by 1:52on average and by up to 3.
1. Introduction
Multicore processor architectures have been delivering per-
formance gains despite the end of Dennard scaling and the
slowdown of Moore’s law in the past two decades. At the same
time, as the data consumed by processors is increasing expo-
nentially, technological breakthroughs have enabled higher-
capacity memory with new media like non-volatile RAM or
via remote memory access over fast networks (e.g., RDMA). A
common technological trade-off with higher-capacity memory
is signiﬁcantly inferior memory access latency and bandwidth
compared to the DDR-based main memory. As a result, servers
continue to predominantly rely on DDR-attached memory for
performance while optionally retaining a slower memory tier
like NVRAM or remote DRAM for capacity expansion.
Equal contribution.The emerging Compute Express Link (CXL) standard
bridges the performance gap between low-bandwidth, high-
capacity memory and DDR-based main memory. By attaching
DRAM modules over the widely deployed high-bandwidth
PCI Express (PCIe) bus, CXL vastly improves memory capac-
ity and bandwidth, while retaining DDR-like characteristics at
a modest access latency overhead. Consequently, there has re-
cently been much interest in architecting CXL-based memory
systems that enable memory pooling and capacity expansion
[3, 16, 25, 33].
CXL owes its high bandwidth to the underlying PCIe-based
serial interface, which currently delivers about 4higher band-
width per processor pin compared to the parallel DDR inter-
face, with technological roadmaps projecting this gap to grow
further. Hence, by repurposing the processor’s DDR-allocated
pins to CXL, it is possible to quadruple the available memory
bandwidth. However, the higher bandwidth comes at the cost
of memory access latency overhead, expected to be as low as
25–30ns [ 9,43], although higher in initial implementations
and systems that multiplex CXL memory devices across mul-
tiple processors. Low access latency is a key requirement
for high-performance memory, which is why CXL’s latency
overhead has biased the research so far to treat the technology
exclusively as a memory expansion technique rather than a
replacement of local DDR-attached memory.
However, we observe that the overall memory access la-
tency in a loaded system is dominated by the queuing delay
at the memory controller, which arbitrates access to the DDR
channel. Modern servers feature between 4 and 12 cores per
memory channel, resulting in contention and signiﬁcant queu-
ing delays even before a request can be launched over the
memory bus. Mitigating these queuing delays by provisioning
more memory channels requires more processor pins and die
area, which are scarce resources. Given rigid pin constraints,
CXL’s bandwidth-per-pin advantage can unlock signiﬁcant
bandwidth and performance gains by rethinking memory sys-
tems to be CXL-centric rather than DDR-centric.
In this paper, we make the key observation that the band-
width boost attainable with CXL drastically reduces memory
access queuing delays, which largely dictate the effective ac-
cess latency of loaded memory systems. In addition to in-
creased average memory access latency, queuing delays also
increase memory access variance, which we show has detri-
mental effects to performance. Driven by this insight, we
argue that a memory system attached to the processor entirely
over CXL is a key enabler for scalable high-performance server
processors that deploy memory-intensive workloads. Our pro-
1arXiv:2305.05033v1  [cs.AR]  8 May 2023posed server design, dubbed COAXIAL, replaces all of the
processor’s direct DDR interfaces with CXL.
By evaluating COAXIALwith a wide range of workloads,
we highlight how CXL-based memory system’s unique char-
acteristics (i.e., increased bandwidth and higher unloaded la-
tency) positively impact performance of processors whose
memory system is typically loaded. Our analysis relies on a
simple but often overlooked fact about memory system be-
havior and its impact on overall performance: that a loaded
memory system’s effective latency is dominated by the im-
pact of queuing effects and therefore signiﬁcantly differs from
the unloaded system’s latency, as we demonstrate in §3.1. A
memory system that offers higher parallelism reduces queuing
effects, which in turn results in lower average latency and
variance, even if its unloaded access latency is higher com-
pared to existing systems. We argue that CXL-based memory
systems offer exactly this design trade-off, which is favorable
for loaded server processors handling memory-intensive ap-
plications, offering strong motivation for a radical change in
memory system design that departs from two decades of DDR
and enables scalable high-performance server architectures.
In summary, we make the following contributions:
•We make the radical proposal of using high-bandwidth CXL
as acomplete replacement of pin-inefﬁcient DDR interfaces
on server processors, showcasing a ground-breaking shift
that disrupts decades-long memory system design practices.
•We show that, despite its higher unloaded memory access la-
tency, COAXIALreduces the effective memory access time
in typical scenarios where the memory system is loaded.
•We demonstrate the promise of COAXIALwith a study of
a wide range of workloads for various CXL bandwidth and
latency design points that are likely in the near future.
•We identify limitations imposed on CXL by the current
PCIe standard, and highlight opportunities a revised stan-
dard could leverage for 20% additional speedup.
Paper outline: §2 motivates the replacement of DDR with
CXL in server processors. §3 highlights the critical impact
of queuing delays on a memory system’s performance and
§4 provides an overview of our proposed COAXIALserver
design, which leverages CXL to mitigate detrimental queuing.
We outline the methodology to evaluate COAXIALagainst a
DDR-based system in §5 and analyze performance results in
§6. We discuss related work in §7 and conclude in §8.
2. Background
In this section, we highlight how DRAM memory bandwidth
is bottlenecked by the processor-attached DDR interface and
processor pin-count. We then discuss how CXL can bridge
this gap by using PCIe as its underlying physical layer.
2.1. Low-latency DDR-based Memory
Servers predominantly access DRAM over the Double Data
Rate (DDR) parallel interface. The interface’s processor pinrequirement is determined by the width of data bus, com-
mand/address bus, and conﬁguration pins. A DDR4 and
DDR5 [ 21] interface is 288 pins wide. While several of
those pins are terminated at the motherboard, most of them
(160+ for an ECC-enabled DDR4 channel [ 20], likely more
for DDR5 [45]) are driven to the processor chip.
The DDR interface’s 64 data bits directly connect to the
processor and are bit-wise synchronous with the memory con-
troller’s clock, enabling a worst-case (unloaded) access latency
of about 50ns. Scaling a DDR-based memory system’s band-
width requires either clocking the channels at a higher rate,
or attaching more channels to the processors. The former
approach results in signal integrity challenges [ 39] and a re-
duction in supported ranks per channel, limiting rank-level
parallelism and memory capacity. Accommodating more chan-
nels requires more on-chip pins, which cost signiﬁcant area
and power, and complicate placement, routing, and packag-
ing [62]. Therefore, the pin-count on processor packages has
only been doubling about every six years [51].
Thus, reducing the number of cores that contend over a
memory channel is difﬁcult without clean-slate technologies,
which we discuss in §7. To this end, the emerging CXL in-
terconnect is bound to bridge this gap by leveraging a widely
deployed high-bandwidth serial interface, as we discuss next.
2.2. The High-bandwidth CXL Memory Interconnect
The Compute Express Link (CXL) is a recent interconnect
standard, designed to present a uniﬁed solution for coherent
accelerators, non-coherent devices, and memory expansion
devices. It represents the industry’s concerted effort for a
standardized interconnect to replace a wide motley collection
of proprietary solutions (e.g., OpenCAPI [ 55], Gen-Z [ 11]).
CXL is rapidly garnering industry adoption and is bound to be-
come a dominant interconnect, as PCIe has been for peripheral
devices over the past twenty years.
CXL brings load-store semantics and coherent memory ac-
cess to high-capacity, high-bandwidth memory for processors
and accelerators alike. It also enables attaching DDR-based
memory (“Type-3” CXL devices) over PCIe to the proces-
sor with strict timing constraints. In this work, we focus on
this capability of CXL. CXL’s underlying PCIe physical layer
affords higher bandwidth per pin at the cost of increased la-
tency. Therefore, most recent works thus far perceive CXL as
a technology enabling an auxiliary slower memory tier directly
attached to the processor. In contrast, we argue that despite its
associated latency overhead, CXL can play a central role in
future memory systems design, replacing , rather than simply
augmenting, DDR-based memory in server processors.
2.3. Scaling the Memory Bandwidth Wall with CXL
CXL’s high bandwidth owes to its underlying PCIe physical
layer. PCIe [ 47] is a high-speed serial interface featuring
multiple independent lanes capable of bi-directional commu-
nication using just 4 pins per lane: two for transmitting data,
2PCIe1.0PCIe2.0PCIe3.0PCIe4.0PCIe5.0PCIe6.0PCIe7.0DDR4-2133DDR4-3200DDR5-4800DDR5-8000DDR6-9600DDR6-160001248163264
2000200520102015202020252030Relative bandwidth per pinYearFigure 1: Bandwidth per processor pin for DDR and CXL (PCIe)
interface, norm. to PCIe-1.0. Note that y-axis is in log scale.
and two for receiving data. Data is sent over each lane as a
serial bit stream at very high bit rates in an encoded format.
Fig. 1 illustrates the bandwidth per pin for PCIe and DDR.
The normalized bandwidth per pin is derived by dividing each
interface’s peak interface bandwidth on JEDEC’s and PCI-
SIG’s roadmap, respectively, by the processor pins required:
160 for DDR and 4 per lane for PCIe.
The 4bandwidth gap is where we are today (PCIe5.0
vs. DDR5-4800). The comparison is conservative, given that
PCIe’s stated bandwidth is per direction , while DDR5-4800
requires about 160 processor pins for a theoretical 38.4GB/s
peak of combined read and write bandwidth. With a third of
the pins, 12 PCIe5.0 lanes (over which CXL operates) offer
48GB/s per direction—i.e., a theoretical peak of 48GB/s for
reads and48GB/s for writes. Furthermore, Fig. 1’s roadmaps
suggest that the bandwidth gap will grow to 8 by 2025.
2.4. CXL Latency Concerns
CXL’s increased bandwidth comes at the cost of increased
latency compared to DDR. There is a widespread assumption
that this latency cost is signiﬁcantly higher than DRAM ac-
cess latency itself. For instance, recent work on CXL-pooled
memory reinforces that expectation, by reporting a latency
overhead of 70ns [ 25]. The expectation of such high added
latency has reasonably led memory system researchers and
designers to predominantly focus on CXL as a technology for
enabling a secondary tier of slower memory that augments
conventional DDR-attached memory. However, such a high
latency overhead does not represent the minimum attainable
latency of the simplest CXL-attached memory and is largely
an artifact of more complex functionality, such as multiplexing
multiple memory devices, enforcing coherence between the
host and memory device, etc.
In this work, we argue that CXL is a perfect candidate to
completely replace the DDR-attached memory for server pro-
cessors that handle memory-intensive workloads. The CXL
3.0 standard sets an 80ns pin-to-pin load latency target for a
CXL-attached memory device [ 9, Table 13-2], which in turn
implies that the interface-added latency over DRAM access in
upcoming CXL memory devices should be about 30ns. Early
implementations of the CXL 2.0 standard demonstrated a 25ns
latency overhead per direction [ 42], and in 2021 PLDA an-
nounced a commercially available CXL 2.0 controller thatonly adds 12ns per direction [ 43]. Such low latency over-
heads are attainable with the simplest CXL type-3 devices that
are not multiplexed across multiple hosts and do not need to
initiate any coherence transactions. Our key insight is that a
memory access latency penalty in the order of 30ns often pales
in comparison to queuing delays at the memory controller that
are common in server systems, and such queuing delays can
be curtailed by CXL’s considerable bandwidth boost.
3. Pitfalls of Unloaded and Average Latency
It is evident from current technological trends that systems
with CXL-attached memory can enjoy signiﬁcantly higher
bandwidth availability compared to conventional systems
with DDR-attached memory. A key concern hindering broad
adoption—and particularly our proposed replacement of DDR
interfaces on-chip with CXL—is CXL’s increased memory
access latency. However, in any system with a loaded memory
subsystem, queuing effects play a signiﬁcant role in determin-
ing effective memory access latency. On a loaded system,
queuing (i) dominates the effective memory access latency,
and (ii) introduces variance in accessing memory, degrading
performance. We next demonstrate the impact of both effects.
3.1. Queuing Dictates Effective Memory Access Latency
Fig. 2a shows a DDR5-4800 channel’s memory access latency
as its load increases. We model the memory using DRAM-
Sim [ 46] and control the load with random memory accesses
of conﬁgurable arrival rate. The resulting load-latency curve
is shaped by queuing effects at the memory controller.
When the system is unloaded, a hypothetical CXL interface
adding 30ns to each memory access would correspond to
a seemingly prohibitive 75% latency overhead compared to
the approximated unloaded latency of 40ns. However, as
the memory load increases, latency rises exponentially, with
average latency increasing by 3and4at 50% and 60%
load, respectively. p90 tail latency grows even faster, rising by
4:7and7:1at the same load points. In a loaded system,
trading off additional interface latency for considerably higher
bandwidth availability can yield signiﬁcant net latency gain.
To illustrate, consider a baseline DDR-based system operat-
ing at 60% of memory bandwidth utilization, corresponding
to 160ns average and 285ns p90 memory access latency. A
CXL-based alternative offering a 4memory bandwidth boost
would shrink the system’s bandwidth utilization to 15%, cor-
responding to 50% lower average and 68% lower p90 memory
access latency compared to baseline, despite the CXL inter-
face’s 30ns latency premium.
Fig. 2a shows that a system with bandwidth utilization as
low as 20% experiences queuing effects, that are initially re-
ﬂected on tail latency; beyond 40% utilization, queuing effects
also noticeably affect average latency. Utilization beyond such
level is common, as we show with our simulation of a 12-
core processor with 1 DDR5 memory channels over a range
of server and desktop applications (methodological details in
30 10 20 30 40 50 60
Memory Bandwidth Utilization as 
% of Theoretical Peak0100200300400500Memory Access Latency (ns) Average Latency
90% Latency(a) Average and p90 memory access latency in a DDR5-
4800 channel (38.4GB/s) at varying bandwidth utiliza-
tion points. p90 grows faster than the average latency.
0100200300400Average Memory
Access Latency (ns)484429
Queuing Delay
Access Service Time
ST_copyST_scaleST_addST_triad  
Comp-scCompPR-DBCPR
RadiiBFSCCBellmFBFS
BFS-BVTriangleMIS  
fluida
facesimraytracesclustercanneal  
lbm
bwavescactuBfotonikcam4wrfmcfromspop2
omnetppxalancgcc  
masstreekmeans0255075Memory Bandwidth
Utilization(b) Memory latency breakdown (DRAM access time and queuing delay) and memory bandwidth
utilization for a range of workloads. Higher utilization increases queuing delay.
Figure 2: Queuing drastically affects memory access time on a loaded system.
§5). Fig. 2b shows that with all processor cores under use, the
vast majority of workloads exceed 30% memory bandwidth
utilization, and most exceed 50% utilization (except several
workloads from SPEC and PARSEC benchmarks).
Fig. 2b also breaks down the average memory access time
seen from the LLC miss register into DRAM service time and
queuing delay at the memory controller. We observe a trend in
high bandwidth consumption leading to long queuing delays,
although queuing delay doesn’t present itself as a direct func-
tion of bandwidth utilization. Queuing delay is also affected
by application characteristics such as read/write pattern and
spatial and temporal distribution of accesses. For example, in
an access pattern where processor makes majority of memory
access requests in a short amount of time, followed by a period
of low memory activity, the system would temporarily be in
a high bandwidth utilization state when memory requests are
made, experiencing contention and high queuing delay, even
though the average bandwidth consumption would not be as
high. Even in such cases, provisioning more bandwidth would
lead to better performance, as it would mitigate contention
from the temporary bursts. In Fig. 2b’s workloads, queuing de-
lay constitutes 72% of the memory access latency on average,
and up to 91% in the case of lbm.
3.2. Memory Latency Variance Impacts Performance
In addition to their effect on average memory access latency,
spurious queuing effects at the memory controller introduce
higher memory access latency ﬂuctuations (i.e., variance).
Such variance is closely related to the queueing delay stem-
ming from high utilization, as discussed in §3.1. To demon-
strate the impact of memory access latency variance on per-
formance, we conduct a controlled experiment where the av-
erage memory access latency is kept constant, but the latency
ﬂuctuation around the average grows. The baseline is a toy
memory system with a 150ns ﬁxed access latency and we
evaluate three additional memory systems where memory ac-
bwaves ST_triad masstreeBFSraytracegm0.00.51.0Performance norm.
to Fixed Latency(100ns,350ns)
(75ns,450ns)
(50ns,550ns)Figure 3: Performance of workloads for synthetic memory ac-
cess latency following three (X, Y) bimodal distributions with
4:1 X:Y ratio, all with 150ns average latency, normalized to a
memory system with ﬁxed 150ns latency. “gm" refers to geo-
metric mean. Higher latency variance degrades performance.
cess latency follows a bimodal distribution with 80%/20%
probability of being lower/higher than the average. We keep
average latency constant in all cases ( 80%low_lat+20%
high_lat=150ns) and we evaluate (low_lat ;high_lat)for
(100ns;350ns);(75ns;450ns);(50ns;550ns), resulting in dis-
tributions with increasing standard deviations (stdev) of 100ns,
150ns, and 200ns. Variance is the square of stdev and denotes
how spread out the latency is from the average.
Fig. 3 shows the relative performance of these memory
systems for ﬁve workloads of decreasing memory bandwidth
intensity. As variance increases, the average performance rel-
ative to the ﬁxed-latency baseline noticeably drops to 86%,
78%, and 71%. This experiment highlights that solely rely-
ing on typical average metrics like Average Memory Access
Time (AMAT) to assess a memory system’s performance is an
incomplete method of evaluating a memory system’s perfor-
mance. In addition to average values, the variance of memory
access latency is a major performance determinant and there-
fore an important quality criterion for a memory system.
4Table 1: Area of processor
components at TSMC 7nm
(rel. to 1MB of L3 cache).
L3 cache (1MB) 1
Zen 3 Core6.5(incl. 512 KB L2)
x8 PCIe (PHY + ctrl) 5.9
DDR channel (PHY + ctrl) 10.8Table 2: DDR-based versus alternative C OAXIAL server conﬁgurations.
Server designCore LLC Memory Relative RelativeCommentcount per core interfaces mem. BW area
DDR-based
1442 MB12 DDR 1 1 baseline
COAXIAL-5 60 x8 CXL 5 1.17 iso-pin
COAXIAL-2 24 x8 CXL 2
1.01 iso-areaiso-LLC
COAXIAL-4
1 MB48 x8 CXL 4 balanced
COAXIAL-asym48 x8 CXL-asym asym. R/Wmax BW(see §4.3)
Summary: The signiﬁcant memory bandwidth boost at-
tainable with CXL-attached memory more than compen-
sates for the interface’s higher latency in loaded systems,
where queuing dictates the memory system’s effective
access latency. By decreasing queuing, a CXL-based
memory system reduces average memory access time and
variance, both of which improve performance.
4. The C OAXIAL Server Architecture
We leverage CXL’s per-pin bandwidth advantage to replace all
of the DDR interfaces with PCIe-based CXL interfaces in our
proposed COAXIALserver. Fig. 4b depicts our architecture
where each on-chip DDR5 channel is replaced by several
CXL channels, providing 2–4 higher aggregate memory
bandwidth to the processor. Fig. 4a shows the baseline DDR-
based server design for comparison. Each CXL channel is
attached to a “Type-3“ CXL device, which features a memory
controller that manages a regular DDR5 channel that connects
to DRAM. The processor implements the CXL.mem protocol
of the CXL standard, which orchestrates data consistency and
memory semantics management. The implementation of the
caches and cores remains unchanged, as the memory controller
still supplies 64B cache lines.
4.1. Processor Pin Considerations
A DDR5-4800 channel features a peak uni-directional band-
width of 38.4GB/s and requires more than 160 processor pins
to account for data and ECC bits, command/address bus, data
strobes, clock, feature modes, etc., as described in §2.1. A
full 16-lane PCIe connection delivers 64GB/s of bi-directional
bandwidth. Moreover, PCIe is modular, and higher-bandwidth
channels can be constructed by grouping independent lanes
together. Each lane requires just four processor pins: two each
for transmitting and receiving data.
The PCIe standard currently only allows groupings of 1,
2, 4, 8, 12, 16, or 32 lanes. To match DDR5’s bandwidth
of 38.4GB/s, we opt for an x8 conﬁguration, which requires
32 pins for a peak bandwidth of 32GB/s, 5fewer than the
160 pins required for the DDR5 channel. As PCIe can sus-
tain 32GB/s bandwidth in each direction, the peak aggregate
bandwidth of 8 lanes is 64GB/s, much higher than DDR5’s
38.4GB/s. Considering a typical 2:1 Read:Write ratio, only
25.6GB/s of a DDR5 channel’s bandwidth would be used in
cores, caches, NoC, etc.DDR
DDRDDRDDR
......(a) Baseline DDR-based server.
.........type-3CXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLcores, caches, NoC, etc.
type-3
(b)COAXIALreplaces each DDR channel with several CXL channels. Each
CXL channel connects to a type-3 device with one DDR memory channel.
Figure 4: Overview of the baseline and C OAXIAL systems.
the DRAM-to-CPU direction, and about 13GB/s in the oppo-
site direction. Furthermore, peak sustainable bandwidth for
DDR controllers typically achieve around 70% to 90% of the
theoretical peak. Thus, even after factoring in PCIe and CXL’s
header overheads which reduce the practically attainable band-
width [ 48] to 26GB/s in the DRAM-to-CPU direction and
13GB/s in the other direction, the x8 conﬁguration supports a
full DDR5 channel without becoming a choke point.
4.2. Silicon Area Considerations
When it comes to processor pin requirements, COAXIALal-
lows replacement of each DDR channel (i.e., PHY and mem-
ory controller) with ﬁve x8 PCIe PHY and controllers, for
a5memory bandwidth boost. However, the relative pin
requirements of DDR and PCIe are not directly reﬂected in
their relative on-chip silicon area requirements. Lacking pub-
licly available information, we derive the relative size of DDR
and PCIe PHYs and controllers from AMD Rome and Intel
Golden Cove die shots [29, 53].
Table 1 shows the relative silicon die area different key
components of the processor account for. Assuming linear
scaling of PCIe area with the number of lanes, as appears to be
the case from the die shots, an x8 PCIe controller accounts for
54% of a DDR controller’s area. Hence, replacing each DDR
controller with four x8 PCIe controllers requires 2.19 more
5silicon area than what is allocated to DDR. However, DDR
controllers account for a small fraction of the total CPU die.
Leveraging Table 1’s information, we now consider a num-
ber of alternative COAXIALserver designs, shown in Table 2.
We focus on high-core-count servers optimized for throughput,
such as the upcoming AMD EPYC Bergamo (128 cores) [ 37],
and Intel Granite Rapids (128 cores) and Sierra Forest (144
cores) [ 38]. All of them feature 12 DDR5 channels, resulting
in a core-to-memory-controller (core:MC) ratio of 10.7:1 to
12:1. A common design choice to accommodate such high
core counts is a reduced LLC capacity; e.g., moving from the
96-core Genoa [ 52] to the 128-core Bergamo, AMD halves the
LLC per core to 2MB. We thus consider a 144-core baseline
server processor with 12 DDR5 channels and 2MB of LLC
per core (Table 2, ﬁrst row).
With pin count as its only limitation, COAXIAL-5re-
places each DDR channel with 5 x8 CXL interfaces, for a
5bandwidth increase. Unfortunately, that results in a 17%
increase in die area to accommodate all the PCIe PHYs and
controllers. Hence, we also consider two iso-area alterna-
tives. COAXIAL-2leverages CXL to double memory band-
width without any microarchitectural changes. COAXIAL-4
quadruples the available memory bandwidth compared to the
baseline CPU by halving the LLC from 288MB to 144MB.
4.3. C OAXIAL Asymmetric Interface Optimization
A key difference between CXL and DDR is that the former
provisions dedicated pins and wires for each data movement
direction (RX and TX). The PCIe standard deﬁnes a one-to-
one match of TX and RX pins: e.g., an x8 PCIe conﬁguration
implies 8 TX and 8 RX lanes. We observe that while uniform
bandwidth provisioning in each direction is reasonable for a
peripheral device like a NIC, it is not the case for memory traf-
ﬁc. Because (i) most workloads read more data than they write
and (ii) every cache block that is written must typically be read
ﬁrst, R:W ratios are usually in the 3:1 to 2:1 range rather than
1:1. Thus, in the current 1:1 design, read bandwidth becomes
the bottleneck and write bandwidth is underutilized. Given
this observation and that serial interfaces do not fundamentally
require 1:1 RX:TX bandwidth provisioning [ 59], we consider
aCOAXIALdesign with asymmetric RX/TX lane provision-
ing to better match memory trafﬁc characteristics. While the
PCIe standard currently disallows doing so, we investigate
the potential performance beneﬁts of revisiting that statutory
restriction. We call such a channel CXL-asym .
We consider a system leveraging such CXL-asym channels
to compose an additional COAXIAL-asym conﬁguration. An
x8 CXL channel consists of 32 pins, 16 each way. Without
the current 1:1 PCIe restriction, CXL-asym repurposes the
same pin count to use 20 RX pins and 12 TX pins, resulting in
40GB/s RX and 24GB/s TX of raw bandwidth. Accounting
for PCIe and CXL’s header overheads, the realized bandwidth
is approximately 32GB/s for reads (compared to 26GB/s in
x8 CXL channel) and 10GB/s for writes [ 48]. To utilize theTable 3: System parameters used for simulation on
ChampSim.
DDR baseline CoaXiaL-*
CPU 12 OoO cores, 2GHz, 4-wide, 256-entry ROB
L1 32KB L1-I & L1-D, 8-way, 64B blocks, 4-cycle access
L2 512 KB, 8-way, 12-cycle access
LLCshared & non-inclusive, 16-way, 46-cycle access
2 MB/core 1–2 MB/core (see Table 2)
MemoryDDR5-4800 [36], 128 GB per channel, 2 sub-channels
per channel, 1 rank per sub-channel, 32 banks per rank
1 channel2–4 CXL-attached channels (see Table 2)
8 channels for C OAXIAL-asym (see §4.3)
additional read bandwidth, we provision two DDR controllers
per CXL-asym channel on the type-3 device. Therefore, the
number of CXL channels on the processor (as well as their
area overhead) remains unchanged. While the 32GB/s read
bandwidth of CXL-asym is insufﬁcient to support two DDR
channels at their combined read bandwidth of about 52GB/s
(assuming a 2:1 R:W ratio), queuing delays at the DDR con-
troller typically become signiﬁcant at a much lower utiliza-
tion point, as shown in Fig. 2a. Therefore, COAXIAL-asym
still provides sufﬁcient bandwidth to eliminate contention at
queues by lowering the overall bandwidth utilization, while
providing higher aggregate bandwidth.
4.4. Additional Beneﬁts of C OAXIAL
Our analysis focuses on the performance impact of a CXL-
based memory system. While a memory capacity and cost
analysis is beyond the scope of this paper, COAXIALcan have
additional positive effects on those fronts that are noteworthy.
Servers provisioned for maximum memory capacity deploy
two high-density DIMMs per DDR channel. The implica-
tions are two-fold. First, two-DIMMs-per-channel (2DPC)
conﬁgurations increase capacity over 1DPC at the cost of
15% memory bandwidth. Second, DIMM cost grows super-
linearly with density; for example, 128GB/256GB DIMMs
cost 5/20more than 64GB DIMMs. By enabling more
DDR channels, COAXIALallows the same or higher DRAM
capacity with 1DPC and lower-density DIMMs.
5. Evaluation Methodology
System conﬁgurations. We compare our COAXIALserver
design, which replaces the processor’s DDR channels with
CXL channels, to a typical DDR-based server processor.
•DDR-based baseline . We simulate 12 cores and one DDR5-
4800 memory channel as a scaled-down version of Table 2’s
144-core CPU.
• C OAXIALservers . We evaluate several servers that re-
place the on-chip DDR interfaces with CXL: COAXIAL-2,
COAXIAL-4, and C OAXIAL-asym (Table 2).
We simulate the above system conﬁgurations using
6ChampSim [1] coupled with DRAMsim3 [ 26]. Table 3 sum-
marizes the conﬁguration parameters used.
CXL performance modeling. ForCOAXIAL, we model
CXL controllers and PCIe bus on both the processor and the
type-3 device. Each CXL controller comprises a CXL port
that incurs a ﬁxed delay of 12ns accounting for ﬂit-packing,
encoding-decoding, packet processing, etc. [ 43]. The PCIe
bus incurs traversal latency due to the limited channel band-
width and bus width. For an x8 channel, the peak 32GB/s
bandwidth results in 26/13 GB/s RX/TX goodput when header
overheads are factored in, and 32/10 GB/s RX/TX in the case
of CXL-asym channels. The corresponding link traversal
latency is 2.5/ 5.5 ns RX/TX for an x8 channel and 2/ 9 ns
RX/TX for CXL-asym. Additionally, the CXL controller main-
tains message queues to buffer requests. Therefore, in addition
to minimum latency overhead of about 30ns (or more, in our
sensitivity analysis), queuing effects at the CXL controller are
also captured and reﬂected in the performance.
Workloads. We evaluate 35 workloads from various bench-
mark suites. We deploy the same workload instance on all
cores and simulate 200 million instructions per core after fast-
forwarding each application to a region of interest.
•Graph analytics: We use 12 workloads from the LIGRA
benchmark suite [49].
•STREAM: We run the four kernels ( copy, scale, add, triad )
from the STREAM benchmark [ 34] to represent bandwidth-
intensive matrix operations in which ML workloads spend a
signiﬁcant portion of their execution time.
•SPEC & PARSEC: We evaluate 13 workloads from the
SPEC-speed 2017 [ 50] benchmark suite in refmode, as
well as ﬁve PARSEC workloads [5].
•We evaluate masstree [32] and kmeans [28] to represent key
value store and data analytics workloads, respectively.
Table 4 summarizes all our evaluated workloads, along with
their IPC and MPKI as measured on the DDR-based baseline.
6. Evaluation Results
We ﬁrst compare our main COAXIALdesign, COAXIAL-4,
with the DDR-based baseline by analyzing the impact of re-
duced bandwidth utilization and queuing delays on perfor-
mance in §6.1. §6.2 highlights the effect of memory access
pattern and distribution on performance. §6.3 presents the per-
formance of alternative COAXIALdesigns, COAXIAL-2
andCOAXIAL-asym, and §6.4 demonstrates the impact of
a more conservative 50ns CXL latency penalty. §6.5 evalu-
ates C OAXIAL at different server utilization points, and §6.6
analyzes C OAXIAL’s power implications.
6.1. From Queuing Reduction to Performance Gains
Fig. 5 (top) shows the performance of COAXIAL-4relative
to the baseline DDR-based system. Most workloads exhibit
Note that, throughout §6’s evaluation, reference to “ COAXIAL” without
a following qualiﬁer implies the C OAXIAL-4conﬁguration.Table 4: Workload Summary.
Application IPCLLC
MPKIApplication IPCLLC
MPKI
Ligra SPEC
PageRank 0.36 40 lbm 0.14 64
PageRank
Delta0.31 27 bwaves 0.33 14
Components
-shortcut0.34 48 cactusBSSN 0.68 8
Components 0.36 48 fotonik3d 0.33 22
BC 0.33 34 cam4 0.87 6
Radii 0.41 33 wrf 0.61 11
BFSCC 0.68 17 mcf 0.793 13
BFS 0.69 15 roms 0.783 6
BFS-Bitvector 0.84 15 pop2 1.55 3
BellmanFord 0.86 9 omnetpp 0.51 10
Triangle 0.65 21 xalancbmk 0.55 12
MIS 1.37 8 gcc 0.31 19
STREAM PARSEC
Stream-copy 0.17 58 ﬂuidanimate 0.78 7
Stream-scale 0.21 48 facesim 0.74 6
Stream-add 0.16 69 raytrace 1.17 5
Stream-triad 0.18 59 streamcluster 0.99 14
KVS & Data analytics canneal 0.66 7
Masstree 0.37 21
Kmeans 0.50 36
signiﬁcant speedup, up to 3forlbmand1:52on average.
10 of the 35 workloads experience more than 2speedup.
Four workloads lose performance, with gcc most signiﬁcantly
impacted at 26% IPC loss. Workloads most likely to suffer
a performance loss are those with low to moderate memory
trafﬁc and heavy dependencies among memory accesses.
Fig. 5 (bottom) shows memory bandwidth utilization for
the DDR-based baseline and COAXIAL-4, which provides
4higher bandwidth than the baseline. COAXIALdistributes
memory requests over more channels which reduces the band-
width utilization of the system, in turn reducing contention
for the memory bus. The lower bandwidth utilization and con-
tention drastically reduces the queuing delay in COAXIALfor
memory-intensive workloads. Fig. 5 (middle) demonstrates
this reduction with a breakdown of the average memory ac-
cess latency (as measured from the LLC miss register) into the
DRAM service time, queuing delay, and CXL interface delay
(only applicable to C OAXIAL).
In many cases, COAXIALenables the workload to drive
signiﬁcantly more aggregate bandwidth from the system. For
instance, stream-copy is bottlenecked by the baseline system’s
constrained bandwidth, resulting in average queuing delay
exceeding 300ns that largely dictates the overall access la-
tency (the total height of the stacked bars). COAXIALreduces
queuing delay to just 55ns for this workload, more than com-
pensating for the 30ns CXL interface latency overhead. The
overall average access latency for stream-copy reduces from
348ns in baseline to just 120ns, enabling COAXIALto drive
memory requests at a 2:9higher rate versus the baseline,
70.00.51.01.52.0Normalized
PerformanceGM_STRMGM_LIGRAGM_PARSGM_SPECGM_ALL2.45
1.45
1.121.551.522.72.6 3.02.6
050100150200250Average Memory
Access Latency (ns)Queuing Delay Access Service Time CXL Interface Delay
ST_copyST_scaleST_addST_triad  
Comp-scCompPR-DBCPR
RadiiBFSCCBellmFBFS
BFS-BVTriangleMIS  
fluidafacesimraytracesclustercanneal  
lbm
bwavescactuBfotonikcam4wrfmcfromspop2
omnetppxalancgcc  
masstreekmeans020406080Memory Bandwidth
Utilization Baseline
CoaXiaLFigure 5: Normalized performance of C OAXIAL over DDR-based baseline (top), memory access latency breakdown (middle), and
memory bandwidth utilization (bottom). Workloads are grouped into their benchmark suite. “gm” refers to geometric mean.
COAXIAL offers 1:52average speedup due to 4higher bandwidth, lowering utilization and mitigating queuing effects.
thus achieving commensurate speedup.
Despite provisioning 4more bandwidth, COAXIALre-
duces average bandwidth utilization from 54% to 34% for
workloads that have more than 2performance improvement,
highlighting the extra bandwidth is indeed utilized by these
workloads. For most of the other workloads, COAXIAL’s aver-
age memory access latency is much lower than the baseline’s,
despite the CXL interface’s latency overhead.
On average, workloads experience 144ns in queuing delay
on top of 40ns DRAM service time. By slashing queuing
delay to just 31ns on average, COAXIALreduces average
memory access latency, thereby boosting performance. Over-
all, Fig. 5’s results conﬁrm our key insight (see §3.1): queuing
delays largely dictate the average memory access latency.
Takeaway #1: COAXIALdrastically reduces queuing
delays, resulting in lower effective memory access latency
for bandwidth-hungry workloads.
6.2. Beyond Average Bandwidth Utilization and Access
Latency
While most of COAXIAL’s performance gains can be justi-
ﬁed by the achieved reduction in average memory latency, a
compounding positive effect is reduction in latency variance
as evidenced in §3.2. For each of the four evaluated workload
groups, Fig. 6a shows the mean average latency and standard
deviation (stdev) for COAXIALand the DDR-based baseline.
As already seen in §6.1, COAXIALdelivers a 45–60% reduc-tion to average memory access latency. Fig. 6a shows that
COAXIALalso achieves a similar reduction in stdev, indicat-
ing lower dispersion and fewer extreme high-latency values.
To further demonstrate the impact of access latency distribu-
tion and temporal effects, we study a few workloads in more
depth. Streamcluster presents an interesting case because
its performance improves despite a slightly higher average
memory access latency of 76ns compared to the baseline’s
69ns (see Fig. 5). Fig. 6b shows the Cumulative Distribution
Function (CDF) of Streamcluster’s memory access latencies,
illustrating that the baseline results in a higher variance than
COAXIAL(stdev of 88 versus 76), due to imbalanced queu-
ing across DRAM banks. The tighter distribution of memory
access latency allows COAXIALto outperform the baseline
despite a 10% higher average memory access latency.
Some workloads beneﬁt from COAXIALmore than other
workloads with similar or higher memory bandwidth utiliza-
tion (Fig. 5 (bottom)). For example, bwaves uses a mere
32% of the baseline’s available bandwidth but suffers an over-
whelming 390ns queuing delay. Even though bwaves uses less
bandwidth on average compared to workloads (e.g., radii with
65% bandwidth utilization), it exhibits bursty behavior that
incurs queuing spikes which can be more effectively absorbed
byCOAXIAL.Kmeans exhibits the opposite case. Despite
having the highest bandwidth utilization in the baseline sys-
tem, it experiences a relatively low average queuing delay of
50ns and exhibits one of the lowest latency variance values
across workloads, indicating an even distribution of accesses
8STREAMLIGRAPARSECSPECALL0100200300400500Average Access Latency 
and Standard Deviation (ns)Baseline
CoaXiaL(a) Average memory access la-
tency per workload group, and stdev
shown as error bars.
mean stdev
Baseline 69 88
CoaXiaL 76 76
0 100 200 300
Memory Access Time (ns)0.20.40.60.81.0CDF
Baseline
CoaXiaL(b) Cumulative Distribution Func-
tion (CDF) of memory access
time for Streamcluster.
Figure 6: Memory access latency distribution.
over time and across DRAM banks. Kmeans is also an outlier
with near-zero write trafﬁc, thus avoiding the turnaround over-
head from the memory controller switching between read and
write mode that results in bandwidth underutilization.
6.3. Alternative C OAXIAL designs
Fig. 7 evaluates the two alternative COAXIALdesigns intro-
duced in §4— COAXIAL-2andCOAXIAL-asym—in addi-
tion to our default COAXIAL-4.COAXIAL-2achieves
a1:26average speedup over the baseline, down from
COAXIAL-4’s1:52gain. This conﬁrms our intuition that
doubling memory bandwidth availability at the cost of halving
the LLC is beneﬁcial for virtually all workloads. COAXIAL-
asym improves performance by 1:67on average—a consid-
erable 15% gain on top of COAXIAL-4—and no workload is
negatively affected by COAXIAL-asym’s reduced write band-
width. This result implies an exciting opportunity to improve
bandwidth efﬁciency in memory devices attached via serial
interconnects by provisioning the interfaces in a manner that
is aware of the workloads’ read versus write demands.
Takeaway #2: Provisioning the lanes in read/write de-
mand aware manner considerably improves performance
compared to the default 1:1 read:write provisioning ratio.
6.4. Sensitivity to CXL’s Latency Overhead
While we base our main evaluation on a 30ns roundtrip CXL
interface latency off the CXL 3.0 speciﬁcation and current
industry expectations (see §2.4), we also evaluate a more pes-
simistic latency overhead of 50ns, in case early products do not
meet the 30ns target. Such latency may also better represent
CXL-attached memory devices located at a longer physical
distance from the CPU, or devices with an additional multi-
plexing overhead (e.g., memory devices shared by multiple
servers—a scenario CXL intends to enable [16, 25]).
Fig. 8 shows COAXIAL’s performance at 30ns (our default)
and 50ns CXL interface latency overhead, normalized to the
DDR-based baseline. Although increasing latency overhead to
50ns reduces COAXIAL’s average speedup, it remains signiﬁ-
cant at 1:33. Memory-intensive workloads continue to enjoy
drastic speedups of over 50%, but more workloads (nine, upfrom four with 30ns latency penalty) take a performance hit.
These results imply that while a COAXIALwith a higher CXL
latency is still worth pursuing, it should be used selectively
for memory-intensive workloads. Deploying different classes
of servers for different optimization goals is common practice
not only in public clouds [ 15] but also in private clouds (e.g.,
different web and backend server conﬁgurations) [12, 18].
Takeaway #3: Even with a 50ns CXL latency overhead,
COAXIALachieves a considerable 1:3average speedup
across all workloads.
6.5. Sensitivity to Core Utilization
Fig. 9 evaluates COAXIAL’s performance under varying lev-
els of system utilization by provisioning proportionately less
work on a fraction of cores of the system. We ﬁrst study the
extreme case of using a single core on our 12-core simulated
system ( 8%utilization). In this scenario, virtually all work-
loads suffer performance degradation with COAXIAL, for a
17% average slowdown. Xalancbmk exhibits a corner case
where the working set ﬁts in the LLC when only one instance
is running, removing most memory accesses. The extreme
single-core experiment showcases COAXIAL’s worst-case be-
havior, where the memory system is the least utilized.
We then increase the system utilization to 33% and 66%,
by deploying workload instances on 4 and 8 cores of the
12-core CPU, respectively. We also show results for 100%
utilization (all cores used) again as a point of comparison.
COAXIAL’s bandwidth abundance gradually starts paying
off, by eliminating the slowdown at 33% utilization for most
workloads, and then delivering signiﬁcant gains—1.27 on
average and up to 2.62 —even at 66% utilization. The 66%
utilization point can also be considered as a good proxy for
a fully loaded system where cores and DDR controllers are
provisioned at an 8:1 ratio. An 8:1 core:MC ratio is the design
point of many server processors with fewer than 100 cores
today, such as AMD EPYC Milan and Genoa [ 8,52]. Thus,
the66% utilization results imply that COAXIAL’s approach
is applicable beyond high-end throughput-oriented processors
that already exhibit 12:1 core:MC oversubscription.
Takeaway #4: Even at 66% server utilization—or 8:1
core:MC ratio—C OAXIAL delivers a 1 :27speedup.
6.6. Power Requirements and Energy Efﬁciency
Although COAXIAL’s added serial links and 4more
DIMMs increase the server’s power consumption, our sys-
tem also affords much higher throughput. To take this power
increase into account, we compute the Energy Delay Product
(EDP = system power CPI2)of the baseline and COAXIAL-
4. A lower EDP value indicates a more efﬁcient system that
9ST_copyST_scaleST_addST_triad  
Comp-scCompPR-DBCPR
RadiiBFSCCBellmFBFS
BFS-BVTriangleMIS  
fluidafacesimraytracesclustercanneal  
lbm
bwavescactuBfotonikcam4wrfmcfromspop2
omnetppxalancgcc  
masstreekmeansGM_STRMGM_LIGRAGM_PARSGM_SPECGM_ALL0.00.51.01.52.02.53.0Norm. PerformanceCoaXiaL-2X CoaXiaL-4X CoaXiaL-asymFigure 7: C OAXIAL’s performance at different design points, norm. to the DDR-based server baseline. C OAXIAL-4outperforms
COAXIAL-2, despite its halved LLC size. C OAXIAL-asym considerably outperforms our default C OAXIAL-4design.
ST_copyST_scaleST_addST_triad  
Comp-scCompPR-DBCPR
RadiiBFSCCBellmFBFS
BFS-BVTriangleMIS  
fluidafacesimraytracesclustercanneal  
lbm
bwavescactuBfotonikcam4wrfmcfromspop2
omnetppxalancgcc  
masstreekmeansGM_STRMGM_LIGRAGM_PARSGM_SPECGM_ALL0.00.51.01.52.02.53.0Norm. Performance30ns CXL Buffer Delay
50ns CXL Buffer Delay
Figure 8: C OAXIAL’s performance for different CXL latency premium, norm. to the DDR-based server. Even with a 50ns interface
latency penalty, C OAXIAL yields a 1:33average speedup.
consumes less energy to complete the same work, even if it
operates at a higher power.
We model power for a manycore processor similar to
AMD EPYC Bergamo (128 cores) [ 37] or Sierra Forest (144
cores) [ 38]. The latter is expected to have a 500W TDP, which
is in line with current processors (e.g., 96-core AMD EPYC
Genoa [ 52] has a TDP of 360W). While the memory controller
and interface require negligible power compared to the proces-
sor, we include them for completeness. We estimate controller
and interface power per DDR5 channel to be 0.5W and 0.6W,
respectively [ 57], or 13W in total for a baseline processor with
12 channels. Similarly, PCIe 5.0’s interface power is 0.2W
per lane [ 4], or 77W for the 384 lanes required to support
COAXIAL’s 48 DDR5 channels.
A signiﬁcant fraction of a large-scale server’s power is
attributed to memory. We use Micron’s power calculator
tool [ 35] to compute our baseline’s and CXL system’s DRAM
power requirement by taking the observed average mem-
ory bandwidth utilization of 52% for baseline and 21% for
COAXIALinto account. As this tool only computes power
up to DDR4-3200MT/s modules, we model a 64GB 2-rank
DDR4-3200 DIMM (16GB 2-rank module for CXL) and dou-ble the power to obtain power consumption of a 128 GB DDR5
channel (32 GB channel for CXL). While COAXIALemploys
4more DIMMs than the baseline, its power consumption is
only 1 :75higher due to lower memory utilization.
Table 5 summarizes the key power components for the
baseline and COAXIALsystems. The overall system power
consumption is 713W for the baseline system and 1.18kW
forCOAXIAL, a 66% increase. Crucially, COAXIALmas-
sively boosts performance, reducing CPI by 34%. As a result,
COAXIALreduces the baseline’s EDP by a considerable 28%.
Takeaway #5: In addition to boosting performance,
COAXIALaffords a more efﬁcient system with a 28%
lower energy-delay product.
6.7. Evaluation Summary
CXL-based memory systems hold great promise for manycore
server processors. Replacing DDR with CXL-based memory
that offers 4higher bandwidth at a 30ns latency premium
achieves a 1:52average speedup across various workloads.
Furthermore, a COAXIAL-asym design demonstrates oppor-
ST_copyST_scaleST_addST_triad  
Comp-scCompPR-DBCPR
RadiiBFSCCBellmFBFS
BFS-BVTriangleMIS  
fluidafacesimraytracesclustercanneal  
lbm
bwavescactuBfotonikcam4wrfmcfromspop2
omnetppxalancgcc  
masstreekmeansGM_STRMGM_LIGRAGM_PARSGM_SPECGM_ALL0.00.51.01.52.02.53.0Norm. Performance1 Core  (8%)
4 Cores (33%) 8  Cores (66%)
12 Cores (100%)
Figure 9: Performance of C OAXIAL as a function of active cores, norm. to DDR-based server baseline at the same active cores.
10Table 5: Energy Delay Product (EDP =System power CPI2)
comparison for target 144-core server. Lower EDP is better.
EDP Component Baseline COAXIAL
Processor Package power 500W 500W
DDR5 MC & PHY power (all) 13W 52W
DDR5 DIMM power (static and access) 200W 551W
CXL’s Interface power (idle and dynamic) N/A 77W
Total system power 713W 1,180W
Average CPI (all workloads) 2.02 1.33
EDP (all workloads) 2,909 2,087 (0.72 )
tunity for additional gain ( 1:67average speedup), assuming
a modiﬁcation to the PCIe standard to allow departure from
the rigid 1:1 read:write bandwidth provisioning to allow an
asymmetric, workload-aware one. Even if COAXIALincurs
a 50ns latency premium, it promises substantial performance
improvement ( 1:33on average). We show that our bene-
ﬁts stem from reduced memory contention: by reducing the
utilization of available bandwidth resources, COAXIALmit-
igates queuing effects, thus reducing both average memory
access latency and its variance.
7. Related Work
We discuss recent works investigating CXL-based memory
system solutions, prior memory systems leveraging serial in-
terfaces, as well as circuit-level and alternative techniques to
improve bandwidth and optimize the memory system.
Emerging CXL-based memory systems. Industry is rapidly
adopting CXL and already investigating its deployment in
production systems to reap the beneﬁts of memory expansion
and memory pooling. Microsoft leverages CXL to pool mem-
ory across servers, improving utilization and thus reducing
cost [ 25]. In the same vein, Gouk et al. [ 16] leverage CXL to
prototype a practical instance of disaggregated memory [ 27].
Aspiring to use CXL as a memory expansion technique that
will enable a secondary memory tier of higher capacity than
DDR, Meta’s recent work optimizes data placement in this
new type of two-tier memory hierarchy [ 33]. Using an FPGA-
based prototype of a CXL type-3 memory device, Ahn et al.
evaluate database workloads on a hybrid DDR/CXL memory
system and demonstrate minimal performance degradation,
suggesting that CXL-based memory expansion is cost-efﬁcient
and performant [ 3]. Instead of using CXL-attached memory
as a memory system extension, our work stands out as the ﬁrst
one to propose CXL-based memory as a complete replace-
ment of DDR-attached memory for server processors handling
memory-intensive workloads.
Memory systems leveraging serial interfaces. There have
been several prior memory system proposals leveraging serial
links for high-bandwidth, energy-efﬁcient data transfers. Mi-
cron’s HMC was connected to the host over 16 SerDes lanes,
delivering up to 160GB/s [ 41]. IBM’s Centaur is a memory
capacity expansion solution, where the host uses SerDes toconnect to a buffer-on-board, which in turn hosts several DDR
channels [ 54]. FBDIMM [ 14] leverages a similar concept to
Centaur’s buffer-on-board to increase memory bandwidth and
capacity. An advanced memory buffer (AMB) acts as a bridge
between the processor and the memory modules, connecting
to the processor over serial links and featuring an abundance
of pins to enable multiple parallel interfaces to DRAM mod-
ules. Similar to CXL-attached memory, a key concern with
FBDIMM is its increased latency. Open Memory Interface
(OMI) is a recent high-bandwidth memory leveraging serial
links, delivering bandwidth comparable to HBM but without
HBM’s tight capacity limitations [ 7]. Originally a subset of
OpenCAPI, OMI is now part of the CXL Consortium.
Researchers have also proposed memory system architec-
tures making use of high-bandwidth serial interfaces. In MeS-
SOS’ two-stage memory system, high-bandwidth serial links
connect to a high-bandwidth DRAM cache, which is then
chained to planar DRAM over DDR [ 58]. Ham et al. pro-
pose disintegrated memory controllers attached over SerDes,
aiming to make the memory system more modular and fa-
cilitate supporting heterogeneous memory technologies [ 17].
Alloy combines parallel and serial interfaces to access memory,
maintaining the parallel interfaces for lower-latency memory
access [ 59]. Unlike our proposal of fully replacing DDR
processor interfaces with CXL for memory-intensive servers,
Alloy’s approach is closer to the hybrid DDR/CXL memory
systems that most ongoing CXL-related research envisions.
Circuit-level techniques to boost memory bandwidth.
HBM [ 23] and die-stacked DRAM caches offer an order of
magnitude higher bandwidth than planar DRAM, but suffer
from limited capacity [ 22,30,44]. BOOM [ 60] buffers out-
puts from multiple LPDDR ranks to reduce power and sus-
tain server-level performance, but offers modest gains due to
low frequency LPDDR and limited bandwidth improvement.
Chen et al. [ 6] propose dynamic reallocation of power pins to
boost data transfer capability from memory during memory-
intensive phases, during which processors are memory bound
and hence draw less power. Pal et al. [ 40] propose packageless
processors to mitigate pin limitations and boost the memory
bandwidth that can be routed to the processor. Unlike these
proposals, we focus on conventional processors, packaging,
and commodity DRAM, aiming to reshape the memory sys-
tem of server processors by leveraging the widely adopted
up-and-coming CXL interconnect.
Other memory system optimizations. Transparent memory
compression techniques are a compelling approach to increas-
ing effective memory bandwidth [ 61]. Malladi et al. [ 31]
leverage mobile LPDDR DRAM devices to design a more
energy-efﬁcient memory system for servers without perfor-
mance loss. These works are orthogonal to our proposed
approach. Storage-class memory, like Phase-Change Mem-
ory [13] or Intel’s Optane [ 19], has attracted signiﬁcant interest
as a way to boost a server’s memory capacity, triggering re-
search activity on transforming the memory hierarchy to best
11accommodate such new memories [ 2,10,24,56]. Unlike our
work, such systems often trade off bandwidth for capacity.
8. Conclusion
Technological trends motivate a server processor design where
all memory is attached to the processor over the emerging
CXL interconnect instead of DDR. CXL’s superior bandwidth
per pin helps bandwidth-hungry server processors scale the
bandwidth wall. By distributing memory requests over 4
more memory channels, CXL reduces queueing effects on
the memory bus. Because queuing delay dominates access
latency in loaded memory systems, such reduction more than
compensates for the interface latency overhead introduced by
CXL. Our evaluation on a diverse range of memory-intensive
workloads shows that our proposed COAXIALserver delivers
1:52speedup on average, and up to 3 .
References
[1]“ChampSim.” [Online]. Available: https://github :com/ChampSim/
ChampSim
[2]N. Agarwal and T. F. Wenisch, “Thermostat: Application-transparent
Page Management for Two-tiered Main Memory,” in Proceedings of
the 22nd International Conference on Architectural Support for Pro-
gramming Languages and Operating Systems (ASPLOS-XXII) , 2017,
pp. 631–644.
[3]M. Ahn, A. Chang, D. Lee, J. Gim, J. Kim, J. Jung, O. Rebholz,
V . Pham, K. T. Malladi, and Y .-S. Ki, “Enabling CXL Memory Expan-
sion for In-Memory Database Management Systems,” in Proceedings
of the 18th International Workshop on Data Management on New
Hardware (DaMoN) , 2022, pp. 8:1–8:5.
[4]M. Bichan, C. Ting, B. Zand, J. Wang, R. Shulyzki, J. Guthrie,
K. Tyshchenko, J. Zhao, A. Parsafar, E. Liu, A. Vatankhahghadim,
S. Shariﬁan, A. Tyshchenko, M. D. Vita, S. Rubab, S. Iyer, F. Spagna,
and N. Dolev, “A 32Gb/s NRZ 37dB SerDes in 10nm CMOS to Sup-
port PCI Express Gen 5 Protocol,” in Proceedings of the 2020 IEEE
Custom Integrated Circuits Conference , 2020, pp. 1–4.
[5]C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The PARSEC benchmark
suite: characterization and architectural implications,” in Proceedings
of the 17th International Conference on Parallel Architecture and
Compilation Techniques (PACT) , 2008, pp. 72–81.
[6]S. Chen, Y . Hu, Y . Zhang, L. Peng, J. Ardonne, S. Irving, and A. Sri-
vastava, “Increasing off-chip bandwidth in multi-core processors with
switchable pins,” in Proceedings of the 41st International Symposium
on Computer Architecture (ISCA) , 2014, pp. 385–396.
[7]T. M. Coughlin and J. Handy, “Higher Performance and Capacity with
OMI Near Memory,” in Proceedings of the 2021 Annual Symposium
on High-Performance Interconnects , 2021, pp. 68–71.
[8]D. I. Cutress and A. Frumusanu, “Amd 3rd gen epyc milan review:
A peak vs per core performance balance,” 2021. [Online]. Available:
https://www :anandtech :com/show/16529/amd-epyc-milan-review
[9]CXL Consortium, “Compute Express Link (CXL) Spec-
iﬁcation, Revision 3.0, Version 1.0,” 2022. [On-
line]. Available: https://www :computeexpresslink :org/_ﬁles/ugd/
0c1418_1798ce97c1e6438fba818d760905e43a :pdf
[10] S. Dulloor, A. Roy, Z. Zhao, N. Sundaram, N. Satish, R. Sankaran,
J. Jackson, and K. Schwan, “Data tiering in heterogeneous memory
systems,” in Proceedings of the 2016 EuroSys Conference , 2016, pp.
15:1–15:16.
[11] EE Times, “CXL will absorb Gen-Z,” 2021. [Online]. Available:
https://www :eetimes :com/cxl-will-absorb-gen-z/
[12] Engineering at Meta, “ Introducing “Yosemite”: the ﬁrst open
source modular chassis for high-powered microservers,” 2015.
[Online]. Available: https://engineering :fb:com/2015/03/10/core-
data/introducing-yosemite-the-ﬁrst-open-source-modular-chassis-
for-high-powered-microservers/
[13] S. W. Fong, C. M. Neumann, and H.-S. P. Wong, “Phase-change mem-
ory—towards a storage-class memory,” IEEE Transactions on Electron
Devices , vol. 64, no. 11, pp. 4374–4385, 2017.[14] B. Ganesh, A. Jaleel, D. Wang, and B. L. Jacob, “Fully-Buffered
DIMM Memory Architectures: Understanding Mechanisms, Over-
heads and Scaling,” in Proceedings of the 13th IEEE Symposium on
High-Performance Computer Architecture (HPCA) , 2007, pp. 109–120.
[15] Google Cloud, “Machine families resource and comparison guide.”
[Online]. Available: https://cloud :google :com/compute/docs/machine-resource
[16] D. Gouk, S. Lee, M. Kwon, and M. Jung, “Direct Access, High-
Performance Memory Disaggregation with DirectCXL,” in Proceed-
ings of the 2022 USENIX Annual Technical Conference (ATC) , 2022,
pp. 287–294.
[17] T. J. Ham, B. K. Chelepalli, N. Xue, and B. C. Lee, “Disintegrated
control for energy-efﬁcient and heterogeneous memory systems,” in
Proceedings of the 19th IEEE Symposium on High-Performance Com-
puter Architecture (HPCA) , 2013, pp. 424–435.
[18] K. M. Hazelwood, S. Bird, D. M. Brooks, S. Chintala, U. Diril,
D. Dzhulgakov, M. Fawzy, B. Jia, Y . Jia, A. Kalro, J. Law, K. Lee, J. Lu,
P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang, “Applied Ma-
chine Learning at Facebook: A Datacenter Infrastructure Perspective,”
inProceedings of the 24th IEEE Symposium on High-Performance
Computer Architecture (HPCA) , 2018, pp. 620–629.
[19] Intel Corporation, “Intel Optane DC Persistent Memory.” [On-
line]. Available: https://www :intel :com/content/www/us/en/products/
memory-storage/optane-dc-persistent-memory :html
[20] J. Jaffari, A. Ansari, and R. Beraha, “Systems and methods for a hybrid
parallel-serial memory access,” 2015, US Patent 9747038B2.
[21] JEDEC, “DDR5 SDRAM standard (JESD79-5B),” 2022.
[22] D. Jevdjic, S. V olos, and B. Falsaﬁ, “Die-stacked DRAM caches for
servers: hit ratio, latency, or bandwidth? have it all with footprint
cache,” in Proceedings of the 40th International Symposium on Com-
puter Architecture (ISCA) , 2013, pp. 404–415.
[23] J. Kim and Y . Kim, “HBM: Memory solution for bandwidth-hungry
processors,” in Hot Chips Symposium , 2014, pp. 1–24.
[24] B. C. Lee, E. Ipek, O. Mutlu, and D. Burger, “Architecting phase
change memory as a scalable dram alternative,” in Proceedings of the
36th International Symposium on Computer Architecture (ISCA) , 2009,
pp. 2–13.
[25] H. Li, D. S. Berger, S. Novakovic, L. Hsu, D. Ernst, P. Zardoshti,
M. Shah, S. Rajadnya, S. Lee, I. Agarwal, M. D. Hill, M. Fontoura,
and R. Bianchini, “Pond: CXL-Based Memory Pooling Systems for
Cloud Platforms,” Proceedings of the 28th International Conference
on Architectural Support for Programming Languages and Operating
Systems (ASPLOS-XXVIII) , 2023.
[26] S. Li, Z. Yang, D. Reddy, A. Srivastava, and B. L. Jacob, “DRAM-
sim3: A Cycle-Accurate, Thermal-Capable DRAM Simulator,” IEEE
Comput. Archit. Lett. , vol. 19, no. 2, pp. 110–113, 2020.
[27] K. T. Lim, J. Chang, T. N. Mudge, P. Ranganathan, S. K. Reinhardt,
and T. F. Wenisch, “Disaggregated memory for expansion and sharing
in blade servers,” in Proceedings of the 36th International Symposium
on Computer Architecture (ISCA) , 2009, pp. 267–278.
[28] S. P. Lloyd, “Least squares quantization in PCM,” IEEE Trans. Inf.
Theory , vol. 28, no. 2, pp. 129–136, 1982.
[29] Locuza, “Die walkthrough: Alder Lake-S/P and a touch of Zen
3,” 2022. [Online]. Available: https://locuza :substack :com/p/die-
walkthrough-alder-lake-sp-and
[30] G. H. Loh and M. D. Hill, “Efﬁciently enabling conventional block
sizes for very large die-stacked DRAM caches,” in Proceedings of the
44th Annual IEEE/ACM International Symposium on Microarchitec-
ture (MICRO) , 2011, pp. 454–464.
[31] K. T. Malladi, F. A. Nothaft, K. Periyathambi, B. C. Lee, C. Kozyrakis,
and M. Horowitz, “Towards energy-proportional datacenter memory
with mobile DRAM,” in Proceedings of the 39th International Sympo-
sium on Computer Architecture (ISCA) , 2012, pp. 37–48.
[32] Y . Mao, E. Kohler, and R. T. Morris, “Cache craftiness for fast multi-
core key-value storage,” in Proceedings of the 2012 EuroSys Confer-
ence, 2012, pp. 183–196.
[33] H. A. Maruf, H. Wang, A. Dhanotia, J. Weiner, N. Agarwal, P. Bhat-
tacharya, C. Petersen, M. Chowdhury, S. O. Kanaujia, and P. Chauhan,
“TPP: Transparent Page Placement for CXL-Enabled Tiered Memory,”
Proceedings of the 28th International Conference on Architectural Sup-
port for Programming Languages and Operating Systems (ASPLOS-
XXVIII) , 2023.
[34] J. D. McCalpin, “Memory Bandwidth and Machine Balance in Current
High Performance Computers,” IEEE Computer Society Technical
Committee on Computer Architecture (TCCA) Newsletter , 1995.
[35] Micron Technology Inc., “System Power Calculators,” https://
www :micron :com/support/tools-and-utilities/power-calc.
12[36] Micron Technology Inc., “DDR5 SDRAM Datasheet,” 2022. [Online].
Available: https://media-www :micron :com/-/media/client/global/
documents/products/data-sheet/dram/ddr5/ddr5_sdram_core :pdf
[37] H. Mujtaba, “AMD EPYC Bergamo ‘Zen 4C’ CPUs Being
Deployed In 1H 2023 To Tackle Arm CPUs, Instinct MI300
APU Back In Labs,” 2022. [Online]. Available: https:
//wccftech :com/amd-epyc-bergamo-zen-4c-cpus-being-deployed-in-
1h-2023-tackle-arm-instinct-mi300-apu-back-in-labs/amp/
[38] H. Mujtaba, “Intel Granite Rapids & Sierra Forest Xeon
CPU Detailed In Avenue City Platform Leak: Up To 500W
TDP & 12-Channel DDR5,” 2023. [Online]. Available: https:
//wccftech :com/intel-granite-rapids-sierra-forest-xeon-cpu-detailed-
in-avenue-city-platform-leak-up-to-500w-tdp-12-channel-ddr5/
[39] B. Nitin, W. Randy, I. Shinichiro, F. Eiji, R. Shibata, S. Yumiko, and
O. Megumi, “DDR5 design challenges,” in 2018 IEEE 22nd Workshop
on Signal and Power Integrity (SPI) , 2018, pp. 1–4.
[40] S. Pal, D. Petrisko, A. A. Bajwa, P. Gupta, S. S. Iyer, and R. Kumar,
“A Case for Packageless Processors,” in Proceedings of the 24th IEEE
Symposium on High-Performance Computer Architecture (HPCA) ,
2018, pp. 466–479.
[41] J. T. Pawlowski, “Hybrid memory cube (HMC),” in Hot Chips Sympo-
sium , 2011, pp. 1–24.
[42] PLDA, “Breaking the PCIe Latency Barrier with CXL,” 2020. [Online].
Available: https://www :brighttalk :com/webcast/18357/434922
[43] PLDA and AnalogX, “PLDA and AnalogX Announce Market-leading
CXL 2.0 Solution featuring Ultra-low Latency and Power,”
2021. [Online]. Available: https://www :businesswire :com/news/
home/20210602005484/en/PLDA-and-AnalogX-Announce-Market-
leading-CXL-2 :0-Solution-featuring-Ultra-low-Latency-and-Power
[44] M. K. Qureshi and G. H. Loh, “Fundamental Latency Trade-off in
Architecting DRAM Caches: Outperforming Impractical SRAM-Tags
with a Simple and Practical Design,” in Proceedings of the 45th Annual
IEEE/ACM International Symposium on Microarchitecture (MICRO) ,
2012, pp. 235–246.
[45] R. Rooney and N. Koyle, “Micron ®DDR5 SDRAM: New Features,”
Micron Technology Inc., Tech. Rep , 2019.
[46] P. Rosenfeld, E. Cooper-Balis, and B. L. Jacob, “DRAMSim2: A
Cycle Accurate Memory System Simulator,” IEEE Comput. Archit.
Lett., vol. 10, no. 1, pp. 16–19, 2011.
[47] D. D. Sharma, “PCI Express ®6.0 Speciﬁcation at 64.0 GT/s with PAM-
4 signaling: a low latency, high bandwidth, high reliability and cost-
effective interconnect,” in Proceedings of the 2020 Annual Symposium
on High-Performance Interconnects , 2020, pp. 1–8.
[48] D. D. Sharma, “Compute Express Link ®: An open industry-standard
interconnect enabling heterogeneous data-centric computing,” in Pro-
ceedings of the 2022 Annual Symposium on High-Performance Inter-
connects , 2022, pp. 5–12.
[49] J. Shun and G. E. Blelloch, “Ligra: a lightweight graph processing
framework for shared memory,” in Proceedings of the 18th ACM SIG-
PLAN Symposium on Principles and Practice of Parallel Programming
(PPoPP) , 2013, pp. 135–146.[50] Standard Performance Evaluation Corporation, “SPEC CPU2017
Benchmark Suite.” [Online]. Available: http://www :spec :org/cpu2017/
[51] P. Stanley-Marbell, V . C. Cabezas, and R. P. Luijten, “Pinned to the
walls: impact of packaging and application properties on the memory
and power walls,” in Proceedings of the 2011 International Symposium
on Low Power Electronics and Design , 2011, pp. 51–56.
[52] StorageReview, “4th Gen AMD EPYC Review (AMD Genoa),” 2022.
[Online]. Available: https://www :storagereview :com/review/4th-gen-
amd-epyc-review-amd-genoa
[53] TechPowerUp, “AMD "Matisse" and "Rome" IO Con-
troller Dies Mapped Out,” 2020. [Online]. Avail-
able: https://www :techpowerup :com/266287/amd-matisse-and-rome-
io-controller-dies-mapped-out
[54] The Next Platform, “IBM POWER Chips Blur the
Lines to Memory and Accelerators,” 2018. [Online].
Available: https://www :nextplatform :com/2018/08/28/ibm-
power-chips-blur-the-lines-to-memory-and-accelerators/#:~:text=
The%20Centaur%20memory%20adds%20about
[55] The Register, “CXL absorbs OpenCAPI on the road to interconnect
dominance,” 2022. [Online]. Available: https://www :theregister :com/
2022/08/02/cxl_absorbs_opencapi/
[56] D. Ustiugov, A. Daglis, J. Picorel, M. Sutherland, E. Bugnion,
B. Falsaﬁ, and D. N. Pnevmatikatos, “Design guidelines for high-
performance SCM hierarchies,” in Proceedings of the 2018 Interna-
tional Symposium on Memory Systems (MEMSYS) , 2018, pp. 3–16.
[57] S. V olos, “Memory Systems and Interconnects for Scale-Out Servers,”
Ph.D. dissertation, EPFL, Switzerland, 2015.
[58] S. V olos, D. Jevdjic, B. Falsaﬁ, and B. Grot, “Fat Caches for Scale-Out
Servers,” IEEE Micro , vol. 37, no. 2, pp. 90–103, 2017.
[59] H. Wang, C.-J. Park, G. Byun, J. H. Ahn, and N. S. Kim, “Alloy:
Parallel-serial memory channel architecture for single-chip heteroge-
neous processor systems,” in Proceedings of the 21st IEEE Symposium
on High-Performance Computer Architecture (HPCA) , 2015, pp. 296–
308.
[60] D. H. Yoon, J. Chang, N. Muralimanohar, and P. Ranganathan,
“BOOM: Enabling mobile memory based low-power server DIMMs,”
inProceedings of the 39th International Symposium on Computer
Architecture (ISCA) , 2012, pp. 25–36.
[61] V . Young, S. Kariyappa, and M. K. Qureshi, “Enabling Transparent
Memory-Compression for Commodity Memory Systems,” in Proceed-
ings of the 25th IEEE Symposium on High-Performance Computer
Architecture (HPCA) , 2019, pp. 570–581.
[62] Q. Zhu, S. Venkataraman, C. Ye, and A. Chandrasekhar, “Package
design challenges and optimizations in density efﬁcient (Intel ®Xeon ®
processor D) SoC,” in 2016 IEEE Electrical Design of Advanced Pack-
aging and Systems (EDAPS) , 2016.
13This paper is included in the Proceedings of the  
2023 USENIX Annual Technical Conference.
July 10–12, 2023 • Boston, MA, USA
978-1-939133-35-9
Open access to the Proceedings of the 
2023 USENIX Annual Technical Conference 
is sponsored byCXL-ANNS: Software-Hardware Collaborative 
Memory Disaggregation and Computation for 
Billion-Scale Approximate Nearest Neighbor Search
Junhyeok Jang, Computer Architecture and Memory Systems Laboratory, KAIST;  
Hanjin Choi, Computer Architecture and Memory Systems Laboratory, KAIST and 
Panmnesia, Inc.;  Hanyeoreum Bae and Seungjun Lee, Computer Architecture 
and Memory Systems Laboratory, KAIST;  Miryeong Kwon and Myoungsoo Jung, 
Computer Architecture and Memory Systems Laboratory, KAIST and Panmnesia, Inc.
https://www.usenix.org/conference/atc23/presentation/jangCXL-ANNS: Software-Hardware Collaborative Memory Disaggregation and
Computation for Billion-Scale Approximate Nearest Neighbor Search
Junhyeok Jang⇤, Hanjin Choi⇤†, Hanyeoreum Bae⇤, Seungjun Lee⇤, Miryeong Kwon⇤†, Myoungsoo Jung⇤†
⇤Computer Architecture and Memory Systems Laboratory, KAIST
†Panmnesia, Inc.
Abstract
We propose CXL-ANNS , a software-hardware collaborative
approach to enable highly scalable approximate nearest neigh-
bor search (ANNS) services. To this end, we ﬁrst disaggregate
DRAM from the host via compute express link (CXL) and
place all essential datasets into its memory pool. While this
CXL memory pool can make ANNS feasible to handle billion-
point graphs without an accuracy loss, we observe that the
search performance signiﬁcantly degrades because of CXL’s
far-memory-like characteristics. To address this, CXL-ANNS
considers the node-level relationship and caches the neighbors
in local memory, which are expected to visit most frequently.
For the uncached nodes, CXL-ANNS prefetches a set of nodes
most likely to visit soon by understanding the graph traversing
behaviors of ANNS. CXL-ANNS is also aware of the archi-
tectural structures of the CXL interconnect network and lets
different hardware components therein collaboratively search
for nearest neighbors in parallel. To improve the performance
further, it relaxes the execution dependency of neighbor search
tasks and maximizes the degree of search parallelism by fully
utilizing all hardware in the CXL network.
Our empirical evaluation results show that CXL-ANNS
exhibits 111.1 ⇥higher QPS with 93.3% lower query latency
than state-of-the-art ANNS platforms that we tested. CXL-
ANNS also outperforms an oracle ANNS system that has
DRAM-only (with unlimited storage capacity) by 68.0% and
3.8⇥, in terms of latency and throughput, respectively.
1 Introduction
Dense retrieval (also known as nearest neighbor search) has
taken on an important role and provides fundamental sup-
port for various search engines, data mining, databases, and
machine learning applications such as recommendation sys-
tems [ 1–8]. In contrast to the classic pattern/string-based
search, dense retrieval compares the similarity across differ-
ent objects using their distance and retrieves a given number
of objects, similar to the query object, referred to as k-nearest
neighbor ( kNN)[9–11]. To this end, dense retrieval embeds
input information into a few thousand dimensional spaces
of each object, called a feature vector . Since these vectors
can encode a wide spectrum of data formats (e.g., images,
documents, sounds, etc.), dense retrieval understands an in-
put query’s semantics, resulting in more context-aware and(a) Previous studies. (b) CXL-based approaches.
Figure 1: Various billion-scale ANNS characterizations.
accurate results than traditional search [ 6,12,13].
Even though kNN is one of the most frequently used search
paradigms in various applications, it is a costly operation
taking linear time to scan data [ 14,15]. This computation
complexity unfortunately makes dense retrieval with a billion-
point dataset infeasible. To make the kNN search more practi-
cal,approximate nearest neighbor search (ANNS) restricts
a query vector to search only a subset of neighbors with a
high chance of being the nearest ones [ 15–17]. ANNS ex-
hibits good vector searching speed and accuracy, but it sig-
niﬁcantly increases memory requirement and pressure. For
example, many production-level recommendation systems
already adopt billion-point datasets, which require tens of
TB of working memory space for ANNS; Microsoft search
engines (used in Bing/Outlook) require 100B+ vectors, each
being explained by 100 dimensions, which consume more
than 40TB memory space [ 18]. Similarly, several of Alibaba’s
e-commerce platforms need TB-scale memory spaces to ac-
commodate their 2B+ vectors (128 dimensions) [ 19].
To address these memory pressure issues, modern ANNS
techniques leverage lossy compression methods or employ
persistent storage, such as solid state disks (SSDs) and per-
sistent memory (PMEM), for their memory expansion. For
example, [ 20–23] split large datasets and group them into
multiple clusters in an ofﬂine time. This compression ap-
proach only has product quantized vectors for each cluster’s
centroid and searches kNN based on the quantized informa-
tion, making billion-scale ANNS feasible. On the other hand,
the hierarchical approach [ 24–28] accommodates the datasets
to SSD/PMEM, but reduces target search spaces by referring
to a summary in its local memory (DRAM). As shown in
Figure 1a, these compression and hierarchical approaches
can achieve the best kNN search performance and scalabil-
USENIX Association 2023 USENIX Annual Technical Conference    585ity similar to or slightly worse than what an oracle1system
offers. However, these approaches suffer from a lack of accu-
racy and/or performance, which unfortunately hinders their
practicality in achieving billion-scale ANNS services.
In this work, we propose CXL-ANNS , a software-hardware
collaborative approach that enables scalable approximate near-
est neighbor search (ANNS). As shown in Figure 1b, the main
goal of CXL-ANNS is to offer the latency of billion-point
kNN search even shorter than the oracle system mentioned
above while achieving high throughput without a loss of ac-
curacy. To this end, we disaggregate DRAM from the host
resources via compute express link (CXL) and place all essen-
tial datasets into its memory pool; CXL is an open-industry
interconnect technology that allows the underlying working
memory to be highly scalable and composable with a low
cost. Since a CXL network can expand its memory capacity
by having more endpoint devices (EPs) in a scalable manner,
a host’s root-complex (RC) can map the network’s large mem-
ory pool (up to 4PB) into its system memory space and use it
just like a locally-attached conventional DRAM.
While this CXL memory pool can make ANNS feasible
to handle billion-point graphs without a loss of accuracy, we
observe that the search performance degrades compared to
the oracle by as high as 3.9 ⇥(§3.1). This is due to CXL’s
far-memory-like characteristics; every memory request needs
a CXL protocol conversion (from CPU instructions to one
or more CXL ﬂits), which takes a time similar to or longer
than a DRAM access itself. To address this, we consider the
relationship of different nodes in a given graph and cache the
neighbors in the local memory, which are expected to visit
frequently. For the uncached nodes, CXL-ANNS prefetches a
set of nodes most likely to be touched soon by understanding
the unique behaviors of the ANNS graph traversing algo-
rithm. CXL-ANNS is also aware of the architectural struc-
tures of the CXL interconnect network and allows different
hardware components therein to simultaneously search for
nearest neighbors in a collaborative manner. To improve the
performance further, we relax the execution dependency in the
KNN search and maximize the degree of search parallelism
by fully utilizing all our hardware in the CXL network.
We summarize the main contribution as follows:
•Relationship-aware graph caching. Since ANNS traverses
a given graph from its entry-node [ 10,19], we observe that
the graph data accesses, associated with the innermost edge
hops, account for most of the point accesses (§ 3.2). Inspired
by this, we selectively locate the graph and feature vectors in
different places of the CXL memory network. Speciﬁcally,
CXL-ANNS allocates the node information closer to the entry
node in the locally-attached DRAMs while placing the other
datasets in the CXL memory pool.
•Hiding the latency of CXL memory pool. If it needs to tra-
verse (uncached) outer nodes, CXL-ANNS prefetches the
1In this paper, the term “Oracle” refers to a system that utilizes ample
DRAM resources with an unrestricted memory capacity.datasets of neighbors, most likely to be processed in the next
step of kNN queries from the CXL memory pool. However,
it is non-trivial to ﬁgure out which node will be the next
to visit because of ANNS’s procedural data processing de-
pendency. We propose a simple foreseeing technique that
exploits a unique graph traversing characteristic of ANNS
and prefetches the next neighbor’s dataset during the current
kNN candidate update phase.
•Collaborative kNN search design in CXL. CXL-ANNS sig-
niﬁcantly reduces the time wasted for transferring the feature
vectors back and forth by designing EP controllers to calcu-
late distances. On the other hand, it utilizes the computation
power of the CXL host for non-beneﬁcial operations in pro-
cessing data near memory (e.g., graph traverse and candidate
update). This collaborative search includes an efﬁcient design
of RC-EP interfaces and a sharding method being aware of
the hardware conﬁgurations of the CXL memory pool.
•Dependency relaxation and scheduling. The computation
sequences of ANNS are all connected in a serial order, which
makes them unfortunately dependent on execution. We exam-
ine all the activities of kNN query requests and classify them
into urgent/deferrable subtasks. CXL-ANNS then relaxes the
dependency of ANN computation sequences and schedules
their subtasks in a ﬁner granular manner.
We validate all the functionalities of CXL-ANNS’s soft-
ware and hardware (including the CXL memory pool) by
prototyping them using Linux 5.15.36 and 16nm FPGA, re-
spectively. To explore the full design spaces of ANNS, we also
implement the hardware-validated CXL-ANNS in gem5 [ 29]
and perform full-system simulations using six billion-point
datasets [ 30]. Our evaluation results show that CXL-ANNS
exhibits 111.1 ⇥higher bandwidth (QPS) with 93.3% lower
query latency, compared to the state-of-the-art billion-scale
ANNS methods [ 20,24,25]. The latency and throughput be-
haviors of CXL-ANNS are even better than those of the oracle
system (DRAM-only) by 68.0% and 3.8 ⇥, respectively.
2 Background
2.1 Approximate Nearest Neighbor Search
The most accurate method to get k-nearest neighbors (kNN)
in a graph is to compare an input query vector with all data
vectors in a brute-force manner [ 9,31]. Obviously, this sim-
ple dense retrieval technique is impractical mainly due to
its time complexity [ 10,24]. In contrast, approximate near-
est neighbor search (ANNS) restricts the query vector to re-
trieve only a subset of neighbors that can be kNN with a high
probability. To meet diverse accuracy and performance re-
quirements, several ANNS algorithms such as tree-structure
based [ 32,33], hashing based [ 11,34,35] and quantization
based approaches [ 20–23] have been proposed over the past
decades. Among the various techniques, ANNS algorithms
using graphs [ 10,19,24] are considered as the most promising
586    2023 USENIX Annual Technical Conference USENIX AssociationFigure 2: Distance.Algorithm 1: Best-ﬁrst Search
Input: query ,
k:= number of nearest neighbor to ﬁnd
Output: knearest neighbor of query
1distance =calcDist (query ,startNode )
2CandidateArr ={startNode ,distance }
3curNode =startNode
4CandidateArr .markVisited (curNode )
/*Main iteration (line 5 ⇠10) */
5while !candidateArr. allVisited ()do
6neighbors =Graph .neighbors (curNode )
7distances =calcDist (query ,neighbors )
8CandidateArr .insert (neighbors ,distances )
9curNode =CandidateArr .getNextNode ()
10CandidateArr .markVisited (curNode )
11return CandidateArr [:k]
solution, with great potential2. This is because graph-based
approaches can better describe neighbor relationships and
traverse fewer points than the other approaches that operate
in a Euclidean space [ 19,36–39].
Distance calculations. While there are various graph con-
struction algorithms for ANNS [ 10,19,24], the goal of their
query search algorithms is all the same or similar to each
other; it is simply to ﬁnd knumbers of neighbors in the target
graph, which are expected to have the shortest distance from a
given feature vector, called query vector . There are two most
common methods to deﬁne such a distance between the query
vector and neighbor’s feature vector (called data vector ): i)
L2 (Euclidean) distance and ii) angular distance. As shown in
Figure 2, these methods map the nodes that we compare into
a temporal dimension space using their own vector’s feature
elements. Let us suppose that there are nnumbers of features
for each vector. Then, L2 and angular distances are calculated
byÂi(Query i Data i)2andÂi(Query i·Data i), respectively;
where Query iandData iare the ithfeature of a given query
and data vectors, respectively ( in). These distance deﬁni-
tions are simpliﬁed to reduce their calculation latency, which
differs from the actual distances in a multi-dimensional vector
space. This simpliﬁcation works well since ANNS uses the
distances only for a relative comparison to search kNN.
Approximate kNN query search. Algorithm 1explains the
graph traversing method that most ANNS employs [ 10,19,24].
The method, best-ﬁrst search (BFS) [ 38,40], traverses from
anentry-node (line ∏) and moves to neighbors getting closer
to the given query vector (lines ∫⇠ø). While the brute-force
search explores a full space of the graph by systematically
enumerating all the nodes, ANNS uses a preprocessed graph
and visits a limited number of nodes for each hop. The graph is
constructed (preprocessed) to have the entry-node that arrives
all the nodes of its original graph within the minimum number
of average edge hops; this preprocessed graph guarantees that
there exists a path between the entry-node and any of the
given nodes. To minimize the overhead of graph traverse, BFS
employs a candidate array that includes the neighbors whose
2For the sake of the brevity, we use “graph-based approximate kNN
methods” and “ANNS” interchangeably.(a) Compression-based approach. (b) Hierarchical approach.
Figure 3: Existing billion-scale ANNS methods.
distances (from the query vector) are expected to be shorter
than others. For each node visiting, BFS checks this candidate
array and retrieves unvisited node from the array (line ª,
æ). It then calculates the distances of the node’s neighbors
(line º) by retrieving their vectors from the embedding table.
After this distance calculation, BFS updates the candidate
array with the new information, neighbors and distances (line
Ω). All these activities are iterated (line ∫) until there is no
unvisited node in the candidate array. BFS ﬁnally returns the
knumber of neighbors in the candidate array.
2.2 Towards Billion-scale ANNS
While ANNS can achieve good search speed and reasonable
accuracy (as it only visits the nodes in the candidate array),
it still requires maintaining all the original graph and vectors
in its embedding table. This renders ANNS difﬁcult to have
billion-point graphs that exhibit high memory demands in
many production-level services [ 18,19]. To address this issue,
there have been many studies proposed [ 20–27], but we can
classify them into two as shown in Figures 3aand3b.
Compression approaches. These approaches [ 20–23] reduce
the embedding table by compressing its vectors. As shown in
Figure 3a, they logically split the given graph into multiple
sub-groups, called clusters ; the nodes A and E are classiﬁed
in the cluster X whereas the others are grouped as the cluster
Y. For each cluster, these approaches then encode the corre-
sponding vectors into a single, representative vector (called
centroid) by averaging all the vectors in the cluster. They then
replace all the vectors in the embedding table with their clus-
ter ID. Since the distances are calculated by the compressed
centroid vectors (rather than original data vectors), it exhibits
a low accuracy for the search. For example, the node E can be
selected as one of kNN although the node B sits closer to the
query vector. Another issue of these compression approaches
is the limited reduction rate in the size of the graph datasets.
Since they quantize only the embedding table, their billion-
point graph data have no beneﬁt of the compression or even
get slightly bigger to add a set of shortcuts into the original
graph.
Hierarchical approaches. These approaches [ 24–27] store
all the graph and vectors (embedding table) to the underly-
ing SSD/PMEM (Figure 3b). Since SSD/PMEM are prac-
USENIX Association 2023 USENIX Annual Technical Conference    587(a) Types of CXL EP. (b) CXL-based memory pool.
Figure 4: CXL’s sub-protocols and endpoint types.
tically slower than DRAM by many orders of magnitude,
these methods process kNN queries in two separate phases:
i)low-accuracy search and ii) high-accuracy search . The
former only refers to compressed or simpliﬁed datasets, simi-
lar to the datasets that the compression approaches use. The
low-accuracy search quickly ﬁnds out one or more nearest
neighbor candidates (without a storage access) thereby reduc-
ing the search space that the latter needs to process. Once it
has been completed, the high-accuracy search refers to the
original datasets associated with the candidates and processes
the actual kNN queries. For example, DiskANN [ 24] ’s low
accuracy search ﬁnds the kNN candidates using the com-
pressed datasets in DRAM. The high-accuracy search then
re-examines and re-ranks the order of kNN candidates by vis-
iting their actual vectors stored in SSD/PMEM. On the other
hand, HM-ANN [ 25] simpliﬁes the target graph by adding
several shortcuts (across multiple edge hops) into the graph.
HM-ANN’s low-accuracy search scans a candidate closer to
the given query vector from the simpliﬁed graph. Once HM-
ANN detects the candidate, the high-accuracy search checks
its kNN by referring to all the graph and data vectors recorded
in SSD/PMEM.
2.3 Compute Express Link for Memory Pool
CXL is an open standard interconnect which can expand
memory over the existing PCIe physical layers in a scalable
option [ 41–43]. As shown in Figure 4a, CXL consists of three
sub-protocols: i) CXL.io , ii)CXL.cache , and iii) CXL.mem .
Based on which sub-protocols are used for the main commu-
nication, CXL EPs can be classiﬁed as Types.
Sub-protocols and endpoint types. CXL.io is basically the
same as the PCIe standard, which is aimed at enumerating the
underlying EPs and performing transaction controls. It is thus
used for all the CXL types of EPs to be interconnected to the
CXL CPU’s root-complex ( RC) through PCIe. On the other
hand, CXL.cache is for an underlying EP to make its states
coherent with those of a CXL host CPU, whereas CXL.mem
supports simple memory operations (load/store) over PCIe.
Type 1 is considered by a co-processor or accelerator that does
not have memory exposed to CXL RC while Type 2 employs
internal memory, accessible from CXL RC. Thus, Type 1 only
uses CXL.cache (in addition to CXL.io), but Type 2 needs toFigure 5: Accuracy.BigANNYandex-TYandex-DMeta-SMS-TRand050100150Normalized Latency
Optane Compute BigANNYandex-TYandex-DMeta-SMS-TDiskANNHM-ANNMS-SFigure 6: Latency.
use both CXL.cache and CXL.mem. A potential example of
Type 1 and 2 can be FPGAs and GPUs, respectively. On the
other hand, Type 3 only uses CXL.mem (read/write), which
means that there is no interface for a device-side compute unit
to update its calculation results to CXL CPU’s RC and/or get
a non-memory request from the RC.
CXL endpoint disaggregation. Figure 4bshows how we
can disaggregate DRAM from host resources using CXL EPs,
in particular, Type 3; we will discuss why Type 3 is the best
device type for the design of CXL-ANNS, shortly. Type 3’s in-
ternal memory is exposed as a host-managed device memory
(HDM), which can be mapped to the CXL CPU’s host phys-
ical address (HPA) in the system memory just like DRAM.
Therefore, applications running on the CXL CPU can access
HDM (EP’s internal memory) through conventional mem-
ory instructions (loads/stores). Thanks to this characteristic,
HDM requests are treated as traditional memory requests in
CXL CPU’s memory hierarchy; the requests are ﬁrst cached
in CPU cache(s). Once its cache controller evicts a line as-
sociated with the address space of HDM, the request goes
through to the system’s CXL RC. RC then converts one or
more memory requests into a CXL packet (called ﬂit) that can
deal with a request or response of CXL.mem/CXL.cache. RC
passes the ﬂit to the target EP using CXL.mem’s read or write
interfaces. The destination EP’s PCIe and CXL controllers
take the ﬂit over, convert it to one or more memory requests,
and serve the request with the EP’s internal memory (HDM).
Type consideration for scaling-out. To expand the mem-
ory capacity, the target CXL network can have one or more
switches that have multiple ports, each being able to connect
a CXL EP. This switch-based network conﬁguration allows
an RC to employ many EPs (upto 4K), but only for Type 3.
This is because CXL.cache uses virtual addresses for its cache
coherence management unlike CXL.mem. As the virtual ad-
dresses (brought by CXL ﬂits) are not directly matched with
the physical address of each underlying EP’s HDM, the CXL
switches cannot understand where the exact destination is.
3 A High-level Viewpoint of CXL-ANNS
3.1 Challenge Analysis of Billion-scale ANNS
Memory expansion with compression. While compression
methods allow us to have larger datasets, it is not scalable
since their quantized data signiﬁcantly degrades the kNN
search accuracy. Figure 5analyzes the search accuracy of
billion-point ANNS that uses the quantization-based com-
588    2023 USENIX Annual Technical Conference USENIX AssociationFigure 7: CXL baseline architecture.
BigANNYandex-TYandex-DMeta-SMS-TMS-S012345SlowdownFigure 8: CXL.
pression described in § 2.2. In this analysis, it reduces the em-
bedding table by 2 ⇥⇠16⇥. We use six billion-point datasets
from [ 30]; the details of these datasets and evaluation envi-
ronment are the same as what we used in § 6.1. As the density
of the quantized data vectors varies across different datasets,
the compression method exhibits different search accuracies.
While the search accuracies are in a reasonable range to ser-
vice with low compression rates, they signiﬁcantly drop as
the compression rate of the dataset increases. It cannot even
reach the threshold accuracy that ANNS needs to support
(90%, recommended by [ 30]) after having 45.8% less data
than the original. This unfortunately makes the compression
impractical for billion-scale ANNS at high accuracy.
Hierarchical data processing. Hierarchical approaches can
overcome this low accuracy issue by adding one more search
step to re-rank the results of kNN search. This high-accuracy
search however increases the search latency signiﬁcantly as
it eventually requires traversing the storage-side graph and
accessing the corresponding data vectors (in storage) en-
tirely. Figure 6shows the latency behaviors of hierarchical
approaches, DiskANN [ 24] and HM-ANN [ 25]. In this test,
we use 480GB Optane PMEM [ 44] for DiskANN/HM-ANN
and compare their performance with the performance of an
oracle ANNS that has DRAM-only (with unlimited storage
capacity). One can observe from this ﬁgure that the storage
accesses of the high-accuracy search account for 87.6% of the
total kNN query latency, which makes the search latency of
DiskANN and HM-ANN worse than that of the oracle ANNS
by 29.4 ⇥and 64.6 ⇥, respectively, on average.
CXL-augmented ANNS. To avoid the accuracy drop and
performance depletion, this work advocates to directly have
billion-point datasets in a scalable memory pool, disaggre-
gated using CXL. Figure 7shows our baseline architecture
that consists of a CXL CPU, a CXL switch, and four 1TB
Type 3 EPs that we prototype (§ 6.1). We locate all the billion-
point graphs and corresponding vectors to the underlyingType 3 EPs (memory pool) while having ANNS metadata
(e.g. candidate array) in the local DRAM. This baseline al-
lows ANNS to access the billion-point datasets on the remote-
side memory pool just like conventional DRAMs thanks to
CXL’s instruction-level compatibility. Nevertheless, it is not
yet an appropriate option for practical billion-scale ANNS
due to CXL’s architectural characteristics that exhibit lower
performance than the local DRAM.
To be precise, we compare the kNN search latency of the
baseline with the oracle ANNS, and the results are shown
in Figure 8. In this analysis, we normalize the latency of the
baseline to that of the oracle for better understanding. Even
though our baseline does not show severe performance deple-
tion like what DiskANN/HM-ANNS suffer from, it exhibits
3.9⇥slower search latency than the oracle, on average. This is
because all the memory accesses associated with HDM(s) in-
sist the host RC convert them to a CXL ﬂit and revert the ﬂit to
memory requests at the EP-side. The corresponding responses
also requires this memory-to-ﬂit conversion in a reverse order
thereby exhibiting the long latency for graph/vector accesses.
Note that this 3.6 ⇠4.6⇥performance degradation is not ac-
ceptable in many production-level ANNS applications such
as recommendation systems [ 45] or search engines [ 46].
3.2 Design Consideration and Motivation
The main goal of this work is to make the CXL-augmented
kNN search faster than in-memory ANNS services working
only with locally-attached DRAMs (cf. CXL-ANNS vs. Ora-
cle as shown in Figure 8). To achieve this goal, we propose
CXL-ANNS, a software-hardware collaborative approach,
which considers the following three observations: i) node-
level relationship, ii) distance calculation, and iii) vector re-
duction.
Node-level relationship. While there are diverse graph struc-
tures [ 10,19,24] for the best-ﬁrst search traverses (cf. Algo-
rithm 1), all of the graphs starts their traverses from a unique,
single entry-node as described in § 2.1. This implies that the
graph traverse of ANNS visits the nodes closer to the entry-
node much more frequently. For example, as shown in Figure
9a, the node B is always accessed to serve a given set of kNN
queries targeting other nodes listed in the graph branch while
the node G is difﬁcult to visit. To be precise, we examine the
average count to visit nodes in all the billion-point graphs
that this work evaluate when there are a million kNN query
(a) Traverse.(b) Nodes’ visit count.
Figure 9: Graph traverse.Figure 10: End-to-end break-
down analysis.(a) Reduction example.(b) Reduction ratio.
Figure 11: Data reduction.
USENIX Association 2023 USENIX Annual Technical Conference    589requests. The results are shown in Figure 9b. One can observe
from this analysis that the nodes most frequently accessed
during the 1M kNN searches reside in the 2 ⇠3 edge hops. By
appreciating this node-level relationship, we will locate the
graph and vector data regarding inner-most nodes (from the
entry-node) to locally-attached DRAMs while allocating all
the others to the underlying CXL EPs.
Distance calculation. To analyze the critical path of billion-
point ANNS, we decompose the end-to-end kNN search task
into four different sub-tasks, i) candidate update, ii) memory
access and iii) computing fractions of distance calculation,
and iv) graph traverse. We then measure the latency of each
sub-tasks on use in-memory, oracle system, which are shown
in Figure 10. As can be seen from the ﬁgure, ANNS dis-
tance calculation signiﬁcantly contributes to the total execu-
tion time, constituting an average of 81.8%. This observation
stands in contrast to the widely held belief that graph traver-
sal is among the most resource-intensive operations [ 47–49].
The underlying reason for this discrepancy is that distance
calculation necessitates intensive embedding table lookups to
determine the data vectors of all nodes visited by ANNS. No-
tably, while these lookup operations have the same frequency
and pattern as graph traversal, the length of the data vectors
employed by ANNS is 2.0 ⇥greater than that of the graph
data due to their high dimensionality. Importantly, although
distance calculation exhibits considerable latency, it does not
require substantial computational resources, thus making it a
good candidate for acceleration using straightforward hard-
ware solutions.
Reducing data vector transfers. We can take the overhead
brought by distance calculations off the critical path in the
kNN search by bringing only the distance that ANNS needs
to check for each iteration its algorithm visits. As shown
in Figure 11a, let’s suppose that CXL EPs can compute a
distance between a given query vector and data vectors that
ANNS is in visit. Since ANNS needs the distance, a simple
scalar value, instead of all the full features of each data vector,
the amount of data that the underlying EPs transfer can be
reduced as many as each vector’s dimensional degrees. Figure
11banalyzes how much we can reduce the vector transfers
during services of the 1M kNN queries. While the vector
dimensions of each dataset varies (96 ⇠256), we can reduce
the amount of data to load from the EPs by 73.3 ⇥, on average.
3.3 Collaborative Approach Overview
Motivated by the aforementioned observations, CXL-ANNS
ﬁrst caches datasets considering a given graph’s inter-node
relationship and performs ANNS algorithm-aware CXL
prefetches (§ 4.1). This makes the performance of a naive
CXL-augmented kNN search comparable with that of the or-
acle ANNS. To go beyond, CXL-ANNS reduces the vector
transferring latency signiﬁcantly by letting the underlying EPs
to calculate all the ANNS distances near memory (§ 5.1). As
this near-data processing is achieved in a collaborative man-/g24/g94/g4/g24/g90/g4/g68/g3/g373/g381/g282/g437/g367/g286/g90/g18/g882/g400/g349/g282/g286/g28/g87/g882/g400/g349/g282/g286/g89/g437/g286/g396/g455/g3/g400/g272/g346/g286/g282/g437/g367/g286/g396/g60/g286/g396/g374/g286/g367/g3/g282/g396/g349/g448/g286/g396/g87/g44/g122/g3/g272/g410/g396/g367/g856/g18/g121/g62/g286/g374/g336/g349/g374/g286/g24/g90/g4/g68/g3/g272/g410/g396/g367/g856/g18/g258/g374/g282/g349/g282/g258/g410/g286/g3/g437/g393/g282/g258/g410/g286/g24/g349/g400/g410/g258/g374/g272/g286/g3/g272/g258/g367/g272/g437/g367/g258/g410/g349/g381/g374
/g44/g87/g4/g44/g24/g68/g18/g121/g62/g856/g373/g286/g373/g395/g437/g286/g396/g455/g4/g69/g69/g94/g3/g258/g393/g393/g856/g894/g286/g856/g336/g856/g853/g3/g396/g286/g272/g856/g3/g400/g455/g400/g410/g286/g373/g895
/g87/g18/g47/g286/g3/g400/g393/g258/g272/g286/g47/g876/g38/g3/g396/g286/g336/g856/g18/g121/g62/g856/g349/g381/g75/g437/g410/g286/g396/g3/g374/g381/g282/g286/g94/g876/g116/g3/g400/g410/g258/g272/g364/g24/g258/g410/g258/g3/g393/g367/g258/g272/g286/g373/g286/g374/g410/g44/g876/g116/g3/g258/g272/g272/g286/g367/g286/g396/g258/g410/g349/g381/g374/g87/g258/g396/g258/g367/g367/g286/g367/g3/g286/g454/g286/g272/g856/g47/g374/g374/g286/g396/g3/g374/g381/g282/g286/g18/g121/g62/g882/g4/g69/g69/g94/g3/g28/g87/g856/g856/g856/g18/g121/g62/g882/g4/g69/g69/g94/g3/g28/g87/g87/g28/g87/g28/g856/g856/g856/g18/g121/g62/g3/g90/g18/g24/g90/g4/g68/g18/g121/g62/g3/g18/g87/g104
/g87/g381/g381/g367/g3/g373/g258/g374/g258/g336/g286/g396/g39/g396/g258/g393/g346/g3/g410/g396/g258/g448/g286/g396/g400/g286Figure 12: Overview.
ner between EP controllers and RC-side ANNS algorithm
handler, the performance can be limited by the kNN query
service sequences. CXL-ANNS thus schedules kNN search
activities in a ﬁne-grained manner by relaxing their execu-
tion dependency (§ 5.3). Putting all together, CXL-ANNS is
designed for offering high-performance even better than the
oracle ANNS without an accuracy loss.
Figure 12shows the high-level viewpoint of our CXL-
ANNS architecture, which mainly consists of i) RC-side soft-
ware stack and ii) EP-side data processing hardware stack.
RC-side software stack. This RC-side software stack is com-
posed of i) query scheduler, ii) pool manager, and iii) kernel
driver. At the top of CXL-ANNS, the query scheduler handles
all kNN searches requested from its applications such as rec-
ommendation systems. It splits each query into three subtasks
(graph traverse, distance calculation, and candidate update)
and assign them in different places. Speciﬁcally, the graph
traverse and candidate update subtasks are performed at the
CXL CPU side whereas the scheduler allocates the distance
calculation to the underlying EP by collaborating with the
underlying pool manager. The pool manager handles CXL’s
HPA for the graph and data vectors by considering edge hop
counts, such that it can differentiate graph accesses based on
the node-level relationship. Lastly, the kernel driver manages
the underlying EPs and their address spaces; it enumerates
the EPs and maps their HDMs into the system memory’s HPA
that the pool manager uses. Since all memory requests for
HPA are cached at the CXL CPU, the driver maps EP-side
interface registers to RC’s PCIe address space using CXL.io
instead of CXL.mem. Note that, as the PCIe spaces where the
memory-mapped registers exist is in non-cacheable area, the
underlying EP can immediately recognize what the host-side
application lets the EPs know.
EP-side hardware stack. EP-side hardware stack includes
a domain speciﬁc accelerator (DSA) for distance calculation
in addition to all essential hardware components to build a
CXL-based memory expander. At the front of our EPs, a phys-
ical layer (PHY) controller and CXL engine are implemented,
which are responsible for the PCIe/CXL communication con-
trol and ﬂit-to-memory request conversion, respectively. The
converted memory request is forwarded to the underlying
memory controller that connects multiple DRAM modules at
its backend; in our prototype, an EP has four memory con-
trollers, each having a DIMM channel that has 256GB DRAM
590    2023 USENIX Annual Technical Conference USENIX AssociationFigure 13: Data placement.
modules. On the other hand, the DSA is located between the
CXL engine and memory controllers. It can read data vectors
using the memory controllers while checking up the operation
commands through CXL engine’s interface registers. These
interface registers are mapped to the host non-cacheable PCIe
space such that all the commands that the host writes can be
immediately visible to DSA. DSA calculates the approximate
distance for multiple data vectors using multiple processing
elements (PEs), each having simple arithmetic units such as
adder/subtractor and multiplier.
4 Software Stack Design and Implementation
From the memory pool management viewpoint, we have to
consider two different system aspects: i) graph structuring
technique for the local memory and ii) efﬁcient space mapping
method between HDM and graph. We will explain the design
and implementation details of each method in this section.
4.1 Local Caching for Graph
Graph construction for local caching. While the pool man-
ager allocates most graph data and all data vectors to the
underlying CXL memory pool, it caches the nodes, expected
to be most frequently accessed, in local DRAMs as much
as the system memory capacity can accommodate. To this
end, the pool manager considers how many edge hops (i.e.,
calculating the number of edge hops) exist from the ﬁxed
entry-node to each node for its relationship-aware graph cache.
Figure 13explains how the pool manager allocates the nodes
in a given graph to different places (local memory vs. CXL
memory pool). When constructing the graph, the pool man-
ager calculates per-node hop counts by leveraging a single
source shortest path (SSSP) algorithm [ 50,51]; it ﬁrst lets
all the nodes in the graph have a negative hop count (e.g.,
-1). Starting from the entry-node, the pool manager checks
all the nodes in one edge hop and increases its hop count.
It visits each of the nodes and iterates this process for them
until there is no node to visit in a breadth-ﬁrst search manner.
Once each node has its own hop count, the pool manager
sorts them based on the hop count in an ascending order and
allocates the nodes from the top (having the smallest hop
count) to local DRAMs as many as it can. The available size
of the local DRAMs can be simply estimated by referring
to system conﬁguration variables ( sysconf() ) of the total
number of pages ( _SC_AVPHYS_PAGES ) and the size of each
page ( _SC_PAGESIZE ). It’s important to mention that in this
study, the pool manager uses several threads within the user
space to execute SSSP, aiming to reduce the construction timeFigure 14: Memory management.
to a minimum. Once the construction is done, the threads are
terminated to make sure they do not consume CPU resources
when a query is given.
4.2 Data Placement on the CXL Memory Pool
Preparing CXL for user-level memory. When mapping
HDM to the system memory’s HPA, CXL CPU should be
capable of recognizing different HDMs and their size whereas
each EP needs to know where its HDM is assigned in HPA.
As shown in Figure 14, our kernel driver checks PCIe con-
ﬁguration space and ﬁgures out CXL devices at the PCIe
enumeration time. The driver then checks RC information
from the system’s data structure describing the hardware com-
ponents that show where the CXL HPA begins (base), such
as device tree [ 52] or ACPI [ 53]. From the base, our kernel
driver allocates each HDM as much as it deﬁnes in a con-
tiguous space. It lets the underlying EPs know where each of
corresponding HDM is mapped in HPA, such that they can
convert the address of memory requests (HPA) to its original
HDM address. Once all the HDMs are successfully mapped
to HPA, the pool manager allocates each HDM to different
places of user-level virtual address space that the query sched-
uler operates on. This memory-mapped HDM, called CXL
arena , guarantees per-arena continuous memory space and
allows the pool manager to distinguish different EPs at the
user-level.
Pool management for vectors/graph. While CXL arenas
directly expose the underlying HDMs of CXL EPs to user-
level space, it should be well managed to accommodate all
the billion-point datasets appreciating their memory usage
behaviors. The pool manager considers two aspects of the
datasets; the data vectors (i.e., embedding table) should be
located in a substantially large and consecutive memory space
while the graph structure requires taking many neighbor lists
with variable length (16B ⇠1KB). The pool manager employs
stack-like and buddy-like memory allocators, which grow up-
ward and downward in each CXL arena, respectively. The
former allocator has a range pointer and manages memory
for the embedding table, similar to stack. The pool manager
allocates the data vectors across multiple CXL arenas in a
round-robin manner by considering the underlying EP archi-
tecture. This vector sharding method will be explained in § 5.1.
In contrast, the buddy-like allocator employs a level pointer,
USENIX Association 2023 USENIX Annual Technical Conference    591(a) PE architecture. (b) Sharding.
Figure 15: Distance calculation.
each level consisting of a linked list, which connects data
chunks with different size (from 16B to 1KB). Like Linux
buddy memory manager [ 54], it allocates the CXL memory
spaces as much as each neighbor list exactly requires and
merge/split the chunk(s) based on the workload behaviors. To
make each EP balanced, the pool manager allocates the neigh-
bor lists for each hop in round-robin manner across different
CXL arenas.
5 Collaborative Query Service Acceleration
5.1 Accelerating Distance Calculation
Distance computing in EP. As shown in Figure 15a,a
processing element (PE) of DSA has arithmetic logic tree
connecting a multiplier and subtractor at each terminal for
element-wise operations. Depending on how the dataset’s
features are encoded, the query and data vectors are routed
differently to the two units as input. If the features are encoded
for the Euclidean space, the vectors are supplied to the subtrac-
tor for L2 distance calculation. Otherwise, the multiplexing
logics directly deliver the input vectors to the multiplier by
bypassing the subtractor such that it can calculate the angular
distance. Each terminal simultaneously calculates individual
elements of the approximate distance, and the results are ac-
cumulated by going through the arithmetic logic tree network
from the terminal to its root. In addition, each PE’s terminal
reads data from all four different DIMM channels in parallel,
thus maximizing the EP’s backend DRAM bandwidth.
Vector sharding. Even though each EP has many PEs (10 in
our prototype), if we locate the embedding table from the start
address of an EP in a consecutive order, EP’s backend DRAM
bandwidth can be bottleneck in our design. This is because
each feature vector in the embedding table is encoded by high
dimensional information ( ⇠256 dimensions, taking around
1KB). To address this, our pool manager shards the embedding
table in a column wise and stores different parts of the table
across the different EPs. As shown in Figure 15b, this vector
sharding splits each vector into multiple sub-vectors based
on each EP’s I/O granularity (256B). Each EP simultaneously
computes its sub-distance from the split data vector that the
EP accommodates. Later, the CXL CPU accumulates the sub-
distances to get the ﬁnal distance value. Note that, since the
L2 and angular distances are calculated by accumulating the
output of element-wise operations, the ﬁnal distance is theFigure 16: Interface.
same as the results of the sub-distance accumulation using
vector sharding.
Interfacing with EP-level acceleration. Figure 16shows
how the interface registers are managed to let the underlying
EPs compute a distance where the data vectors exist. There
are two considerations for the interface design and implemen-
tation. First, multiple EPs perform the distance calculation
for the same neighbors in parallel thanks to vector sharding.
While the neighbor list contains many node ids ( 200), it
is thus shared by the underlying EPs. Second, handling in-
terface registers using CXL.io is an expensive operation as
CPU should be involved in all the data copies. Considering
these two, the interface registers handle only the event of com-
mand arrivals, called doorbell whereas each EP’s CXL engine
pulls the corresponding operation type and neighbor list from
the CPU-side local DRAM (called a command buffer) in an
active manner. This method can save the time for CPU to
move the neighbor list to each EP’s interface registers one by
one as the CXL engine brings all the information if there is
any doorbell update. The CXL engine also pushes results of
distance calculation to the local DRAM such that the RC-side
software directly accesses the results without an access of
the underlying CXL memory pool. Note that all these com-
munication buffers and registers are directly mapped to the
user-level virtual addresses in our design such that we can
minimize the number of context switches between user and
kernel mode.
5.2 Prefetching for CXL Memory Pool
Figure 17ashows our baseline of collaborative query service
acceleration, which lets EPs compute sub-distances while
ANNS’s graph traverse and candidate update (including sub-
distance accumulation) are handled at the CPU-side. This
scheduling pattern is iterated until there is no kNN candidates
to visit futher (Algorithm 1). A challenge of this baseline
approach is traversing graph can be started once the all node
information is ready at the CPU-side. While local caching
Figure 17: Prefetching.Figure 18: Next
node’s source.
592    2023 USENIX Annual Technical Conference USENIX AssociationFigure 18: Resource
utilization.Figure 19: Query scheduling.
of our pool manager addreses this, it yet shows a limited
performance. It is required to go through the CXL memory
pool to get nodes, which does not sit in the innermost edge
hops. As its latency to access the underlying memory pool is
long, the graph traverse can be postponed comparably. To this
end, our query scheduler prefetches the graph information
earlier than the actual traverse subtask needs, as shown in
Figure 17b.
While this prefetch can hide the long latency imposed by
the CXL memory pool accesses, it is non-trivial as the prefetch
requires knowing the nodes that the next (future) iteration of
the ANNS algorithm will visit. Our query scheduler specu-
lates the nodes to visit and brings their neighbor information
by referring to the candidate array, which is inspired by an
observation that we have. Figure 18shows which nodes are
accessed in the graph traverse of the next iteration across all
the datasets that we tested. We can see that 82.3% of the to-
tal visiting nodes are coming from the candidate array (even
though its information is not yet updated for the next step).
5.3 Fine-Granular Query Scheduling
Our collaborative search query acceleration can reduce the
amount of data to transfer signiﬁcantly and successfully hide
the long latency imposed by the CXL memory pool. However,
computing kNN search in different places makes the RC-side
ANNS subtasks pending until EPs complete their distance
calculation. Figure 18shows how much the RC-side subtasks
(CXL CPU) stay idle, waiting for the distance results. In
this evaluation, we use Yandex-D as a representative of the
datasets, and its time series are analyzed for the time visiting
only ﬁrst two nodes for their neighbor search. The CXL CPU
performs nothing while EPs calculate the distances, which
take 42% of the total execution time for processing those two
nodes. This idle time cannot be easily removed as candidate
update cannot be processed without having their distance.
To address this, our query scheduler relaxes the execution
dependency on the candidate update and separates such an
update into urgent and deferrable procedures. Speciﬁcally,
the candidate update consists of i) inserting (updating) the
array with the candidates, ii) sorting kNN candidates based
on their distance, and iii) node selection to visit. The node
selection is an important process because the following graph
traverse requires knowing the nodes to visit (urgent). However,
sorting/inserting kNN candidates maintain the knumbers of
neighbors in the candidate array, which are not to be done
Figure 21: Prototype.CPU40 O3 cores, ARM v8, 3.6GHz
L1/L2 $: 64KiB/2MiB per core
Local memory 128GiB, DDR4-3200
CXL memory 1 CXL switch
pool 256GiB/device, DDR4-3200
Storage 4⇥Intel Optane 900P 480 GB
CXL-ANNS1 GHz, 10 ANNS PE/device,
2 distance calc. unit/PE
Table 1: Simulation setup.
immediately. Thus, as shown in Figure 19, the query sched-
uler performs the node selection before the graph traverse,
but it executes the deferrable operations during the distance
calculation time by delaying them in a ﬁne-granular manner.
6 Evaluation
6.1 Evaluation Setup
Prototype and Methodology. Given the lack of a publicly
available, fully functional CXL system, we constructed and
validated the CXL-ANNS software and hardware in an oper-
ational real system (Figure 21). This hardware prototype is
based on a 16nm FPGA. To develop our CXL CPU prototype,
we adapted the RISC-V CPU [ 55]. The prototype integrates
4 ANNS EPs, each equipped with four memory controllers,
linked to the CXL CPU via a CXL switch. The system’s
software for the prototype, including the kernel driver, is com-
patible with Linux 5.15.36. For the ANNS execution, we
adjusted Meta’s open ANNS library, FAISS v1.7.2 [ 56].
Unfortunately, the prototype system does not offer the ﬂex-
ibility needed to explore various ANNS design spaces. As a
remedy, we also established a hardware-validated full-system
simulator [ 29] that represents CXL-ANNS, which was uti-
lized for evaluation. This model replicates all operational
cycles extracted from the hardware prototype and is cross-
validated with our real system at the cycle level. We conducted
simulation-based studies in this evaluation, the system details
of which are outlined in Table 1. Notably, the system emulates
the server utilized in Meta’s production environment [ 57]. Al-
though our system by default uses 4 EPs, our system increases
their count for speciﬁc workloads (e.g., Meta-S) that neces-
sitate larger memory spaces (more than 2TB) compared to
others.
Workloads. We use billion-scale ANNS datasets from Bi-
gANN benchmark [ 30], a public ANNS benchmark that mul-
tiple companies (e.g., Microsoft, Meta) participate in. Their
important characteristics are summarized in Table 2. In addi-
tion, since ANNS-based services often need different number
Candidate arr. sizeDataset Dist.Num.
vecs.Emb.
dim.Avg. num.
neighbors k=1 k=5 k=10Num.
devices.
BigANN L2 1B 128 31.6 30 75 150 4
Yandex-T Ang. 1B 200 29.0 440 900 2500 4
Yandex-D L2 1B 96 66.9 300 700 1700 4
Meta-S L2 1B 256 190 1200 2800 5600 8
MS-T L2 1B 100 43.1 60 130 250 4
MS-S L2 1B 100 87.4 580 100 200 4
Table 2: Workloads.
USENIX Association 2023 USENIX Annual Technical Conference    593xxx xx x x0.010.1110100Normalized QPSComp Hr-D Hr-H Orcl Base EPAx Cache CXLA K=5 K=10K=1BigANN Yandex-T Yandex-D Meta-S MS-T MS-SX indicates failure (recall@k lower than 0.9)K=5 K=10K=1K=5 K=10K=1K=5 K=10K=1K=5 K=10K=1K=5 K=10K=1Figure 22: Throughput (queries per second).
(a) BigANN. (b) Yandex-D.
Figure 23: Recall-QPS curve.Figure 24: Single query latency ( k= 10).Dataset BaseCXL-
ANNS
BigANN 3.0 0.3
Yandex-T 66.0 7.4
Yandex-D 55.7 5.3
Meta-S 1121.2 34.2
MS-T 6.0 0.6
MS-S 107.2 8.6
*unit: ms
Table 3: Latency.
of nearest neighbors [ 3,19], we evaluated our system on the
various k(e.g., 1, 5, 10). We generated the graph for ANNS
by using state-of-the-art algorithm, NSG, employed by pro-
duction search services in Alibaba [ 19]. Since the accuracy
and performance of BFS can vary on the size of the candidate
array, we only show the performance behavior of our system
when its accuracy is 90%, as recommended by the BigANN
benchmark. The accuracy is deﬁned as recall@k ; the ratio
of the exact knumber of neighbors that are included in the k
number of output nearest neighbors of ANNS.
Conﬁgurations. We compare CXL-ANNS with 3 state-of-the-
art large-scale ANNS systems. For the compression approach,
we use a representative algorithm, product quantization [ 20]
(Comp ). It compresses the data vector by replacing the vec-
tor with the centroid of its closest cluster (see § 2.2). For the
hierarchical approach, we use DiskANN [ 24](Hr-D ) and
HM-ANN [ 25](Hr-H ) for the evaluation. The two methods
employ compressed embedding table and simpliﬁed graphs
to reduce the number of SSD/PMEM accesses, respectively.
For fair comparison, we use the same storage device, Intel
Optane [ 44], for both Hr-D/H. For CXL-ANNS, we evalu-
ated its multiple variants to distinguish the effect of each
method we propose. Speciﬁcally, Base places the graph and
embedding table in CXL memory pool and lets CXL CPU
execute the subtasks of ANNS. Compared to Base, EPAx per-
forms distance calculation by using DSA inside the ANNS EP.
Compared to EPAx, Cache employs relationship-aware graph
caching and prefetching. Lastly, CXLA employs all the meth-
ods we propose, including ﬁne-granular query scheduling. In
addition, we compare oracle system ( Orcl) that uses unlim-
ited local DRAM. We will show that CXL-ANNS makes the
CXL-augmented kNN search faster than Orcl.
6.2 Overall Performance
We ﬁrst compare the throughput and latency of various sys-
tems we evaluated. We measured the systems’ throughput by
counting the number of processed queries per second (QPS,in short). Figure 22shows the QPS of all ks, while Figure 24
digs the performance behavior deeper for k=10 by breaking
down the latency. We chose k=10 following the guide from
BigANN benchmark. The performance behavior for k=1,5
are largely same with when k=10. For both ﬁgures, we nor-
malized the values by that of Base when k=10. The original
latencies are summarized in Table 3.
As shown in Figure 22, the QPS gets lower when the k
increases for all the systems we tested. This is because, the
BFS visits more nodes to ﬁnd more nearest neighbors. On the
other hand, while Comp exhibits comparable QPS to Orcl, it
fails to reach the target recall@k (0.9) for 7 workloads. This is
because Comp cannot calculate the exact distance since it re-
places the original vector with the centroid of a cluster nearby.
This can also be observed in Figure 23. The ﬁgure shows
the accuracy and QPS when we vary the size of candidate
array for two representative workloads. BigANN represents
the workloads that Comp does reach the target recall, while
Yandex-D represents the opposite. We can see that Comp
converges at low recall@10 of 0.92 and 0.58, respectively,
while other systems reach the maximum recall@10.
In contrast, hierarchical approaches (Hr-D/H) reaches the
target recall@k for all the workloads we tested, by re-ranking
the search result. However, they suffer from the long la-
tency of underlying SSD/PMEM while accessing their un-
compressed graph and embedding table. Such long latency
signiﬁcantly depletes the QPS of Hr-D and Hr-H by 35.9 ⇥
and 77.6 ⇥compared to Orcl, respectively. Consider Figure
24to better understand; Since Hr-D only calculates the dis-
tance for the limited number of nodes in the candidate array, it
exhibits 20.1 ⇥shorter distance calculation time compared to
Hr-H, which starts a new BFS on original graph stored in SS-
D/PMEM. However, Hr-D’s graph traverse takes longer time
than that of Hr-H by 16.6 ⇥. This is because, Hr-D accesses
the original graph in SSD/PMEM for both low/high-accuracy
search while Hr-H accesses the original graph only for their
high-accuracy search.
594    2023 USENIX Annual Technical Conference USENIX AssociationFigure 25: Data transfer.Figure 26: Local
caching.Figure 27: Cache miss
handling time.
050100Utilization (%)CacheCPU idleSingle Iteration  CXL CPU   PE   CXL engine 02468 1 0050100Time (us)ReducedCXLAFigure 28: CPU/PE utilization
(Yandex-D).
As shown in Figure 22, Base does not suffer from accuracy
drop or the performance depletion of Hr-D/H since it employs
a scalable memory pool that CXL offers. Therefore, it signif-
icantly improves the QPS by 9.4 ⇥and 20.3 ⇥, compared to
Hr-D/H, respectively. However, Base still exhibits 3.9 ⇥lower
throughput than Orcl. This is because Base experiences the
long latency of memory-to-ﬂit conversion while accessing
the graph/embedding in CXL memory pool. Such conversion
makes Base’s graph traverse and distance calculation longer
by 2.6 ⇥and 4.3 ⇥, respectively, compared to Orcl.
Compared to Base, EPAx signiﬁcantly diminishes the dis-
tance calculation time by a factor of 119.4 ⇥, achieved by
reducing data vector transfer through the acceleration of dis-
tance calculation within the EPs. While this EP-level accel-
eration introduces an interface overhead, this overhead only
represents 5.4% of the Base’s distance calculation latency.
Hence, EPAx reduces the query latency by 7.5 ⇥on average,
relative to Base. It’s important to highlight that EPAx’s la-
tency is 1.9 ⇥lower than Orcl’s, which has unlimited DRAM.
This discrepancy stems from Orcl’s insufﬁcient grasp of the
ANNS algorithm and its behaviour, which results in consider-
able data movement overhead during data transfer between
local memory and the processor complex. Additional details
can be found in Figure 25, depicting the volume of data trans-
fer via PCIe for the CXL-based systems. The ﬁgure shows that
EPAx eliminates data vector transfer, thereby cutting down
data transfer by 21.1 ⇥.
Further, Cache improves EPAx’s graph traversal time by
3.3⇥, thereby enhancing the query latency by an average of
32.7%. This improvement arises because Cache retains in-
formation about nodes anticipated to be accessed frequently
in the local DRAM, thereby handling 59.4% of graph traver-
sal within the local DRAM (Figure 26). The ﬁgure reveals a
particularly high ratio for BigANN and Yandex-T, at 92.0%.
As indicated in Table 2, their graphs have a relatively small
number of neighbors (31.6 and 29.0, respectively), resulting
in their graphs being compact at an average of 129.3GB. In
contrast, merely 13.8% of Meta-S’s graph accesses are ser-
viced from local memory, attributable to its extensive graph.
Nevertheless, even for Meta-S, Cache enhances graph traver-
sal performance by prefetching graph information before ac-
tual visitation. As depicted in Figure 24, this prefetching can
conceal CXL’s prolonged latency, reducing Meta-S’s graph
traversal latency by 72.8%. While prefetching would intro-
duce overhead in speculating the next node visit, it is insignif-icant, accounting for only 1.3% of the query latency. These
caching and prefetching techniques yield graph processing
performance similar to that of Orcl. We will explain the details
of prefetching shortly.
Lastly, as depicted in Figure 22, CXLA boosts the QPS
by 15.5% in comparison to Cache. This is due to CXLA’s
enhancement of hardware resource utilization by executing
deferrable subtasks and distance calculations concurrently
in the CXL CPU and PE, respectively. As illustrated in Fig-
ure24, such scheduling beneﬁts Yandex-T, Yandex-D, and
Meta-S more so than others. This is attributable to their use
of a candidate array that is, on average, 16.3 ⇥larger than
others, which allows for the overlap of updates with distance
calculation time. Overall, CXLA attains a signiﬁcantly higher
QPS than Orcl, surpassing it by an average factor of 3.8 ⇥.
6.3 Collaborative Query Service Analysis
Prefetching. Figure 27compares the L1 cache miss handling
latency while accessing the graph for the CXL-based systems
we tested. We measured the latency by dividing the total L1
cache miss handling time of CXL CPU by the number of
L1 cache access. The new system, NoPrefetch, disables the
prefetching from Cache. As shown in Figure 27, EPAx’s la-
tency is as long as 75.4ns since it accesses slow CXL memory
whenever there is a cache miss. NoPrefetch alleviates such
problem thanks to local caching, shortening the latency by
45.8%. However, when the dataset uses a large graph (e.g.
Meta-S, MS-S), only 24.5% of the graph can be cached in
local memory. This makes NoPrefetch’s latency 2.3 ⇥higher
than that of Orcl. In contrast, Cache signiﬁcantly shortens
the latency by 8.5 ⇥which is even shorter than that of Orcl.
This is because Cache can foresee the next visiting nodes and
loads the graph information in the cache in advance. Note
that, Orcl accesses local DRAM on demand on cache miss.
Utilization. Figure 28shows the utilization of CXL CPU,
PE, CXL engine on a representative dataset (Yandex-D). To
clearly provide the behavior of our ﬁne-granule scheduling,
we composed a CXL-ANNS with single-core CXL CPU and
single PE per device and show their behavior in a timeline.
The upper part of the ﬁgure shows the behavior of Cache
that does not employ the proposed scheduling. We plot CXL
CPU’s utilization as 0 when it polls the distance calculation
results of PE, since it does not perform any useful job during
that time. As shown in the ﬁgure, CXL CPU idles for 42.0%
of the total time waiting for the distance calculation result. In
USENIX Association 2023 USENIX Annual Technical Conference    595Figure 29: Device scal-
ing (Yandex-D).Figure 30: Host Sensitivity.
contrast, CXLA reduces the idle time by 1.3 ⇥, relaxing the
dependency between ANNS subtasks. In the ﬁgure, we can
see that the CXL CPU’s candidate update time overlaps with
the time CXL engine and PE handling the command. As a
result, CXLA improves the utilization of hardware resources
in CXL network by 20.9%, compared to Cache.
6.4 Scalability Test
Bigger Dataset. To evaluate the scalability of CXL-ANNS,
we increase the number of data vectors in Yandex-D by 4B,
and connect more EP to CXL CPU to accommodate their data.
Since there is no publicly available dataset that is as large
as 4B, we synthetically generated additional 3B vectors by
adding noise to original 1B vectors. As shown in Figure 29,
we can see that the latency of Orcl increases as we increase the
scale of dataset. This is because larger dataset makes BFS visit
more nodes to maintain the same level of recall. On the other
hand, we can see the interface overhead of CXLA increases
as we employ more devices to accommodate bigger dataset.
This is because the CXL CPU should notify more devices for
the command arrival by ringing the doorbell. Despite such
overhead, CXLA exhibits 2.7 ⇥lower latency than Orcl thanks
to its efﬁcient collaborative approach.
Multi-host. In a disaggregated system, a natural way to in-
crease the system’s performance is to employ more host CPUs.
Thus, we evaluate the CXL-ANNS that supports multiple
hosts in the CXL network. Speciﬁcally, we split EP’s re-
sources such as HDM and PEs and then allocate each of
them to one the the CXL hosts in the network. For ANNS, we
partition the embedding table and make each host responsible
for ﬁnding kNN from different partitions. Once all the CXL
hosts ﬁnd the kNN, the system gathers them all and rerank
the neighbors to ﬁnally select kNN among them.
Figure 30shows the QPS of multi-host ANNS. The QPS
is normalized to that when we use single CXL host with
the same number of EPs that we used before. Note that we
also show the QPS when we employ more number of EPs
than we used before. When the number of EPs stays the
same, the QPS increases until we connect 4 CXL hosts in the
system. However the QPS drops when the number of CXL
hosts is 6. This is because the distance calculation by limited
number of PEs became the bottleneck; the commands from
the host pends since there is no availabe PE. Such problem
can be addressed by having more EPs in the system, thereby
distributing the computation load. As we can see in the ﬁgure,when we double the number of EPs in the network, we can
improve the QPS when we have 6 CXL hosts in the system.
7 Discussion and Acknowledgments
GPU-based distance calculation. Recent research has be-
gun to leverage the massive parallel processing capabilities
of GPUs to enhance the efﬁciency of graph-based ANNS
services [ 58,59]. While GPUs generally exhibit high perfor-
mance, our argument is that it’s not feasible for CPU+GPU
memory to handle the entirety of ANNS data and tasks, as
detailed in Section 1. Even under the assumption that ANNS
is functioning within an optimal in-memory computing envi-
ronment, there are two elements to consider when delegating
distance computation to GPUs. The ﬁrst point is that GPUs
require interaction with the host’s software and/or hardware
layers, which incurs a data transfer overhead for computation.
Secondly, ANNS distance computations can be carried out us-
ing a few uncomplicated, lightweight vector processing units,
making GPUs a less cost-efﬁcient choice for these distance
calculation tasks.
In contrast, CXL-ANNS avoids the burden of data move-
ment overhead, as it processes data in close proximity to its
actual location and returns only a compact result set. This
approach to data processing is well established and has been
validated through numerous application studies [ 48,60–66].
Moreover, CXL-ANNS effectively utilizes the cache hierar-
chy and can even decrease the frequency of accesses to the
underlying CXL memory pool. It accomplishes this through
its CXL-aware and ANNS-aware prefetching scheme, which
notably enhances performance.
Acknowledgments. The authors thank anonymous reviewers
for their constructive feedback as well as Panmnesia for their
technical support. The authors also thank Sudarsun Kannan
for shepherding this paper. This work is supported by Panm-
nesia and protected by one or more patents. Myoungsoo Jung
is the corresponding author (mj@camelab.org)
8 Conclusion
We propose CXL-ANNS, a software-hardware collaborative
approach for scalable ANNS. CXL-ANNS places all the
dataset into its CXL memory pool to handle billion-point
graphs while making the performance of the kNN search com-
parable with that of the (local-DRAM only) oracle system. To
this end, CXL-ANNS considers inter-node relationship and
performs ANNS-aware prefetches. It also calcualate distances
in its EP while scheduling the ANNS subtasks to utilize all
the resources in the CXL network. Our empirical results show
that CXL-ANNS exhibits 111.1 ⇥better performance com-
pared to the state-of-the-art billion-scale ANNS methods and
3.8⇥better performance than oracle system, respectively.
596    2023 USENIX Annual Technical Conference USENIX AssociationReferences
[1]Rihan Chen, Bin Liu, Han Zhu, Yaoxuan Wang, Qi Li,
Buting Ma, Qingbo Hua, Jun Jiang, Yunlong Xu,
Hongbo Deng, et al. Approximate nearest neighbor
search under neural similarity metric for large-scale rec-
ommendation. In Proceedings of the 31st ACM Interna-
tional Conference on Information & Knowledge Man-
agement (CIKM) , 2022.
[2]Yanhao Zhang, Pan Pan, Yun Zheng, Kang Zhao, Yingya
Zhang, Xiaofeng Ren, and Rong Jin. Visual search at
alibaba. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining (KDD) , 2018.
[3]Jianjin Zhang, Zheng Liu, Weihao Han, Shitao Xiao,
Ruicheng Zheng, Yingxia Shao, Hao Sun, Hanqing
Zhu, Premkumar Srinivasan, Weiwei Deng, et al. Uni-
retriever: Towards learning the uniﬁed embedding based
retriever in bing sponsored search. In Proceedings of
the 28th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD) , 2022.
[4]Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin,
Peng Xu, Shengjun Li, Xiangyu Wang, Xiangzhou Guo,
Chengming Li, Xiaohai Xu, et al. Milvus: A purpose-
built vector data management system. In Proceedings
of the 2021 International Conference on Management
of Data (SIGMOD) , 2021.
[5]Minjia Zhang and Yuxiong He. Grip: Multi-store
capacity-optimized high-performance nearest neighbor
search for vector search engine. In Proceedings of the
28th ACM International Conference on Information and
Knowledge Management (CIKM) , 2019.
[6]Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia,
David Zhang, Philip Pronin, Janani Padmanabhan,
Giuseppe Ottaviano, and Linjun Yang. Embedding-
based retrieval in facebook search. In Proceedings of
the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining (KDD) , 2020.
[7]Jiawen Liu, Zhen Xie, Dimitrios Nikolopoulos, and
Dong Li. RIANN: Real-time incremental learning
with approximate nearest neighbor on mobile devices.
In2020 USENIX Conference on Operational Machine
Learning (OpML 20) , 2020.
[8]Chieh-Jan Mike Liang, Hui Xue, Mao Yang, Lidong
Zhou, Lifei Zhu, Zhao Lucis Li, Zibo Wang, Qi Chen,
Quanlu Zhang, Chuanjie Liu, et al. Autosys: The de-
sign and operation of learning-augmented systems. In
Proceedings of the 2020 USENIX Conference on Usenix
Annual Technical Conference , pages 323–336, 2020.[9]Vincent Garcia, Eric Debreuve, and Michel Barlaud.
Fast k nearest neighbor search using gpu. In 2008 IEEE
Computer Society Conference on Computer Vision and
Pattern Recognition Workshops . IEEE, 2008.
[10] Yu A Malkov and Dmitry A Yashunin. Efﬁcient and
robust approximate nearest neighbor search using hierar-
chical navigable small world graphs. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2018.
[11] Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al.
Similarity search in high dimensions via hashing. In
VLDB , 1999.
[12] Pandu Nayak. Understanding searches better than ever
before. https://blog .google/products/search/
search-language-understanding-bert/ , 2019.
[13] Charlie Waldburger. As search needs evolve, microsoft
makes ai tools for better search available to researchers
and developers. https://news .microsoft .com/
source/features/ai/bing-vector-search/ ,
2019.
[14] Roger Weber, Hans-Jörg Schek, and Stephen Blott.
A quantitative analysis and performance study for
similarity-search methods in high-dimensional spaces.
InVLDB , 1998.
[15] Piotr Indyk and Rajeev Motwani. Approximate near-
est neighbors: towards removing the curse of dimen-
sionality. In Proceedings of the thirtieth annual ACM
symposium on Theory of computing , 1998.
[16] Sunil Arya, David M Mount, Nathan S Netanyahu, Ruth
Silverman, and Angela Y Wu. An optimal algorithm
for approximate nearest neighbor searching ﬁxed dimen-
sions. Journal of the ACM (JACM) , 45(6), 1998.
[17] Ting Liu, Andrew Moore, Ke Yang, and Alexander Gray.
An investigation of practical approximate nearest neigh-
bor algorithms. In L. Saul, Y. Weiss, and L. Bottou,
editors, Advances in Neural Information Processing Sys-
tems (NIPS) , 2004.
[18] Harsha Simhadri. Research talk: Approximate
nearest neighbor search systems at scale. https:
//www .youtube .com/watch?v=BnYNdSIKibQ&list=
PLD7HFcN7LXReJTWFKYqwMcCc1nZKIXBo9&index=9 ,
2021.
[19] Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai.
Fast approximate nearest neighbor search with the navi-
gating spreading-out graph. Proceedings of the VLDB
Endowment , 2019.
[20] Herve Jegou, Matthijs Douze, and Cordelia Schmid.
Product quantization for nearest neighbor search. IEEE
USENIX Association 2023 USENIX Annual Technical Conference    597Transactions on Pattern Analysis and Machine Intelli-
gence , 2010.
[21] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,
David Simcha, Felix Chern, and Sanjiv Kumar. Ac-
celerating large-scale inference with anisotropic vector
quantization. In International Conference on Machine
Learning (ICML) , 2020.
[22] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Op-
timized product quantization for approximate nearest
neighbor search. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) ,
2013.
[23] Artem Babenko and Victor Lempitsky. The inverted
multi-index. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2014.
[24] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vard-
han Simhadri, Ravishankar Krishnawamy, and Rohan
Kadekodi. Diskann: Fast accurate billion-point near-
est neighbor search on a single node. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information
Processing Systems (NeurIPS) , 2019.
[25] Jie Ren, Minjia Zhang, and Dong Li. Hm-ann: Efﬁcient
billion-point nearest neighbor search on heterogeneous
memory. Advances in Neural Information Processing
Systems (NeurIPS) , 2020.
[26] Aditi Singh, Suhas Jayaram Subramanya, Ravis-
hankar Krishnaswamy, and Harsha Vardhan Simhadri.
Freshdiskann: A fast and accurate graph-based ANN
index for streaming similarity search. arXiv preprint
arXiv:2105.09613 , 2021.
[27] Siddharth Gollapudi, Neel Karia, Varun Sivashankar,
Ravishankar Krishnaswamy, Nikit Begwani, Swapnil
Raz, Yiyong Lin, Yin Zhang, Neelam Mahapatro,
Premkumar Srinivasan, et al. Filtered-diskann: Graph
algorithms for approximate nearest neighbor search with
ﬁlters. In Proceedings of the ACM Web Conference 2023
(WWW 23) , 2023.
[28] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li,
Chuanjie Liu, Zengzhong Li, Mao Yang, and Jingdong
Wang. Spann: Highly-efﬁcient billion-scale approxi-
mate nearest neighbor search. In 35th Conference on
Neural Information Processing Systems (NeurIPS 2021) ,
2021.
[29] Jason Lowe-Power, Abdul Mutaal Ahmad, Ayaz Akram,
Mohammad Alian, Rico Amslinger, Matteo Andreozzi,
Adrià Armejach, Nils Asmussen, Brad Beckmann,
Srikant Bharadwaj, et al. The gem5 simulator: Version
20.0+. arXiv preprint arXiv:2007.03152 , 2020.[30] Harsha Vardhan Simhadri, George Williams, Martin
Aumüller, Matthijs Douze, Artem Babenko, Dmitry
Baranchuk, Qi Chen, Lucas Hosseini, Ravishankar Kr-
ishnaswamny, Gopal Srinivasa, et al. Results of the
neurips’21 challenge on billion-scale approximate near-
est neighbor search. In NeurIPS 2021 Competitions and
Demonstrations Track . PMLR, 2022.
[31] Sunil Arya and David M Mount. Approximate nearest
neighbor queries in ﬁxed dimensions. In Proceedings of
the fourth annual ACM-SIAM symposium on Discrete
algorithms (SODA) , 1993.
[32] Marius Muja and David G Lowe. Scalable nearest neigh-
bor algorithms for high dimensional data. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence ,
2014.
[33] Jingdong Wang, Naiyan Wang, You Jia, Jian Li, Gang
Zeng, Hongbin Zha, and Xian-Sheng Hua. Trinary-
projection trees for approximate nearest neighbor search.
IEEE transactions on pattern analysis and machine in-
telligence , 36(2):388–403, 2013.
[34] Qiang Huang, Jianlin Feng, Yikai Zhang, Qiong Fang,
and Wilfred Ng. Query-aware locality-sensitive hashing
for approximate nearest neighbor search. Proceedings
of the VLDB Endowment , 2015.
[35] Yifang Sun, Wei Wang, Jianbin Qin, Ying Zhang, and
Xuemin Lin. Srs: solving c-approximate nearest neigh-
bor queries in high dimensional euclidean space with a
tiny index. Proceedings of the VLDB Endowment , 2014.
[36] Martin Aumüller, Erik Bernhardsson, and Alexander
Faithfull. Ann-benchmarks: A benchmarking tool for
approximate nearest neighbor algorithms. In Interna-
tional conference on similarity search and applications .
Springer, 2017.
[37] Karima Echihabi, Kostas Zoumpatianos, Themis Pal-
panas, and Houda Benbrahim. Return of the lernaean
hydra: experimental evaluation of data series approxi-
mate similarity search. Proceedings of the VLDB En-
dowment , 2019.
[38] Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxi-
ang Wang. A comprehensive survey and experimental
comparison of graph-based approximate nearest neigh-
bor search. arXiv preprint arXiv:2101.12631 , 2021.
[39] Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Mingjie Li,
Wenjie Zhang, and Xuemin Lin. Approximate nearest
neighbor search on high dimensional data—experiments,
analyses, and improvement. IEEE Transactions on
Knowledge and Data Engineering , 2019.
598    2023 USENIX Annual Technical Conference USENIX Association[40] Kiana Hajebi, Yasin Abbasi-Yadkori, Hossein Shahbazi,
and Hong Zhang. Fast approximate nearest-neighbor
search with k-nearest neighbor graph. In Twenty-Second
International Joint Conference on Artiﬁcial Intelligence
(IJCAI) , 2011.
[41] CXL Consortium. Compute ex-
press link 3.0 white paper. https://
www.computeexpresslink .org/_files/ugd/
0c1418_a8713008916044ae9604405d10a7773b .pdf,
2022.
[42] Huaicheng Li, Daniel S Berger, Lisa Hsu, Daniel Ernst,
Pantea Zardoshti, Stanko Novakovic, Monish Shah,
Samir Rajadnya, Scott Lee, Ishwar Agarwal, et al. Pond:
Cxl-based memory pooling systems for cloud platforms.
InProceedings of the 28th ACM International Confer-
ence on Architectural Support for Programming Lan-
guages and Operating Systems , 2023.
[43] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Jo-
hannes Weiner, Niket Agarwal, Pallab Bhattacharya,
Chris Petersen, Mosharaf Chowdhury, Shobhit Kanaujia,
and Prakash Chauhan. Tpp: Transparent page placement
for cxl-enabled tiered-memory. In Proceedings of the
28th ACM International Conference on Architectural
Support for Programming Languages and Operating
Systems, Volume 3 , pages 742–755, 2023.
[44] Intel. Optane ssd 9 series. https://www .intel .com/
content/www/us/en/products/details/memory-
storage/consumer-ssds/optane-ssd-9-
series .html , 2021.
[45] Michael Anderson, Benny Chen, Stephen Chen, Summer
Deng, Jordan Fix, Michael Gschwind, Aravind Kalaiah,
Changkyu Kim, Jaewon Lee, Jason Liang, et al. First-
generation inference accelerator deployment at face-
book. arXiv preprint arXiv:2107.04140 , 2021.
[46] Jeffrey Dean and Luiz André Barroso. The tail at scale.
Communications of the ACM , 2013.
[47] Guohao Dai, Tianhao Huang, Yuze Chi, Ningyi Xu,
Yu Wang, and Huazhong Yang. Foregraph: Explor-
ing large-scale graph processing on multi-fpga archi-
tecture. In Proceedings of the 2017 ACM/SIGDA In-
ternational Symposium on Field-Programmable Gate
Arrays (FPGA) , 2017.
[48] Junwhan Ahn, Sungpack Hong, Sungjoo Yoo, Onur
Mutlu, and Kiyoung Choi. A scalable processing-in-
memory accelerator for parallel graph processing. In
Proceedings of the 42nd Annual International Sympo-
sium on Computer Architecture (ISCA) , 2015.[49] Shaﬁur Rahman, Nael Abu-Ghazaleh, and Rajiv Gupta.
Graphpulse: An event-driven hardware accelerator for
asynchronous graph processing. In 2020 53rd Annual
IEEE/ACM International Symposium on Microarchitec-
ture (MICRO) . IEEE, 2020.
[50] Mikkel Thorup. Undirected single-source shortest paths
with positive integer weights in linear time. J. ACM ,
46(3):362–394, 1999.
[51] Andy Diwen Zhu, Xiaokui Xiao, Sibo Wang, and Wen-
qing Lin. Efﬁcient single-source shortest path and
distance queries on large graphs. In The 19th ACM
SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, KDD 2013 . ACM, 2013.
[52] Linaro. The devicetree speciﬁcation. .https://
www.devicetree .org/ .
[53] Inc UEFI Forum. Advanced conﬁguration and power
interface (acpi) speciﬁcation version 6.4. https://
uefi .org/specs/ACPI/6 .4/, 2021.
[54] Kenneth C. Knowlton. A fast storage allocator. Com-
munications of the ACM , 1965.
[55] Christopher Celio, Pi-Feng Chiu, Borivoje Nikolic,
David A Patterson, and Krste Asanovic. Boomv2: an
open-source out-of-order risc-v core. In First Work-
shop on Computer Architecture Research with RISC-V
(CARRV) , 2017.
[56] Herve Jegou, Matthijs Douze, Jeff Johnson, Lu-
cas Hosseini, Chengqi Deng, and Alexandr Guzhva.
Faiss. https://github .com/facebookresearch/
faiss , 2018.
[57] Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim
Naumov, Brandon Reagen, David Brooks, Bradford Cot-
tel, Kim Hazelwood, Mark Hempstead, Bill Jia, et al.
The architectural implications of facebook’s dnn-based
personalized recommendation. In 2020 IEEE Inter-
national Symposium on High Performance Computer
Architecture (HPCA) , pages 488–501. IEEE, 2020.
[58] Weijie Zhao, Shulong Tan, and Ping Li. Song: Approxi-
mate nearest neighbor search on gpu. In 2020 IEEE 36th
International Conference on Data Engineering (ICDE) .
IEEE, 2020.
[59] Yuanhang Yu, Dong Wen, Ying Zhang, Lu Qin, Wen-
jie Zhang, and Xuemin Lin. Gpu-accelerated proximity
graph approximate nearest neighbor search and construc-
tion. In 2022 IEEE 38th International Conference on
Data Engineering (ICDE) . IEEE, 2022.
[60] Miryeong Kwon, Donghyun Gouk, Sangwon Lee, and
Myoungsoo Jung. Hardware/software co-programmable
USENIX Association 2023 USENIX Annual Technical Conference    599framework for computational SSDs to accelerate deep
learning service on large-scale graphs. In 20th USENIX
Conference on File and Storage Technologies (FAST 22) ,
2022.
[61] Youngeun Kwon, Yunjae Lee, and Minsoo Rhu. Tensor-
dimm: A practical near-memory processing architecture
for embeddings and tensor operations in deep learning.
InProceedings of the 52nd Annual IEEE/ACM Inter-
national Symposium on Microarchitecture (MICRO) ,
2019.
[62] Miryeong Kwon, Junhyeok Jang, Hanjin Choi, Sangwon
Lee, and Myoungsoo Jung. Failure tolerant training with
persistent memory disaggregation over cxl. IEEE Micro ,
2023.
[63] Jun Heo, Seung Yul Lee, Sunhong Min, Yeonhong Park,
Sung Jun Jung, Tae Jun Ham, and Jae W Lee. Boss:
Bandwidth-optimized search accelerator for storage-
class memory. In 2021 ACM/IEEE 48th Annual Inter-
national Symposium on Computer Architecture (ISCA) .
IEEE, 2021.
[64] Liu Ke, Udit Gupta, Benjamin Youngjae Cho, David
Brooks, Vikas Chandra, Utku Diril, Amin Firoozshahian,
Kim Hazelwood, Bill Jia, Hsien-Hsin S Lee, et al. Rec-
nmp: Accelerating personalized recommendation with
near-memory processing. In 2020 ACM/IEEE 47th An-
nual International Symposium on Computer Architec-
ture (ISCA) . IEEE, 2020.
[65] Nika Mansouri Ghiasi, Jisung Park, Harun Mustafa,
Jeremie Kim, Ataberk Olgun, Arvid Gollwitzer, Damla
Senol Cali, Can Firtina, Haiyu Mao, Nour Almad-
houn Alserr, et al. Genstore: a high-performance in-
storage processing system for genome sequence anal-
ysis. In Proceedings of the 27th ACM International
Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS) , 2022.
[66] Joel Nider, Craig Mustard, Andrada Zoltan, John Rams-
den, Larry Liu, Jacob Grossbard, Mohammad Dashti,
Romaric Jodin, Alexandre Ghiti, Jordi Chauzi, et al. A
case study of processing-in-memory in off-the-shelf sys-
tems. In USENIX Annual Technical Conference (ATC) ,
2021.
600    2023 USENIX Annual Technical Conference USENIX AssociationCXL and the Return of Scale-Up Database Engines
Alberto Lerner
eXascale Infolab
University of Fribourg, Switzerland
alberto.lerner@unifr.chGustavo Alonso
Systems Group, Department of Computer Science
ETH Zurich, Switzerland
alonso@inf.ethz.ch
ABSTRACT
The growing trend towards specialization has led to a proliferation
of accelerators and alternative processing devices. When embedded
in conventional computer architectures, the PCIe link connecting
the CPU to these devices becomes a bottleneck. Several proposals
for alternative designs have been put forward, with these e ￿orts
having now converged into the Compute Express Link (CXL) spec-
i￿cation. CXL is an interconnect protocol on top of PCIe with a
more modern and powerful interface. While still on version 1.0 in
terms of commercial availability, the potential of CXL to radically
change the underlying architecture has already attracted consid-
erable attention. This attention has been focused mainly on the
possibility of using CXL to build a shared memory system among
the machines in a rack. We argue, however, that such bene ￿ts are
just the beginning of more signi ￿cant changes that will have a ma-
jor impact on database engines and data processing systems. In a
nutshell, while the cloud favored scale-out approaches, CXL brings
back scale-up architectures. In the paper we describe how CXL
enables such architectures, and the research challenges associated
with the emerging scale-up, heterogeneous hardware platforms.
Keywords: Database Engines, Memory Management, Heterogenous
Computing, CXL.
1 INTRODUCTION
The cloud, demanding applications, and an ever larger amount
of data are driving an increasing specialization in hardware [ 37].
Nowadays, many use cases rely on a wide variety of processors
other than the CPU: smart NICs, FPGAs, GPUs, TPUs, DPUs, etc.
In these settings, the conventional CPU-centric architecture is sub-
optimal. Data movement is one of the most expensive operations in
a data center [ 11,12], often the CPU is the least powerful element
[22,44], and the interconnect becomes a major bottleneck [ 19,41].
Parallel to these developments, the cloud has become one of
the bigger market drivers for computing hardware. Consequently,
many current hardware developments are focused on the needs
of cloud providers such as disaggregation, cost e ￿cient use of re-
sources (CPU, memory, storage), and the particular requirements of
running virtualized environments. In such settings, the limitations
mentioned above become even more acute, e.g., the bottleneck cre-
ated by many VMs running on the same machine trying to access a
peripheral or an accelerator (the NIC, the GPU, the local disk, etc.).
This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by
emailing the authors. Copyright is held by the owner/author(s).
Preprint, under review.The last years have seen several competing proposals to im-
prove both the interconnect bandwidth issue as well as the many
architectural imbalances in the cloud: CCIX [ 9], Gen-Z [ 14], Open-
CAPI [ 27], and the Compute Express Link (CXL) [ 31]. Today, all
these speci ￿cations have been merged under the CXL umbrella,
which has become the de-facto standard for future interconnects.
However, CXL is much more than just an interconnect protocol
that physically builds on top of PCIe. CXL supports three classes of
interfaces. The I/O interface extends the capabilities of PCI without
major changes to its semantics (essentially, copying memory re-
gions from one device to another in a non-coherent manner). The
memory interface allows the CPU (host) to coherently access the
memory or storage of peripheral devices. And the cache interface
allows peripheral devices to coherently access and cache data from
the memory of the CPU.
The last two interfaces represent a signi ￿cant change in architec-
ture from the last decades. Cache (memory) coherency allows many
agents to access memory collectively so that no agent misses each
other’s memory modi ￿cations. However, cache coherency meth-
ods are mostly proprietary technologies of CPU vendors and the
application or the operating system cannot interact with it in any
manner. Furthermore, no agents other than CPU cores can partici-
pate in the protocol, thereby establishing coherency domains as clear
boundaries outside which memory cannot be accessed collectively.
This is one of the big architectural barriers that CXL removes,
enabling what are called Type 2 devices (e.g., CPUs, GPUs, or FPGAs)
to directly access the host and each other’s memory in a coherent
fashion. Big as this step is, it not the only one. With version 3.0,
CXL goes well beyond the traditional role of an interconnect and
becomes a rack level networking fabric that is both more performant
than current Ethernet based systems (CXL 3.0 claims 64 GBytes/s
bandwidth versus 100 or 200 Gbits/s in data centers) as well as
more powerful in terms of its interface (coherent access, memory
sharing, encryption, etc., versus simple reliable packet sending and
receiving in TCP/IP or data copying in RDMA).
In this paper we focus on this capability of CXL and discuss
how it enables scale-up (or vertically scaled) database engines that
will signi ￿cantly di ￿er from the scale-out (horizontally scaled) sys-
tems that have dominated the landscape in the last years due to
the constraints imposed by cloud architectures. We ￿rst provide
a brief introduction to CXL and then discuss in detail new archi-
tectural possibilities enabled by novel CXL features as well as the
many challenges that will have to be addressed before they can be
exploited to full advantage.arXiv:2401.01150v1  [cs.DB]  2 Jan 2024Alberto Lerner and Gustavo Alonso
ServerPCIePeripheralmemorycontrollercachecontrollerdirectorycontrollermemorycontroller
ServerCXL
Peripheralmemorycontrollercachecontrollerdirectorycontrollermemorycontrollercachecontroller
cxl.memcxl.cacheCPU coherencydomain
extended coherencydomain
PCIe MRd andMWr transactionsFigure 1: (left) Peripherals connected using PCIe cards are outside the coherency domain even if they contain memory. (right)
CXL extends the coherency domain by allowing the peripheral to communicate with the directory controller.
2 BACKGROUND AND MOTIVATION
In this section, we introduce CXL and its basic mechanisms and
discuss the fundamental changes these mechanisms allow.
2.1 CXL Status
CXL is a public consortium lead by Intel. It has been evolving
quickly since its inception in 2019, and so far, there have been
four major spec releases: 1.1, 2.0, 3.0, and 3.1. The 1.1 version is
centered around local memory expansion, i.e., allowing a server
to access more memory than available in its DIMM slots. Release
2.0 introduced basic forms of CXL interconnects (e.g., switches),
so that memory in a remote chassis is accessible, too, and support
for memory pooling. A server can aggregate memory from several
expanders and vice-versa. Releases 3.0 and 3.1 added support for
more sophisticated networking and for sharing memory across
multiple servers and peripherals, rather than the latter expanding
memory of the former.
At the time of writing, the CXL hardware available is overwhelm-
ingly based on version 1.1, although some vendors have already
adopted selected features of 2.0 (in what some refer to informally
as 1.1+). Versions 3.0 and 3.1 require improvements in PCIe, which
will be accomplished in the already rati ￿ed Gen 6 of that protocol.
Presently, one can hardly avoid CXL since all major x86server class
CPU vendors have adopted it.
2.2 Multicores and Memory Coherency
A multicore server is a distributed system where CPU cores share
the server’s memory. Each core can cache data in small blocks, called
cache lines , occasionally duplicating that data. Naturally, issues of
data consistency may arise. Cache coherency avoids the issues by
maintaining two invariants: (1) writes to the same memory location
are serialized; and (2) every write is eventually made visible to all
cores [ 34]. This means that if a core wishes to modify a cached line,
all other copies residing in di ￿erent cores must be invalidated ￿rst.
Therefore, the system keeps track of the state in which each cached
line is. Read-only copies are deemed Shared (S), while invalidated
copies are called Invalid (I). A unique copy is called Exclusive (E)
while it is still intact, or Modi ￿ed(M) after its contents have been
updated w.r.t. main memory. Other states may exist, but these four
are the most common. Often, coherency protocols announce the
states they use. What we described above is a MESI protocol, which
is what CXL uses.
In current computers, only caching done by CPU cores is tracked.
We say that the coherency domain extends only to the CPU. Data
copies outside this domain are not coherent in the sense that the
hardware will not keep track of whether that copy is exclusive.
Therefore, a con ￿ict may arise if one updates it.2.3 Coherent and Non-Coherent Data Transfers
Non-coherent transfers occur between servers and peripherals
through PCIe exchanges called transactions . A transaction is a se-
quence of messages in the context of a read or a write operation
between a server’s and a peripheral’s memory. Figure 1 (left) shows
how this exchange is done vie PCIe. Note that the communication
takes place directly between memory controllers.
In contrast, coherent transfers between CPU cores and memory
are not direct. They involve additional components that track the
information necessary to maintain coherency. To load a line, a CPU
core uses a local Cache Controller (CC). In turn, the CC noti ￿es
a (socket central) component, called a Directory Controller (DC),
of the intent to access that line. The DC keeps track of all lines
currently cached and can evaluate if, to maintain the invariants,
this new request should trigger any cache invalidation. Only after
the invalidations are performed can the request be forwarded to the
memory controller. Figure 1 (left) also depicts this process, showing
that the peripherals are not on the coherency domain.
CXL extends the type of transactions that can occur between
servers and peripherals. It is backward compatible with PCIe trans-
actions, which it wraps under a sub-protocol called cxl.io , and it
adds two other sub-protocols, cxl.mem andcxl.cache . The server
uses the former to communicate with the peripheral’s memory
controller as if it resided locally on its motherboard. The periph-
eral uses the latter sub-protocol to cache contents managed by the
server’s directory controller. Through these two sub-protocols, any
memory the device o ￿ers can be seamlessly incorporated into the
overall system, and the device can cache data that lives in the sys-
tem’s memory. Figure 1 (right) depicts this scenario. Due to space
constraints, this paper will focus on devices that only use cxl.io
andcxl.mem , called ( Type 3 ) devices. Devices such as CPUs, GPUs,
or FPGAs ( Type 2 ) use both cxl.mem andcxl.cache , and some
devices use cxl.cache only ( Type 1 ).
2.4 CXL Performance Characterization
Current conventional NUMA servers typically comprise two sock-
ets, each containing a CPU and half of the system’s DRAM. Such
tight coupling of CPU and memory is problematic as it prevents
the two sides to be scaled independently. CXL can be used to add
coherent memory without having to increase the number of sockets
in the machine. Storage vendors such as Micron and Samsung are
launching a new device category called Memory Expander [23,30].
A memory expander allows a server to map more memory than
the DRAM DIMMs (memory slots) it carries on its motherboard
by simply plugging in what looks like a special type of SSD device
that carries DRAM memory instead. Memory expanders not onlyCXL and the Return of Scale-Up Database Engines
increase DRAM size, they also improve memory bandwidth because
they e ￿ectively add memory controllers to the system.
Preliminary benchmarks that use actual CXL implementations
are already available [ 35]. Regarding latency, executing a load in-
struction against a given type of CXL-attached memory can be 35%
longer than the equivalent NUMA memory access. Executing a
store under the same conditions can present slightly lower but
equivalent overheads. Regarding bandwidth, it helps to measure the
e￿ciency of the transfer, i.e., what percentage of the nominal mem-
ory bandwidth capacity can be achieved. Transfers from NUMA
nodes can be 70% e ￿cient when considering only load s, compared
to 46% e ￿ciency when reading the same type of memory through
a CXL interconnect. Curiously, store s can be more bandwidth ef-
￿cient if directed to a CXL device than a neighbor NUMA socket,
12% higher. The reason is that stores to a CXL device can bypass
several coherency checks that must occur on a NUMA node.
These micro-benchmark results have been complemented with
more end-to-end studies. In a recent article by Meta [ 21], CXL mem-
ory is used to store cold pages with the operating system swapping
pages back and forth between the host DRAM and the CXL mem-
ory. In essence, CXL memory is used as an additional memory tier
between the DRAM and storage. The results suggest that the band-
width available from CXL memory will be around 64 GB/s with
latency only slightly larger than that of NUMA memory. Similarly,
a study from Microsoft analyzed the impact of CXL memory for
their cloud environment [ 18]. While the results di ￿er from work-
load to workload, the study found that under the expected latency
increases, some 26% of the 158 workloads studied show less than
1% performance penalty due to it, and an additional 17% show less
than 5%. In contrast, 21% of the workloads were a ￿ected by more
than 25% of performance decrease. For database workloads, specif-
ically TPC-H, the overheads are highly query-dependent but are
mostly below 20%. This analysis already considers CXL switches
and shared memory among a large number of machines in a rack.
2.5 CXL as Networking
The studies above suggest another disruptive change that CXL
can bring about [ 18,21,35]. CXL interconnects—fabrics carrying
coherency tra ￿c across servers—are expected to be signi ￿cantly
more e ￿cient than traditional networks, even RDMA based ones.
The reason is that RDMA requires conversions between the In ￿ni-
band/RoCE protocols and the PCIe protocol. In other words, a NIC
uses PCIe to talk to its host but In ￿niband/RoCE is use to interact
with the rest of the system. In contrast, all tra ￿c in CXL is already
carried by PCIe-compatible messages.
Some studies have attempted to quantify these bene ￿ts [15].
The latency of CXL communications was found to be in the three
hundred nanoseconds range, while the fastest exchanges in RDMA
take at least 2.7 microseconds—a di ￿erence of 8.3 ⇥. The study also
hints at a structural advantages that gives CXL more bandwidth
than traditional networking: NICs are sub-dimensioned w.r.t. the
number of PCIe lanes they occupy. For instance, a 400 Gbps NIC
(50 GB/s) will occupy 16 PCIe Gen5 lanes that, in the aggregate,
can o ￿er 64 GB/s [ 24]. Put di ￿erently, over 20% of the available
PCIe bandwidth does not translate into network bandwidth. This
discrepancy has been consistent through di ￿erent network speedsand will likely remain the same for 800 Gbps and 1.6 Tbps NICs
when PCIe Gen 6 and 7 servers are available.
We note that with CXL versions 3.0 and 3.1 the networking works
at a peer-to-peer level as well. Two peer peripherals can access each
other’s memory independently of the server. Most importantly, CXL
3.1 brings the concept of Global Integrated Memory . This feature
allows several servers and peripherals to contribute a region of
memory mapped globally, i.e., many servers and peripherals share
the same area. For those reasons, CXL is poised to become a much
better way to connect CPUs, memory, and storage, at least on a rack
level. CXL strength is allowing coherency domains that encompass
full racks.
3 SHARED MEMORY ARCHITECTURES
In this section we discuss how CXL memory could change the
architecture of database engines and the advantages it would bring.
3.1 Single Machine Memory Expansion
Databases maintain a pool of shared memory available to the
threads where queries or transactions run. The so called Bu￿er
Cache orBu￿er Pool is used as an intermediate stage and cache
between persistent storage and the private space used by every
processing thread. In the Bu ￿er Pool the data is stored and read by
the processing threads, with the engine implementing diverse cache
replacement policies to minimize the number of page misses and
the tra ￿c to storage. This architecture makes sense in an engine
that runs on a single machine. However, it is not without problems:
the working space of every thread, the storage needed for indexes
and auxiliary data structures, and the memory used by the engine
all compete for space with the Bu ￿er Pool.
A simple way to exploit CXL memory follows the tiered memory
approach pursued by Meta and mentioned above [ 21]. The con-
￿guration used is shown in Figure 2(a) where CXL exposes the
memory from a peripheral device (in this case a memory device)
and seamlessly integrates it with the server’s memory. In such a set
up, the CXL memory is logically allocated above the local DRAM
memory and pages are swapped back and forth as needed. The idea
is the same as with a disk but using DRAM and faster interconnect
which provides much lower latencies. In a database engine, the
paging system could be adapted to the needs of data processing
instead or relying on general purpose page replacement policies [ 8].
In general, CXL used in this way brings the bene ￿ts and challenges
of a deeper memory hierarchy and existing techniques should be
revisited accordingly [20, 33].
While a deeper memory hierarchy would facilitate adoption,
CXL memory does not need to be treated as block device. More
interesting con ￿gurations arise when the local DRAM is used to,
e.g., store indexes for fast traversal with the data actually in the
CXL memory where much more capacity can be provided and the
data can be accessed on a tuple basis rather than as a block.
Another important aspect that is often overlooked is that CXL
memory frees up the system from inter-dependencies. In existing
CPUs, the type of DIMM supported is tied to the CPU architecture.
Also, the socket organization of CPUs requires memory capacities
proportional to the number of sockets and cores. CXL memory
removes all these constraints. In terms of capacity, the memory onAlberto Lerner and Gustavo Alonso
Buffer Area
ServerCXL
Peripheral
Buffer Area
ServerCXL
RemotePeripheral
CXLBuffer MgrBuffer MgrDisaggregated Rack
CXL
(a)(b)(c)Figure 2: CXL allows progressive levels of desegregation: (a) local and (b) far memory expansion, and (c) full-rack disaggregation.
CXL is not tied to the CPU architecture so one could add DIMMs
di￿erent from those used in the CPU. It can also be used to simply
add more memory capacity independently of the number of cores.
Doing so o ￿ers more ￿exible options to con ￿gure the database by
varying the proportion of cores to memory.
Finally, main memory databases and hybrid transactional/analytic
systems could greatly bene ￿t from the architecture. An interesting
con￿guration to explore would be to place the transactional work-
load on the local DRAM and use CXL memory for the analytic part,
also providing space for more data as well as structures like data
cubes, materialized tables, de-normalized tables, etc., without that
a￿ecting the transactional workload in any way.
From these ideas, several research directions open up:
•How should the memory extension be handled? As a block device
with pagination or as addressable memory just like DRAM on a
CPU socket? The former makes integration easier but the latter
is likely to being more performance and design opportunities.
•Is the memory expansion fast enough for OLTP or will be suitable
mainly for OLAP? Can it be used to perform both on the same
machine and what are the implications?
•What data structures should be kept in the local memory and
which ones on the memory expansion? Are these data structures
suitable for the increases non-uniformity in memory accesses
that CXL memory creates?
3.2 Disaggregated Memory
The limitations imposed by the memory available in a given ma-
chine have been recognized long ago. In the cloud, the problem
becomes worse due to what has been called stranded memory : the
fact that, in the cloud, a machine often runs out of virtual CPUs to
rent before it runs out memory, resulting in parts of its DRAM being
unused. Since memory is one of the most expensive components in
today’s data centers, this is a major source of ine ￿ciencies [18].
In the context of database engines, how much main memory to
allocate to the engine is crucial for performance purposes. In the
cloud this is made worse by the fact that storage is disaggregated
and swapping of pages involves networking. To minimize this e ￿ect,
many cloud providers support special con ￿gurations for databases
using network attached block storage and disks that are much
faster than those typically used for conventional applications—of
course, with the corresponding increase in price as these services
are costlier than typical cloud systems. The network overhead and
the slow storage needed (for economic reasons) when the data is
very large has led over time to additional layers in the system such
as main memory caches and even specialized solutions with SSDs
and FPGAs directly attached to the network [4].Recently, researchers have starting exploring ways to provide
additional DRAM memory to a sever without necessarily attach-
ing it to this server also in the context of databases [ 2,16,40,43].
The ideas has led to the notions of remote memory ,far memory , or
disaggregated memory . This is implemented in di ￿erent ways. Far
memory is a term typically used to refer to any generic con ￿gu-
ration where the additional DRAM is not local. Remote memory
generally refers to utilizing unused memory of other machines.
Disaggregated memory is memory not allocated to any machine
but available for servers to use. In most cases, the data is exchanged
through RDMA and the remote memory is treated as a block device
with a paging mechanism to move data back and forth between the
host and the far memory.
As Figure 2(b) shows, a CXL peripheral can be remote, i.e., it
can sit outside the server’s chassis and use the CXL interconnect
for integration. This feature allows a server to pool memory from
several CXL devices and, conversely, for a remote device to carve its
memory across di ￿erent servers. In this way, CXL enables disaggre-
gated memory with the advantage of the memory being coherent
which is not the case when disaggregated memory is implemented
through the network. Through the addition of multi-level switches
to CXL, CPUs on di ￿erent machines can access a central memory
expander containing a pool of memory available to all machines
in a rack, very much like disaggregated storage is available in the
cloud. Such an approach allows to create a large scale memory pool
(with coherency enforced by hardware) that is far more e ￿cient
that what can be accomplished in a distributed system.
An intriguing use of such a con ￿guration is database migration
and elasticity across machines. Databases have a large state and
are not very elastic as they are heavily anchored by the data loaded
in memory so that the system is reasonably fast. Disaggregated
memory implemented through CXL would allow to place the Bu ￿er
Pool on the disaggregated memory and use the local DRAM just
for query processing. If more query processing capacity is needed,
new engines can be spawned and connected to the disaggregated
memory so that these engines are immediately ready to run queries
as there is no need to warm up the database. The additional latency
of CXL memory plays only a minor role in databases and is a
good trade-o ￿in return for far more elasticity than it is feasible
with engines where the data resides in local memory. Similarly, a
database engine can be easily migrated when the bu ￿er pool is in
disaggregated memory. If the data structures and state of the engine
itself are maintained in disaggregated memory, then migrating the
entire engine to another machine becomes a far simpler operation.
From a research perspective, interesting questions arise:
•Implementing these features will require rethinking the internal
database architecture to remove the assumption that everythingCXL and the Return of Scale-Up Database Engines
is in local memory. What needs to be local and what can be placed
in disaggregated memory will require extensive experimentation
to determine which part of the engine can tolerate the additional
latency of CXL memory.
•With the suitable architectural approach, engines can become
far more elastic as pointed out above. Should this be done at the
level of entire engines or can the elasticity be pushed down to
the level of threads running queries?
•Threads running queries could be moved from machine to ma-
chine by keeping their state and working space in disaggregated
memory. Alternatively, they can be created as the workload
dictates. How would an engine operate under a dynamically
changing multiprogramming level?
3.3 Shared Memory = Scale Up
Distributed databases are a typical way to implement larger sys-
tems [ 1,28,39]. However, the architecture of the engines discussed
above does not scale as expected: cache invalidation now crosses
machines; updates imply distributed, more onerous locks; there is
wasted memory as the same data is copied into the local bu ￿er cache
of several machines, etc. These problems are addressed through a
mix of replication and sharding, increasingly using RDMA to reduce
the network overhead [ 5,6,36,42]. Invariably a fragile balance is
reached between consistency, symmetry in the system (e.g., with
read-only copies), and data placement to minimize data movement,
thereby limiting architectural freedom (e.g., [38]).
CXL supports building a larger database in a truly scaled-up man-
ner rather than through distribution. This is thanks to CXL’s ability
to integrate devices via a coherency domain that can encompass the
entire rack. In particular, CXL supports Global Integrated Memory
(GIM), where each system component contributes a range of its
own memory to the collective memory, rather than having each
machine expand its memory by monopolizing it out of a pool. Since
now each server plus the memory expanders can share memory, the
boundaries of the machines in a rack get blurred. The entire rack
can be seen as a single machine. Figure 2(c) depicts this scenario.
Rack-level integration is arguably the most impactful change
CXL enables. It liberates the database system from managing con-
sistency across servers by moving the memory uni ￿cation e ￿ort to
hardware. Moreover, as discussed above, CXL o ￿ers a bandwidth
far larger than what is available with today’s networks and with far
lower latency. The result is a “blank canvas” for database architects
with almost none of the disadvantages of the previous scalabil-
ity methods. We expect this to cause a radical change in the way
scalable databases and data processing engines are designed.
Promising as the prospects are, the road ahead is not without
challenges. The last two decades have veered away from scale-up
systems, focusing instead on cloud-native and scale-out approaches.
Moreover, many existing large systems carry biases that do not exist
anymore, e.g., that networks are slow or that I/O is prohibitively
expensive. A fully disaggregated system like the one CXL enables
breaks new ground regarding the algorithms and data structures.
Here are some of the questions it opens:
•Since our fundamental data operations are built around hashing
and sorting, do we know how to conduct these operations on a
rack-level scale? Do we fully understand when to use which?•Given that each core now can access one to two orders of magni-
tude more memory than before, are the data structures we use to
organize and index the data still e ￿ective at these new scales? In
particular, how is the coherency tra ￿cgenerated by a typical data
structure? Given that the invalidation messages can dominate
access time, can it be improved?
•Assuming we now have the freedom to engage a tremendous
amount of resources to solve individual query operators, how do
we schedule the machine resources across competing queries?
The bene ￿ts of answering these questions are signi ￿cant. For
instance, consider transactional workloads. By keeping the data in
one (large) location, transactional updates can be done against a
centralized bu ￿er cache, rather than across a distributed system.
Similarly, many of the data structures the database has to main-
tain can now be centralized in the disaggregated memory instead
needing to be synchronized across the machines involved.
4 NEAR-DATA PROCESSING
CXL indirectly enables another set of important features beyond in-
tegrating local, far, and disaggregated DRAM. To understand these
features, it helps to look into how a CXL device is implemented—
and a comparison to Persistent Memory (PMem) devices is relevant
here. Persistent Memory solutions like Intel’s Optane carry a com-
plex controller that is embedded into the PMem DIMM. In contrast,
CXL support involves wrapping existing DRAM DIMMs (or other
types of memory) within a specialized hardware memory controller
(cf. Figure 1 (right)) using a variety of possible PCIe device form
factors. The device’s controller, which is transparent to the appli-
cation, must implement the CXL protocol and potentially manage
several logical devices carved out of the physical one, to cite one
functionality. The controller will likely utilize a processor that im-
plements the protocol, performs tra ￿c management, and arbitrates
the devices’ requests. The CXL memory controller is likely to be
rather sophisticated, probably in the same way that SSD controllers
are far more than simple data relays to ￿ash storage.
We argue that a sophisticated controller, potentially using a
specialized processor, opens a tremendous opportunity: part of the
CXL controller’s computing power can be co-opted to perform near-
data processing. Near-data processing is known to optimize away
unnecessary data movements, making the entire architecture far
more e ￿cient than one that simply integrates memory. Putting it
di￿erently, a CXL controller can functionally behave as a smart NIC,
following the idea that CXL could replace the network. However,
unlike smart NICs, the CXL controller will be sitting not only next
to compute devices (CPUs, FPGAs, GPUs) but also close to memory
and storage. Figure 3 (top) depicts this scenario.
This idea is very much aligned with current trends. In research
there have been e ￿orts to, e.g., explore the advantages for databases
of placing a small processor near memory [ 13] or in disaggregated
memory [ 16]. Oracle had a processor (SPARC S7) with Data Analyt-
ics Accelerators (DAX) placed between the cores and the memory
where basic relational operators could be o ￿oaded to ￿lter data be-
fore it hits the processor caches [ 29]. Recently, Amazon developed
AQUA, a network attached SSD caching layer for Redshift that used
an FPGA to o ￿oad relational operators to the caching layer [4].Alberto Lerner and Gustavo Alonso
ServerCXL
AcceleratorQuery Exec
Query Exec
Lock TableQuery data
ServerCXL
Accelerator w/ VirtualizationQuery Exec
Query Accel
Lock TableQuery data
Figure 3: Near-data processing under CXL. (top) A processor
or FPGA managing the expanded memory can be co-opted to
execute a portion of a query. (bottom) A unique opportunity
for acceleration exists through virtual memory regions.
CXL o ￿ers a way to better implement these ideas but not without
challenges, some of which we list below:
•How to perform query processing near the data? This possibility
is not new, as several query operators have been shown to have
near-data implementations. Compression and decompression,
encryption and decryption, selection, projection, ￿ltering with
LIKE predicates, and a wide range of other relational operators
that have been demonstrated to bring substantial advantages in
practice [4, 7, 13, 16].
•Do we leverage the controller to implement the functionality or
do we attach an accelerator in the CXL path? How does such a
controller or such an accelerator look like?
•If the lock table is also placed in the shared memory region, then
even updates to common data structures from both sides of the
query could be performed. What are suitable data structures
that can be manipulated by both “sides?” In other words, are the
invalidation tra ￿c of traditional data structures acceptable, if
they are updated by several processors?
•Lastly, a very intriguing idea is to use the CXL controller to
implement a virtual memory region that does not correspond to
actual content but to a service that takes data from the memory
and transforms it before sending it to the requester to make
it look like it was stored in memory all along. Figure 3 (bot-
tom) illustrates such a use case. In databases this can be used
to implement, e.g., view materialization on-the- ￿y; data type
transformations; data transpositions from column to row, row to
column, or matrix transpositions [ 16]; on-the- ￿y data cubes and
statistical summaries; or even to better integrate databases and
smart storage systems [17].
5 HETEROGENEOUS ARCHITECTURES
GPUs, TPUs, DPUs, smart NICs, and FPGAs con ￿gured to perform
di￿erent types of near-data processing or processing in memory
acceleration have been shown to be advantageous in several real
systems scenarios. If anything, this trend has exposed many of the
limitations of CPU-centric computer architectures and, as discussed,
the limitations of existing interconnects. With around 64 GB/s
bandwidth per slot and a typical server with 4 to 6 slots, CXL enables
the creation of a computing platform much more ￿exible than a
rack of servers with shared memory. A federation of heterogeneous
processing nodes can now operate on a unique coherency domain.This composability possibility that CXL creates opens a research
￿eld of its own: if computational devices are independently con-
nected, what should a heterogeneous machine look like? Can we
(and should we) build machines that accommodate speci ￿c work-
loads better than others? For instance, Machine Learning (ML) is
taxing database engines because the data often has to be taken out
of the database to run it through ML tools. With a heterogeneous
architecture that seamlessly integrates CPUs and GPUs, it becomes
possible to implement ML operators directly on the database engine
while still taking advantage of suitable hardware. This will require
changes to the engine design and architecture. However, given the
powerful compute fabric that CXL enables, it will likely lead to a
new generation of scale-up database engines.
6 RELATED EFFORTS
CXL is, in no small part, the result of consolidating several projects
that came before it, most notably CCIX [ 9], GenZ [ 14], and Open-
CAPI [ 27]. An overwhelming number of institutions and companies
are working together to advance the protocol [ 10]. The consolida-
tion, however, has not been complete. Some proprietary memory
coherency interconnects still exist, mainly involving GPGPU ven-
dors. AMD supports an interconnect called In ￿nity Architecture
across its GPGPUs [ 3]. NVidia has alternative interconnect tech-
nology in the form of NVLink [25] and NVlink-C2C [26].
The argument for developing these highly specialized intercon-
nects, especially for GPGPUs, is that they can provide (a) a much
higher bandwidth than what is possible with PCIe or the network
and (b) a uni ￿ed memory between the host and the accelerator to
be able to exchange pointers and to avoid having to pin memory
when exchanging data. While these arguments may have been valid
in older versions of PCIe, the latter standard has been upgraded
at an unprecedented pace. PCIe Gen 7, expected to be available in
2025, will support 128MT/s per lane, i.e., 242GB/s in a ⇥16 card [ 32].
Even if the proprietary interconnect improved their bandwidth,
they would still carry the usual drawbacks: outside of the few
CPU/GPGPUs they support, they bring ine ￿cient memory use, data
copying overheads, consistency issues, etc. It remains to be seen
how these competing standards will evolve, merge, or co-exists.
7 CONCLUSION
In this paper, we argued that the availability of CXL technology
will upend at least two decades of investments in scale-out data-
base systems. The reason is that, through a quite natural memory
integration, CXL allows database systems to scale up instead. The
appeal of this direction change is that it turns what is now complex
distributed system development into familiar centralized system
development instead. We discussed how CXL supports a range of
disaggregated—but coherent!—memory settings in a system, from
local- to far-memory expansion to full rack-level disaggregation. At
each of these steps, we presented several research questions these
changes open, along with the bene ￿ts of addressing them. Lastly,
we argued that CXL can also support near-data processing and
heterogeneous platforms in uncomplicated ways never available
before. Given the magnitude of these possibilities, we expect CXL to
foster the design of an entirely new generation of database systems
with unprecedented scalability, e ￿ciency, and integration.CXL and the Return of Scale-Up Database Engines
REFERENCES
[1] Josep Aguilar-Saborit et al .2020. POLARIS: The Distributed SQL Engine in Azure
Synapse. Proc. VLDB Endow. 13, 12 (2020), 3204–3216. https://doi.org/10.14778/
3415478.3415545
[2] Marcos K. Aguilera, Emmanuel Amaro, Nadav Amit, Erika Hunho ￿, Anil Yelam,
and Gerd Zellweger. 2023. Memory disaggregation: why now and what are the
challenges. ACM SIGOPS Oper. Syst. Rev. 57, 1 (2023). https://doi.org/10.1145/
3606557.3606563
[3] AMD. [n.d.]. In ￿nity Architecture: A New Era in Accelerated System Connectiv-
ity. https://www.amd.com/en/technologies/in ￿nity-architecture.
[4] Je￿Barr. 2021. AQUA (Advanced Query Accelerator) – A Speed Boost for Your Ama-
zon Redshift Queries . https://aws.amazon.com/blogs/aws/new-aqua-advanced-
query-accelerator-for-amazon-redshift/
[5] Claude Barthels, Ingo Müller, Konstantin Taranov, Gustavo Alonso, and Torsten
Hoe￿er. 2019. Strong consistency is not hard to get: Two-Phase Locking and
Two-Phase Commit on Thousands of Cores. Proc. VLDB Endow. 12, 13 (2019).
https://doi.org/10.14778/3358701.3358702
[6] Carsten Binnig, Andrew Crotty, Alex Galakatos, Tim Kraska, and Erfan Zamanian.
2016. The End of Slow Networks: It’s Time for a Redesign. Proc. VLDB Endow. 9,
7 (2016). https://doi.org/10.14778/2904483.2904485
[7]Monica Chiosa, Fabio Maschi, Ingo Müller, Gustavo Alonso, and Norman May.
2022. Hardware Acceleration of Compression and Encryption in SAP HANA. Proc.
of the VLDB Endowment 15, 12 (2022). https://doi.org/10.14778/3554821.3554822
[8] Hong-Tai Chou and David J. DeWitt. 1985. An Evaluation of Bu ￿er Management
Strategies for Relational Database Systems. In VLDB’85, Proceedings of 11th
International Conference on Very Large Data Bases, August 21-23, 1985, Stockholm,
Sweden . https://doi.org/10.1007/BF01840450
[9] CCIX Consortium. [n.d.]. An Introduction to CCIX. https://www.ccixconsortium.
com/wp-content/uploads/2019/11/CCIX-White-Paper-Rev111219.pdf.
[10] CXL. [n.d.]. Consortium Member List. https://www.computeexpresslink.org/
members.
[11] Bill Dally. 2011. Power, programmability, and granularity: The challenges of
exascale computing. In 2011 IEEE International Test Conference . IEEE Computer
Society, 12–12. https://doi.org/10.1109/IPDPS.2011.420
[12] William J Dally, Yatish Turakhia, and Song Han. 2020. Domain-speci ￿c hardware
accelerators. Commun. ACM 63, 7 (2020), 48–57. https://doi.org/10.1145/3361682
[13] Yuanwei Fang, Chen Zou, and Andrew A. Chien. 2019. Accelerating Raw Data
Analysis with the ACCORDA Software and Hardware Architecture. Proceedings
of the VLDB Endowment 12, 11 (2019). https://doi.org/10.14778/3342263.3342634
[14] GenZ. [n.d.]. GenZ Archive. https://www.computeexpresslink.org/projects-3.
[15] Donghyun Gouk, Sangwon Lee, Miryeong Kwon, and Myoungsoo Jung. 2022.
Direct Access, High-Performance Memory Disaggregation with DirectCXL
(USENIX ATC’22’) . https://www.usenix.org/conference/atc22/presentation/gouk
[16] Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin Taranov,
Dejan S. Milojicic, and Gustavo Alonso. 2022. Farview: Disaggregated Memory
with Operator O ￿-loading for Database Engines. In 12th Conference on Innovative
Data Systems Research, CIDR 2022, Chaminade, CA, USA, January 9-12, 2022 .
https://www.cidrdb.org/cidr2022/papers/p11-korolija.pdf
[17] Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudré-Mauroux.
2024. Database Kernels: Seamless Integration of Database Systems and Fast
Storage via CXL. In 14th Conference on Innovative Data Systems Research, CIDR
2024, Chaminade, CA, USA, January 9-12, 2022 . http://exascale.info/assets/pdf/
lee2024cidr.pdf
[18] Huaicheng Li, Daniel S. Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko
Novakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, Mark D.
Hill, Marcus Fontoura, and Ricardo Bianchini. 2023. Pond: CXL-Based Memory
Pooling Systems for Cloud Platforms (ASPLOS 2023) . https://doi.org/10.1145/
3575693.3578835
[19] Clemens Lutz, Sebastian Breß, Ste ￿en Zeuch, Tilmann Rabl, and Volker Markl.
2020. Pump Up the Volume: Processing Large Data on GPUs with Fast Intercon-
nects (SIGMOD’20) . https://doi.org/10.1145/3318464.3389705
[20] Stefan Manegold, Peter A. Boncz, and Martin L. Kersten. 2000. Optimizing
Database Architecture for the New Bottleneck: Memory Access. The VLDB
Journal 9, 3 (dec 2000). https://doi.org/10.1007/s007780000031
[21] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agar-
wal, Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanau-
jia, and Prakash Chauhan. 2023. TPP: Transparent Page Placement for CXL-
Enabled Tiered-Memory (ASPLOS 2023) . https://doi.org/10.1145/3582016.3582063[22] Fabio Maschi and Gustavo Alonso. 2023. The Di ￿cult Balance Between Modern
Hardware and Conventional CPUs (DaMoN’23) . https://doi.org/10.1145/3592980.
3595314
[23] Micron. [n.d.]. Flexible memory capacity expansion for data intensive workloads.
https://www.micron.com/solutions/server/cxl.
[24] NVidia. [n.d.]. NVidia ConnectX-7 400G Ethernet. https://www.nvidia.com/
content/dam/en-zz/Solutions/networking/ethernet-adapters/connectx-7-
datasheet-Final.pdf.
[25] NVidia. [n.d.]. NVLink and NVSwitch: The building blocks of advanced multi-
GPU communication—within and between servers. https://www.nvidia.com/en-
us/data-center/nvlink/.
[26] NVIDIAG. [n.d.]. NVIDIA Opens NVLink for Custom Silicon Integra-
tion. https://nvidianews.nvidia.com/news/nvidia-opens-nvlink-for-custom-
silicon-integration.
[27] OpenCAPI. [n.d.]. OpenCAPI Archive. https://www.computeexpresslink.org/occ-
archive.
[28] Oracle. [n.d.]. Why Oracle Exadata platforms are the best for Oracle Database.
https://www.oracle.com/engineered-systems/exadata/.
[29] Oracle. 2015. SPARC S7 Processor. https://www.oracle.com/a/ocom/docs/servers/
sparc/sparc-s7-processor-ds-3042417.pdf.
[30] Samsung. [n.d.]. Samsung Electronics Introduces Industry’s First 512GB
CXL Memory Module. https://news.samsung.com/global/samsung-electronics-
introduces-industrys- ￿rst-512gb-cxl-memory-module.
[31] Debendra Das Sharma. [n.d.]. Compute Express Link. https://docs.wixstatic.
com/ugd/0c1418_d9878707bbb7427786b70c3c91d5fbd1.pdf.
[32] PCIe SIG. [n.d.]. Announcing the PCIe 7.0 Speci ￿cation. https:
//pcisig.com/blog/announcing-pcie%C2%AE-70-speci ￿cation-doubling-
data-rate-128-gts-next-generation-computing.
[33] Utku Sirin, Pinar Tözün, Danica Porobic, and Anastasia Ailamaki. 2016. Micro-
Architectural Analysis of In-Memory OLTP (SIGMOD ’16) . https://doi.org/10.
1145/2882903.2882916
[34] Daniel J Sorin, Mark D Hill, and David A Wood. 2011. A Primer on Memory
Consistency and Cache Coherence . Morgan & Claypool Publishers. https://doi.
org/10.1007/978-3-031-01764-3
[35] Yan Sun, Yifan Yuan, Zeduo Yu, Reese Kuper, Chihun Song, Jinghan Huang,
Houxiang Ji, Siddharth Agarwal, Jiaqi Lou, Ipoom Jeong, Ren Wang, Jung Ho
Ahn, Tianyin Xu, and Nam Sung Kim. 2023. Demystifying CXL Memory with
Genuine CXL-Ready Systems and Devices (MICRO’23) . https://doi.org/10.1145/
3613424.3614256
[36] Yacine Taleb, Ryan Stutsman, Gabriel Antoniu, and Toni Cortes. 2018. Tailwind:
Fast and Atomic RDMA-Based Replication (USENIX ATC’18) . https://www.
usenix.org/conference/atc18/presentation/taleb
[37] Neil C. Thompson and Svenja Spanuth. 2021. The Decline of Computers as a
General Purpose Technology. Commun. ACM 64, 3 (feb 2021). https://doi.org/
10.1145/3430936
[38] Alexandre Verbitski et al .2018. Amazon Aurora: On Avoiding Distributed
Consensus for I/Os, Commits, and Membership Changes (SIGMOD ’18) . https:
//doi.org/10.1145/3183713.3196937
[39] Alexandre Verbitski, Anurag Gupta, Debanjan Saha, Murali Brahmadesam, Ka-
mal Gupta, Raman Mittal, Sailesh Krishnamurthy, Sandor Maurice, Tengiz
Kharatishvili, and Xiaofeng Bao. 2017. Amazon Aurora: Design Considera-
tions for High Throughput Cloud-Native Relational Databases (SIGMOD’17) .
https://doi.org/10.1145/3035918.3056101
[40] Ruihong Wang, Jianguo Wang, Stratos Idreos, M. Tamer Özsu, and Walid G. Aref.
2022. The Case for Distributed Shared-Memory Databases with RDMA-Enabled
Memory Disaggregation. Proc. VLDB Endow. 16, 1 (2022). https://doi.org/10.
14778/3561261.3561263
[41] Yuan Yuan, Rubao Lee, and Xiaodong Zhang. 2013. The Yin and Yang of Pro-
cessing Data Warehousing Queries on GPU Devices. Proceedings of the VLDB
Endowment 6, 10 (2013), 817–828. https://doi.org/10.14778/2536206.2536210
[42] Erfan Zamanian, Xiangyao Yu, Michael Stonebraker, and Tim Kraska. 2019.
Rethinking Database High Availability with RDMA Networks. Proc. VLDB
Endow. 12, 11 (2019). https://doi.org/10.14778/3342263.3342639
[43] Qizhen Zhang, Philip A. Bernstein, Daniel S. Berger, and Badrish Chandramouli.
2021. Redy: Remote Dynamic Memory Cache. Proc. VLDB Endow. 15, 4 (2021).
https://doi.org/10.14778/3503585.3503587
[44] Mark Zhao et al .2022. Understanding data storage and ingestion for large-
scale deep recommendation model training: industrial product (ISCA’22) . https:
//doi.org/10.1145/3470496.3533044Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for
Memory Expansion (CXL-SSD)
Myoungsoo Jung
Computer Architecture and Memory Systems Laboratory ,
Korea Advanced Institute of Science and Technology (KAIST)
http://camelab.org
ABSTRACT
Compute express link (CXL) is the ﬁrst open multi-protocol
method to support cache coherent interconnect for differ-
ent processors, accelerators, and memory device types. Even
though CXL manages data coherency mainly between CPU
memory spaces and memory on attached devices, we argue
that it can also be useful to reform existing block storage as
cost-efﬁcient, large-scale working memory. Speciﬁcally, this
paper examines three different sub-protocols of CXL from a
memory expander viewpoint. It then suggests which device
type can be the best option for PCIe storage to bridge its
block semantics to memory-compatible, byte semantics. We
then discuss how to integrate a storage-integrated memory
expander into an existing system and speculate how much
effect it does have on the system performance. Lastly, we visit
various CXL network topologies and explore a new opportu-
nity to efﬁciently manage the storage-integrated, CXL-based
memory expansion.
1 INTRODUCTION
Cache coherence interconnects are recently emerged to inte-
grate different CPUs, accelerators, and memory components
into a heterogeneous, single computing domain. Speciﬁcally,
the interconnect technologies maintain data coherency be-
tween CPU memory and private memory attached to devices,
deﬁning a new type of globally shared memory and network
space. While there have been several efforts to coherently
connect different hardware components, such as Gen-Z [ 1]
and CCIX [ 2],Compute Express Link (CXL) is the ﬁrst open
interconnect protocol supporting various types of processors
and device endpoints [ 3]. CXL has absorbed Gen-Z [ 4] and
has become one of the most promising interconnect interfaces
thanks to its high-speed coherence control and full compati-
bility with the existing bus standard, PCIe. A broad spectrum
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from permissions@acm.org.
HotStorage ’22, June 27–28, 2022, Virtual Event, USA
© 2022  Association  for Computing  Machinery.  
ACM  ISBN  978-1-4503-9399-7/22/06.  . . $15.00  
https://doi.org/10.1145/3538643.353974 5of datacenter-scale hardware such as CPU, GPU, FPGA, and
domain-speciﬁc ASIC is thus expected to take signiﬁcant
advantage of CXL [ 5–7]. CXL consortium announces that it
can also disaggregate memory by pooling DRAM and byte-
addressable persistent memory (PMEM).
While CXL can handle diverse computing resources and
memory components, it sets block storage aside and leaves a
question on whether the storage can reap the beneﬁts of CXL
or not. A primary question that storage designers and system
architects may have is i) why and what can the block storage
beneﬁt from CXL? . If there is an advantage, we should be able
to answer the following questions: ii) how can we connect the
underlying block storage to the host’s system memory bus? ,
iii)what kind of CXL device type should be used for the block
storage and memory expander? , and iv) what does CXL need
to improve for better utilization of the block storage? .
In this paper, we argue that CXL is helpful in leveraging
PCIe-based block storage to incarnate a large, scalable work-
ing memory by answering all the four questions mentioned
above. We believe CXL is a cost-effective and practical in-
terconnect technology that can bridge PCIe storage’s block
semantics to memory-compatible, byte semantics. To this end,
we should carefully integrate the block storage into its inter-
connect network by being aware of the diversity of device
types and protocols that CXL supports. This paper ﬁrst dis-
cusses what a mechanism makes the PCIe storage impractical
and unable to be used for a memory expander (§2). Then, we
explore all the CXL device types and their protocol interfaces
to answer which conﬁguration would be the best for the PCIe
storage to expand the host’s CPU memory (§3).
Even though CXL can be the most promising interface for
the block storage in getting closer to CPU, it is non-trivial
to speculate how much effect a storage-integrated memory
expander does have on system performance. As there is no
CPU and fabric for CXL yet, it is also unclear for the storage
designers and system architects to see how CXL-enabled
storage can be implemented and interact with CPU. To answer
this, we discuss what a PCIe storage device needs to change,
how it can be connected to the host over CXL, and how users
can access the device through load/store instructions (§4).
We then project the performance of the storage-integrated
memory expander by prototyping CXL agents and controllers
in different FPGA nodes, all connected by a PCIe network.
45
HotStorage ’22, June 27–28, 2022, Virtual Event, USA Myoungsoo Jung
After examining the potential beneﬁts of converting the
block semantic to byte semantic over CXL, we further discuss
how to disaggregate PCIe storage resources from computing
resources using CXL (§6). In particular, we visit three differ-
ent network topologies and argue the pros and cons of each
disaggregation option. We lastly explore a new opportunity
to manage the PCIe storage efﬁciently at the host-side (§7).
Speciﬁcally, we suggest two additional states, determinism
andbufferability , and debate why these additional states can
be beneﬁcial to handle the PCIe storage in expanding the
host’s CPU memory with CXL.
2 WHY CXL FOR PCIE STORAGE
Byte-addressability. It is a long-standing dream for PCIe
storage to have byte-addressability and be a part of working
memory devices [ 8–17]. For example, an industry prototype
and NVMe standard [ 18–20] offer the byte-addressability by
exposing SSD’s internal memory/buffer to PCIe base address
registers (BARs). Since BARs can be directly mapped to the
system memory space, the host-side kernel and applications
can access the exposed memory/buffer resources just like the
local memory (using load/store instructions) rather than block
storage. To hide long latency imposed by SSD’s backend
block media (e.g., Z-NAND, Flash, Optane SSD), they can
use the internal memory/buffer as a write-back inclusive cache
of the backend [21–25].
Limits with non-cacheable accesses. PCIe bandwidth is fast
enough to be far memory (e.g., 63GB/s ⇠121GB/s for Gen5/6
16⇥[26]). However, PCIe considers the block storage devices
as just one of the peripherals that the host-side CPU needs
to manage and communicate with. Thus, while the storage
devices can handle load/store requests through the PCIe’s
BARs, they are all limited to being used as working mem-
ory in a real system. Speciﬁcally, as the memory-mapped
BARs are only the interface for the host to let the under-
lying storage know what it requests or controls, CPU must
make the load/store requests uncached and directly acces-
sible. This non-cacheable characteristic severely degrades
the performance of all memory accesses targeting the BARs.
If CPU can cache/buffer the memory requests targeting the
PCIe address space, there is no way for the PCIe storage to
catch their arrivals. This can introduce an unexpected situ-
ation such as a system failure or storage disconnection. To
prevent such a failure, x86 instruction set architectures of both
Intel and AMD do not allow PCIe-related memory requests to
be cached at the CPU side. Unfortunately, this nature enforces
the storage-integrated memory expanders be excluded from
the conventional memory hierarchy anddisables them from
taking advantage of CPU caches .
Compute express link. CXL is a cache coherent interconnec-
tion technology, designed initially toward supporting variousaccelerators and memory devices therein [ 3]. Speciﬁcally,
CXL can offer one or more memory address spaces in the
PCIe network domain coherently, which can consistently be
accessed by different processors and hardware accelerators
over its multi-protocol technology. The multi-protocol over-
rides the I/O semantic of the existing PCIe interface, thereby
making all device types of CXL compatible with most exist-
ing PCIe devices, including SSDs. We will explain details of
each device type in §3.
Even though CXL is built upon PCIe, it basically guar-
antees that all the caches across different computing com-
plexes in the same CXL hierarchy are coherent. This can
make the load/store requests (heading to the PCIe address
space) cacheable in contrast to PCIe. The current CXL con-
siders only DRAM or PMEM for its memory pooling, but we
advocate that CXL can open a new door that changes PCIe
storage’s block interface to a memory-like, byte interface. As
CXL’s multi-protocol can integrate PCIe storage into its cache
coherent memory space, it can create a much bigger memory
pool than DRAM-based or PMEM-based memory expansion
technologies.
3MULTI-PROTOCOL AND CXL DEVICES
CXL provides three different sub-protocols, i) CXL.io , ii)
CXL.mem , and iii) CXL.cache , which can also deﬁne three
different types of CXL devices ( Type 1 ⇠Type 3 ).
Multi-protocol. CXL.io is the fundamental protocol that all
CXL-attached devices and host CPUs require to communi-
cate. Fundamentally, it employs full features of PCIe as a
non-coherent load/store interface for I/O devices (e.g., device
discovery/enumeration and host address conﬁguration). To
this end, CXL.io amends PCIe’s hierarchical communication
layers and creates a high-speed I/O channel, called FlexBus .
FlexBus converts received CXL data to an appropriate for-
mat to leverage the physical PCIe layers (e.g., transaction,
data, and link). On the other hand, CXL.cache and CXL.mem
respectively add coherent cache and memory access capa-
bilities into FlexBus, which can fan out to support multiple
device domains and remote memory management. While
CXL overrides PCIe, its root port ( CXL RP ) allows one or
more memory addresses (exposed by the underlying CXL
devices) to be mapped to a target host’s cacheable system
memory space. While this capability is designed toward uni-
fying multi-domain memory devices into a single pool over
coherent cache management, it can be leveraged for a memory
expander using different storage technologies.
CXL device types. Based on how to combine the multi-
protocol features of CXL, it declares three different device
types, Type 1, Type 2, and Type 3. Figure 1a shows all the
CXL device types and the protocols each device type uses.
46Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for Memory Expansion HotStorage ’22, June 27–28, 2022, Virtual Event, USA/g18/g121/g62/g90/g87/g100/g455/g393/g286/g1007/g94/g455/g400/g410/g286/g373/g3/g393/g346/g455/g400/g349/g272/g258/g367/g3/g373/g286/g373/g381/g396/g455/g3/g373/g258/g393/g62/g381/g272/g258/g367/g3/g24/g90/g4/g68/g44/g24/g68/g17/g4/g90/g18/g381/g374/g296/g856/g876/g18/g258/g393/g258/g393/g258/g271/g349/g367/g349/g410/g455/g18/g121/g62/g3/g90/g87/g3/g396/g286/g400/g286/g396/g448/g286/g282/g3/g3/g3/g3/g44/g24/g68/g17/g4/g90/g18/g258/g393/g258/g271/g349/g367/g349/g410/g455/g18/g381/g374/g296/g349/g336/g856/g38/g116/g28/g87/g100/g455/g393/g286/g3/g1007/g3/g68/g286/g373/g381/g396/g455/g3/g68/g258/g393/g68/g286/g373/g381/g396/g455/g17/g367/g381/g272/g364/g18/g121/g62/g3/g272/g381/g374/g410/g396/g381/g367/g367/g286/g396/uni2776/uni2777/uni2778/uni2779/g3
/g18/g87/g104/g3/g400/g455/g400/g410/g286/g373/g3/g373/g286/g373/g381/g396/g455/g3/g400/g393/g258/g272/g286/g100/g455/g393/g286/g1005/g100/g455/g393/g286/g1006/g100/g455/g393/g286/g1007/g68/g286/g373/g381/g396/g455/g68/g286/g373/g381/g396/g455/g87/g18/g44/g18/g87/g104/g44/g381/g400/g410/g3/g18/g87/g104/g3/g18/g381/g373/g393/g367/g286/g454
/g62/g381/g272/g258/g367/g3/g24/g90/g4/g68/g44/g24/g68/g44/g24/g68/g68/g286/g373/g381/g396/g455(a) Protocols and devices. (b) System memory mapping.
Figure 1: CXL multi-protocol and system map.
Type 1 is CXL devices that have a local cache without
employing an internal DRAM component. Type 1 is valuable
if the endpoint device requires a fully coherent cache, which
implies that the device can access the corresponding host’s
memory data through its own caches in an active manner.
Most domain-speciﬁc accelerators for computationally inten-
sive applications, such as tensor processing units [ 27], can be
classiﬁed by this Type 1 device. Note that Type 1 devices use
CXL.cache and CXL.io to manage their full cache coherence
capability.
Type 2 is for discrete acceleration devices that internally
employ high-performance memory modules. These memory
components are referred to as host-managed device memory
(HDM) in CXL. While the host can, in default, communicate
with Type 2 devices using CXL.io (over PCIe), CXL.cache
and CXL.mem are respectively used for the device to access
its host-side memory and for the host to access HDM. Note
that HDM differs from private memory modules employed by
conventional acceleration devices such as GPUs. Even though
a host can access GPU’s device-side memory (e.g., GDDR),
it is only performed by legacy memory copies. In contrast, a
CXL-enabled host can manage HDM using cache-coherent
load/store instructions. It is also expected for Type 2 devices
to actively access the host’s CPU memory by utilizing all the
features that CXL’s multi-protocol supports.
Lastly, Type 3 is designed for non-acceleration devices
that only employ HDM without a processing component.
CXL regulates these Type 3 devices operate primarily with
CXL.mem to serve load/store requests issued by a host; it does
not allow the Type 3 devices to make a request (to the host)
over CXL.cache. While Type 3 does not employ CXL.cache,
it can be used for expanding the host-side memory. This is
because CXL.mem includes basic memory read and write
interfaces for HDM. We will explain how to use HDM in §3.
Note that CXL allows the Type 3 devices to manage CXL.io
at the device side in an attempt to accommodate various I/O
speciﬁc demands in a ﬂexible manner.
4 INTEGRATING STORAGE INTO CXL
Device type consideration. Generally speaking, PCIe storage
is not a simple, passive device. In addition to its backend’sblock media, PCIe storage employs large internal DRAMs
to buffer/cache incoming requests and corresponding data. It
also has computation capability used for diverse data process-
ing tasks such as address translation or reliability management
[28–32]. Type 2 can probably be a good option for utilizing
HDM and/or integrating data processing capabilities into the
storage by being aware of the semantics of host CPUs. In this
paper, we however advocate Type 3 for a storage-integrated
memory expander in CXL.
There are three reasons why we believe that Type 3 de-
vices are better than Type 2 devices for the storage-integrated
memory expander. First, even though Type 2 allows the host
to handle the storage-side HDM directly, Type 2 is designed
for computationally intensive applications. Because of this,
only one device (per CXL RP) can be connected to a host
system, which makes Type 2 devices not scalable as Type 3 de-
vices can do. Second, having full features of CXL.cache and
CXL.mem can introduce another type of communication bur-
den, thereby degrading the overall performance of the storage-
integrated memory expander. Speciﬁcally, all load/store re-
quests require checking the cache states of PCIe storage’s
computing complex, which exhibits multiple CXL transac-
tions for every I/O service. Even though it is crucial for the
PCIe storage to manage internal DRAM efﬁciently, it does not
require coherently managing the host’s CPU caches. Third,
if we integrate a PCIe storage device into CXL as Type 2,
the device should ask permission from the host whenever the
storage side computing resources access its memory. This is
because Type 2’s CXL.cache manages both the host’s local
memory and HDM in a fully coherent manner, which makes
the device-level I/O performance even worse than before.
Storage-side modiﬁcation. PCIe storage devices typically
employ a PCIe endpoint and NVMe controllers to parse the in-
coming requests and transfer data between a host and SSD’s
internal DRAMs [ 33,34]. Thus, a hardware change at the
storage side can be simple, which in turn makes most storage
devices easily support Type 3 with a minor modiﬁcation. For
example, we can compose a CXL storage controller to handle
CXL transaction packet formatting and CXL.io control by
leveraging the existing PCIe endpoint logic. Similarly, the
existing NVMe controller’s capabilities, such as command
parsing and page memory copies, can be simpliﬁed to im-
plement the read and write interfaces of CXL.mem. Note
that the NVMe speciﬁcation allows PCIe storage to realize
its controllers in either ﬁrmware or hardware [ 35,36]. How-
ever, we believe it is better to automate the service routine
of CXL.mem’s reads and writes over hardware while letting
ﬁrmware manage the internal DRAMs and the back-end block
media.
System integration. Figure 1b shows how CXL can connect
a PCIe storage device to a host and explains how the host-side
users directly access the storage device through load/store
47HotStorage ’22, June 27–28, 2022, Virtual Event, USA Myoungsoo Jung
instructions. In this example, the host’s system bus employs
a CXL RP connecting a PCIe storage device as Type 3; we
will discuss a system option to disaggregate many storage
devices from the host resources in §6. When the host boots, it
enumerates CXL devices connected to its RP and initializes
the devices by mapping their internal memory spaces to the
system memory. Speciﬁcally, the host retrieves the size of
CXL BAR and HDM from the PCIe storage devices and then
maps them into its system memory space (CXL RP reserved).
In particular, HDM is mapped to a cacheable memory ad-
dress space such that the users can access it using load/store
instructions. As CXL BAR and HDM are mapped to differ-
ent addresses from what a Type 3 device initially manages,
the CXL RP requires letting the underlying CXL controller
know where they have been mapped [ 1]. The host can do
this address space synchronization by writing the correspond-
ing information (e.g., remapped address offset) to the target
storage’s CXL capability/conﬁguration areas.
When an application loads or stores data on the system
memory (mapped to HDM), CXL RP generates a message,
called CXL ﬂit , and sends it to the target’s CXL storage con-
troller via CXL.mem [ 2]. The underlying endpoint and CXL
controllers then parse the ﬂit and extract the request infor-
mation (e.g., command and target address) [ 3]. Using the
request information, the controllers can serve the data by
collaborating with underlying storage ﬁrmware [ 4].
5 PERFORMANCE PROJECTION
Prototype. Since there is unfortunately no processor com-
plex that yet supports CXL.mem and CXL.io, we prototype
a CXL-enabled CPU and CXL storage, each taking the role
of a host and storage-integrated memory expander. Speciﬁ-
cally, the CPU and storage are fabricated into two separate,
custom FPGA boards, which are connected through a tailored
PCIe backplane. We integrate CXL.mem and CXL.io agents
into an in-house RISC-V CPU (64-bit O3 dual-core architec-
ture that uses 128KB L1 and 4MB L2 caches), and 32GB
OpenExpress-based NVMe storage [ 24] in 16nm FPGA for
the host node and storage node, respectively. OpenExpress’s
backend media emulates Z-NAND [ 37] while buffering the
incoming CXL requests into its internal DRAMs. In addition
to this prototype ( CXL), we evaluate a local DRAM-only sys-
tem ( DRAM ) and PCIe-based memory expander ( PCIe ).PCIe
andCXLuse the same backend storage, but their RP’s address
is mapped to different places of the host’s system memory.
Workloads. We use Apex-Map , a global memory access
benchmark for large-scale computing [ 38]. The benchmark
allows us to test underlying memory with different locality
and request size levels (i.e., the parameter, a). We conﬁgure
the request size as 64B, which is the same as the last-level
cacheline size of our CPU. In this performance projection, we(a)a= 0.001. (b) Average a. (c) a= 1.
Figure 2: Performance of different memory systems.
exclude time-consuming activities of internal tasks such as
garbage collection; we will discuss how CXL can alleviate the
long latency imposed by such internal tasks in §7. Apex-Map
generates 512 million memory instructions synthetically by
ranging afrom 0.001 (highest locality) to 1 (lowest locality).
Result analysis. Figures 2a, 2b, and 2c show each system’s
latency (in terms of CPU cycles) for the best case ( a=0.001),
the average case ( 0.001a1), and the worst case ( a=1),
respectively. The best-case performance shows the reason
why CXLcan be more beneﬁcial than a PCIe-based memory
expander. While most memory requests in this test are hit
from the CPU caches, PCIe cannot take any advantage of the
host CPU caches, thereby exhibiting 129.5 ⇥longer latency
thanCXL. In contrast, CXLenjoys the CPU caches and shows
excellent latency behaviors comparable with DRAM .
CXLis also better than PCIe by 3⇥for the average case.
Note that, even though the performance of CXLis 9.3 ⇥worse
than that of DRAM , we believe it is still in a reasonable range
by considering the fact that CXLleverages the block storage.
When there is no locality (the worst-case), CXLcannot hide
the underlying Z-NAND latency because of the benchmark’s
access pattern (fully random), which exhibits 84.1 ⇥worse
thanDRAM . However, CXLshows still better performance com-
pared to PCIe by 1.6 ⇥as it does not handle all the memory
requests (on PCIe’s BAR) in a synchronized fashion.
We are somewhat disappointed with the results as CXL’s
worst-case latency characteristics are far away from DRAM’s
behaviors. However, most workloads exhibit high locality
except for a speciﬁc application like graph processing. Con-
sidering the large capacity that the storage-integrated memory
expander offers, we believe many applications can reap the
beneﬁts of CXL. We also believe that there is an opportunity
to optimize this long latency by wisely using PCIe storage’s
internal DRAMs and backend block media (§7).
6 STORAGE DISAGGREGATION
This section discusses how a system can disaggregate CXL
controllers and storage devices from its computing resources
while keeping their byte-addressability.
Pooling storage over the byte interface. To make the inter-
connect network scalable, CXL 2.0 allows FlexBus to employ
one or more CXL switches, each being able to have multiple
48Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for Memory Expansion HotStorage ’22, June 27–28, 2022, Virtual Event, USA
(a) Switch. (b) Multi-switch. (c) Virtual hierarchy.
Figure 3: Storage disaggregation conﬁgurations.
upstream ports (USPs) and downstream ports (DSPs). Even
though CXL yet leaves a question undecided on how to imple-
ment a switch and its internal components, USPs and DSPs
can be simply interconnected by a reconﬁgurable crossbar
switch. Speciﬁcally, a USP can be connected to a CXL RP or
another switch’s DSP over FlexBus, and it internally returns
the incoming message to one or more underlying DSPs as
soon as possible. In contrast, a DSP links a lower-level hard-
ware module such as a storage device’s CXL endpoint or a
different switch’s USP. Using a switch buffer, it can control
multiple CXL messages going through the DSP(s).
Figure 3a shows how a host can expand its local mem-
ory by having multiple storage devices. Speciﬁcally, each
DSP connects to a different storage device, whereas a USP
is linked to all the DSPs and exposes them to the host’s CXL
RP. For this type of storage-integrated memory expansion, the
host should map each HDM to different places of its physical
memory and be aware of the mapping information when the
system enumerates CXL devices to conﬁgure CXL capabil-
ity/conﬁguration (BAR/HDM). While this network topology
is simple enough to connect multiple PCIe storage devices,
the number of lanes that a CXL switch can support is limited.
Typically, a switch supports 64 ⇠128 lanes, and thus, only
4⇠8 ports are available for high-performance storage devices
(using 16 lanes). In this case, as shown in Figure 3b, it can
expand the host memory by adding one or more switches to
the CXL network. The top switch is used to bridge the host’s
CXL RP and all other lower-level switches, while the leaf
switches are employed to manage many PCIe storage devices.
Note that the number of storage devices that a network can
handle varies based on the size of the devices and the memory
capacity that CXL deals with (currently, it is 4PB).
Multi-host connection management. To better utilize the
storage resources, we can also connect arbitrary numbers of
host CPUs to the CXL network. Since the switch’s crossbar
(called fabric manager ) remembers each connection between
USPs and DSPs, we can fabricate a unique routing path begin-
ning from a host to one or more storage devices, called virtual
hierarchy (VH). Each VH guarantees that a storage device
can be mapped to a host, which is attached anywhere in the
CXL network. Thus, VHs allow the system to completely
disaggregate many PCIe storage devices from its multi-hostcomputing resources for the memory expansion. While these
reconﬁgurable VHs can realize a fully scale-out architecture,
memory resources expanded by the storage devices are un-
fortunately tricky to control ﬁnely. Since the storage device
should only be associated with a host, it can be underutilized
and/or unbalanced across different CPUs.
Storage device virtualization. To address this issue, we can
virtualize each storage device to be shared by different hosts.
Speciﬁcally, as shown in Figure 3c, CXL allows a system to
logically splits each endpoint into multiple Type 3 devices (up
to 16), called multiple logical device (MLD). Thus, we can
make each MLD deﬁne its own HDM, which can be mapped
to a different place of any host of system memory, similar
to a physical storage device. As each MLD associated with
the same storage device can be a part of different VHs, it is
expected to utilize the underlying storage resources better by
allocating the memory expanders in a ﬁne granular manner.
A disadvantage of multi-host VHs can be bandwidth shar-
ing and/or trafﬁc congestion. To support MLDs, PCIe storage
may require partitioning the underlying backend and internal
DRAMs, lowering the level of parallelism, and this can un-
fortunately reduce the bandwidth of each MLD. In addition,
as a single storage device (and a CXL switch) can be shared
by multiple hosts, the endpoint’s fabric can be congested
more than before. Since the performance of this multi-host
memory expansion varies based on diverse perspectives and
hardware conﬁgurations of CXL, it does need careful network
and storage designs.
7 EXTENSION FOR STORAGE CONTROL
As CXL’s Type 3 is designed for memory pooling, not block
storage, there are two issues that we can further consider and
discuss: i) latency ﬂuctuation and ii) data persistence .
CXL.mem and CXL.io do not strictly manage the turn-
around time of loads/stores as their CXL memory requests
can be served asynchronously. However, long latency is still
undesirable and can degrade the host’s overall performance.
For example, the PCIe storage device that we used for the
performance projection assumes that there are no internal
tasks (§5). The latency of internal tasks varies based on how
ﬁrmware operates, but all they can make the responsiveness
signiﬁcantly worse than usual. In addition, if host-side li-
braries such as PMDK [ 39] insist on data persistence, the cur-
rent ﬂushing mechanism of CXL can be insufﬁcient to handle
the underlying PCIe storage. Speciﬁcally, CXL provides a
global persistent ﬂush (GPF) register, which enforces all data
residing in the CXL network and SSD’s internal DRAMs to
be immediately written back to the backend media. This can
also make latency behaviors of the storage-integrated memory
expander(s) severely longer.
49HotStorage ’22, June 27–28, 2022, Virtual Event, USA Myoungsoo Jung
To overcome this, we suggest two simple features, i) deter-
minism and ii) bufferability , which can be annotated to CXL
messages and hint the host semantic to the underlying CXL
controllers. Note that CXL allows Type 3 to manage CXL.io
for diverse I/O speciﬁc demands (§3), and CXL.mem has a
reserved ﬁeld, which can be used for the annotation.
Latency and persistence controls. Determinism can be de-
ﬁned by two states, deterministic (DT) and non-deterministic
(ND). DT means a host wants Type 3 devices to serve the
tagged request without internal task involvement, whereas
ND makes the corresponding requests ﬁre-and-forget. For
example, if DT is speciﬁed, the target storage can schedule
one or more internal tasks to operate with the subsequent re-
quests (annotated by ND) or in idles. Bufferability can also be
composed by two states, bufferable (BF) and non-bufferable
(NB). When BF is annotated, the corresponding requests can
be cached or buffered in SSD’s internal DRAMs, while the
requests annotated by NB consider persistence as ﬁrst-class
citizens for their service. PCIe storage can then selectively
write the requests back to its block media, which can avoid
the situation where globally ﬂushing all data (sitting on its
large, internal DRAMs) to the block media at a time.
User scenarios. To cover diverse user scenarios, determin-
ism, bufferability, and GPF can be used in either an individ-
ual or a combination (e.g., BF+DT, BF+ND, NB+DT, and
NB+ND). For example, databases and transactional memories
(e.g., libpmem andlibpmemobj ) log the data when a trans-
action begins. Since the data associated with the log is not
necessary to be persistent before its commit, the host can log
the transactions with either BF+ND or NB+ND. During this
time, the storage can secure a time to perform internal tasks
by buffering all incoming writes. When there is a transaction
commit, it can conﬁgure GPF to ﬂush all buffered data and
write the commit (if needed) with NB+DT.
Since an operator of most instructions waits for its operand
arrivals, loads can typically take advantage of DT. However,
if there is no subsequent instruction that uses a result of
the instruction issued in the previous (i.e., read-after-write
dependency), the current loads do not need to be synchronized.
We can thus precisely use BF+DT or BF+ND for the loads,
which allows the storage to prefetch the data into its internal
DRAMs. For example, since data are somewhat engaged with
spatial/temporal localities in a loop code segment (e.g., matrix
calculation), we can let the storage know that the data will be
hit by the internal DRAMs sooner or later again.
Another use scenario to take advantage of the annotation
is lock and synchronization management. Their mechanisms
(e.g., spin, fence, and barrier) do not need persistence in
most cases, but the latency is the matter. For example, a spin-
lock uses an atomic instruction such as compare-and-swap or
compare-and-exchange, which is composed of a memory read
and a write. While the spinlock does not place its parametersin the CPU cache, the corresponding atomic instruction keeps
iterating to access the same memory address. Thus, it would
be better for both the loads/stores to access Type 3 devices
with BF+DT. Memory fences and barriers are also similar
to the spinlock as their lifetime is bounded to the running
process rather than the data.
8 CONCLUSION AND FUTURE WORK
This paper examines CXL from a memory expander view-
point and explores different conﬁgurations to transfer PCIe’s
block semantic to memory-compatible byte semantic. As our
performance projection is imperfect and limited to studying
diverse perspectives of a storage-integrated expander, we con-
sider extending this work by accommodating various software
and hardware environments. We also believe that the several
characteristics of CXL-based memory expansion that this pa-
per discussed will lead to many architectural changes in both
software and hardware, which can be worthwhile to study in
the near future.
REFERENCES
[1]Gen-Z Consortium. Gen-Z Final Speciﬁcations. https://genzconsortium.
org/speciﬁcations/.
[2]CCIX Consortium. CCIX Base Speciﬁcation 1.1. https://www.
ccixconsortium.com/library/speciﬁcation/.
[3]CXL Consortium. Compute Express Link Speciﬁcation Revision 2.0.
https://www.computeexpresslink.org/download-the-speciﬁcation.
[4]Gen-Z Consortium. Exploring the Future: CXL Consortium and Gen-Z
Consortium Sign Letter of Intent to Advance Interconnect Technology.
https://bit.ly/3tXPIod.
[5]Zhiyuan Guo, Yizhou Shan, Xuhao Luo, Yutong Huang, and Yiying
Zhang. Clio: A hardware-software co-designed disaggregated memory
system. In Proceedings of the 27th ACM International Conference
on Architectural Support for Programming Languages and Operating
Systems (ASPLOS) , 2022.
[6]Irina Calciu, M. Talha Imran, Ivan Puddu, Sanidhya Kashyap, Hasan Al
Maruf, Onur Mutlu, and Aasheesh Kolli. Rethinking software run-
times for disaggregated memory. In Proceedings of the 26th ACM
International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS) , 2021.
[7]Zixuan Wang, Joonseop Sim, Euicheol Lim, and Jishen Zhao. En-
abling efﬁcient large-scale deep learning training with cache coherent
disaggregated memory systems. In Proceedings of The 28th IEEE
International Symposium on High-Performance Computer Architecture
(HPCA) , 2022.
[8]Jie Zhang, Miryeong Kwon, Donghyun Gouk, Sungjoon Koh,
Nam Sung Kim, Mahmut Taylan Kandemir, and Myoungsoo Jung.
Revamping storage class memory with hardware automated memory-
over-storage solution. In Proceedings of the 48th Annual International
Symposium on Computer Architecture (ISCA) , 2021.
[9]Changmin Lee, Wonjae Shin, Dae Jeong Kim, Yongjun Yu, Sung-
Joon Kim, Taekyeong Ko, Deokho Seo, Jongmin Park, Kwanghee Lee,
Seongho Choi, Namhyung Kim, Vishak G, Arun George, Vishwas
V, Donghun Lee, Kangwoo Choi, Changbin Song, Dohan Kim, Insu
Choi, Ilgyu Jung, Yong Ho Song, and Jinman Han. Nvdimm-c: A
byte-addressable non-volatile memory module for compatibility with
50Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for Memory Expansion HotStorage ’22, June 27–28, 2022, Virtual Event, USA
standard ddr memory interfaces. In 2020 IEEE International Sympo-
sium on High Performance Computer Architecture (HPCA) , 2020.
[10] Ahmed Abulila, Vikram Sharma Mailthody, Zaid Qureshi, Jian Huang,
Nam Sung Kim, Jinjun Xiong, and Wen-mei Hwu. Flatﬂash: Exploiting
the byte-accessibility of ssds within a uniﬁed memory-storage hierar-
chy. In Proceedings of the Twenty-Fourth International Conference
on Architectural Support for Programming Languages and Operating
Systems (ASPLOS) , 2019.
[11] Xiaojian Liao, Youyou Lu, Erci Xu, and Jiwu Shu. Write dependency
disentanglement with {HORAE }. In 14th USENIX Symposium on
Operating Systems Design and Implementation (OSDI) .
[12] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K Aguilera, and Adam
Belay. {AIFM }:{High-Performance },{Application-Integrated }far
memory. In 14th USENIX Symposium on Operating Systems Design
and Implementation (OSDI) .
[13] Anastasios Papagiannis, Giorgos Xanthakis, Giorgos Saloustros, Mano-
lis Marazakis, and Angelos Bilas. Optimizing memory-mapped {I/O}
for fast storage devices. In 2020 USENIX Annual Technical Conference
(USENIX ATC) .
[14] Anirudh Badam and Vivek S Pai. {SSDAlloc }: Hybrid {SSD/RAM }
memory management made easy. In 8th USENIX Symposium on Net-
worked Systems Design and Implementation (NSDI) , 2011.
[15] Kan Wu, Zhihan Guo, Guanzhou Hu, Kaiwei Tu, Ramnatthan Alagap-
pan, Rathijit Sen, Kwanghyun Park, Andrea C Arpaci-Dusseau, and
Remzi H Arpaci-Dusseau. The storage hierarchy is not a hierarchy:
Optimizing caching on modern storage devices with orthus. In 19th
USENIX Conference on File and Storage Technologies (FAST) .
[16] S Kazama, S Gokita, S Kuwamura, E Yoshida, J Ogawa, and Y Honda.
Memory expansion technology using software-controlled ssd. In Proc.
Flash Memory Summit .
[17] Shin-Yeh Tsai, Yizhou Shan, and Yiying Zhang. Disaggregating Per-
sistent Memory and Controlling Them Remotely: An Exploration of
Passive Disaggregated Key-Value Stores . 2020.
[18] Duck-Ho Bae, Insoon Jo, Youra Adel Choi, Joo-Young Hwang,
Sangyeun Cho, Dong-Gi Lee, and Jaeheon Jeong. 2b-ssd: The case for
dual, byte- and block-addressable solid-state drives. In Proceedings of
the 45th Annual International Symposium on Computer Architecture
(ISCA) , 2018.
[19] Chander Chadha. NVMe SSD with Persistent Memory Region . shorturl.
at/hrPS3.
[20] Stephan Bates. Enabling Remote Access to Persitent Memory on an
I/O Subsystem using NVMe and RDMA. https://tinyurl.com/2jykndr6.
[21] Jaeho Kim, Donghee Lee, and Sam H Noh. Towards {SLO}complying
{SSDs }through {OPS}isolation. In 13th USENIX Conference on File
and Storage Technologies (FAST) , 2015.
[22] Bryan S Kim, Jongmoo Choi, and Sang Lyul Min. Design tradeoffs
for{SSD}reliability. In 17th USENIX Conference on File and Storage
Technologies (FAST 19) , 2019.
[23] Jie Zhang, Miryeong Kwon, Michael Swift, and Myoungsoo Jung.
Scalable parallel ﬂash ﬁrmware for many-core architectures. In 18th
USENIX Conference on File and Storage Technologies (FAST) , 2020.
[24] Myoungsoo Jung. {OpenExpress }: Fully hardware automated open
research framework for future fast {NVMe }devices. In 2020 USENIX
Annual Technical Conference (USENIX ATC) , 2020.
[25] Jian Xu and Steven Swanson. {NOV A }: A log-structured ﬁle system
for hybrid {Volatile/Non-volatile }main memories. In 14th USENIX
Conference on File and Storage Technologies (FAST) , 2016.
[26] PCISIG. PCI Express 6.0 Speciﬁcation. https://pcisig.com/pci-express-
6.0-speciﬁcation.
[27] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav
Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden,
Al Borchers, et al. In-datacenter performance analysis of a tensorprocessing unit. In Proceedings of the 44th annual international sym-
posium on computer architecture , 2017.
[28] Gala Yadgar, Eitan Yaakobi, and Assaf Schuster. Write once, get 50%
free: Saving {SSD}erase costs using {WOM }codes. In 13th USENIX
Conference on File and Storage Technologies (FAST) .
[29] Devesh Tiwari, Simona Boboila, Sudharshan Vazhkudai, Youngjae Kim,
Xiaosong Ma, Peter Desnoyers, and Yan Solihin. Active ﬂash: Towards
{Energy-Efﬁcient },{In-Situ }data analytics on {Extreme-Scale }ma-
chines. In 11th USENIX Conference on File and Storage Technologies
(FAST) .
[30] Miryeong Kwon, Donghyun Gouk, Changrim Lee, Byounggeun Kim,
Jooyoung Hwang, and Myoungsoo Jung. {DC-Store }: Eliminating
noisy neighbor containers using deterministic {I/O}performance and
resource isolation. In 18th USENIX Conference on File and Storage
Technologies (FAST) , 2020.
[31] Qiuping Wang, Jinhong Li, Wen Xia, Erik Kruus, Biplob Debnath,
and Patrick PC Lee. Austere ﬂash caching with deduplication and
compression. In 2020 USENIX Annual Technical Conference (USENIX
ATC) , 2020.
[32] Mohit Saxena, Yiying Zhang, Michael M Swift, Andrea C Arpaci-
Dusseau, and Remzi H Arpaci-Dusseau. Getting real: Lessons in
transitioning research simulations into hardware systems. In 11th
USENIX Conference on File and Storage Technologies (FAST) , 2013.
[33] Jie Zhang and Myoungsoo Jung. Flashabacus: a self-governing ﬂash-
based accelerator for low-power systems. In Proceedings of the Thir-
teenth EuroSys Conference , 2018.
[34] Myoungsoo Jung. Exploring design challenges in getting solid state
drives closer to cpu. IEEE Transactions on Computers , 2014.
[35] NVM Express Work Group. NVMe Base Speciﬁcation. https:
//nvmexpress.org/developers/nvme-speciﬁcation/.
[36] Gyuyoung Park and Myoungsoo Jung. Automatic-ssd: full hardware
automation over new memory for high performance and energy efﬁcient
pcie storage cards. In Proceedings of the 39th International Conference
on Computer-Aided Design , 2020.
[37] Wooseong Cheong, Chanho Yoon, Seonghoon Woo, Kyuwook Han,
Daehyun Kim, Chulseung Lee, Youra Choi, Shine Kim, Dongku Kang,
Geunyeong Yu, Jaehong Kim, Jaechun Park, Ki-Whan Song, Ki-Tae
Park, Sangyeun Cho, Hwaseok Oh, Daniel D.G. Lee, Jin-Hyeok Choi,
and Jaeheon Jeong. A ﬂash memory controller for 15us ultra-low-
latency ssd using high-speed 3d nand ﬂash with 3us read time. In 2018
IEEE International Solid State Circuits Conference (ISSCC) , 2018.
[38] E. Strohmaier and Hongzhang Shan. Apex-map: A global data access
benchmark to analyze hpc systems and parallel programming paradigms
(sc). In SC ’05: Proceedings of the 2005 ACM/IEEE Conference on
Supercomputing , 2005.
[39] Piotr Balcer. Persistent Memory Development Kit. https://pmem.io/
pmdk/.
51