{"docstore/metadata": {"b8cdebc3-981b-4217-a17f-fea2b3ce2ae7": {"doc_hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e"}, "ed6b6146-adac-499d-816a-1f74002b652c": {"doc_hash": "1fc7329da2e64f46fb0f63b950f84c52b789a9f8b50537d4d8680f4e8a5aa06a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "5da63216-b706-4685-9b8c-a0ad025c5806": {"doc_hash": "627d8541616f898fe21a5ba3074f57876843194d868e90b3e24e09ee265389f0", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c4f0d33c-0bbf-4a42-94fe-f53290f97758": {"doc_hash": "c14056ec4c0c58de97f53d59c2a329006549ca5f890c8afe74acbc490195aaeb", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c897fe5f-62f7-4bbf-8043-4ded07eea5e8": {"doc_hash": "0415b0f2659ee804cf042f8ac8b342f5ab0f3d68d11510d9bc675f588c451c88", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "bf48a779-ad85-4a9d-87bf-f94676b6728c": {"doc_hash": "798dce38933339521eb58f5bbcc53c4e491967fcf2b3112697ce83f94e8a6a39", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ec07150a-4831-4ef1-9819-591e267ee9ae": {"doc_hash": "448aaa4837711192481360db9c739e018d385c5e437a76a5f0fe6f530f403fdd", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "81836ed1-268a-4395-b138-205ac7f38a9c": {"doc_hash": "69ac03f40c490d73cf4539dde4c0e91284c374eb8205729acab4d1ee3b856a4a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "2ace6239-1ed7-4fca-8302-a95d18b1fafb": {"doc_hash": "fcae3c0208cd955f421079676a5a8d0b362e824af011b5ec28acf841faca7056", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "312b52ec-ad6b-41e2-856c-5c10cceadd38": {"doc_hash": "d5a970b6c0167c694be4308a06a2dd40897fe5b63758a2ef36be2c7312df343a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "11083665-f5eb-4a69-9a43-7c7255760241": {"doc_hash": "9a024e2ab2717e5f2f214712a1ba547c73c3b6645b545d7fd01f15c3ecc69f82", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "fe1f3513-cead-4c57-ae2a-68cda690a3d5": {"doc_hash": "ab77eabfb6b38bddf141f06df399ecc878d0be9b8a6aa9650357c2127317159a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "75d812fb-050b-4b65-9b13-29d14f2818a8": {"doc_hash": "e78b924d1d911a5c916e6d506fbd53853fab09ee5808a76efd0b92f413424c91", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "b9a53f0c-581d-4523-a78a-1f19ef79a7d7": {"doc_hash": "16d8f900176598d329e952171dbc27cdc521aaae4b722fd6d7002537d9b8c566", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "41bea6a3-1276-4edd-b33f-3f418e137bc9": {"doc_hash": "e50066624faba28098ac0d1b9483e39d4ce7d3bca4c061da78c1b520e29998fc", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "cfc06553-66c9-49ba-84f3-67616316a58d": {"doc_hash": "20e05470eb1a700fe138f5b970630cabe2a3829fef291d9869e8fd1034c62c43", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "016d1938-7134-46c1-8ecd-cef6acb466f5": {"doc_hash": "a6434fb479d387feed3803506fb07f7fdd01b378f34f7b2e56e1640970b200aa", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6d8593cb-0fd2-48e5-b80e-80e1d5aba226": {"doc_hash": "d49bc6fddff77287b977dccdc19ffcb330d5afbcdb243c5390475d3d43b8503f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e9e42b0b-217f-4338-9bbb-2f5807b86140": {"doc_hash": "36c9a13770d86b6d86f7a76c1284d4f66c669649392291325ef28d37034b7c3d", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d45df45a-1dd0-4c95-ba2c-2c68f2c7fa59": {"doc_hash": "4244ab120c1fa8e1cb18b4b8f708cf9d3f3c808f84196ae2958a0e3f6b2e326f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "887b03f6-95f2-434f-9bd9-27a1933719b8": {"doc_hash": "a7c5511f6b671e47da748a85e2b11b9ccd673942ab9b5c3798c17ec02680429f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "15a50813-ab7e-4446-9bbc-49d70ad11d31": {"doc_hash": "af94d7ed4398689737725220ba78a682c1575f12a237229578e16b451c6b49f8", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "a0af8f88-b4cf-4e9e-93ea-a875a5690e70": {"doc_hash": "14bfb65fe6f5a2e8809b95e7e55d869bee4daabbd64c3c3fc4374374735e9fc3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "59363e3b-4ff5-465f-a964-3a75f2d3ae6b": {"doc_hash": "a794534e2026f4206afebf5841cd934c70abe88172f6c6a33be266cca9d1faee", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0f5fa7b7-c0fb-47d4-8277-429cc49db8f1": {"doc_hash": "5fb5c9e4c6122bc7cced5c036273fc8c71eecbf5657c097b9667afe24458ee96", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ea8b4e8b-002e-4241-95ae-0b649616fc5d": {"doc_hash": "69903fead55c731294a91c40b710ead512ab55be1f4b238e53a6de00f3c8d852", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "73af9a00-d3f2-40b5-b2cf-5c45fc4b34ae": {"doc_hash": "f1eda2e59b8043737f010b4ba90969622de59454a1f08c29fe31e2235b2fd6f1", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "bc32b152-847a-49d4-a552-4a5b9853fd6a": {"doc_hash": "4dfac36c1023d5abfc12a9d1a2d0665eaed7f9fbb0def3b8dc301ec22e884177", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e6fc7030-49ed-45fa-a00a-d44b3f90001a": {"doc_hash": "db72e9f03b2b0a78c93668ae0bfc8637a46ac3b1bfb758916f2fca5a5c8bd26a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "96242707-6156-484e-a363-11d4f6942758": {"doc_hash": "1d107058be47e8acfa081c7d4257cd66bf64e2febf359b5e6f7488c7662d2e41", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "7dc46b2f-7b61-4d28-b26d-e2a3e672f772": {"doc_hash": "8946cdeb6b8c719628c6402ecb9186e7f0e4aa1e8a3f089122939bc7ef8c6cb1", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "41ffe974-2995-4f92-b39c-0abb35266fa2": {"doc_hash": "39d64c3399a829bea023b0aebcec229717d7c90d831f11f6e1a4dd4cb82c96e8", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c45d1533-f68e-4420-88f2-81bec47e3ed1": {"doc_hash": "892f40e668fea933eb53151fc9c7954b24cf6c053df21270ed36eb913b08d11c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "bd25aaa0-33b5-4274-baf1-ff09840de5c7": {"doc_hash": "8db37e8df295dbddac4279b02f7307802629be986c164e4b97a2a78e69f86067", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4fa95e3c-09bf-48ad-8a3a-cbf000a257bb": {"doc_hash": "2ceef4642811f18d25316623d87f620c00cb90c869525eff9533198ac6f21d96", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "9074039c-bf1f-40b7-b8ed-e11dd11a54e2": {"doc_hash": "751d76d09acada6d8755b43e77031f46f57a281cf767455eeba45ecdc49910c8", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e7a70987-015e-4694-ace1-7f054cce01cb": {"doc_hash": "9765edd818e3f504eae99ce984fecf484f15d646a8e8b8658f929a9aecd0d43c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "97faf31f-b25b-4ef8-bc78-7823842030a2": {"doc_hash": "78211abc55a9735eb3c721f604931aa6a89e37ad991d0a0a98684684f37fce7c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "56c6d236-6baa-4a64-ab27-d608c2c1ebad": {"doc_hash": "83444a0e4758474f400976da348513e50d19bc5f2cd2f7aadcad46782d9af0c1", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0013c566-9315-4dc6-a738-5a7574f3c2fc": {"doc_hash": "bc532af7f6c8f2c5b16dc61fa1e938e379029a574d71ef303948050304dd61ed", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "35f42536-787c-4c6f-925e-995f9ee1e05a": {"doc_hash": "52402262e99406a5a6eaa40a26d4d68bd82b3bdc53669c9b5b825ec10c6467ae", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f647646d-ed60-48f7-a378-2bb5cd7f479f": {"doc_hash": "cabc2a652911fe1305a352821419ef2fe2dccf21943889254ccd2d2658e11d4b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ddcb1049-d906-4483-8f9f-8bfe4084dea8": {"doc_hash": "5fdab019381fc57e0f94dc5a6b40fde000c86254acf4b190f6285cd5c170d179", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "184d9667-8304-41ba-badd-fd153c1ace58": {"doc_hash": "e42d59952a1361920eacdebfdcc692c8840648eb6787078093f582a385cba6f5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f11dba25-2128-4c36-ace8-893438bbc078": {"doc_hash": "768d61c93ecee512918f27589da2f1bee5617987717dca712a1b259891ac4237", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "06a72cc0-adc8-412e-8104-032a5b7da43b": {"doc_hash": "caa005c56a56bd095d4cd3d67de68cbe428ec458b1b745d0fb87da9c33c13a88", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "75c0f980-7f75-4323-9278-0a5f07aaa926": {"doc_hash": "5ff1cfb257e1cbe1e72b12ae7fe459a7187c3ad53708a80b724d99c2a4e4dc14", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "7bf5bdac-2348-40e0-9b47-c2b450dd21c3": {"doc_hash": "00657adc4f122f9193087f2482938457e684e8210cc8b5e4a81868fe7fa825fa", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6ccfe094-f374-4f17-aa3d-e3bb886cd1a2": {"doc_hash": "455a575c2e086742360fa157df1391050d85a2207da13cfcfbcf5cfef44cd369", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4d275945-bc9f-4c98-843f-b54d2c2d8027": {"doc_hash": "ae9ce6f1ce6e42300dad38f84d4fcd8026c1f8478d2e8f5864e6732bada598ad", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ba3feacb-988e-4add-9fc0-12dbb00113e3": {"doc_hash": "d71075d40d6510a5e47d48878549c2134ec3ce9117d2df857e8a770074084338", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "cd66b23d-bb5b-4842-9e96-a7402bee9c83": {"doc_hash": "a3b5a1e15cceb61b22e5f18390f0462b54060f7b8476c0a6cbaf3a9286ccbe9a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "a70687a4-4339-4530-8c1d-6692d20abeb4": {"doc_hash": "05c19d869215f41adc9939340f28a55915ded346dd80a47561dfcd88691c6f0d", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ceffa9b3-773c-459c-9dfc-5a0b37c914a9": {"doc_hash": "6e93868da967a79f7a1306105e43c2621b0afb032244749e4a212ab16d300207", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "87db7a65-c790-4496-ab37-297be4091b39": {"doc_hash": "7c699bf4e223d729ac17df83a6645f0ea77328a7db317f54898396dcac227da5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "2e737753-ff6d-43e0-9d47-d6e09794c88f": {"doc_hash": "e4ca88302f8cea033172ee77b791db3605c3457c97d4d8599ae8977883a475e6", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "2bdb7f1f-8d70-49a3-aa9e-6cf9cb5cb675": {"doc_hash": "eb313a89f133c5cbd18a93b37eefc35724dd6756224f97734f46a781b213d822", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "04e3b033-ce68-4fa3-85ed-208e76d24aa4": {"doc_hash": "9c93ae0c9bf3f5ab82e9677de9bd61c472cf86d884f4b03db11957465ba0edb4", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c16220b3-782a-4dca-a043-d67d87e13ef5": {"doc_hash": "efc527b2797a35ff8eaef3c8dcea653432fed8631da0becba7e6319836b4d6e9", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "1c22c8b2-917a-4bc4-9b05-ffbc00996994": {"doc_hash": "6d67b7fa3de825c5f165c4a5f9de32fa5f962f6eb101853a79e8e816be391b93", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c8121bbc-0bcf-4685-ae53-91cd00fee759": {"doc_hash": "a51e78c29d4c29602cfe3dbe2c6a86c9eb041147d35e1933bf138f96e7ff3d64", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "bdbf5660-da8a-45d5-b71c-48b697ac05a0": {"doc_hash": "a0a6c6d1b6baf853a82d1382ab1e1db29e3501d47759fbb1b7862c4135e08d2c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "adf863d0-375c-4ee2-8f06-32abf56f015e": {"doc_hash": "03bef61ce4691f6e33a255203f255d08e558a2ddbda865a8510444f8044145ca", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "fae38f8c-f20d-43f8-8a84-ab06b6ee8e72": {"doc_hash": "352f0314e24f03c0f37381f28f2d7e37a8742f96f2721afd0542b77e1da7a4d2", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "bf71de70-e0dd-4720-9836-00a3a285651a": {"doc_hash": "f2269fa66abe339766d9aac2755675722fadd098cb705d46d14571c864624e90", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ba191825-e457-4f48-8512-4404c8d3d07d": {"doc_hash": "b1655a24b44cc7a2937ebf13aa426c3e943ecb6af19c41419bec424225035c05", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "eed987cc-15d5-45e8-b75f-bcd083e6bd52": {"doc_hash": "190b46330d4ab87aa41d80e01abee1456072b85791da09502ffc0b6a64296bad", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "51e4fc95-119f-4c1e-ab3c-dbc6f1d9e016": {"doc_hash": "897c3b117d076da78b229da6871e93424e2d6b73699d31ce98050ee87221193b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f30fca24-6fc0-4279-86a4-2dd10e29cc95": {"doc_hash": "83b34114c0f1ce0ee684034fe3abde609f3120e18d0156e1d0361cca91e5c658", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6b845fca-806e-4a2d-a56e-adef1d1a801e": {"doc_hash": "a1869cae6ed37dd474cff7d0507e7c10ee12b12e6ea4e34d8f7239ccbd1c8d33", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "9cd57eaf-b137-471c-ad3d-1992d042c545": {"doc_hash": "10348ef0ff7b9157199bcc884eaf729a20847a3d7063601dd6a29852dabbf920", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "8f02924f-892d-4188-98d1-f076d9da3d7b": {"doc_hash": "46d2eeb6cb5b1123cf5862d91ba3256a5e17759a92754182fd1210ce336f1b0e", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "677ff985-63dc-4501-92d1-70a7d67bc0da": {"doc_hash": "af2e9dcb8f86bc9780c925a2810fdc913d67f011a8bec3817ab324c43002ba92", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "a111e300-1b3a-4824-bcb1-959aa96ef9e2": {"doc_hash": "18606538ef7978dcd7f6046f6a0a641d86d305ed2e5b4f4f60f361633f76d6e3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "56f47d77-b474-4ad6-b976-9c2c6ecf7241": {"doc_hash": "a122bced1a3b32e24fe8964b76c0490565f4055c62f0ef3199bd080c34882d5d", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "94081e9e-93f6-410a-9549-c877c44fd91f": {"doc_hash": "6abfdba09793776fcc0831690e8c06ad2c791d148ee702d0ac67e507178da341", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e159add2-82e8-47b7-9131-723a58681f56": {"doc_hash": "b1ae7772b8cfcde66363d7768b729720664682b443b93605e23bfaa12f998c00", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4e3f37ab-7553-4323-bf4f-44a7fefc570d": {"doc_hash": "4c1082da7df79deec9faf4c583eb56dbf8c50f190eb43246e98c8133be2a60c8", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "937b97a3-2a00-4f64-afd0-ba687e2576c5": {"doc_hash": "fa4904bc6a30ef1fec1e7c5883c88f9a0d74285516ee9f7d0b6daf3b4a36671c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "eb0df7ac-47f5-44ea-a6a6-6ac85621d23d": {"doc_hash": "e2b2528cc351835400ebc4cacad5e4a6d1f56d2e390235f0afb721bf9f14c538", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e40169f7-be14-405b-b487-c47098368872": {"doc_hash": "03952b252ec2a4fa5f618bd697152de61432468ac8717d14622c358ef27dec60", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "eb6a610b-80a9-48fc-818a-9cc1462a6ce5": {"doc_hash": "9ea5ad1ccdf6f04ace7640f9623a2d9b09813fa20d0018ab2e26808d255fc665", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "a5bccb49-caae-496e-83b0-b00a7b7e2a94": {"doc_hash": "05a74acb5dab1154fbbba6499d54eeb8a262aa6f714504f086952a1c7790850c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d68afe09-1348-4e69-b047-0b9d33aaebc3": {"doc_hash": "5a5d43528ab91521a0d4f28a888af3128a6aaedc917f941c53c583c8a3a55401", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "5f6e8d4e-22e7-45f2-b464-ada1bc70641f": {"doc_hash": "d95ca65caf8904b5fed28d2f37d94439c97c53c199133e2e00c5cd308d347642", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0790d16d-c003-4978-9ed9-11405ee0326c": {"doc_hash": "a67d0239a89b919157b6daebf190fb612b1ab83efe612e47813679fd07ee857b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0497b5fd-6d94-4a24-bde8-10e8ece24a24": {"doc_hash": "46a5d8173f7a00a3e4b98cb6593b618e4b394b8df03f5462f623bd42bab27f87", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "58c3e2d1-9ecd-4012-900b-206e9e1dc999": {"doc_hash": "acd1a388ea6c1efec27e8c4a81822e8afb92233e9fedd4343ce64b029e149704", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "5a4b1129-a3a2-42a1-90d6-7731f4589f32": {"doc_hash": "9d42c526f58269ba82d5ad342abc3d89593041abfb843c7f744cde09d56f6f64", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "7def49cb-991a-4787-854d-e651725ba458": {"doc_hash": "32a1bcb0eb30f53f898560116c8859ae89f9797b410e0f8af53de664472b2fdf", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3d4cd4f1-ed22-4aba-b0ce-1a8cbee31620": {"doc_hash": "af1f79375e479c4b20ac3586f258665c2aa07f00650be12c2a3f5ddecb23682a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4779ff58-fafa-48b0-87f6-f5ceba14be18": {"doc_hash": "921a00385e4ec4b6958df204d54081fed03356f793df2d678d7dc751ec7165fa", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ea7b9ea5-a5ff-4798-9593-b5ef1e2b349d": {"doc_hash": "2c1c87af658e478704c8b0a2a2c4decd277fd5b337002255f907e19b9b399800", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "48ae52fc-9f5f-449c-8859-76fcffb3dbe7": {"doc_hash": "84c56e7f03d85509d608ba664cdd074991dc21ff19424163dc2b1c3a4da0997a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4575b79e-aa35-4cc0-bcc4-ce7b079d113f": {"doc_hash": "44a4283ab43018a6b77f3c06dafd045525555b09ffa0ebf275c3a99f1424c5da", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0cf51d9e-9b84-4b14-8b4b-bc3ee0310262": {"doc_hash": "e665a07d661078718106cb17c5eb7b1409a121ebfca2c603e2430b63c5ed30e3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "b1da21a9-4a9b-469e-9def-821dc49ed7fe": {"doc_hash": "de5214d5ed8b6bba9009161659ec9abeb8ef0df6682000d5c65585c53a280ee0", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4021aae7-49d7-4a21-80fc-3d35da841496": {"doc_hash": "fb1485165b6b1667fddb88ecfbeec522d262f13ca03be364bb5b3fb5b075d9bb", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3a430a03-545c-4596-ab32-2cdac57973b7": {"doc_hash": "8db9ef6ff10c721e69a37c61a6d3a03fc37db02218d5083042ab227c8707347a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "bc9d39b9-60a0-4e3e-b480-9da132b25203": {"doc_hash": "3860bf5b881ef7ae8be2c9fbf85cd676dd4f77c8a444d1caad9adfc3cd626fcd", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d8896edc-6004-4c68-9228-f16cfeaff75e": {"doc_hash": "074e2ec1a3672b6b07d708e70363d1d98835228abc96a4c55e5e9f019735830a", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "67db5ecd-6e7b-4aee-9e82-48f02b2e03b8": {"doc_hash": "fb1d05212b3cc1b947dac3fb0302507f5ee6a1fdafe3905aaddca3b70fd61480", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "37b81762-f267-4f8d-9017-9db8d509e975": {"doc_hash": "a6ea767b19cf411ecf142dab1607e6063b9206fc86df51dd3a077ffa4a75bad8", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0e779ca7-4861-4d33-855b-b3b21063742a": {"doc_hash": "4c596d7e639961b46eafafcca4ddf47a82dcd2a2cf6fe1a9453df1259213263e", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6641cf14-a7b1-4e2b-807c-99393c7e8ac0": {"doc_hash": "45185d99e7aab1576e9592bed6213caaa2ffcc96e07a70ead0e1d09d90ab0b32", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d17977e2-d35b-462f-ade4-b269e29460e4": {"doc_hash": "72a03d9ea5d5f0bf199e7ac2eed20437d301dcee276120dc574b8181cb36c028", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "602cd835-3c9c-4857-8f70-c7edb7a955df": {"doc_hash": "aa5f8bc874ea8334b9e03dd4aefb66a6781e2a739c64feb4b4399d380dfb8d27", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "deeedbde-9e19-4aba-a3f7-ddbf39e73d2d": {"doc_hash": "2e3ae57cd5e42f7a96b6ceab141792cd98ec8b6c6fe28cd5829176ecd9099349", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0efb634c-c11e-4c57-8645-4c5a7ca6d9a9": {"doc_hash": "4e366fab10b1eb04d5989a0ecb7de8d0827d84187922fbedf80c8e1df604c7c2", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "37e79361-c5f8-4833-b9a5-794209ac3182": {"doc_hash": "c37840327c7716f9280a01b3fec53aad66d6e14a73ddf4da60cb98bc515747e1", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "45199acd-ee0e-4343-aac1-1cd58e81fd8d": {"doc_hash": "9867c0c5107db64fe758c3430e05d13c30d7313fb352bbb028ec251a8064cc12", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "b3d8d233-48d7-4a03-936e-388edd575300": {"doc_hash": "72e20a9cfb1ce418054a2be832808fc9b28fe51c95fed9755d3d9645fc8e1bf8", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "a4ca2cdf-0048-46af-85f5-fc2137dabd26": {"doc_hash": "c53a1706cc63f855276d2d007342e525b20a06c3bff50d357f8c07b4b47b9348", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "5c993fdc-0b5f-4dd6-947f-b8fcd33a290f": {"doc_hash": "d96b48d0ab3cd68539ee645744b40b00790e7157d2d13ac43271e564c231b0f1", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "71c1feb2-4a79-4fa8-8250-d7b035288089": {"doc_hash": "8309a91738ddc09cfd3183022267c884184558488d3083f585921611f03c2745", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e234e9ff-705f-460d-9cae-c30e20f143d2": {"doc_hash": "6f4c91fa162d5091f4328c3a58a6ffa50e0519a2996e7329ceeea3072c0af933", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "423a9b60-d907-4785-998a-de5a66e423f6": {"doc_hash": "b39375632cab66d00d86bcb25e52ff9f932206f316d6506d08988243ac8c8ed8", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3950ad46-d5e6-4104-8928-d0389d63a5a0": {"doc_hash": "c0f9c8c45cd33b8e05dfed88b0834893acb77a33d53cce2f1646199bd1546971", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "8598ad6f-8201-461a-b638-280ae196404b": {"doc_hash": "e2c64d9c1b5aeb9b6dc8267587064e1245482037d7124a1117ff8870c2633782", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c3241294-0319-40f5-9621-552eed1e6f31": {"doc_hash": "f5b74623978e103946a0a9a08f15b82be7845f9cfe11b224d21111ed5c1f0cb0", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f4fc44dd-3527-468d-a425-d38d53a4dc60": {"doc_hash": "f0fa6ad4465ecd1ab7ed0d3ff921f95d37371e452ea8122830d99707d1c51c8e", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "044588df-9bf0-4ec1-ad00-fc7beb27d862": {"doc_hash": "239c2e95d01de5d6ce331b2000571773c66ef217c400ba70323474a670a2af43", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "faf07e91-0dfd-40f7-92dc-9e297004be7a": {"doc_hash": "3e7cc416cef4dc8728958e9cf87ae3ce64ee81d12f93d698d1dadce5a450921f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "9c138416-d3d9-4e8c-8d99-45e5a27ec7ef": {"doc_hash": "f6e8fe8f17c87331ec00859e15f53fa3e03cd9afdedd1ea21e5f1d7638b94b74", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "1296a4eb-0be6-49ee-90c6-e3cdf452a49f": {"doc_hash": "6514e7f2f6529e639f294f5a6aa4163df2556152d8f382d1f08bdbd445b93318", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "90249a5e-78b3-4491-9e0d-44026ffa83b1": {"doc_hash": "093633a6955248b575e0a0f5ac8068a5117b333857b26e385dd05e8494449a99", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f1ef47a4-90ea-4539-91cf-381e6cc9ecfc": {"doc_hash": "2c463089f600e0d68031b71ef94a2bc0120beecbb795c4303cbb230a925926ff", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "1eb8427b-71a5-4afc-b935-7c7fdd2381ab": {"doc_hash": "118dd94f213b6f6167095b919204839a409c88888eac0f4c749dd2775dbdfef5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "687a1876-f339-459e-bae0-5720824202fd": {"doc_hash": "001791e478f99d7fae0c21ac6e962e6cc970e625c80921cc532ee7be2d73f7de", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d2a7991e-013d-40df-92bb-55b47af7f667": {"doc_hash": "862882a27c5c046a956db6f755a58dec0536219831fd8b1fb1418029c007b8ee", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "639b4890-c430-4b46-80ea-217d668af544": {"doc_hash": "6841bf5b9048540ae66630ae22ab33eaebcd21f623fbce9c4341afb01d21de53", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4ccb9eb1-172d-4e68-99db-dc6a1d19c336": {"doc_hash": "a10e250ef5979c4234662610c936b301956b2f038f39454dc8d7c9c3421e3997", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3b971bea-5642-4838-9732-9c5331254a9c": {"doc_hash": "2db17f12eeb7c02e69d13c2fadb19c76c24d0bdfa3b33c9711843a92cc64d7d7", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "77524f49-007e-4937-b8f0-dd689bdd8b87": {"doc_hash": "65e155af3bd16ca6aaeae0399a40a408e31046f7acde2d5afd36796a335a4e65", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "259d203e-0612-44ee-931f-5824178bcbf0": {"doc_hash": "f68e413015a664dfef7cc544b12a6504d21278904640b725247093596f137ecd", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e1a8719f-215b-40bf-9f98-6974070adfc9": {"doc_hash": "f8222d0c7f280bd8510573042627ff6df86d85f015ff00e8bf3382ae5142464f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c5e732c1-b84d-466a-95ba-baf106746856": {"doc_hash": "642be1485e02d8e2aff01ca2016617e1a919d7f5c3c14bbfca4d358062af416d", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "503456f6-e46d-47f5-bbf9-efbea49c4860": {"doc_hash": "d536a4993782daee92c6f636c26ba68f73e124b7018f51f0ee895c5fd7b19e6f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "30375e0b-837a-4a1b-afcf-b29ec201e7d1": {"doc_hash": "dc048d4beb7cb5c082426a749af3ad27ea430633d9a3886d5fad7e569941f3cc", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e17d91ef-49a3-4489-a260-247b78a20864": {"doc_hash": "f158dcf7a8fe880303e08a50795ded8a3cdd2059dd80902fe67b53a141f48c57", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "fd654f44-dcfd-4468-a3b5-f30bb6a8e976": {"doc_hash": "b2b5fa3658bc0faf8e3d852637c5742dcb338187558d8b5c82ebfda1dd6051da", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ae1bfc67-d98d-47f1-88fc-cf43b4791fc7": {"doc_hash": "aeb7e5478086634fa811cbae4bde2613b1493cf68f1c1b7b345989bfc419d8cd", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "07ddc96d-d9c3-4dab-b48f-48225e9d7a34": {"doc_hash": "eab34f89fbbeb399314cd55763fccc15302b2fcaa91447191884f384a10e9e0d", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0fa91dd3-e817-4c11-b195-03ea4dd8ca61": {"doc_hash": "d8f56901df85921cabee92daa15ad346409aba91d221276f54ff67a53757aeac", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "a307a07e-b57b-4d32-98e1-bcb7d8794f0b": {"doc_hash": "bf1d635da8bee82b564be14bf97a20eda0f05e648a1e61b587ef5e26cf9b8947", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "5e61b449-85e3-43ab-a1ce-2e0264918692": {"doc_hash": "9f575310b82304e29d5ac3fef5dac7442218d85cefb66db21d94e7bf2cd0e8a3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3e947418-3303-46b3-8413-10f44f2bd0ca": {"doc_hash": "f9302462d498eff1263220e6f85a3f1c44dbdcbbdfde7fb39696a4754b6da4ad", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "48844f4c-16e1-4406-9fcb-2895e6a62933": {"doc_hash": "6d01f9ad68aff0b5827483324a51d5cc199a00f77e02e1056a6af5878d2ea866", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "cf07094a-e885-4242-ae58-6185d8c2e187": {"doc_hash": "ab0ca26331c6f3cf5d1ba6a51eb6421f71d32979156093493e172a5d6539601e", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f07c154e-8ac2-4e81-856e-5e2ac0359d72": {"doc_hash": "14e1fde9cb754064a096a1ddaed38d80ee7ee920cd0d6af7675fdc6e8bdc0fff", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f55a8490-a1f0-4c19-b905-43b4af61b4d3": {"doc_hash": "3c8c9752f4475882d8310b8b91a19c3111f1317d8215796c6a0f7f58df6f87ea", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d2ec76ca-350f-4e78-8b63-760941b6050f": {"doc_hash": "d6739ddc3bf91469e4e901ca0f1a778be7184f6fededc8e476cd552b321cc191", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d918ccac-5314-4ce4-b20d-7d9e71fef1c5": {"doc_hash": "3b3a5214e7e9b1aacefeab565216ecab3a957f7d2898cd6729453dca665696fb", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "2bcdcd5f-8502-4f88-b03c-d4cf1903d2f9": {"doc_hash": "e9125e3efbfe6cf45b1d73d8aed42a683e852aee5aee4389aedcf9fc5130c410", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ce6c6699-936b-4bf0-97c7-d487c74e5449": {"doc_hash": "b0d7a7ed197f0da55b4c85c4571307afeba3220455d88157d66dcefba53fcb65", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0316081b-bcb9-4172-b58e-6ac439bf5871": {"doc_hash": "5e4245dd4c3024fbc016768f36cfed38bdeb30211582fa5ff939b9919c35ac3d", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4cc89d25-1d1a-440f-a270-5c8c662d68b2": {"doc_hash": "8675d6a82b99350612bf2f069590e4c9eafea8f1b3d4a0f661a45799443dcf64", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f4af30c0-29b2-4a8c-b56a-b594d9a18fd7": {"doc_hash": "1e4306bcd34c5a804b2801c3368029587afbd7efe9d1c228b8d037dd00e89f23", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "467bd94b-41ad-4b0a-a709-c9bd3bf6f5b5": {"doc_hash": "8219f2a66149244780cf990ca4467df88e18245d94352d3d26d3acdac7356991", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "78451caa-56b9-446f-83db-c4248359c47f": {"doc_hash": "e39f02341071cd78d0124208334e69910a3fe48dff2fe2bf54e55cf904721b39", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "7e5a5b45-efbe-4187-9e4b-0ba344ea2b4a": {"doc_hash": "42df2277c429be95e21629e4ad571af994dd8ec22c3e358ff54c140f02675e6b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "eb2fab86-fbcd-43bf-a248-1045590d6439": {"doc_hash": "e6c75c0ee16032579b5ab7e10b311dd72efa3eebdaad9b7c19835bd589dcc844", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "098a79dc-c1f1-412c-adcd-035eb5d1c2b9": {"doc_hash": "a36fa093f698d3f7e0d5de981e1041b0dc84dfcce129f666ba7ff4e298d4997f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6e0a5b1d-80a0-41de-9c79-563bfb4ceaf3": {"doc_hash": "98884d8074b1a667a6b201170026e750e533f34f4df4d8ddfea666978b37b450", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0fe6f05b-2220-444b-b458-4f60e288f309": {"doc_hash": "e4038436923816cfa1814b29deb355766237e6539f3e494c53f737959c20868b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "8c5546bf-644d-4c3d-bc04-821d9fe301d8": {"doc_hash": "6d8b3bf408ea24db1c277345297c7327549641d64ea121522fb60a904376c5fb", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3ad1ca74-d927-4467-9603-8209d5b0c177": {"doc_hash": "8842d4474afd15da0ab394c8935feaea284a8729924646b1c388029c060db175", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "fb57fbb8-c588-4ef3-bfa7-5368b8056ec8": {"doc_hash": "ba3eed62e55fd073b007834d1d5bc091f755fb4c7e349f81f9590cdc1cbac864", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "7063425c-d2a7-437e-b7c7-08499595b055": {"doc_hash": "fffaca73eacd3e0738718738060bf77e23b7c56839bd8eac9d60ce88141a845c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "32132b13-671e-400b-ac63-5e4e8baf9ebe": {"doc_hash": "5d774b2bdbbb0bc38b8d82dbcb897fb3ef391653254e1dbbded4de4f4170ecb3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "71ea6d5c-e554-49bf-b7cb-ed90207bd8ba": {"doc_hash": "2d82d9a84656a369bea77bc9cdfedf6753c7ef14ab0724269894ca1ef208f81c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f8e3e49d-e209-46da-8ca0-2ec2e1651030": {"doc_hash": "4b3391cbb95d337d48a9f28a1531c1ef9132709a123fed1932c072d3931b975c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0ade1b98-f700-472b-92bd-2e1ff740d4be": {"doc_hash": "5e28f36de92895733e9981ba03bc1885fb414c44bd4cf16ef45aef4a21e10dad", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "85904e96-d6fa-4d3e-a12d-30718b7725d0": {"doc_hash": "67ff560077ed30d89ee106b49742870ab3ae8e57afb3c46aaecac75517e185e9", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6f9b0d31-3ba2-4f83-9a0c-2bcec1c61d97": {"doc_hash": "0613c74fe2ffbbc3272f4608e298830ee91992aa5c8a74c438082f4345135444", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "84f2cd68-89d1-43e4-9be9-7aeb8b5a54c5": {"doc_hash": "3fbecb6fdb1af2d62a7ae5b426c7a6cc71e348626c50be746722bb5ec0820f6f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "5494beb7-f295-408e-a525-90452352328e": {"doc_hash": "7a7959d4c52aef1600dd931ce51644e3ab7073e52a3ac23322999c3a95aac639", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3825240b-86c6-4ed6-bc66-9c6f428ac893": {"doc_hash": "c7d7b78ba103ca5fb7a5cee0b304971abdd9b915f1c4780f0a63b51b00500728", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "90580101-1853-46bb-8bdb-f1a6905e8017": {"doc_hash": "6aaf47a2947647bfe9d858b5177c23a9e8e39029e25e3d1eebe2d55154ca68ac", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "7cd155e9-ea68-46a1-a4a5-eec25df2a6cd": {"doc_hash": "3abbbfbadc76f1d644dba3dcf5dc2b61848aaaa9161072cc0cc6ea6272e67811", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6a6b4007-a624-498d-8967-9012ac02aa30": {"doc_hash": "7ce14ce82c9c6639e9232c41a3e8eaf1ca49b72ea22d4e1c1ca134386ab7e51b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ac430d4f-1d10-4fb8-ad2a-7b57b2aec456": {"doc_hash": "5749635adeecaeef40c3a7e008623a11e5cd77c60fcd7debe52ac0f5a6703af0", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d33fe1bc-f856-494d-893f-4e9b941e93e0": {"doc_hash": "0061f63daa198b64843b5c6f48a6c2d655d024f762492437151a727d21761b94", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "9a8e8823-6aff-40ca-9ef3-0243719fc978": {"doc_hash": "a9732c8df01c324fb0eb9afaa16db4fc9616e901da94a6b9dfb1ebd2ff60723d", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4289d59c-11ae-426b-9242-63779a78e1e6": {"doc_hash": "e562e2e83f6c735bc24a528e4bddcd89a2556534809c5bdbc3f4c1ac38432919", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6c385b07-63a7-47a8-8cb6-28f8b9451eee": {"doc_hash": "707510997cdec1363361cd2e77bbd070505a0b956f930dc12849a17e30ea5d50", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3cbd77c1-40db-4bc4-9d24-fd207b4e9aad": {"doc_hash": "58e3bd543480842e56f8c4cc07f8b17d061a663a6d72bce3df52b9e721bc6ef4", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "35e8b006-71a1-4d0a-85d2-77d804f95787": {"doc_hash": "3057718aa7a63bd1cccbae83ea8b35ba1c503ae62410da5db8b8bfae91975ec1", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "bdd6d74c-21de-44ed-a1eb-bf097c06ddc3": {"doc_hash": "992a46fc159502702da2128982ed6cf30430e5ee25b5223eb38eb325cf6faff1", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "37593358-e774-4741-9978-f10dced5d88e": {"doc_hash": "61246e9b228e0043dec901960120499fcd29b0cd08986a00b3ef92eeb82a2b0f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "356aa619-1ff6-42d0-9af2-a450d6e2082f": {"doc_hash": "3f560f147b75e7e55be77f4505588c1d378be360cdeddc47e21d2f0161af64b3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "163c397a-21c7-4cc0-a296-82647ae298dc": {"doc_hash": "0acadee43e7280c1c498680331890a5caada7e54387fd42ca3c4cd345a5f0298", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "09c535d1-6be3-4eb9-acdc-dd6a90cec0fc": {"doc_hash": "7a5195e1a96e2dd92ee833d0d22f0717021767dd981afc53d87a0dfac9b852ca", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3c31b763-6004-43ef-ac2d-5d78e0d62b79": {"doc_hash": "28f538d6994fccf4f883a52583adc43904acb59329b76cff3b259f2fbbcfbbbf", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "91245482-9d09-44f6-8934-e9550d494389": {"doc_hash": "e79153ddae18d24dc9f8747bdfefccd02d43234adb14a86c5d7bd7eb1202f9bd", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "84b35d94-701e-4ee9-90ba-d565c8694fd4": {"doc_hash": "c5b70e272f5f4523c2917305c4b9215709b4386988f2b36e3de6d2762b2d8ff9", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "b5720807-474f-45d7-903e-0bbd458ae2b3": {"doc_hash": "8e0deab758c7653056778e2082d0a70e9533841c4e51645f70e68bc85299eeb0", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "96e2d7a6-f347-4f7f-9d4a-19d8f735486e": {"doc_hash": "f90c75c7ea18f3337b5e2f03c5add7d531d4238df6415c871ebee35a98e794e7", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "696b14d9-e527-415a-8d2f-6b3ba25095ad": {"doc_hash": "1caf6addf332631c0df56f2b9b9d3b22a801d3ee7276b85c78a0bfecf1d7a447", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ad9d8f3c-92f8-431f-af0a-3b7caa9f335d": {"doc_hash": "701865267d63d2a218cdc8a15c1a26a68a48d7a0d98776a396aad7861331cc8f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "60d4c6d4-91e6-40d5-9b5d-ea8a95590c6f": {"doc_hash": "a2c726de57d8d495744e41f5082466233f7e1a5918c09b68349c41576ce8bb4c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "9aaf7c42-afb3-4904-9588-745ece879401": {"doc_hash": "f6955b6558f4d4459e334b48e63f57d3afd132ed73839e2400e9e15b4966c99b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ca1fa7bd-214d-4245-b566-04c9c43df42e": {"doc_hash": "3ae46b159723bc1368d75d9299643be03c036b2412feeae2ebde6b74709c931b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "49dff4df-dac0-4281-9efd-87bab0dc3ac8": {"doc_hash": "f0271dc1d7b1e8f4daba2053d46a0af5e67f060b41856bfc3e58ed70fa1052f5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e055fe99-5e2e-4852-9b2a-25289d61df72": {"doc_hash": "e528f4ac77227708e061a73465d6d085b485435212c8592c42e70d3d99d0e45c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "5460990b-5c6b-45df-9565-72a1d63de2ee": {"doc_hash": "2e3cec3236381f18465d7673e710eaf9b8d907a9efd126d333b9aed6b6d63fc9", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d6ce0406-fac2-41cc-b343-86f73a1db1f1": {"doc_hash": "ff946de745ff6c3fb7e9f25401a3cb0e8068b340ebd08234eb8c3857b021eabf", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0a0beae8-dabe-442d-b522-7cfac485e7ac": {"doc_hash": "25b353e102088036bc3f0b5473de2a342ce3fca238f92ef9d63afbb8f1c76990", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "17fabd49-d262-49a2-9f36-8fe70014a2bc": {"doc_hash": "a4e4f017a4f05316a6518ab90804f5e6c41a196bd6fbfb8fe30d85985ff2413b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "87d72e2f-7ac5-4e21-baf5-d8ef597aed14": {"doc_hash": "f4caea33f6113dac5fb40d46421f9cfb034865ef8ef0892a162b34746a5db040", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "89d8e7fb-e9cb-42da-9274-f9ed68a60e1a": {"doc_hash": "faf561da7f47dd326f80d446d1615ed5748f2de7b70bb5bf86a4c09c5aee5d33", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "2e3a7c3b-4d72-4ea5-b0d2-92bf530d6f9a": {"doc_hash": "320a159a7249a13aa1b731c169efcb0c34cb0e886c7c656ea0939d36b3554804", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6975f378-2f02-4f65-9436-4a502e37551d": {"doc_hash": "5869656811343b92938327ec9336c7b473574c81a364a4107f7cb578a5622aa7", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "59e3dc07-2910-4b08-841b-f9992c27f228": {"doc_hash": "e6a357b63aec81512eeea45e891ed822cdafe604ba8e69340d29bcc6bd2c2cf3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c3e6c2dd-2f73-40a6-bfad-708f8713dbeb": {"doc_hash": "50b6547b4646f62de08463a06477a6db128a5666ad2fbde128aa670cbcdf4fcd", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "392e83ab-859f-44eb-ad37-ff068c803e7b": {"doc_hash": "f981f914a4d11e7a037aa1e6883d44254856c850cfc71d2cc5e10930b34241ea", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6ae2e016-f5fa-43ea-a216-ecb7cdc0055f": {"doc_hash": "ea9cf5d3286b7a0c8a28deca87c43c718a92413502d0e7617bdb232de57db2b4", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "71aef2a4-f1a3-4925-9932-4c2ac63ae0da": {"doc_hash": "73649c54f07c722d2be87a15b93ed527a82dd8c25861853d8b4bafab511c5498", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "05dbe910-58b1-43a4-84f0-dc7e660f0e9f": {"doc_hash": "2609347f8e0da78b00f0405497377b5b5a6bb2a1336840015a63fcacbdaaf4a2", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f46f385c-a3ed-4a43-805d-f8af0d42f416": {"doc_hash": "a4ef5c01217f75abe6c064f6d1abd6dce700ae05660cf286ff78c53b082734e0", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "79401572-9926-4979-b3fe-c2f90ee105f4": {"doc_hash": "bbad6d981d39c406ba0f3d51b8c7b990ce0e73a1757259d8233669be858d307b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "1d8f8216-43b3-4568-be9a-cd6b484885b9": {"doc_hash": "03f41b0afcc870f2cc7d780d676e0f474f97f828a4741b1bf6dcea0abf01852f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "aefba818-f1c4-4371-aaf0-624bd968fc18": {"doc_hash": "79f10f83d8512ae7a7c2660ceb37d84617e4285f9090734aa4211311f3cad04d", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "2ff0315a-d337-44a6-a6be-f98a5e646955": {"doc_hash": "0efd67ac134636feae4806f80c9a777f7faa272113990a2c1188a295dbb2d018", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "a460a119-a678-4bc9-81f1-9912a05f19ef": {"doc_hash": "25138522b2820c54ec25ce77d933a281cda7cd845de4f5bb4cf6a6f4b65e5ca3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "15e8d1b8-6c9b-4ae5-aed4-31ef7bb4d1e8": {"doc_hash": "ef4c87397d6e7bf2d2bbb4e06a643a4e9caab1f5696e48c9fe04340b254a760d", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "5fc3ec5a-38dd-40fa-9aef-93cae98bec0e": {"doc_hash": "7b8431920b46fe7bdd3f8a97546b93049cbcf6b65dfe5ef7b7c0b57747b6804b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c3d3940f-f5cc-41fa-ad2e-2b604ddae237": {"doc_hash": "4583fc3ea6c6d5b2c08de14a8c3b44d5ff05dd6c4c8a67fd4a85c7de5d2fdc42", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "2db55bd0-ecf2-4264-9a95-5433d1420db1": {"doc_hash": "c201088d7a5f760663754f538e29642d1c3c2b7b5e4cd133aeb0117bda835897", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3a717654-feba-4956-8460-66b0e3671c4c": {"doc_hash": "4b09d90a021a8bfa54e7cd2dbb15ed341100b2c9a8a0594aaf8fa0bfc562c4b6", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3d7b6c9b-6212-46cc-bedc-e2e648660b41": {"doc_hash": "6f7ab93d430be223fb34686ea3fe070a321b38e628afcb5466f19588139c44e5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "9ab5b89e-8fbf-45e8-9e38-03c77736fee1": {"doc_hash": "77650bd925801568b338e7e5e1e0bad73a0c023b6d410d1af117ca0205926aed", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d25a75be-4b62-4959-869b-399102cf98ab": {"doc_hash": "cd72a73cf06cfd1389915139accac1171c803b24d3ec4f66650c22b2b1559657", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "631b6cfa-6712-4278-bf12-1133d1ae1807": {"doc_hash": "305f24129a83504c979e86305cf63b4758bc84d1999ebad825286e4bee5eecff", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "759a38f6-2a24-499d-92ce-a8cac70e106a": {"doc_hash": "6389ef5fb710bb62629754f7c56af89257f29f91abc7ff8b1dc14f776febd027", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "832ae1c3-1f84-4b41-a045-332652b591b2": {"doc_hash": "f6780b753a7aa5c014da7f501f1e35574a7c37e1cabfd1dfd1e2b4e70330cb94", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "577be5fd-a953-43b9-917d-43dc8e883e94": {"doc_hash": "89361cea810329ed84ba8c95a6564e32815be28db88c4bc3430096dff2b6d230", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "032e2300-232a-489d-ae43-b273d57c7d8b": {"doc_hash": "ad378b9530ecb7e47d36dab12965199946a750da3cc9bb9af7ddb445511b4c47", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "af0a0fe2-c935-4b04-aa8d-ea4b96785156": {"doc_hash": "f265783dd05686ce1b25a3907c7711c38c58755f633c7edb206ec7a77f31f4fc", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "bc406730-5119-44c8-9dbd-9cc762f8de33": {"doc_hash": "117706da1257fff62409992d1118e77e27eb5ff963d333429527d1e7698f3555", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "1969d826-5009-4b11-b1c7-2ed8b5f381e5": {"doc_hash": "f40281d2e73da20e6ebf9fab09a6f89335c119d44b1e7f889cbdb7389e1a971f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "bff5e5c2-5657-4588-8f9e-1e56a8582a25": {"doc_hash": "009bc9b672e4ce179510c9bf4058e1368559fa20ba7fab4ad3fd15af676f5f99", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "dc4da986-c189-4e74-8983-3fcc37d2d951": {"doc_hash": "8022489fe8e2db16317e920309e210638f010c0567da8b5408f1979ef390ba26", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f1939abc-3068-4062-8999-db7f605a6f6f": {"doc_hash": "4e219b0c89fde0c279548d30bb3d19dcdfdc135c1ba21ebeb900c6e611fd2d59", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "9e610229-8ee9-42e2-972d-3d37f2fd19f4": {"doc_hash": "64aacd7bd02aa23d0ad3746c5ceb5f1a38039a76d20405bc7f4b7d72b5a362af", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "12725f5a-45d6-44fd-82dd-2b0c016ac228": {"doc_hash": "3d9279f142d907d536957ae236e2903694fd98578f81d4b732e4e68c33f942b6", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "db80f62f-cfa7-4b6a-b805-062abacd7c14": {"doc_hash": "bdb16019da8873f070f24b2be210ad4ee34ffd49e8b5233925df03ac3063de42", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "271daadc-b16d-4439-8562-b15c922217e7": {"doc_hash": "13a302b26932404d054b4132669cc42d20a22964a79a6477441fcdcac251b9ba", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "7d165a60-ef73-4cd8-b9c2-1a04bd8b341b": {"doc_hash": "a80fb59e71c5a4a753550db48d83d63663d4699e0af0285d0860003534b63624", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "98b04653-5c73-492c-b6c2-9845d7e5c473": {"doc_hash": "cf873025ce865a5873a4dd5b3a7097668c09c5166f8760d8fa1a494622573bab", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "b49c16f8-767f-4aa5-8171-882aa2c3957b": {"doc_hash": "f70efff7c04a4c2121144859f027356d80ec75d856ae1743f281cf730217e2ce", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "4357edb1-44a6-4852-b254-7bc057a45b06": {"doc_hash": "fb5f7359101cff72bc8f981d25c9931a9e674ecad4a1a194fcfbb8734d1923bf", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "3d5bddb2-5e95-4c83-bd82-5a0c55e89bfc": {"doc_hash": "172210867886cddb3fc87dcd694c3bd5a40df274033d088422a27e30f7bbb134", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "55a6877f-bba3-4632-afbe-fb596c192eba": {"doc_hash": "e9229a92273acde2f5827afc86479bf749fd674f42e40434242d803e67260174", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "b6f88f6c-0bd8-41c1-8e61-90254833090b": {"doc_hash": "39f7a6fa6dd09b4475eb238a699bb4136e8d6f6225a2153ff9e4733c8f70f0bf", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c4ccd43c-9507-414f-8209-3cfc13d4c33e": {"doc_hash": "bc7f80d86b818675735eb5448bfc5dc0cbd26cfa071a3c7de00739e6bb8b46a5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "9d5c4a3d-a0f2-46f5-9c78-3720a8619ddf": {"doc_hash": "b42a1ffe88200f2b389bd10d94d2071b6e32eaf555296b4385aba35da26c4c16", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "cd49931a-c2a3-4080-a659-b68803bb0ea6": {"doc_hash": "a2561b6595c6666c2442ecded94b9e6a5b54ae0a9fbf41ff361a195636ea44d4", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "c0ab135b-6bbf-4bd5-b69b-962a17564feb": {"doc_hash": "b870602ace805e838108d23dd103429cfb89d7d3bda74af7fb51e69822ce1ee4", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ff82f36a-dc76-4f19-bde9-d91745588e8e": {"doc_hash": "5bd55e3ee6e0c280ceb9fbd0b6ad2d9be005091d6ff703f5acb76cacb3fbb7f5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d48e9fd4-6545-4925-acc9-b09c1084c723": {"doc_hash": "e74e93ee22a961366ced42670217ba112b859834dd0d23ef709df2d042569aba", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "fcd46c2b-93cc-492f-950a-1e7efd34d570": {"doc_hash": "5bd1c188f79883619720908159c3830be973282f2a348e75c730c99768709936", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e6159ad8-db64-4ae7-85c8-929baa03bb63": {"doc_hash": "271dacab9c39213e0d24f5836bb96ed84a7e06141688502782fd20c159940ee2", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "be6925c6-1cd0-4cf3-ab20-8c6045c2eb31": {"doc_hash": "525e4ec0d4513e878712a5b74913769b5ed244a9c718453bc9c2126654865b34", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "a3d968bb-a506-48da-a5c8-225f5348d10a": {"doc_hash": "7a81202b1d6928edcd889c4f3a51c37bc3c022afedc3c667c2f2c200cf671ff3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "69544297-4a13-4b41-8ce0-aa956e7bddb3": {"doc_hash": "7803c483760f3d839e3c2ae8da2b93c65678afbcd4a7b3ce7bf1efddd525d402", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "85c1ea8c-277e-422c-bbea-ad64e3d9ccf4": {"doc_hash": "835f81653cd9d3e824593786ce6f620ccd80ba440cc4fee0a5b9173243f7daaf", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "b9c97d73-0746-42c5-9d92-1e1a92d40f57": {"doc_hash": "ff8d015c0373f70441763dc62ebbe2e27a88dd786e0790f4b88596c8a0beb1d4", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ee67b3a0-96ab-4e50-9187-5a8f19546007": {"doc_hash": "38783a3f509c3d556f95eaa0116235cccb0e450821be97dff7dc62adf59a853e", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "01f6c210-ed62-4f33-bee4-b1ef79d81c9c": {"doc_hash": "7a3e9ad8aa786a32e00760b0ef181987319f8f7138818bf3e5e238bc3355c54f", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d2c4c761-7918-4556-9024-0c7898104281": {"doc_hash": "18869e5462614c54268a5844c01fbcd6e99f1b339cc4fd8c483c07ac4d66f4b5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "7db2f6fc-9d06-42ad-8294-ed694d34dba9": {"doc_hash": "d51f69d81b5296b5cd704bff348130509330aa70ab45975834b7a7ee15d769d5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "988e432b-37e4-4c42-9522-717f773fc9ca": {"doc_hash": "ac349119959246d3fed446be0fadd4beaab58a4d22e8ce72bdad696f3ac1d931", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "edc4c9cb-a263-4cb3-b6ea-cb2d70c81e4d": {"doc_hash": "1342660d631e6dd18ade993d98510b7a6d66074b2e157c3c7e0a0612567a37c4", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "12f32004-bdcd-4657-8bcd-053633e52ae1": {"doc_hash": "5aa4212f00815ebf56af493a16aef8850eae6c34083e03c0ebe76e7193081633", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "6d66d72e-b250-4661-9673-82c0c8a94b2a": {"doc_hash": "e99ae8a7ff2f06b10829001747a14ce7ec332d890e983fb9b1da3f5731ef7346", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f2c314d4-700e-42a8-b403-662f0008cdaa": {"doc_hash": "d19ced3b83f71c6dda960e6a76a4d8da41075ceb683098dcf9ee91f963ea82e5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "e6dbd063-7965-4b57-943c-7c8008782357": {"doc_hash": "22b662bbc4757f6b170f0b30c71eab127b5843b4b3c9b00fe08c7251c91ca5e5", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "0f966a2c-1a1b-4965-886f-04e342cbc4be": {"doc_hash": "8b2256137e366ec3606e2eca4dca0c2ffb7c0d160f435dff01ab0eb63b72e70b", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "80f791aa-a554-4d8b-9d41-4a99a3dfdf02": {"doc_hash": "68697313275b9bcfd61f96a882ae3ba3e319401f605c63590b2d5fc7701b6342", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d72d4d7e-6c62-486c-a766-c62aa4220b80": {"doc_hash": "6b5544be409181a4018267a6c54af9a89cd1efc774329897e4a6420ca329be62", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "37dc5d59-786a-4d97-a00c-decef3c14044": {"doc_hash": "7262d76e7bfbafbac2f1318b6c3308edb87c8d605d55d58b7e01198b63ec174c", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "45a5ba89-93bb-4830-ab19-9eeb929975aa": {"doc_hash": "dda7fa94e9461ae8c522b58fa129e9862ebdd99d536801c88e8f1505e8d0cdc4", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "ee6c29b6-655d-4e6d-a0cd-6a84f2fd266d": {"doc_hash": "b04663bc7715355a924446fc9f6f5dae1f8c71db19de84457f6efd5fe1e7d6b3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d3cb58db-d464-4777-bc31-60da5a2d59b7": {"doc_hash": "42b7eefbbebbde912b6bbbd4003ec3bab698c05351664d39c32773a83a4604d3", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "a162ddeb-1914-4296-abe0-6421a200c26a": {"doc_hash": "8e4e2bb082eaeba9f5f18375b929a5f388add60e9631760f48446dade700e14e", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "74b69fa3-6432-4f14-bfbc-d242a237d105": {"doc_hash": "361e766c8732b18a6d8a36e92af975b85df8c43e14015d8748562a0ab04417ab", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "dcfe8f80-2f0f-4bc7-bafd-aef45633a0e0": {"doc_hash": "d76f48de1d8655cbaadf3e5345721e3c730545774bef4ce0c91eed10a01154c8", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "d2938a49-c750-4863-8efa-00ed0108f415": {"doc_hash": "06edddb1ad2c3107c1bb47cfc44d8d34dbdf862807d94abf2bb0bf21ed2a3fdb", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "07ae80e5-94c5-469f-bef8-9fea1c95fb69": {"doc_hash": "eec83d94b3c8a644caa3a40a59030de73a79da800a600f438009e9f3353344af", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f562a64c-233a-4e75-9927-b4d7b0e26e53": {"doc_hash": "42e40d6f42f78f89a391be987d3133cf34f396263bc4e33b42d47a29aea2e5fb", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f2cff24f-cf84-412f-b16a-3dbe47592559": {"doc_hash": "8c52b12c01339a65838ed193b3da90b1057a583be5da1e0f6a79a8b72b7019de", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "1532427d-b090-4254-ae46-36777f93dd5e": {"doc_hash": "45b8ade30135a7c2913122eb0d602b19d4d79f01e836a15128f815c9285bf4f7", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "f0d92fc7-fe58-4967-bac4-3288c881f1f2": {"doc_hash": "24ed72730f174fa103f4dbad4818b94edcfc01a2327c339ebe77750f6a772b53", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "51dbf7a7-f319-4899-9b4c-65245f44fa27": {"doc_hash": "d4cebf247aa36c75d29ffb72ad230349456905773ed2511f0cbc17a169221aaa", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}, "1bcaacaa-970b-442c-bb6d-0e3b86535d4f": {"doc_hash": "a3a21aaf06c9bce06bd1b52719639bebe113c3a3411b47d7929af5f9b5891e55", "ref_doc_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7"}}, "docstore/data": {"ed6b6146-adac-499d-816a-1f74002b652c": {"__data__": {"id_": "ed6b6146-adac-499d-816a-1f74002b652c", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5da63216-b706-4685-9b8c-a0ad025c5806", "node_type": "1", "metadata": {}, "hash": "c5cd77671000d06e2ba8682946cd3c5a83319eb9835b0171b6e2e24b9adf75b9", "class_name": "RelatedNodeInfo"}}, "text": "D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff : Seamless Integration of\nDatabase Systems and Fast Storage via CXL\nSangjin Lee1Alberto Lerner1Philippe Bonnet2Philippe Cudr\u00e9-Mauroux1\n1University of Fribourg2University of Copenhagen\nSwitzerland Denmark\nABSTRACT\nFlash memory is the de facto standard for data persistence in data-\nintensive systems. Despite its bene \uffffts, this type of memory has at\nleast one severe disadvantage: it is o \uffffered only as part of tightly\nclosed Solid-State Drives (SSDs). To access an SSD, applications\nneed to resort to one of many possible I/O frameworks, which\nthemselves are wrappers around a block interface abstraction, the\nNVMe standard. These levels of indirection impact how applications\nare structured and prevent them from bene \uffffting from the full power\nof Flash-based devices.\nIn this paper, we argue that SSDs should instead interact with ap-\nplications via CXL. CXL is a new technology driven by an Intel-led\nconsortium that allows systems to maintain coherence between a\nhost\u2019s memory and memory from attached peripherals. With CXL,\na device can expose a range of Flash-backed addresses through the\nserver\u2019s memory. One implementation option is to allow applica-\ntions to read and write to that range and let the device convert\nthem to Flash operations. In our SSD, however, we pick a di \ufffferent\noption. The device exposes what we call a D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff (DBK)\nthrough a CXL-backed memory range. Read/writes against a kernel\nwould trigger database-centric computations that the kernel would\nperform inside the device. We show examples of DBKs to support\ndi\ufffferent database functionalities and discuss their bene \uffffts. We\nbelieve that CXL and D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff can support a new gen-\neration of heterogeneous database platforms with unprecedented\ne\uffffciency, performance, and functionality.\n1 INTRODUCTION\nStorage layers API galore. Applications nowadays can access\nmany di \ufffferent memory types, ranging from caches, DRAM (local\nand remote), Persistent Memory (ditto), and NAND Flash. This\nbreadth of alternatives is necessary because each memory type sup-\nports a storage layer with a distinct size, speed, latency, persistence,\nand cost trade-o \uffff. Since no single memory type can outperform all\nthe others in all criteria, having many storage layers gives applica-\ntions some necessary \uffffexibility.\nDealing with most of those storage layers is relatively straight-\nforward. For instance, caches and DRAM are transparent to appli-\ncations; they see these layers as a continuous memory region that\ncan be directly read and written with simple load s and store s in-\nstructions. However, this simplicity is sacri \uffffced when applications\nrequire persistent memory. The storage layers providing persistence\ncome with much heavier abstractions.\nThis paper is published under the Creative Commons Attribution 4.0 International\n(CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their\npersonal and corporate Web sites with the appropriate attribution, provided that you\nattribute the original work to the authors and CIDR 2024. 14th Annual Conference on\nInnovative Data Systems Research (CIDR \u201924). January 14-17, 2024, Chaminade, USA.\nCPUCXLPCIe5Database SystemDatabaseKernels\nMemory\nExpanded Memory\nSSDFigure 1: CXL allows a host\u2019s memory to be expanded by a\nperipheral device. Both host and device can update the mem-\nory, i.e., CXL guarantees coherence. D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff use\nthe memory to provide services to the database by changing\nthe semantics of speci \uffffc regions of CXL memory. Writing to\na particular area may use a low-latency data path\u2014useful\nfor logging, for instance. Reading from another area may\nreturn the result of a table scan instead of the table itself.\nReading/Writing to a third area may decompress/compress\nthe contents along the way.", "start_char_idx": 0, "end_char_idx": 3779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5da63216-b706-4685-9b8c-a0ad025c5806": {"__data__": {"id_": "5da63216-b706-4685-9b8c-a0ad025c5806", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed6b6146-adac-499d-816a-1f74002b652c", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "1fc7329da2e64f46fb0f63b950f84c52b789a9f8b50537d4d8680f4e8a5aa06a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4f0d33c-0bbf-4a42-94fe-f53290f97758", "node_type": "1", "metadata": {}, "hash": "eb8297b710f256b8011c9553e0faa7b3cd15a5035066f3370a02f67de041437a", "class_name": "RelatedNodeInfo"}}, "text": "January 14-17, 2024, Chaminade, USA.\nCPUCXLPCIe5Database SystemDatabaseKernels\nMemory\nExpanded Memory\nSSDFigure 1: CXL allows a host\u2019s memory to be expanded by a\nperipheral device. Both host and device can update the mem-\nory, i.e., CXL guarantees coherence. D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff use\nthe memory to provide services to the database by changing\nthe semantics of speci \uffffc regions of CXL memory. Writing to\na particular area may use a low-latency data path\u2014useful\nfor logging, for instance. Reading from another area may\nreturn the result of a table scan instead of the table itself.\nReading/Writing to a third area may decompress/compress\nthe contents along the way.\nCurrently, the best persistent memory option is arguably NAND\nFlash, the underlying storage medium of SSDs. To use fast SSDs,\nand thus Flash, an application must resort to NVMe and, most\nlikely, an additional I/O framework to issue fast block reads and\nwrites [ 9]. These layers force applications to be structured around\nasynchronous API calls instead of the much simpler load/store\ninstructions. Even with these APIs, properly coupling a database\nsystem with an SSD requires much e \uffffort and may not unlock the\ndevice\u2019s entire performance [15].\nCXL as a storage layer uni \uffffer.Recently, a technology called\nCompute Express Link (CXL) [ 7] emerged that can bring back\nsimplicity. CXL promises to allow memory of any type sitting on\nperipheral cards or even in remote machines to be accessed directly\nby applications as if they were local DRAM. At its heart, CXL is a\ncache coherence protocol [ 31]. For decades, these protocols have\nallowed multi-socket servers to o \uffffer applications a uni \uffffed view\nof memory. These protocols, however, have been closely guarded\nand have all been proprietary. CXL\u2019s most signi \uffffcant advantage is,\narguably, that it is public. It promises to support interoperability\nacross di \ufffferent manufacturers\u2019 devices.\nThis allows third parties to build, for instance, so-called memory\nexpanders , PCIe form factor daughter cards that contribute extra\nmemory to a host, be it Intel- or AMD-based, or any future server\nthat would support the protocol [ 24,29]. Figure 1 (top) depicts this\nscenario. CXL characteristics are already known [ 33], and major\nhyperscalers, such as Microsoft and Meta, have already announced\npotential adoption plans [21, 23].CIDR\u201924, January 14-17, 2024, Chaminade, USA Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudr\u00e9-Mauroux\nSSD storage can join CXL but do more than memory expan-\nsion. The CXL standard, however, does not clearly address how to\nincorporate a persistence storage layer such as the one provided by\nan SSD. Flash has widely di \ufffferent characteristics than DRAM and\nrequires completely di \ufffferent maintenance procedures. Even so, one\ncan imagine that a CXL memory expander could simply mask these\ndi\ufffferences and use Flash to back a CXL memory address range.\nAn I/O against a Flash-backed CXL address would be somehow\nconverted into an I/O against Flash. Put di \ufffferently, an application\nwould not need intermediate frameworks to access this device and\ncould thus have a more natural structure. This semantics is viable\nand is one of the CXL-Flash integration alternatives proposed by\nthis paper and some prototypes [14, 28].\nThis paper, however, goes beyond this level of integration. We\nintroduce a new CXL-Flash coupling option that allows parts of\na database system to be implemented in the device, in what is\ncommonly referred to as Near-Data Processing (NDP) [ 2]. Storage\ndevices have been steadily gaining such capabilities [ 10,11,18,\n19]. We call our new abstraction D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff (DBK).DBK\nleverage the CXL technology to export an address space to the\ndatabase system. However, instead of implementing a 1:1 mapping\ninto Flash, I/Os against this area trigger computations inside the\ndevice.", "start_char_idx": 3120, "end_char_idx": 6959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4f0d33c-0bbf-4a42-94fe-f53290f97758": {"__data__": {"id_": "c4f0d33c-0bbf-4a42-94fe-f53290f97758", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5da63216-b706-4685-9b8c-a0ad025c5806", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "627d8541616f898fe21a5ba3074f57876843194d868e90b3e24e09ee265389f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c897fe5f-62f7-4bbf-8043-4ded07eea5e8", "node_type": "1", "metadata": {}, "hash": "57d7642460a3107a93af4aa2f6170b8872491c2765fd416fb59122e2f474e510", "class_name": "RelatedNodeInfo"}}, "text": "This semantics is viable\nand is one of the CXL-Flash integration alternatives proposed by\nthis paper and some prototypes [14, 28].\nThis paper, however, goes beyond this level of integration. We\nintroduce a new CXL-Flash coupling option that allows parts of\na database system to be implemented in the device, in what is\ncommonly referred to as Near-Data Processing (NDP) [ 2]. Storage\ndevices have been steadily gaining such capabilities [ 10,11,18,\n19]. We call our new abstraction D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff (DBK).DBK\nleverage the CXL technology to export an address space to the\ndatabase system. However, instead of implementing a 1:1 mapping\ninto Flash, I/Os against this area trigger computations inside the\ndevice. Figure 1 also depicts this possibility.\nTheDBKabstraction allows moving di \ufffferent aspects of database\nsystems into the device. For instance, early predicate execution\ncan be implemented as a DBK. The device could take a predicate\ndescription through an address within the CXL-backed window\nand present a materialized view of the result in the rest of the area.\nAs with other kernels, we expect the execution to consume fewer\nresources (e.g., transfers), to be faster, and to liberate the host CPU\nfor other tasks. We will discuss how di \ufffferent classes of DBKs can\nextend the database functionality into the device.\nTo execute DBKs, we proposed a new SSD architecture. This SSD\nis more modular than a traditional one and opens access to some\nof the device\u2019s internal interfaces. The interfaces are designed to\nshield non-specialized programmers from unnecessary intricacies\nwhile allowing them to develop NDP database functionality.\nThe structure of this paper follows its contributions, which are\nsummarized below:\n\u2022We start by brie \uffffy explaining the main CXL tenets and present\ntechniques to extend the use of CXL to storage devices (Section 2).\n\u2022We introduce the concept of D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff as a means to ex-\necute data-intensive tasks in a near-data-processing fashion and\nshow a device architecture to support such kernels (Section 3).\n\u2022We present the di \ufffferent types of CXL features that can be lever-\naged by DBKs (Section 4).\n\u2022We show examples of DBKin the context of database systems\n(Section 5).\n\u2022We discuss the viability of implementing D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff\ndevices in practice (Section 6).\n\u2022We enumerate the research questions that require further in-\nvestigation to realize D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff supporting devices\n(Section 7).\nWe discuss related work in Section 8 and conclude in Section 9.2 BACKGROUND & MOTIVATION\nMemory Coherence and Caching. Applications perceive mem-\nory as a continuous range of addresses that can be freely accessed.\nProviding this level of abstraction in the modern memory hierarchy\nis not trivial. The issue is that modern CPUs have many cores, each\nwith a private cache. When several cores access a given memory\naddress simultaneously, the data at the given address may be copied\nand possibly updated in multiple caches. The discipline that gov-\nerns how cores access data copies for the same memory address is\ncalled Memory (or Cache) Coherence.\nFormally, coherence can be de \uffffned in various ways through\ninvariants [ 31]. A simple de \uffffnition is the following: (1) writes to\nthesame memory location are serialized; and (2) every write\nis eventually made visible to all cores1. Informally, a typical im-\nplementation of coherence allows many copies of a piece of data\nto exist in di \ufffferent caches, provided no core modi \uffffes them. If a\ncore wants to modify its copy, it must acquire an exclusive version,\ninvalidating all existing copies before proceeding.\nSuch an implementation relies on two components. The Direc-\ntory Controller keeps track of which memory addresses are cached,\nby whom, and in which state. The Cache Controller requests cache\naddresses to the directory controller and receives invalidation re-\nquests from it. Figure 2 (left) depicts a simpli \uffffed implementation\nof this architecture.", "start_char_idx": 6250, "end_char_idx": 10195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c897fe5f-62f7-4bbf-8043-4ded07eea5e8": {"__data__": {"id_": "c897fe5f-62f7-4bbf-8043-4ded07eea5e8", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4f0d33c-0bbf-4a42-94fe-f53290f97758", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "c14056ec4c0c58de97f53d59c2a329006549ca5f890c8afe74acbc490195aaeb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf48a779-ad85-4a9d-87bf-f94676b6728c", "node_type": "1", "metadata": {}, "hash": "73f6f7bb6c7b3071ca38fe03786b2c30dad0c62eaec0cf74a674f1b21dcbdb11", "class_name": "RelatedNodeInfo"}}, "text": "A simple de \uffffnition is the following: (1) writes to\nthesame memory location are serialized; and (2) every write\nis eventually made visible to all cores1. Informally, a typical im-\nplementation of coherence allows many copies of a piece of data\nto exist in di \ufffferent caches, provided no core modi \uffffes them. If a\ncore wants to modify its copy, it must acquire an exclusive version,\ninvalidating all existing copies before proceeding.\nSuch an implementation relies on two components. The Direc-\ntory Controller keeps track of which memory addresses are cached,\nby whom, and in which state. The Cache Controller requests cache\naddresses to the directory controller and receives invalidation re-\nquests from it. Figure 2 (left) depicts a simpli \uffffed implementation\nof this architecture. Note that this implementation scales well to\nmulti-socket systems, by associating a Directory Controller to each\nsocket and a Cache controller to each cache. The memory addresses\nare assigned to Directory Controllers in such a way that each Cache\nController can tell if a request should be directed to the local or a\nremote Directory Controller. Figure 2 (left) depicts this scenario.\nCXL versions and Device Types. The\uffffrst version of CXL, called\n1.1, extends Memory Coherence to caches located on local peripher-\nals. We are starting to see the \uffffrst products emerging in the market\nthat support that CXL version, e.g., servers [ 12] and memory ex-\npanders [ 28]. Two additional versions of CXL are already rati \uffffed.\nCXL 2.0 enables single-level switching, i.e., Memory Coherence is\nsupported across multiple hosts and multiple devices connected\nthrough a single switch. CXL 3.0 extends Memory Coherence to\nmultiple switches over various interconnects and fabrics protocols.\nCXL also adopts the concept of a Directory and a Cache control-\nlers\u2014although it uses di \ufffferent terminology\u2014 dividing the message\ntypes that make up the protocol into two. The messages originating\nfrom the Cache Controller form the subprotocol named cxl.cache .\nThe messages originating from the Directory controller form the\ncxl.mem sub-protocol. A peripheral can implement only the cxl.\ncache protocol as a Type 1 device , both protocols as a Type 2 device ,\nor only the cxl.mem protocol as a Type 3 device .\nFigure 2 (right) depicts a Type 3 device. It is suitably called a\nmemory expander because its goal is to provide additional memory\nto a server without caching the latter\u2019s memory. In other words,\nonly CPU cores on the server side will cache contents of the memory\nthe device is providing. For that reason, it only needs to implement\ncxl.mem . The \uffffgure shows that the device implements a Directory\nController\u2014called a Device Controller here.\n1Note the emphasis on a single memory location. The discipline that governs the order\nof accesses to multiple addresses is a di \ufffferent one: Memory Consistency [ 25]. Memory\ncoherence and consistency are orthogonal concepts.D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff : Seamless Integration of Database Systems and Fast Storage via CXL CIDR\u201924, January 14-17, 2024, Chaminade, USA\nCacheControllerDirectoryController\nCache\nCoreload/storesGet/PutData/Ack\nMemory\nDirectoryControllerData/Ack\nMemory\nsymmetricinterconnectCPU 1CPU 2\nCacheControllerDirectoryController\nCache\nCoreload/storesGet/PutData/Ack\nMemory\nDeviceControllerData/CmPMemory\nCXL.memasymmetricinterconnectCPU 1CXL MemoryExpander Device\nMemRd/MemWr\u2460\u2461\u2462\u2460\u2461\u2464\u2462\u2462\u2463Figure 2: (Left) Coherence across sockets: To access or modify the contents of a memory address, a core brings a copy of it to its\ncache 1\u0000. This can be triggered by issuing a load or astore instruction. Upon receiving the instruction, the Cache Controller\nissues a request to either get a copy or put (write) its copy of the modi \uffffed content from/back to memory 2\u0000. The Directory\nController receives this message and executes the required memory access, either sending a copy of the read data to the Cache\nController or acknowledging that the modi \uffffed data was written 3\u0000. The Cache Controller can then signal to the core that the\ninstruction is complete.", "start_char_idx": 9415, "end_char_idx": 13469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf48a779-ad85-4a9d-87bf-f94676b6728c": {"__data__": {"id_": "bf48a779-ad85-4a9d-87bf-f94676b6728c", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c897fe5f-62f7-4bbf-8043-4ded07eea5e8", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "0415b0f2659ee804cf042f8ac8b342f5ab0f3d68d11510d9bc675f588c451c88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec07150a-4831-4ef1-9819-591e267ee9ae", "node_type": "1", "metadata": {}, "hash": "bd3e58f4b6ad6bc44a2f5e0bc4affe0208ae09a6d8b9ddef4cc367bfea1eb9e2", "class_name": "RelatedNodeInfo"}}, "text": "This can be triggered by issuing a load or astore instruction. Upon receiving the instruction, the Cache Controller\nissues a request to either get a copy or put (write) its copy of the modi \uffffed content from/back to memory 2\u0000. The Directory\nController receives this message and executes the required memory access, either sending a copy of the read data to the Cache\nController or acknowledging that the modi \uffffed data was written 3\u0000. The Cache Controller can then signal to the core that the\ninstruction is complete. Note that if the address required were held by a remote Directory Controller, the Cache Controller\nwould have targeted it instead 3\u0000. (Right) Coherence with a memory expander device: The Cache Controller asks or sends a\ncache line as before but is unaware of who is backing that address. Upon noticing that the request is for the expanded memory\narea, the Directory Controller issues the proper command to the Device Controller 3\u0000, which in turn interacts with the local\nmemory 4\u0000and responds. It is the Directory Controller that sends the cache line or the acknowledgment back as if the line\naccessed was local 5\u0000.\nCuriously, CXL imposes a hierarchy of Directory Controllers.\nA Cache controller cannot talk directly with a Device Controller\nbecause it does not know whether it is accessing a range of memory\nthat it manages. The Directory Controller on the host mediates all\ncommunications. This type of coherence protocol is called asymmet-\nric. Note that in Figure 2 (left), we portray a symmetric coherence\nprotocol; there is no hierarchy between the Directory Controllers\nof each CPU. There have been debates on the relative merits of the\napproaches. CXL proponents argue that asymmetric protocols are\nsimpler. Symmetric protocol proponents argue that more balanced\nsystems may be built if every peripheral that o \uffffers memory to\nthe system controls its own memory. It seems, however, that the\nIndustry decided to proceed with asymmetric protocols.\nA naive type 3 Flash-based CXL device. The standard recognizes\nmemory expanders, but nothing in it constrains what type of mem-\nory can back the address range the expander adds to the Directory\nController. In fact, recent versions of the standard even stipulate\nranges of tolerable latency for transactions (request-response mes-\nsage pairs) issued against the device. Some of these ranges are well\nwithin the response times of NAND Flash-based devices. Naturally,\none can conceive of a Type 3 device whose address range would be\nbacked by that kind of memory and, therefore, be persistent (e.g.,\nSamsung\u2019s recent Memory-Semantics SSD Prototype [28]).\nAlthough the exercise of building such a storage device is inter-\nesting, we think it would deliver subpar response times. As we will\nsubstantiate shortly, the device would only be noti \uffffed of a write\noperation when it is about to complete, leaving very little time\nto hide the Flash-memory latency. Instead, we claim that a device\nwith this functionality is better realized by a Type 2 device that\nis allowed to hold a cache\u2014and implement a cache controller\u2014of\nits own. Cache Coherence here is a means to give early notice of\non-going write operations to the NAND-based device.3 DEVICE ARCHITECTURE\nWe propose a storage device that supports simple and e \uffffcient ac-\ncess to expanded memory (including Flash\u2014but not only) through\nmemory reads and writes, and rich semantics associated to opera-\ntions on a given memory range.\nCXL Integration and Storage Options. Our storage device is a\nCXL Type 2 device. It exposes memory regions to the server through\ncxl.mem , and it caches memory from the host through cxl.cache ,\nas depicted in Figure 3. We provide a simple API to map physical\naddresses from the device onto process memory. Once this mapping\nis done, applications access these memory ranges \u201cas usual.\u201d\nInternally, however, the device o \uffffers a powerful indirection\nmechanism. It associates a range of addresses with a kernel. A\nkernel is a function that provides well-de \uffffned semantics for reads and\nwrites . A kernel that implements memory expansion semantics will\nsimply redirect reads/writes to a selected memory type. We will\nprovide such a kernel with the device that can opt between DRAM\nor NAND-Flash as backing memory. As Figure 3 shows, the device\ncan still present itself to the system as an NVMe device and o \uffffer a\ntraditional data path. Nothing prevents a legacy application from\nusing it that way.", "start_char_idx": 12954, "end_char_idx": 17391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec07150a-4831-4ef1-9819-591e267ee9ae": {"__data__": {"id_": "ec07150a-4831-4ef1-9819-591e267ee9ae", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf48a779-ad85-4a9d-87bf-f94676b6728c", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "798dce38933339521eb58f5bbcc53c4e491967fcf2b3112697ce83f94e8a6a39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81836ed1-268a-4395-b138-205ac7f38a9c", "node_type": "1", "metadata": {}, "hash": "85aff5514b8986da83d0a9e8d93d61b467dceec69b062a88389a3636cabef07d", "class_name": "RelatedNodeInfo"}}, "text": "We provide a simple API to map physical\naddresses from the device onto process memory. Once this mapping\nis done, applications access these memory ranges \u201cas usual.\u201d\nInternally, however, the device o \uffffers a powerful indirection\nmechanism. It associates a range of addresses with a kernel. A\nkernel is a function that provides well-de \uffffned semantics for reads and\nwrites . A kernel that implements memory expansion semantics will\nsimply redirect reads/writes to a selected memory type. We will\nprovide such a kernel with the device that can opt between DRAM\nor NAND-Flash as backing memory. As Figure 3 shows, the device\ncan still present itself to the system as an NVMe device and o \uffffer a\ntraditional data path. Nothing prevents a legacy application from\nusing it that way.\nThe device can also associate standard or application-speci \uffffc\nkernels to a chosen memory range. We focus on kernels that are\nrelevant for data-intensive applications that we denote as D\uffff\uffff\uffff\uffff\uffff\uffff\uffff\nK\uffff\uffff\uffff\uffff\uffff\uffff . These kernels can provide much more than 1:1 address\nmapping. They can host functionality that would otherwise be\nperformed by the database system on the server host. D\uffff\uffff\uffff\uffff\uffff\uffff\uffff\nK\uffff\uffff\uffff\uffff\uffff\uffff can manipulate memory directly but can also rely on the\nhelp of local Direct Memory Access (DMA) engines, capable of\ne\uffffciently transferring data in, out, and across the storage options,\nincluding fast memory, such as SRAM, in addition to DRAM and\nNAND-Flash.CIDR\u201924, January 14-17, 2024, Chaminade, USA Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudr\u00e9-Mauroux\nFast Staging Memory\nFlashChannel\nDMA\nDRAMMemory\nDMA\nMemoryController\nCacheController\nFast Staging Memory\nFlashChannel\nDRAMMemory\nNVMeControllercxl.cachemessagescxl.memmessagesNVMemessages\nSSDSemantics\nExpanderSemantics\nDMAkernel area\ufb01xed kernels\n\u2026\nDMA\u2026\u2026\ndatabase kernels\n Message Routerrequeststo cacheinvali-dationsmemreadsmemwritespagereadspagewrites\nFigure 3: A D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff supporting device. The device\ncan be accessed as a conventional SSD or through CXL. In the\nlatter case, the messages to a given memory address range\nwill be directed to its assigned kernel. The kernel can choose\nwhich kind of storage type to use and how.\nNote that a kernel does not need to map that range onto storage\naddresses directly. Instead, it can tie an address range to a virtually\nmaterialized result of a given computation . In other words, if we\nimagine that the device is holding a table, a kernel can expose only\nsome of these table\u2019s rows, as if it had applied a predicate on behalf\nof the application. The kernel can also expose data that is not stored\n\u2014 if it knows how to calculate it from the data that is. We will discuss\nmore examples in Section 5.\nWhy use a Type 2 device? So far, we have only discussed memory\naccesses in what could be perfectly accomplished by a Type 3 device.\nHowever, we propose developing D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff on a Type\n2 device. As explained above, this type of device can contribute\nmemory to the system and also cache data from the system locally.\nTo do so, it implements a Cache Controller that interacts with the\nsystem\u2019s Directory Controller via cxl.cache . Interestingly, Type\n2 devices release control of their memory range to the Directory\nController (on the host), which forces the device to notify the Di-\nrectory Controller if it wishes to cache its own memory. This is\ncalled Host Bias [7].\nIn our device, a kernel has the option to request shared cached\nlines on any portion of the address ranges it exposes. The bene \ufffft\nof this arrangement is subtle but powerful. If a core on the host\nwishes to access a cached memory address in exclusive mode\u2014e.g.,\nit wishes to write a new entry in an exposed area\u2014the device can\nbe noti \uffffed of this intent through a cache invalidation message.", "start_char_idx": 16618, "end_char_idx": 20373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81836ed1-268a-4395-b138-205ac7f38a9c": {"__data__": {"id_": "81836ed1-268a-4395-b138-205ac7f38a9c", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec07150a-4831-4ef1-9819-591e267ee9ae", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "448aaa4837711192481360db9c739e018d385c5e437a76a5f0fe6f530f403fdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ace6239-1ed7-4fca-8302-a95d18b1fafb", "node_type": "1", "metadata": {}, "hash": "0f7736eddc12f886f39b00360f5ae12900ec5db92a5f257a72c32e18a21ae614", "class_name": "RelatedNodeInfo"}}, "text": "As explained above, this type of device can contribute\nmemory to the system and also cache data from the system locally.\nTo do so, it implements a Cache Controller that interacts with the\nsystem\u2019s Directory Controller via cxl.cache . Interestingly, Type\n2 devices release control of their memory range to the Directory\nController (on the host), which forces the device to notify the Di-\nrectory Controller if it wishes to cache its own memory. This is\ncalled Host Bias [7].\nIn our device, a kernel has the option to request shared cached\nlines on any portion of the address ranges it exposes. The bene \ufffft\nof this arrangement is subtle but powerful. If a core on the host\nwishes to access a cached memory address in exclusive mode\u2014e.g.,\nit wishes to write a new entry in an exposed area\u2014the device can\nbe noti \uffffed of this intent through a cache invalidation message.\nFigure 4 illustrates this case This early noti \uffffcation of an intent to\nwrite gives the device much more time to prepare for the write than\nit would have if it learned about the write as it was requested.OwningCacheControllerDirectoryControllerSharingCacheControllersDeviceControllerask ExclusiveInvalidade sharersackExclusivegrantedputMemWrCmpackearly noticeof upcomingwrite\nType 3DeviceType 2DeviceCore  Reques-ting ExclusiveOwership store\ncxl.cachetraf\ufb01ccxl.memtraf\ufb01cFigure 4: A Type 3 device would not learn about a write oper-\nation until the Directory Controller requested it. In contrast,\nif the same device could cache memory as well, i.e., if it were\na Type 2 device, it would learn about the early intent to write.\nThe reason is that, to give a core exclusive access to a memory\naddress, the Directory Controller must invalidate all accesses\ngiven before. The invalidation is an early signal to the Flash-\nbased Type 2 device that it should prepare to hear a write\nrequest for that address in the short future, giving it ample\ntime to prepare.\n4 DATABASE KERNEL TYPES\nThe discussion above about choosing a Type 2 device for its moni-\ntoring capabilities suggests that there are other CXL mechanisms\nthan coherence that could be attractive for kernel development. We\ndivide the kernels in categories according to the CXL features they\nuse and discuss these categories next.\nClassic Kernels. These are kernels that expose a memory address\nrange backed by device\u2019s DRAM to the database system. There\ncould be processes inside the device that issue load and store\ninstructions against these addresses without di \ufffferentiating whether\nan address sits on the host system or the device DRAM. Because\nthese kernels rely on the coherence mechanisms of CXL, we call\nthem classic CXL kernels .\nIn a classic kernel, the memory semantics and its backing imple-\nmentation are the same as those of traditional memory. Ultimately,\nthese kernels allow extending the database functionality into the\ndevice, simply by moving wholesale processes into it. We will dis-\ncuss more speci \uffffc examples in the following section, but generally\nspeaking, any database process one would spin in a multi-socket\nmachine\u2014where remote access to memory occurs transparently\u2014\ncould technically be spun inside of the device as well. What de \uffffnes\nthese kernels is access to coherent memory.D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff : Seamless Integration of Database Systems and Fast Storage via CXL CIDR\u201924, January 14-17, 2024, Chaminade, USA\nAdvanced Kernels. Database kernels, however, can leverage other\naspects of the CXL protocol than coherence to build more advanced\nkernels . We comment on at least three features that enable new\nkernel functionality, summarized in Table 1.\nThe\uffffrst feature pertains to how CXL is built upon an extremely\nlow-latency messaging system. Arguably, the most important CXL\ncomponent that supports this feature is the Arbiter . The Arbiter\nis a hardware component that sits on the lowest part of the CXL\nstack and decides which type of message\u2014 cxl.io ,cxl.mem , or\ncxl.cache \u2014will use the PCIe bus next. In practice, the Arbiter will\nprioritize cache coherence tra \uffffc, even, for instance, during heavy\nDMA operations in the PCIe bus. The very presence of the Arbiter\nis what di \ufffferentiates PCIe Gen 5 and cxl.io . Another CXL aspect\nthat makes it low latency is that its messages are small.", "start_char_idx": 19508, "end_char_idx": 23746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ace6239-1ed7-4fca-8302-a95d18b1fafb": {"__data__": {"id_": "2ace6239-1ed7-4fca-8302-a95d18b1fafb", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81836ed1-268a-4395-b138-205ac7f38a9c", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "69ac03f40c490d73cf4539dde4c0e91284c374eb8205729acab4d1ee3b856a4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "312b52ec-ad6b-41e2-856c-5c10cceadd38", "node_type": "1", "metadata": {}, "hash": "2e30d3092f1b5653a95c06278dc0d07312c21909cd894dcd03107319d101d571", "class_name": "RelatedNodeInfo"}}, "text": "We comment on at least three features that enable new\nkernel functionality, summarized in Table 1.\nThe\uffffrst feature pertains to how CXL is built upon an extremely\nlow-latency messaging system. Arguably, the most important CXL\ncomponent that supports this feature is the Arbiter . The Arbiter\nis a hardware component that sits on the lowest part of the CXL\nstack and decides which type of message\u2014 cxl.io ,cxl.mem , or\ncxl.cache \u2014will use the PCIe bus next. In practice, the Arbiter will\nprioritize cache coherence tra \uffffc, even, for instance, during heavy\nDMA operations in the PCIe bus. The very presence of the Arbiter\nis what di \ufffferentiates PCIe Gen 5 and cxl.io . Another CXL aspect\nthat makes it low latency is that its messages are small. They occur\nin\uffffits, or 68 bytes in CXL 1.1 (although \uffffits will grow in later CXL\nstandard iterations) [ 30]. We have seen above how this combination\nof low latency and small messages enable the protocol to monitor\nthe status of data regions at a cache line granularity and prepare\nfor upcoming memory modi \uffffcation events before they have been\nconcluded. Simply put, the coherence tra \uffffc can sometimes predict\nthe memory tra \uffffc, and it does so with low latency. There could be\nkernels that use this prediction mechanism.\nThe second feature that can unlock advanced kernel possibilities\nis memory independence. The standard does not dictate the type of\nmemory a device associates with exported address ranges. Servers\nsupporting CXL on the market at the time of this writing invariably\nuse DDR5 as the memory standard. However, some CXL-enabled\ndevices that started appearing can o \uffffer DDR4 or even high band-\nwidth memory (HBM). Other CXL-enable devices are built with\nFPGAs, which carry some SRAM variations. Moreover, CXL devices\ncan o \uffffer persistence by backing addresses with persistent memory\nor with NAND Flash memory. A DBKmay use any type of memory\navailable in the device or even a combination thereof, for instance,\nimplementing some sort of memory layering inside the device with\nanti-caching semantics [ 8]. The kernel is free to implement a con-\ntract (semantics) that the application relies on when issuing reads\nand writes against that kernel memory. This contract may even\nvary across di \ufffferent addresses of the same kernel.\nThe third CXL feature that enables advanced D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff\nis perhaps the most powerful. The standard neither dictates the\nkind of memory to back up addresses o \uffffered by a CXL device, nor\ndoes it mandate that there should be some memory backing the\naddress! This allows an address to hold the results of a dynamic\ncomputation, performed only when the address is accessed. A typi-\ncal usage of this kernel is data compression. Such a kernel would\naccept uncompressed writes to a memory region but persist them\nin a compressed way. In turn, reading from that region would de-\ncompress the necessary addresses only. Another interesting kernel\nusing this feature is to allow a given data structure to be seen as\ncolumn-oriented through one memory region but row-oriented\nthrough another region. Most importantly, if either gets updated,\nthe kernel responsible for these two regions would invalidate the\ncorresponding cached lines from both regions.5 KERNEL EXAMPLES\nThe previous section discussed how certain CXL features, both\ncentered on coherence and not, can unlock several useful D\uffff\uffff\uffff\uffff\uffff\uffff\uffff\nK\uffff\uffff\uffff\uffff\uffff\uffff . In this section, we provide a functional description of a\nfew of them.\nBu\uffffer Manager Extensions. Perhaps the most intuitive kernel\nto add to a system is one that expands the amount of memory it\ncan use. Technically, there is no need for a D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff to\nachieve this. Any type 3 device can be attached to the system whose\nmemory could be promptly used by the system\u2019s Bu \uffffer Manager\nsimply through a larger bu \uffffer frame.\nFlushing bu \uffffers in this arrangement, however, could be less than\ne\uffffcient.", "start_char_idx": 23004, "end_char_idx": 26887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "312b52ec-ad6b-41e2-856c-5c10cceadd38": {"__data__": {"id_": "312b52ec-ad6b-41e2-856c-5c10cceadd38", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ace6239-1ed7-4fca-8302-a95d18b1fafb", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "fcae3c0208cd955f421079676a5a8d0b362e824af011b5ec28acf841faca7056", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11083665-f5eb-4a69-9a43-7c7255760241", "node_type": "1", "metadata": {}, "hash": "7ef43b70fea01b3923efd14c72897ec3607462f018db9ca9dc357541e49413de", "class_name": "RelatedNodeInfo"}}, "text": "In this section, we provide a functional description of a\nfew of them.\nBu\uffffer Manager Extensions. Perhaps the most intuitive kernel\nto add to a system is one that expands the amount of memory it\ncan use. Technically, there is no need for a D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff to\nachieve this. Any type 3 device can be attached to the system whose\nmemory could be promptly used by the system\u2019s Bu \uffffer Manager\nsimply through a larger bu \uffffer frame.\nFlushing bu \uffffers in this arrangement, however, could be less than\ne\uffffcient. If the device o \uffffering the memory has storage capabilities,\nthe database system may not have the means to transfer data from\nthe expanded memory into the persistent area easily. It would, most\nlikely, treat the CXL device as two, the volatile and the persistent\nstorage device areas. It would stage data from the expanded memory\nand send it to a persistent area\u2014when this data movement could\nvery well be performed intra-device. The goal of a Bu \uffffer Manager\nExtension kernel is to enable such optimizations. In other words,\nthe kernel would implement a Flush operation to move data from\nDRAM into Flash.\nNote that this kernel may allow for exciting data placement\npossibilities. The system may know upfront that certain pages are\nbeing retrieved that may be updated, e.g., as part of a SQL UPDATE\ncommand, while others would not, e.g., in a SQL query. Pages\nnot already loaded in the system may be allocated accordingly,\nwith likely writable pages being allocated from the device memory\npool. The bene \ufffft of such a data placement scheme is to reduce I/O\nbandwidth.\nQuery Execution Worker. The idea of pushing predicates down\nis as old as query optimization. Naturally, this type of optimization\nwas one of the \uffffrst to be attempted in-storage [ 16]. With kernels,\nwe can support not only this type of scan and \ufffflter operation but\nalso an extensive array of access paths, as the query operations\nthat interact with the base tables/indexes storage are called. For\ninstance, a kernel could implement an indexed lookup access. It\nwould entail an index tree traversal and a base table page read. The\nkernel could implement the tree traversal in the device bene \uffffting\nfrom cached data, saving index data transfers between the host and\nthe device.\nAs with other typical query workers, the operators this worker\nwould be executing would communicate with each other via pages\npinned in the Bu \uffffer Manager. These pages may or may not reside\ninside the device. A traditional query worker on the system pro-\ncesses the rest. The coherence feature of CXL allows either type of\nworker to access a common set of pages.\nA Fast Transaction Logging Kernel. In the two previous kernels,\nwe took advantage of the memory coherence feature of CXL to\nship some of the database functionality to the device. In this kernel,\nwe mostly rely on CXL\u2019s fast communication messaging system\nin a situation that does not strictly require coherence. Transaction\nlogging typically entails a sensitive operation in a database system,\nas every transaction has to be re \uffffected in a local (and persistent) log\n\uffffle. Quite often, the speed with which the transaction is persistedCIDR\u201924, January 14-17, 2024, Chaminade, USA Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudr\u00e9-Mauroux\nCXL Features Potential Kernel Functionality Kernel Examples\nCoherence Allows di \ufffferent processes to access memory Bu \uffffer manager extensions, query execution worker, etc.\ncoherently.\nFlit-based messaging Support low-latency communication for Fast transaction logging, etc.\ncoherence tra \uffffc.\nBacking-memory freedom Allows mixing memory types within the Data placement for LSM compression, etc.\nsame memory region.\nNon-backed addressing Enables view materialization mechanisms. Compression/decompression, data transposition, coherent\nviews, etc.\nTable 1: CXL features and the kernel functionality they enable.\nis the main bottleneck in a database. Transaction logging is an ideal\noperation to o \uffffoad to a DBK, in the sense that it can use a type\nof memory with low latency and with a battery-backed option for\npersistence. A dedicated database kernel exposing an append-only\nabstraction on top of CXL could streamline this set of operations in\nvarious ways.", "start_char_idx": 26388, "end_char_idx": 30597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11083665-f5eb-4a69-9a43-7c7255760241": {"__data__": {"id_": "11083665-f5eb-4a69-9a43-7c7255760241", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312b52ec-ad6b-41e2-856c-5c10cceadd38", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d5a970b6c0167c694be4308a06a2dd40897fe5b63758a2ef36be2c7312df343a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe1f3513-cead-4c57-ae2a-68cda690a3d5", "node_type": "1", "metadata": {}, "hash": "7660859b4ca8946af84f6a80f6346456b1d6374aabb2c44c625b03646e48fc24", "class_name": "RelatedNodeInfo"}}, "text": "coherently.\nFlit-based messaging Support low-latency communication for Fast transaction logging, etc.\ncoherence tra \uffffc.\nBacking-memory freedom Allows mixing memory types within the Data placement for LSM compression, etc.\nsame memory region.\nNon-backed addressing Enables view materialization mechanisms. Compression/decompression, data transposition, coherent\nviews, etc.\nTable 1: CXL features and the kernel functionality they enable.\nis the main bottleneck in a database. Transaction logging is an ideal\noperation to o \uffffoad to a DBK, in the sense that it can use a type\nof memory with low latency and with a battery-backed option for\npersistence. A dedicated database kernel exposing an append-only\nabstraction on top of CXL could streamline this set of operations in\nvarious ways. For instance, the kernel can stage log entries on SRAM\nand quickly destage them asynchronously to Flash. Comparable\nsemantics can be obtained using a dedicated storage device (such\nas our own X-SSD system [ 17]), but CXL and kernels bring many\nadvantages in this context, including the upfront notice of an intent\nto write (cf. Section 3).\nData Placement Kernel. In column stores, data is traditionally\nstored in two very di \ufffferent memory regions: a write-optimized\nstore that \uffffrst receives all updates, and a compressed, read-optimized\nstore [ 32]. In HTAP systems, the areas that store di \ufffferent data\nrepresentations can be even more disparate [ 26]. Moreover, in LSM-\ntree-based key-value stores, the writing activity in the upper layers\nof the tree is more frequent than in the lower ones, and these in-\nterfere with compaction work [ 3]. As discussed in the previous\nsection, the address ranges exported by a type 3 device may be\nbacked by more than one memory type. Areas with more intensive\nwrite activity could be initially persisted in SLC Flash memories,\nfor instance. Areas with more read activity could be moved to MLC,\nTLC, or QLC Flash memory, which sacri \uffffce write performance in\nthe name of more economical reads. Currently, this \uffffne-grained\ndata placement is not visible to an SSD user, even if it exists in\nsome devices [ 34], but with D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff , it can. A kernel\ncan place di \ufffferent portions of their memory range under distinct\nService Level Agreement, so to speak, just by assigning each portion\nto a di \ufffferent type of backing memory.\nCoherent Virtual Views. This is perhaps the most powerful type\nofD\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff . It can equate memory accesses with per-\nforming computations. The idea with this kernel is that it exports\na memory address range but does not back it up with memory. It\nassociates a computation with the region and performs the com-\nputation as a side-e \uffffect of accessing that region. One example\nwould be to use this functionality to compress data when writing\nand decompressing when reading, both transparently. As the name\nimplies, we say the kernel implements a view over the data.\nThe views may explore coherence in a unique way. Suppose\na view exports a column-oriented representation of an area that\nis, in fact, stored in a row-oriented format. Each virtual view be\naccessible through a di \ufffferent memory range. Updating one of thememory regions implies updating the other as well. If, however,\neither area is being cached anywhere in the system, these cache\nlines need to be invalidated. The kernel can control how the areas\nof the di \ufffferent regions are associated. When one area gets updated,\nbesides asking for exclusive access over that data structure, the\nkernel asks for the same access over the virtual structure. This\ntechnique was pioneered by CCKit [27] and PLayer [5].\n6 PRELIMINARY VIABILITY ANALYSIS\nWe seek a platform that supports CXL to develop a D\uffff\uffff\uffff\uffff\uffff\uffff\uffff\nK\uffff\uffff\uffff\uffff\uffff\uffff storage device. This platform inevitably needs special-\nized hardware because PCIe and CXL protocol messages require\nlow latency. Moreover, memory management requires specialized\nhardware for the same reason.", "start_char_idx": 29813, "end_char_idx": 33740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe1f3513-cead-4c57-ae2a-68cda690a3d5": {"__data__": {"id_": "fe1f3513-cead-4c57-ae2a-68cda690a3d5", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11083665-f5eb-4a69-9a43-7c7255760241", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "9a024e2ab2717e5f2f214712a1ba547c73c3b6645b545d7fd01f15c3ecc69f82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75d812fb-050b-4b65-9b13-29d14f2818a8", "node_type": "1", "metadata": {}, "hash": "6ccbd36d2e79d42b64504f3e200b242025a642ac92c97ef423db50e986a196c6", "class_name": "RelatedNodeInfo"}}, "text": "Updating one of thememory regions implies updating the other as well. If, however,\neither area is being cached anywhere in the system, these cache\nlines need to be invalidated. The kernel can control how the areas\nof the di \ufffferent regions are associated. When one area gets updated,\nbesides asking for exclusive access over that data structure, the\nkernel asks for the same access over the virtual structure. This\ntechnique was pioneered by CCKit [27] and PLayer [5].\n6 PRELIMINARY VIABILITY ANALYSIS\nWe seek a platform that supports CXL to develop a D\uffff\uffff\uffff\uffff\uffff\uffff\uffff\nK\uffff\uffff\uffff\uffff\uffff\uffff storage device. This platform inevitably needs special-\nized hardware because PCIe and CXL protocol messages require\nlow latency. Moreover, memory management requires specialized\nhardware for the same reason. For mostly everything else, there\nare alternatives that can use the software in e \uffffcient ways. This\ncombination of hardware and software makes FPGAs a particularly\npromising platform. We already have access to a platform that \uffffts\nthis pro \uffffle.\nOne of the most challenging aspects of developing using FPGAs\nis predicting the necessary area (how many logic units in the FPGA\nfabric) a given design will have. Therefore, our \uffffrst experiments\nwere to prototype a hardware design that connects the PCIe/CXL\nareas of the card with the memory controller areas. The rationale\nbehind this design is that it can approximate the data path we wish\nthe card to support. The data path is certainly one of the components\nthat will consume the larger area in our design. Table 2 shows the\nresults of this experiment for a Type 2 and a Type 3 design. As\nexpected, the Type 2 design uses more area since it implements a\nCache Controller (cf. Figure 2).\nCase IP(s)Logic Unit Counts % of Total\nLogic Units Ideal Real Total\n1CXL Type 2 179K 213K251K 27.5%2 DDRs + User Logic 31K 38K\n2CXL Type 3 141K 167K204K 22.5%2 DDRs + User Logic 30K 37K\nTable 2: The table shows how many logic units a primary\ntype 2 or 3 device design uses. Each design includes CXL IP,\ntwo channels of DDR 4, and user logic.\nFortunately, the FPGA is comfortably sized; our data path oc-\ncupies only slightly more than 1/4thof the available area, leaving\nample space for the other components.D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff : Seamless Integration of Database Systems and Fast Storage via CXL CIDR\u201924, January 14-17, 2024, Chaminade, USA\nArea, however, is not the only concern. The data path should\nprovide adequate bandwidth to be e \uffffective. In practice, the maxi-\nmum bandwidth of a PCIe card is capped at the width of the PCIe\nconnection. In our case, this is 64GB/s (a Gen5 x16 connection).\nThis bandwidth can roughly support 2 DDR4 memory controllers,\nbut it is a somewhat high bandwidth for an FPGA card. For compar-\nison, this bandwidth is equivalent to \uffffve 100Gbps network ports.\nTherefore, we analyze whether a preliminary design could achieve\nsuch bandwidth. Figure 5 shows the result of this experiment. The\ndata path in that \uffffgure connects (1) the PCIe/CXL controller with\n(2) two memory controllers.\nFigure 5: Floorplan of the FPGA fabric. The \uffffgure shows the\nFPGA fabric in the center (large blue rectangle) and the spe-\ncialized hard blocks in the periphery. The PCIe/CXL block\nis located at the left-center of the FPGA fabric (1). The two\nDDR4 channels are located at the bottom-center (2). The cir-\ncuit placement in the FPGA connecting these areas is shown\nin a green-magenta color range. That range represents the\ndensity of the circuit. In particular, the magenta areas are\nclose to saturating the resources in that area.\nThe FPGA \uffffoorplan shows that the data path is viable but there\nare congested areas within our data path.", "start_char_idx": 32964, "end_char_idx": 36622, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75d812fb-050b-4b65-9b13-29d14f2818a8": {"__data__": {"id_": "75d812fb-050b-4b65-9b13-29d14f2818a8", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe1f3513-cead-4c57-ae2a-68cda690a3d5", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ab77eabfb6b38bddf141f06df399ecc878d0be9b8a6aa9650357c2127317159a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9a53f0c-581d-4523-a78a-1f19ef79a7d7", "node_type": "1", "metadata": {}, "hash": "ce188778cb78f53e74ad0bec9e772755648f5f2636b7d284cf32f2e70426c8fa", "class_name": "RelatedNodeInfo"}}, "text": "The\ndata path in that \uffffgure connects (1) the PCIe/CXL controller with\n(2) two memory controllers.\nFigure 5: Floorplan of the FPGA fabric. The \uffffgure shows the\nFPGA fabric in the center (large blue rectangle) and the spe-\ncialized hard blocks in the periphery. The PCIe/CXL block\nis located at the left-center of the FPGA fabric (1). The two\nDDR4 channels are located at the bottom-center (2). The cir-\ncuit placement in the FPGA connecting these areas is shown\nin a green-magenta color range. That range represents the\ndensity of the circuit. In particular, the magenta areas are\nclose to saturating the resources in that area.\nThe FPGA \uffffoorplan shows that the data path is viable but there\nare congested areas within our data path. In essence, this may\nmean that the FPGA synthesizer may take longer to compile circuit\nde\uffffnitions, trying to place and route them on the FPGA. We believe\nthe drawbacks are minor and that we have a suitable platform to\ndevelop a D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff -supporting device.7 RESEARCH AGENDA\nWe believe D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff is a foundation for future database\nsystems incorporating in-storage processing capabilities. The work\npresented here is but the start of several fundamental research\ndirections that need to be explored, including:\nFollowing CXL evolution. Our current proposal is based on CXL\n1.1, which is the version that is about to become commercially\navailable. However, the future versions of the CXL protocol are\nalready speci \uffffed. The additional features speci \uffffed for versions\n2.0 and 3.0 can unlock further possibilities for DBK. Of particular\ninterest is the possibility of integrating a host with remote storage,\nwhich CXL 3.0 and beyond will allow.\nHardware Support for Application Logic. We would like D\uffff\uffff\uffff\uffff\n\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff to be an inviting and performant environment for\nsoftware development. We can achieve so with a combination of\nuser and pre-installed functions [ 11]. Examples of pre-installed\nfunctions are sorting, merging, \uffffltering, transposition, etc. Some\nfunctions could even deal with serialization and deserialization of\ntraditional \uffffle formats. Since these internal functions are stable and\ngeneric, they may be implemented in hardware. The user functions,\nin contrast, could be developed in a software environment and a\ngeneral-purpose language. To support the latter, the device should\ndedicate one or more cores to run application software. We discuss\nhow to develop in this environment next.\nA Database Kernel Development Kit (DSK). Admittedly, the de-\nvelopment of kernels in our current proposal requires skills that are\nonly available to SSD and FPGA design specialists. The integration\nwith Flash is, for now, too low level, and, for performance, some of\nit must be implemented via hardware. However, there is no funda-\nmental impediment to shifting the development techniques towards\na more software-centric approach. This may require implementing\nclearer software interfaces and wrapping hardware aspects of this\nintegration in something akin to function calls.\nAdditional Memory Technologies. The current D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\n\uffff\uffff\uffff\uffff proposal makes SRAM, DRAM, and NAND-Flash memories\navailable for kernel development. In the future, it should be possible\nto incorporate other types of memory such as HBM\u2014and, perhaps\nfuture formats of persistent memory that replace Optane\u2014should\nthey become more commonly available on development platforms.\nSafety and Security Aspects. With many kernels running in a\ndevice, crash safety and security issues may arise. The kernels\nshould not interfere with one another, and despite being integrated\ninto the device, they should be isolated in a way that does not\ncorrupt or otherwise hamper the proper functioning of the device.\nFostering Interoperability.", "start_char_idx": 35891, "end_char_idx": 39637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9a53f0c-581d-4523-a78a-1f19ef79a7d7": {"__data__": {"id_": "b9a53f0c-581d-4523-a78a-1f19ef79a7d7", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75d812fb-050b-4b65-9b13-29d14f2818a8", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e78b924d1d911a5c916e6d506fbd53853fab09ee5808a76efd0b92f413424c91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41bea6a3-1276-4edd-b33f-3f418e137bc9", "node_type": "1", "metadata": {}, "hash": "45d1fd09730913faa613e0781fdd45269bc8a3cec0512405f5363ad5b6a23244", "class_name": "RelatedNodeInfo"}}, "text": "Additional Memory Technologies. The current D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\n\uffff\uffff\uffff\uffff proposal makes SRAM, DRAM, and NAND-Flash memories\navailable for kernel development. In the future, it should be possible\nto incorporate other types of memory such as HBM\u2014and, perhaps\nfuture formats of persistent memory that replace Optane\u2014should\nthey become more commonly available on development platforms.\nSafety and Security Aspects. With many kernels running in a\ndevice, crash safety and security issues may arise. The kernels\nshould not interfere with one another, and despite being integrated\ninto the device, they should be isolated in a way that does not\ncorrupt or otherwise hamper the proper functioning of the device.\nFostering Interoperability. One factor that could signi \uffffcantly im-\nprove adoption is if di \ufffferent vendors supported D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff\nand competed by o \uffffering di \ufffferent cost vs. performance tradeo \uffffs.\nSimilarly, it would be interesting if a database system boot pro-\ncess could check whether the storage over which it is running\nprovides D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff and adjust accordingly. With such\ninteroperability features, it should be possible for something akin\nto a marketplace of DBKto emerge.CIDR\u201924, January 14-17, 2024, Chaminade, USA Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudr\u00e9-Mauroux\n8 RELATED WORK\nTo the best of our knowledge, this is the \uffffrst work that leverages\nCXL to provide in-storage database functionality. Storage, however,\nhas been historically gaining processing capabilities [4, 11, 19].\nLeveraging that potential for query processing was described by\nDo et al. [ 10]. More recently, Lerner & Bonnet characterized the\narchitectural alternatives to do so [ 18]. There are many examples\nof functionalities that were pushed into storage: joins and \ufffflters [ 6,\n13,16], transaction log acceleration [ 17], LSM compaction [ 22], and\ndevice pro \uffffling [ 20], to cite a few. These are excellent kernel candi-\ndates, and if implemented so, they will bene \ufffft from the uniform\nand transparent interface that CXL can o \uffffer.\nSome works started to speculate about how to use CXL memory\nexpanders to integrate a host\u2019s memory either with storage [ 14,28]\nor directly with a database system [ 1]\u2014but never both, as D\uffff\uffff\uffff\uffff\n\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff do. Abishek et al. have discussed mechanisms to\nmaintain cache coherence across virtually materialized views [ 27].\nThe mechanism is very powerful, and D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff can take\nfull advantage of it within a storage device.\n9 CONCLUSION\nIn this paper, we introduced D\uffff\uffff\uffff\uffff\uffff\uffff\uffff K\uffff\uffff\uffff\uffff\uffff\uffff , the \uffffrst platform\nto embed database functionalities deep into storage devices using\ncoherence technology. The cornerstone of this database-device inte-\ngration is the use of CXL, and in particular its caching capabilities.\nThe device uses coherence tra \uffffc to monitor requests, prepare ahead\nof time, and ultimately answer database queries more e \uffffciently.\nWhile realizing the full potential of DBKs will take years, we are\nalready excited about the new database architectural possibilities\nthat this new technology opens.\nACKNOWLEDGMENTS\nThis work has received funding from the Swiss State Secretariat\nfor Education (SERI) in the context of the SmartEdge EU project\n(Grant agreement No. 101092908).\nREFERENCES\n[1]Minseon Ahn et al .2022. Enabling CXL Memory Expansion for In-Memory\nDatabase Management Systems. In DaMoN . https://doi.org/10.1145/3533737.", "start_char_idx": 38916, "end_char_idx": 42303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41bea6a3-1276-4edd-b33f-3f418e137bc9": {"__data__": {"id_": "41bea6a3-1276-4edd-b33f-3f418e137bc9", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9a53f0c-581d-4523-a78a-1f19ef79a7d7", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "16d8f900176598d329e952171dbc27cdc521aaae4b722fd6d7002537d9b8c566", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfc06553-66c9-49ba-84f3-67616316a58d", "node_type": "1", "metadata": {}, "hash": "52fe8cb057a734766f9db9739c6a10d71e3437671ffe1269196c7d36995f6096", "class_name": "RelatedNodeInfo"}}, "text": "The cornerstone of this database-device inte-\ngration is the use of CXL, and in particular its caching capabilities.\nThe device uses coherence tra \uffffc to monitor requests, prepare ahead\nof time, and ultimately answer database queries more e \uffffciently.\nWhile realizing the full potential of DBKs will take years, we are\nalready excited about the new database architectural possibilities\nthat this new technology opens.\nACKNOWLEDGMENTS\nThis work has received funding from the Swiss State Secretariat\nfor Education (SERI) in the context of the SmartEdge EU project\n(Grant agreement No. 101092908).\nREFERENCES\n[1]Minseon Ahn et al .2022. Enabling CXL Memory Expansion for In-Memory\nDatabase Management Systems. In DaMoN . https://doi.org/10.1145/3533737.\n3535090\n[2]Rajeev Balasubramonian, Jichuan Chang, Troy Manning, Jaime H Moreno,\nRichard Murphy, Ravi Nair, and Steven Swanson. 2014. Near-data processing:\nInsights from a micro-46 workshop. IEEE Micro 34, 4 (2014), 36\u201342.\n[3]Oana Balmau, Florin Dinu, Willy Zwaenepoel, Karan Gupta, Ravishankar Chand-\nhiramoorthi, and Diego Didona. 2019. SILK: Preventing Latency Spikes in\nLog-Structured Merge Key-Value Stores. In USENIX . https://www.usenix.org/\nconference/atc19/presentation/balmau\n[4]Antonio Barbalace and Jaeyoung Do. 2021. Computational Storage: Where Are We\nToday?. In CIDR . https://www.cidrdb.org/cidr2021/papers/cidr2021_paper29.pdf\n[5]Richard Braun, Abishek Ramdas, Michal Friedman, and Gustavo Alonso. 2023.\nPLayer: Expanding Coherence Protocol Stack with a Persistence Layer. In DIMES\nWorkshop . https://doi.org/10.1145/3609308.3625270\n[6]Wei Cao et al .2020. POLARDB Meets Computational Storage: E \uffffciently Support\nAnalytical Workloads in {Cloud-Native }Relational Database. In FAST . https:\n//www.usenix.org/system/ \uffffles/fast20-cao_wei.pdf\n[7]CXL Consortium. 2023. Compute Express Link Speci \uffffcation. https://www.\ncomputeexpresslink.org/download-the-speci \uffffcation.\n[8]Justin DeBrabant, Andrew Pavlo, Stephen Tu, Michael Stonebraker, and Stan\nZdonik. 2013. Anti-caching: A new approach to database management system\narchitecture. 6, 14 (2013), 1942\u20131953. https://doi.org/10.14778/2556549.2556575\n[9]Diego Didona, Jonas Pfe \ufffferle, Nikolas Ioannou, Bernard Metzler, and Animesh\nTrivedi. 2022. Understanding Modern Storage APIs: A Systematic Study of libaio,\nSPDK, and io_uring. In SYSTOR . https://doi.org/10.1145/3534056.3534945[10] Jaeyoung Do, Yang-Suk Kee, Jignesh M. Patel, Chanik Park, Kwanghyun Park,\nand David J. DeWitt. 2013. Query Processing on Smart SSDs: Opportunities and\nChallenges. In SIGMOD . https://doi.org/10.1145/2463676.2465295\n[11] Niclas Hedam, Morten Tychsen Clausen, Philippe Bonnet, Sangjin Lee, and Ken\nFriis Larsen. 2023. Delilah: EBPF-O \uffffoad on Computational Storage. In DaMoN .\nhttps://doi.org/10.1145/3592980.3595319\n[12] Intel. 2023. Sapphire Rapids Family. https://ark.intel.com/content/www/us/en/\nark/products/codename/126212/products-formerly-sapphire-rapids.html.\n[13] Insoon Jo, Duck-Ho Bae, Andre S Yoon, Jeong-Uk Kang, Sangyeun Cho, Daniel DG\nLee, and Jaeheon Jeong. 2016.", "start_char_idx": 41555, "end_char_idx": 44627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfc06553-66c9-49ba-84f3-67616316a58d": {"__data__": {"id_": "cfc06553-66c9-49ba-84f3-67616316a58d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41bea6a3-1276-4edd-b33f-3f418e137bc9", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e50066624faba28098ac0d1b9483e39d4ce7d3bca4c061da78c1b520e29998fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "016d1938-7134-46c1-8ecd-cef6acb466f5", "node_type": "1", "metadata": {}, "hash": "ae51f3da19e94fb1e0ee8b496a5c5a3a7a02bd25153dcff93fab4278db635dc7", "class_name": "RelatedNodeInfo"}}, "text": "In SIGMOD . https://doi.org/10.1145/2463676.2465295\n[11] Niclas Hedam, Morten Tychsen Clausen, Philippe Bonnet, Sangjin Lee, and Ken\nFriis Larsen. 2023. Delilah: EBPF-O \uffffoad on Computational Storage. In DaMoN .\nhttps://doi.org/10.1145/3592980.3595319\n[12] Intel. 2023. Sapphire Rapids Family. https://ark.intel.com/content/www/us/en/\nark/products/codename/126212/products-formerly-sapphire-rapids.html.\n[13] Insoon Jo, Duck-Ho Bae, Andre S Yoon, Jeong-Uk Kang, Sangyeun Cho, Daniel DG\nLee, and Jaeheon Jeong. 2016. YourSQL: a high-performance database system\nleveraging in-storage computing. In VLDB . https://doi.org/10.14778/2994509.\n2994512\n[14] Myoungsoo Jung. 2022. Hello Bytes, Bye Blocks: PCIe Storage Meets Compute\nExpress Link for Memory Expansion (CXL-SSD). In HotStorage . https://doi.org/\n10.1145/3538643.3539745\n[15] Aarati Kakaraparthy, Jignesh M. Patel, Kwanghyun Park, and Brian P. Kroth.\n2019. Optimizing Databases by Learning Hidden Parameters of Solid State Drives.\n(2019). https://doi.org/10.14778/3372716.3372724\n[16] Sungchan Kim, Hyunok Oh, Chanik Park, Sangyeun Cho, Sang-Won Lee, and\nBongki Moon. 2016. In-storage processing of database scans and joins. In Infor-\nmation Sciences . https://doi.org/10.1016/j.ins.2015.07.056\n[17] Sangjin Lee, Alberto Lerner, Andr\u00e9 Ryser, Kibin Park, Chanyoung Jeon, Jinsub\nPark, Yong Ho Song, and Philippe Cudr\u00e9-Mauroux. 2022. X-SSD: A Storage\nSystem with Native Support for Database Logging and Replication. In SIGMOD .\nhttps://doi.org/10.1145/3514221.3526188\n[18] Alberto Lerner and Philippe Bonnet. 2021. Not Your Grandpa\u2019s SSD: The Era of Co-\nDesigned Storage Devices. In SIGMOD . https://doi.org/10.1145/3448016.3457540\n[19] Alberto Lerner, Rana Hussein, Andr\u00e9 Ryser, Sangjin Lee, and Philippe Cudr\u00e9-\nMauroux. 2020. Networking and Storage: The Next Computing Elements in\nExascale Systems?. In IEEE Data Engineering Bulletin . https://exascale.info/\nassets/pdf/lerner20debull.pdf\n[20] Alberto Lerner, Jaewook Kwak, Sangjin Lee, Kibin Park, Yong Ho Song, and\nPhilippe Cudr\u00e9-Mauroux. 2020. It Takes Two: Instrumenting the Interaction\nbetween In-Memory Databases and Solid-State Drives. In CIDR . https://www.\ncidrdb.org/cidr2020/papers/p19-lerner-cidr20.pdf\n[21] Huaicheng Li et al .2023. Pond: CXL-Based Memory Pooling Systems for Cloud\nPlatforms. In ASPLOS . https://doi.org/10.1145/3575693.3578835\n[22] Minje Lim, Jeeyoon Jung, and Dongkun Shin. 2021. LSM-tree Compaction\nAcceleration Using In-storage Processing. https://doi.org/10.1109/ICCE-\nAsia53811.2021.9641965\n[23] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agarwal,\nPallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanaujia,\nand Prakash Chauhan. 2023. TPP: Transparent Page Placement for CXL-Enabled\nTiered-Memory. In ASPLOS . https://doi.org/10.1145/3582016.3582063\n[24] Micron. 2023. CZ120 memory expansion module. https://www.micron.com/\nsolutions/server/cxl.", "start_char_idx": 44113, "end_char_idx": 47037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "016d1938-7134-46c1-8ecd-cef6acb466f5": {"__data__": {"id_": "016d1938-7134-46c1-8ecd-cef6acb466f5", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfc06553-66c9-49ba-84f3-67616316a58d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "20e05470eb1a700fe138f5b970630cabe2a3829fef291d9869e8fd1034c62c43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d8593cb-0fd2-48e5-b80e-80e1d5aba226", "node_type": "1", "metadata": {}, "hash": "8cff8b11263f4f636e81f78345d84b7c5293fc382bd86605d845c62bc3ecf744", "class_name": "RelatedNodeInfo"}}, "text": "2021. LSM-tree Compaction\nAcceleration Using In-storage Processing. https://doi.org/10.1109/ICCE-\nAsia53811.2021.9641965\n[23] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agarwal,\nPallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanaujia,\nand Prakash Chauhan. 2023. TPP: Transparent Page Placement for CXL-Enabled\nTiered-Memory. In ASPLOS . https://doi.org/10.1145/3582016.3582063\n[24] Micron. 2023. CZ120 memory expansion module. https://www.micron.com/\nsolutions/server/cxl.\n[25] Scott Owens, Susmit Sarkar, and Peter Sewell. 2009. A better x86 memory model:\nx86-TSO. In TPHOLs . https://doi.org/10.1007/978-3-642-03359-9_27\n[26] Fatma \u00d6zcan, Yuanyuan Tian, and Pinar T\u00f6z\u00fcn. 2017. Hybrid transac-\ntional/analytical processing: A survey. In SIGMOD . 1771\u20131775. https://doi.org/\n10.1145/3035918.3054784\n[27] Abishek Ramdas. 2023. CCKit: FPGA acceleration in symmetric coherent heteroge-\nneous platforms . Ph. D. Dissertation. ETH Zurich.\n[28] Samsung. 2023. Memory Semantics SSD. https://samsungmsl.com/ms-ssd/.\n[29] Samsung. 2023. Samsung Develops Industry\u2019s First CXL DRAM Supporting CXL\n2.0. https://semiconductor.samsung.com/news-events/news/samsung-develops-\nindustrys- \uffffrst-cxl-dram-supporting-cxl-2-0/.\n[30] Debendra Das Sharma, Robert Blankenship, and Daniel S Berger. 2023. An\nIntroduction to the Compute Express Link (CXL) Interconnect. arXiv preprint\narXiv:2306.11227 (2023).\n[31] Daniel Sorin, Mark Hill, and David Wood. 2011. A primer on memory consistency\nand cache coherence . Morgan & Claypool Publishers. https://doi.org/10.1007/978-\n3-031-01764-3\n[32] Michael Stonebraker et al .2005. C-Store: A Column-oriented DBMS. In VLDB .\nhttps://doi.org/10.5555/1083592.1083658\n[33] Yan Sun, Yifan Yuan, Zeduo Yu, Reese Kuper, Ipoom Jeong, Ren Wang, and\nNam Sung Kim. 2023. Demystifying CXL Memory with Genuine CXL-Ready\nSystems and Devices. In arXiv . https://arxiv.org/abs/2303.15375.\n[34] Xuebin Zhang, Jiangpeng Li, Hao Wang, Kai Zhao, and Tong Zhang. 2016. Reduc-\ning Solid-State Storage Device Write Stress through Opportunistic In-place Delta\nCompression. In FAST . https://www.usenix.org/conference/fast16/technical-\nsessions/presentation/zhang-xuebinDemystifying CXL Memory with\nGenuine CXL-Ready Systems and Devices\nYan Sun\nUniversity of Illinois\nUrbana, U.S.A.\nyans3@illinois.eduYifan Yuan\nIntel Labs\nHillsboro, U.S.A.\nyifan.yuan@intel.comZeduo Yu\nUniversity of Illinois\nUrbana, U.S.A.\nzeduoyu2@illinois.eduReese Kuper\nUniversity of Illinois\nUrbana, U.S.A.\nrkuper2@illinois.edu\nChihun Song\nUniversity of Illinois\nUrbana, U.S.A.\nchihuns2@illinois.eduJinghan Huang\nUniversity of Illinois\nUrbana, U.S.A.\njinghan4@illinois.eduHouxiang Ji\nUniversity of Illinois\nUrbana, U.S.A.\nhj14@illinois.eduSiddharth Agarwal\nUniversity of Illinois\nUrbana, U.S.A.\nsa10@illinois.edu\nJiaqi Lou\nUniversity of Illinois\nUrbana, U.S.A.", "start_char_idx": 46522, "end_char_idx": 49390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d8593cb-0fd2-48e5-b80e-80e1d5aba226": {"__data__": {"id_": "6d8593cb-0fd2-48e5-b80e-80e1d5aba226", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "016d1938-7134-46c1-8ecd-cef6acb466f5", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a6434fb479d387feed3803506fb07f7fdd01b378f34f7b2e56e1640970b200aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9e42b0b-217f-4338-9bbb-2f5807b86140", "node_type": "1", "metadata": {}, "hash": "f6cf82a47a230c81f606c13b387f3646e145c56c13b384d00570d52519710675", "class_name": "RelatedNodeInfo"}}, "text": "yans3@illinois.eduYifan Yuan\nIntel Labs\nHillsboro, U.S.A.\nyifan.yuan@intel.comZeduo Yu\nUniversity of Illinois\nUrbana, U.S.A.\nzeduoyu2@illinois.eduReese Kuper\nUniversity of Illinois\nUrbana, U.S.A.\nrkuper2@illinois.edu\nChihun Song\nUniversity of Illinois\nUrbana, U.S.A.\nchihuns2@illinois.eduJinghan Huang\nUniversity of Illinois\nUrbana, U.S.A.\njinghan4@illinois.eduHouxiang Ji\nUniversity of Illinois\nUrbana, U.S.A.\nhj14@illinois.eduSiddharth Agarwal\nUniversity of Illinois\nUrbana, U.S.A.\nsa10@illinois.edu\nJiaqi Lou\nUniversity of Illinois\nUrbana, U.S.A.\njiaqil6@illinois.eduIpoom Jeong\nUniversity of Illinois\nUrbana, U.S.A.\nipoom@illinois.eduRen Wang\nIntel Labs\nHillsboro, U.S.A.\nren.wang@intel.comJung Ho Ahn\nSeoul National University\nSeoul, Republic of Korea\ngajh@snu.ac.kr\nTianyin Xu\nUniversity of Illinois\nUrbana, U.S.A.\ntyxu@illinois.eduNam Sung Kim\nUniversity of Illinois\nUrbana, U.S.A.\nnskim@illinois.edu\nABSTRACT\n1The ever-growing demands for memory with larger capacity and\nhigher bandwidth have driven recent innovations on memory ex-\npansion and disaggregation technologies based on Compute eX-\npress Link (CXL). Especially, CXL-based memory expansion tech-\nnology has recently gained notable attention for its ability not\nonly to economically expand memory capacity and bandwidth but\nalso to decouple memory technologies from a speci \uffffc memory\ninterface of the CPU. However, since CXL memory devices have\nnot been widely available, they have been emulated using DDR\nmemory in a remote NUMA node. In this paper, for the \uffffrst time,\nwe comprehensively evaluate a true CXL-ready system based on\nthe latest 4th-generation Intel Xeon CPU with three CXL memory\ndevices from di \ufffferent manufacturers. Speci \uffffcally, we run a set of\nmicrobenchmarks not only to compare the performance of true CXL\nmemory with that of emulated CXL memory but also to analyze\nthe complex interplay between the CPU and CXL memory in depth.\nThis reveals important di \ufffferences between emulated CXL memory\nand true CXL memory, some of which will compel researchers to\nrevisit the analyses and proposals from recent work. Next, we iden-\ntify opportunities for memory-bandwidth-intensive applications\nto bene \ufffft from the use of CXL memory. Lastly, we propose a CXL-\nmemory-aware dynamic page allocation policy, Caption to more\ne\uffffciently use CXL memory as a bandwidth expander. We demon-\nstrate that Caption can automatically converge to an empirically\nfavorable percentage of pages allocated to CXL memory, which\n1This work has been accepted by a conference. The authoritative version of this\nwork will appear in the Proceedings of the IEEE/ACM International Symposium\non Microarchitecture (MICRO), 2023. Please refer to https://doi.org/10.1145/3613424.\n3614256 for the o \uffffcial version of this paper.improves the performance of memory-bandwidth-intensive appli-\ncations by up to 24% when compared to the default page allocation\npolicy designed for traditional NUMA systems.\nKEYWORDS\nCompute eXpress Link, tiered-memory management, measurement\n1 INTRODUCTION\nEmerging applications have demanded memory with even larger\ncapacity and higher bandwidth at lower power consumption. How-\never, as the current memory technologies have almost reached their\nscaling limits, it has become more challenging to meet these de-\nmands cost-e \uffffciently. Especially, when focusing on the memory\ninterface technology, we observe that DDR5 requires 288 pins per\nchannel [ 44], making it more expensive to increase the number\nof channels for higher bandwidth under the CPU\u2019s package pin\nconstraint. Besides, various signaling challenges in high-speed par-\nallel interfaces, such as DDR, make it harder to further increase\nthe rate of data transfers. This results in super-linearly increasing\nenergy consumption per bit transfer [ 55] and reducing the number\nof memory modules (DIMMs) per channel to one for the maximum\nbandwidth [ 33].", "start_char_idx": 48841, "end_char_idx": 52726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9e42b0b-217f-4338-9bbb-2f5807b86140": {"__data__": {"id_": "e9e42b0b-217f-4338-9bbb-2f5807b86140", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d8593cb-0fd2-48e5-b80e-80e1d5aba226", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d49bc6fddff77287b977dccdc19ffcb330d5afbcdb243c5390475d3d43b8503f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d45df45a-1dd0-4c95-ba2c-2c68f2c7fa59", "node_type": "1", "metadata": {}, "hash": "a83dabd4decc9fc825b9436380a2c322bca538a65508cdefe5fa7409e256fbe7", "class_name": "RelatedNodeInfo"}}, "text": "How-\never, as the current memory technologies have almost reached their\nscaling limits, it has become more challenging to meet these de-\nmands cost-e \uffffciently. Especially, when focusing on the memory\ninterface technology, we observe that DDR5 requires 288 pins per\nchannel [ 44], making it more expensive to increase the number\nof channels for higher bandwidth under the CPU\u2019s package pin\nconstraint. Besides, various signaling challenges in high-speed par-\nallel interfaces, such as DDR, make it harder to further increase\nthe rate of data transfers. This results in super-linearly increasing\nenergy consumption per bit transfer [ 55] and reducing the number\nof memory modules (DIMMs) per channel to one for the maximum\nbandwidth [ 33]. As the capacity and bandwidth of memory are\nfunctions of the number of channels per CPU package, the number\nof DIMMs per channel, and the rate of bit transfers per channel,\nDDR has already shown its limited bandwidth and capacity scala-\nbility. This calls for alternative memory interface technologies and\nmemory subsystem architectures.\nAmong them, Compute eXpress Link (CXL) [ 73] has emerged\nas one of the most promising memory interface technologies. CXL\nis an open standard developed through a joint e \uffffort by major\nhardware manufacturers and hyperscalers. As CXL is built on thearXiv:2303.15375v4  [cs.PF]  5 Oct 2023Host CPUMemory ctrl.DDRDDRCXL memory module CXL/PCIeCXL.ioCXL.memCXLcontrollerCXL/PCIeMemory ctrl.Mem.deviceMem.deviceMem.device\u00b7\u00b7\u00b7Mem.deviceMem.deviceMem.device\u00b7\u00b7\u00b7Figure 1: CXL memory module architecture.\nstandard PCIe, which is a serial interface technology, it can o \uffffer\nmuch higher bit-transfer rates per pin ( e.g., PCIe 4.0: 16 Gbps/lane\nvs. DDR4-3200: 3.2 Gbps/pin) and consumes lower energy per bit\ntransfer ( e.g., PCIe 4.0: 6 pJ/bit [ 75] vs. DDR4: 22 pJ/bit [ 56]), but at\nthe cost of much longer link latency ( e.g., PCIe 4.0: \u21e040 ns [ 73] vs.\nDDR4: <1 ns [ 15]). Compared to PCIe, CXL implements additional\nfeatures that enable the CPU to communicate with devices and their\nattached memory in a cache-coherent fashion using load and store\ninstructions. Figure 1 illustrates a CXL memory device consisting\nof a CXL controller and memory devices. Consuming \u21e03\u21e5fewer\npins than DDR5, a CXL memory device based on PCIe 5.0 \u21e58 may\nexpand memory capacity and bandwidth of systems cost-e \uffffciently.\nFurthermore, with the CXL controller between the CPU and mem-\nory devices, CXL decouples memory technologies from a speci \uffffc\nmemory interface technology supported by the CPU. This grants\nmemory manufacturers unprecedented \uffffexibility in designing and\noptimizing their memory devices. Besides, by employing retimers\nand switches, a CPU with CXL support can easily access memory in\nremote nodes with lower latency than traditional network interface\ntechnologies like RDMA, e \uffffciently facilitating memory disaggre-\ngation. These advantages position memory-related extension as\none of the primary target use cases for CXL [ 25,32,65,67], and\nmajor hardware manufacturers have announced CXL support in\ntheir product roadmaps [4, 25, 35, 65, 67].\nGiven its promising vision, CXL memory has recently attracted\nsigni \uffffcant attention with active investigation for datacenter-scale\ndeployment [ 59,64]. Unfortunately, due to the lack of commercially\navailable hardware with CXL support, most of the recent research\non CXL memory has been based on emulation using memory in a\nremote NUMA node in a multi-socket system, since CXL memory is\nexposed as such [ 6,59,64]. However, as we will reveal in this paper,\nthere are fundamental di \ufffferences between emulated CXL memory\nand true CXL memory. That is, the common emulation-based prac-\ntice of using a remote NUMA node to explore CXL memory may\ngive us misleading performance characterization results and/or lead\nto suboptimal design decisions.", "start_char_idx": 51989, "end_char_idx": 55833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d45df45a-1dd0-4c95-ba2c-2c68f2c7fa59": {"__data__": {"id_": "d45df45a-1dd0-4c95-ba2c-2c68f2c7fa59", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9e42b0b-217f-4338-9bbb-2f5807b86140", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "36c9a13770d86b6d86f7a76c1284d4f66c669649392291325ef28d37034b7c3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "887b03f6-95f2-434f-9bd9-27a1933719b8", "node_type": "1", "metadata": {}, "hash": "2bd2cd55a97cc51befbf22a038b8d97c53369d5a7f3ab7a7f616040cd12d7829", "class_name": "RelatedNodeInfo"}}, "text": "Given its promising vision, CXL memory has recently attracted\nsigni \uffffcant attention with active investigation for datacenter-scale\ndeployment [ 59,64]. Unfortunately, due to the lack of commercially\navailable hardware with CXL support, most of the recent research\non CXL memory has been based on emulation using memory in a\nremote NUMA node in a multi-socket system, since CXL memory is\nexposed as such [ 6,59,64]. However, as we will reveal in this paper,\nthere are fundamental di \ufffferences between emulated CXL memory\nand true CXL memory. That is, the common emulation-based prac-\ntice of using a remote NUMA node to explore CXL memory may\ngive us misleading performance characterization results and/or lead\nto suboptimal design decisions.\nThis paper addresses a pressing need to understand the capabili-\nties and performance characteristics of true CXL memory, as well\nas their impact on the performance of (co-running) applications\nand the design of OS policies to best use CXL memory. To this\nend, we take a system based on the CXL-ready 4th-generation Intel\nXeon CPU [ 35] and three CXL memory devices from di \ufffferent man-\nufacturers (\u00a73). Then, for the \uffffrst time, we not only compare the\nperformance of true CXL memory with that of emulated CXL mem-\nory, but also conduct an in-depth analysis of the complex interplaybetween the CPU and CXL memory. Based on these comprehensive\nanalyses, we make the following contributions.\nCXL memory <remote NUMA memory (\u00a74). We reveal that true\nCXL memory exhibits notably di \ufffferent performance Characteristics\nfrom emulated CXL memory. (C1) Depending on CXL controller\ndesigns and/or memory technologies, true CXL memory devices\ngive a wide range of memory access latency and bandwidth val-\nues.(C2) True CXL memory can give up to 26% lower latency and\n3\u201366% higher bandwidth e \uffffciency than emulated CXL memory,\ndepending on memory access instruction types and CXL memory\ndevices. This is because true CXL memory has neither caches nor\nCPU cores that modify caches, although it is exposed as a NUMA\nnode. As such, the CPU implements an on-chip hardware structure\nto facilitate fast cache coherence checks for memory accesses to\nthe true CXL memory. These are important di \ufffferences that may\nchange conclusions made by prior work on the performance charac-\nteristics of CXL memory and consequently the e \uffffectiveness of the\nproposals at the system level. (C3)The sub-NUMA clustering (SNC)\nmode provides LLC isolation among SNC nodes (\u00a73) by directing\nthe CPU cores within an SNC node to evict their L2 cache lines from\nits local memory exclusively to LLC slices within the same SNC\nnode. However, when CPU cores access CXL memory, they end up\nbreaking the LLC isolation, as L2 cache lines from CXL memory\ncan be evicted to LLC slices in any SNC nodes. Consequently, ac-\ncessing CXL memory can bene \ufffft from e \uffffectively 2\u20134 \u21e5larger LLC\ncapacity than accessing local DDR memory, notably compensating\nfor the longer latency of accessing CXL memory for cache-friendly\napplications.\nNa\u00efvely used CXL memory considered harmful (\u00a75). Using\na system with a CXL memory device, we evaluate a set of appli-\ncations with diverse memory access characteristics and di \ufffferent\nperformance metrics ( e.g., response time and throughput). Subse-\nquently, we present the following Findings. (F1)Simple applications\n(e.g., key-value-store) demanding `s-scale latency are highly sensi-\ntive to memory access latency. Consequently, allocating pages to\nCXL memory increases the tail latency of these applications by 10\u2013\n82% compared to local DDR memory. Besides, the state-of-the-art\nCXL-memory-aware page placement policy for a tiered memory\nsystem [ 64] actually increases tail latency even further when com-\npared to statically partitioning pages between DDR memory and\nCXL memory. This is due to the overhead of page migration. (F2)\nComplex applications ( e.g., social network microservices) exhibit-\ning<s-scale latency experience a marginal increase in tail latency\neven when most of pages are allocated to CXL memory. This is\nbecause the longer latency of accessing CXL memory contributes\nmarginally to the end-to-end latency of such applications.", "start_char_idx": 55093, "end_char_idx": 59262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "887b03f6-95f2-434f-9bd9-27a1933719b8": {"__data__": {"id_": "887b03f6-95f2-434f-9bd9-27a1933719b8", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d45df45a-1dd0-4c95-ba2c-2c68f2c7fa59", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "4244ab120c1fa8e1cb18b4b8f708cf9d3f3c808f84196ae2958a0e3f6b2e326f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15a50813-ab7e-4446-9bbc-49d70ad11d31", "node_type": "1", "metadata": {}, "hash": "8ffec206a747ebd51f42c4cba09efd9c6654b5233aebd706de6b8200673eb5fd", "class_name": "RelatedNodeInfo"}}, "text": "(F1)Simple applications\n(e.g., key-value-store) demanding `s-scale latency are highly sensi-\ntive to memory access latency. Consequently, allocating pages to\nCXL memory increases the tail latency of these applications by 10\u2013\n82% compared to local DDR memory. Besides, the state-of-the-art\nCXL-memory-aware page placement policy for a tiered memory\nsystem [ 64] actually increases tail latency even further when com-\npared to statically partitioning pages between DDR memory and\nCXL memory. This is due to the overhead of page migration. (F2)\nComplex applications ( e.g., social network microservices) exhibit-\ning<s-scale latency experience a marginal increase in tail latency\neven when most of pages are allocated to CXL memory. This is\nbecause the longer latency of accessing CXL memory contributes\nmarginally to the end-to-end latency of such applications. (F3)Even\nfor memory-bandwidth-intensive applications, na\u00efvely allocating\n50% of pages to CXL memory based on the default OS policy may\nresult in lower throughput, despite higher aggregate bandwidth\ndelivered by using both DDR memory and CXL memory.\nCXL-memory-aware dynamic page allocation policy (\u00a76). To\nshowcase the usefulness of our characterizations and \uffffndings de-\nscribed above, we propose Caption ,aCXL-memory- aware dy-\nnamic page alloca tion policy for the OS to more e \uffffciently use\nthe bandwidth expansion capability of CXL memory. Speci \uffffcally,\n2CXL memory controller\nMemory device(DDR4 or DDR5) Memory I/FMemory ctrl.Memory device(DDR4 or DDR5) Memory ctrl.Memory I/FConfiguration space regs/MMIO spaceCXL link layerCXL transaction layerCXL.memlink layerCXL.iolink layerCXL.memtransaction layerCXL.iotransaction layerDevice-specific buffer/logic RxTxCXL ARB/MUXFlex Bus Physical Layer (PHY)CXL IPFigure 2: CXL.mem controller architecture.\nCaption begins by determining the bandwidth of manufacturer-\nspeci \uffffc CXL memory devices. Subsequently, Caption periodically\nmonitors various CPU counters, such as memory access latency\nexperienced by (co-running) applications and assesses the band-\nwidth consumed by them at runtime. Lastly, based on the moni-\ntored CPU counter values, Caption estimates memory-subsystem\nperformance over periods. When a given application demands an\nallocation of new pages, Caption considers the history of memory\nsubsystem performance and the percentage of pages allocated to\nCXL memory in the past. Then, it adjusts the percentage of the\npages allocated to CXL memory to improve the overall system\nthroughput using a simple greedy algorithm. Our evaluation shows\nthat Caption improves the throughput of a system co-running a\nset of memory-bandwidth-intensive SPEC CPU2017 benchmarks\nby 24%, compared with the default static page allocation policy set\nby the OS.\n2 BACKGROUND\n2.1 Compute eXpress Link (CXL)\nPCIe is the industry standard for a high-speed serial interface be-\ntween a CPU and I/O devices. Each lane of the current PCIe 5.0 can\ndeliver 32 GT/s ( e.g.,\u21e064 GB/s with 16 lanes). Built on the physical\nlayer of PCIe, the CXL standard de \uffffnes three separate protocols:\nCXL.io ,CXL.cache , and CXL.mem .CXL.io uses protocol features\nof the standard PCIe, such as transaction-layer packet (TLP) and\ndata-link-layer packet (DLLP), to initialize the interface between a\nCPU and a device [ 20].CXL.cache andCXL.mem use the aforemen-\ntioned protocol features for the device to access the CPU\u2019s memory\nand for the CPU to access the device\u2019s memory, respectively.\nTheCXL.mem protocol accounts only for memory accesses from\nthe CPU to the device facilitated by the Home Agent (HA) and the\nCXL controller on the CPU and the device, respectively [ 70]. The\nHA handles the CXL.mem protocol and transparently exposes CXL\nmemory to the CPU as memory in a remote NUMA node. That is,\nthe CPU can access CXL memory with load and store instructions in\nthe same way as it accesses memory in a remote NUMA node. This\nhas an advantage over other memory expansion technologies, such\nas RDMA, which involves the device\u2019s DMA engine and thus has\ndi\ufffferent memory access semantics. Lastly, when the CPU accesses\nCXL memory, it caches data from/to the CXL memory in every\nlevel of its cache hierarchy.", "start_char_idx": 58403, "end_char_idx": 62585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15a50813-ab7e-4446-9bbc-49d70ad11d31": {"__data__": {"id_": "15a50813-ab7e-4446-9bbc-49d70ad11d31", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "887b03f6-95f2-434f-9bd9-27a1933719b8", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a7c5511f6b671e47da748a85e2b11b9ccd673942ab9b5c3798c17ec02680429f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0af8f88-b4cf-4e9e-93ea-a875a5690e70", "node_type": "1", "metadata": {}, "hash": "8495c6610d62bf66f8db151e5029773612b1de03f82b84e0d89eb6ee8b28ad32", "class_name": "RelatedNodeInfo"}}, "text": "TheCXL.mem protocol accounts only for memory accesses from\nthe CPU to the device facilitated by the Home Agent (HA) and the\nCXL controller on the CPU and the device, respectively [ 70]. The\nHA handles the CXL.mem protocol and transparently exposes CXL\nmemory to the CPU as memory in a remote NUMA node. That is,\nthe CPU can access CXL memory with load and store instructions in\nthe same way as it accesses memory in a remote NUMA node. This\nhas an advantage over other memory expansion technologies, such\nas RDMA, which involves the device\u2019s DMA engine and thus has\ndi\ufffferent memory access semantics. Lastly, when the CPU accesses\nCXL memory, it caches data from/to the CXL memory in every\nlevel of its cache hierarchy. This has been impossible with any other\nmemory extension technologies except for persistent memory.2.2 CXL-ready Systems and Memory Devices\nCXL requires hardware support from both the CPU and devices.\nBoth the latest 4th-generation Intel Xeon Scalable Processor (Sap-\nphire Rapids) and the latest 4th-generation AMD EPYC Processor\n(Genoa) are among the \uffffrst server-class commodity CPUs to sup-\nport the CXL 1.1 standard [ 4,35]. Figure 2 depicts a typical archi-\ntecture of CXL.mem controllers. It primarily consists of (1) PCIe\nphysical layer, (2) CXL link layer, (3) CXL transaction layer, and (4)\nmemory controller blocks. (2), (3), and other CXL-related compo-\nnents are collectively referred to as CXL IP in this paper. As of today,\nin addition to some research prototypes, multiple CXL memory de-\nvices have been designed by major hardware manufacturers, such\nas Samsung [ 25], SK Hynix [ 77], Micron [ 65], and Montage [ 67].\nTo facilitate more \uffffexible memory functionality and near-memory\ncomputing capability, Intel also enables the CXL protocol in its\nlatest Agilex-I series FPGA [ 40], integrated with hard CXL IPs to\nsupport the CXL.io ,CXL.cache , and CXL.mem [41]. Lastly, unlike\na true NUMA node typically based on a large server-class CPU, a\nCXL memory device does not have any CPU cores, caches, or long\ninterconnects between the CXL IP and the memory controller in\nthe device.\n3 EVALUATION SETUP\n3.1 System and Device\nSystems. We use a server to evaluate the latest commercial hard-\nware supporting CXL memory (Table 1). The server consists of two\nIntel Sapphire Rapids (SPR) CPU sockets. One socket is populated\nwith eight 4800 MT/s DDR5 DRAM DIMMs (128 GB) across eight\nmemory channels. The other socket is populated with only one\n4800 MT/s DDR5 DRAM DIMM to emulate the bandwidth and ca-\npacity of CXL memory. The Intel SPR CPU integrates four CPU\nchiplets, each with up to 15 cores and two DDR5 DRAM channels.\nA user can choose to use the 4 chiplets as a uni \uffffed CPU, or each\nchiplet (or two chiplets) as a NUMA node in the SNC mode. Such\n\uffffexibility is to give users strong isolation of shared resources, such\nas LLC, among applications. Lastly, we turn o \uffffthe hyper-threading\nfeature and set the CPU core clock frequency to 2.1 GHz for more\npredictable performance.\nCXL memory devices. We take three CXL memory devices (\u2018CXL\nmemory devices\u2019 in Table 1), each featuring di \ufffferent CXL IPs (ASIC-\nbased hard IP and FPGA-based soft IP) and DRAM technologies\n(DDR5-4800, DDR4-2400, and DDR4-3200). Since the CXL protocol\nitself does not prescribe the underlying memory technology, it can\nseamlessly and transparently accommodate not only DRAM but\nalso persistent memory, \uffffash [ 72], and other emerging memory\ntechnologies. Consequently, various CXL memory devices may\nexhibit di \ufffferent latency and bandwidth characteristics.\n3.2 Microbenchmark\nTo characterize the performance of CXL memory, we use two\nmicrobenchmarks. First, we use Intel Memory Latency Checker\n(MLC) [ 42], a tool used to measure memory latency and bandwidth\nfor various usage scenarios.", "start_char_idx": 61867, "end_char_idx": 65659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0af8f88-b4cf-4e9e-93ea-a875a5690e70": {"__data__": {"id_": "a0af8f88-b4cf-4e9e-93ea-a875a5690e70", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15a50813-ab7e-4446-9bbc-49d70ad11d31", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "af94d7ed4398689737725220ba78a682c1575f12a237229578e16b451c6b49f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59363e3b-4ff5-465f-a964-3a75f2d3ae6b", "node_type": "1", "metadata": {}, "hash": "11adb69e9b447bb0ff53de5ddd0764d2aa54404ba6c1adb42a838099c4295758", "class_name": "RelatedNodeInfo"}}, "text": "Since the CXL protocol\nitself does not prescribe the underlying memory technology, it can\nseamlessly and transparently accommodate not only DRAM but\nalso persistent memory, \uffffash [ 72], and other emerging memory\ntechnologies. Consequently, various CXL memory devices may\nexhibit di \ufffferent latency and bandwidth characteristics.\n3.2 Microbenchmark\nTo characterize the performance of CXL memory, we use two\nmicrobenchmarks. First, we use Intel Memory Latency Checker\n(MLC) [ 42], a tool used to measure memory latency and bandwidth\nfor various usage scenarios. Second, we use a microbenchmark\ndubbed memo (measuring e\uffffciency of memory subsystems). It\nshares some features with Intel MLC, but we develop it to give more\n3Table 1: System con \uffffgurations.\nDual-socket server system\nComponent Desription\nOS (kernel) Ubuntu 22.04.2 LTS (Linux kernel v6.2)\nCPU2\u21e5Intel\u00aeXeon 6430 CPUs @2.1 GHz [37], 32 cores\nand 60 MB LLC per CPU, Hyper-Threading disabled\nMemorySocket 0: 8 \u21e5DDR5-4800 channels\nSocket 1: 1 \u21e5DDR5-4800 channel (emulated CXL memory)\nCXL memory devices\nDevice CXL IP Memory technology Max. bandwidth\nCXL-A Hard IP DDR5-4800 38.4 GB/s per channel\nCXL-B Hard IP 2 \u21e5DDR4-2400 19.2 GB/s per channel\nCXL-C Soft IP DDR4-3200 25.6 GB/s per channel\ncontrol over characterizing memory subsystem performance in di-\nverse ways. For instance, it can measure the latency and bandwidth\nof a speci \uffffc memory access instruction ( e.g., AVX-512 non-temporal\nload and store instructions).\n3.3 Benchmark\nLatency-sensitive applications. We run Redis [69], a popular\nhigh-performance in-memory key-value store, with YCSB [19]. We\nuse a uniform distribution of keys, ensuring maximum stress on the\nmemory subsystem, unless we explicitly specify the use of other\ndistributions. We also run DeathStarBench ( DSB)[28], an open-\nsource benchmark suite designed to evaluate the performance of\nmicroservices. It uses Docker to launch components of a microser-\nvice, including machine learning (ML) inference logic, web backend,\nload balancer, caching, and storage. Speci \uffffcally, we evaluate three\nDSBworkloads: (1) compose posts , (2)read user timelines , and\n(3)mixed workloads (10% of compose posts , 30% of read user\ntimelines , and 60% of read home timelines ) as a social network\nframework. Lastly, we run FIO[7], an open-source tool used for\nbenchmarking storage devices and \uffffle systems, to evaluate the\nlatency impact of using CXL memory for OS page cache. The page\ncache is supported by the standard Linux storage subsystem, which\nholds recently accessed storage data ( e.g.,\uffffles) in unused main\nmemory space to reduce the number of accesses to slow storage\ndevices.\nThroughput applications. First, we run an inference application\nbased on a deep learning recommendation model ( DLRM ) with the\nsame setup as MERCI [58]. The embedding reduction step in DLRM\ninference is known to have a large memory footprint and is re-\nsponsible for 50\u201370% of the inference latency [ 58]. Second, we take\nthe SPECrate CPU2017 benchmark suite [ 13], which is commonly\nused to evaluate the throughput of systems in datacenters. Then we\nassess misses per kilo instructions (MPKI) of every benchmark and\nrun the four benchmarks with the highest MPKI: (1) fotonik3d , (2)\nmcf, (3)roms , and (4) cactuBSSN . We run multiple instances of a\nsingle benchmark or two di \ufffferent benchmarks.\n4 MEMORY LATENCY AND BANDWIDTH\nCHARACTERISTICS\nIn this section, we \uffffrst evaluate the latency and bandwidth of ac-\ncessing di \ufffferent memory devices: an emulated CXL memory devicebased on DDR5 memory in a remote NUMA node (DDR5-R), and\nthree true CXL memory devices (CXL-A, CXL-B, and CXL-C).", "start_char_idx": 65102, "end_char_idx": 68738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59363e3b-4ff5-465f-a964-3a75f2d3ae6b": {"__data__": {"id_": "59363e3b-4ff5-465f-a964-3a75f2d3ae6b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0af8f88-b4cf-4e9e-93ea-a875a5690e70", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "14bfb65fe6f5a2e8809b95e7e55d869bee4daabbd64c3c3fc4374374735e9fc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f5fa7b7-c0fb-47d4-8277-429cc49db8f1", "node_type": "1", "metadata": {}, "hash": "ba86903b39a33f62e0b330fb54796bd0aa2db84c40299fd6e033a1fd1f7a29d2", "class_name": "RelatedNodeInfo"}}, "text": "Then we\nassess misses per kilo instructions (MPKI) of every benchmark and\nrun the four benchmarks with the highest MPKI: (1) fotonik3d , (2)\nmcf, (3)roms , and (4) cactuBSSN . We run multiple instances of a\nsingle benchmark or two di \ufffferent benchmarks.\n4 MEMORY LATENCY AND BANDWIDTH\nCHARACTERISTICS\nIn this section, we \uffffrst evaluate the latency and bandwidth of ac-\ncessing di \ufffferent memory devices: an emulated CXL memory devicebased on DDR5 memory in a remote NUMA node (DDR5-R), and\nthree true CXL memory devices (CXL-A, CXL-B, and CXL-C). We\nconduct this evaluation to understand the performance characteris-\ntics of various CXL memory devices for di \ufffferent memory access\ninstruction types. Next, we investigate interactions between the\nIntel SPR CPU\u2019s cache hierarchy and the CXL memory devices.\n4.1 Latency\nFigure 3 presents the measured latency values of accessing both\nemulated and true CXL memory devices. The \uffffrst group of bars\nshows average (unloaded idle) memory access latency values mea-\nsured by Intel MLC that performs pointer-chasing ( i.e., getting the\nmemory address of a load from the value of the preceding load) in a\nmemory region larger than the total LLC capacity of the CPU. This\ne\uffffectively measures the latency of serialized memory accesses. The\nremaining four groups of bars show the average memory access\nlatency values measured by memo for four memory access instruc-\ntion types: (1) temporal load ( ld), (2) non-temporal load ( nt-ld ),\n(3) temporal store ( st), and (4) non-temporal store ( nt-st ). For\nthese groups, we \uffffrst execute clflush to\uffffush all cache lines from\nthe cache hierarchy and then mfence to ensure the completion\nof\uffffushing the cache lines. Then, we execute 16 memory access\ninstructions back to back to 16 random memory addresses in a\ncacheable memory region. To measure the execution time of these\n16 memory access instructions, we execute rdtsc , which reads\nthe current value of the CPU\u2019s 64-bit time-stamp counter into a\nregister immediately before and after executing the 16 memory\naccess instructions, followed by an appropriate fence instruction.\nThis e \uffffectively measures the latency of random parallel memory\naccesses for each memory access instruction type for a given mem-\nory device. To obtain a representative latency value, we repeat the\nmeasurement 10,000 times and choose the median value to exclude\noutliers caused by TLB misses and OS activities.\nAnalyzing the latency values shown in Figure 3, we make the\nfollowing Observations.\n(O1) The full-duplex CXL and UPI interfaces reduce memory\naccess latency. memo gives emulated CXL memory 76% lower ld\nlatency than Intel MLC. This di \ufffference arises because serialized\nmemory accesses by Intel MLC cannot exploit the full-duplex capa-\nbility of the UPI interface that connects NUMA nodes. In contrast,\nrandom parallel memory accesses by memo can send memory com-\nmands/addresses and receive data in parallel through the full-duplex\nUPI interface, e \uffffectively halving the average latency cost of going\nFigure 3: Random memory access latency of various memory\ndevices (DDR5-R, CXL-A, CXL-B, and CXL-C), measured by\nIntel MLC and memo .\n4through the UPI interface. Since true CXL memory is also based\non the full-duplex interface ( i.e., PCIe), it enjoys the same bene \ufffft\nas emulated CXL memory. Nonetheless, with memo , CXL-A gets a\n3 percentage points more ldlatency reduction than for DDR5-R.\n(O3) explains the reason for this additional latency reduction.\n(O2) The latency of accessing true CXL memory devices is\nhighly dependent on a given CXL controller design. Figure 3\nshows that CXL-A exhibits only 35% longer ldlatency than DDR5-\nR, while CXL-B and CXL-C present almost 2 \u21e5and 3 \u21e5longer ld\nlatency, respectively. Even with the same DDR4 DRAM technology,\nCXL-C based on DDR4-3200 gives 67% longer ldlatency than CXL-\nB based on DDR4-2400.", "start_char_idx": 68195, "end_char_idx": 72053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f5fa7b7-c0fb-47d4-8277-429cc49db8f1": {"__data__": {"id_": "0f5fa7b7-c0fb-47d4-8277-429cc49db8f1", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59363e3b-4ff5-465f-a964-3a75f2d3ae6b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a794534e2026f4206afebf5841cd934c70abe88172f6c6a33be266cca9d1faee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea8b4e8b-002e-4241-95ae-0b649616fc5d", "node_type": "1", "metadata": {}, "hash": "8d79a04f0eb8f78f332e7fe71fdce962cbb08cfb5de90fcff43b16fd42360af6", "class_name": "RelatedNodeInfo"}}, "text": "Since true CXL memory is also based\non the full-duplex interface ( i.e., PCIe), it enjoys the same bene \ufffft\nas emulated CXL memory. Nonetheless, with memo , CXL-A gets a\n3 percentage points more ldlatency reduction than for DDR5-R.\n(O3) explains the reason for this additional latency reduction.\n(O2) The latency of accessing true CXL memory devices is\nhighly dependent on a given CXL controller design. Figure 3\nshows that CXL-A exhibits only 35% longer ldlatency than DDR5-\nR, while CXL-B and CXL-C present almost 2 \u21e5and 3 \u21e5longer ld\nlatency, respectively. Even with the same DDR4 DRAM technology,\nCXL-C based on DDR4-3200 gives 67% longer ldlatency than CXL-\nB based on DDR4-2400.\n(O3) Emulated CXL memory can give longer memory access\nlatency than true CXL memory. When issuing memory requests\nto emulated CXL memory, the local CPU must \uffffrst check with\nthe remote CPU, which is connected through the inter-chip UPI\ninterface, for cache coherence [ 62,66]. Moreover, the memory re-\nquests must travel through a long intra-chip interconnect within\nthe remote CPU to reach its memory controllers [ 83]. These over-\nheads increase with more CPU cores, i.e., more caches and a longer\ninterconnect path. For example, the ldlatency values of DDR5-R\nwith 26- and 40-core Intel SPR CPUs are 29% lower and 19% higher,\nrespectively, than those of DDR5-L with the 32-core Intel SPR CPU\nused for our primary evaluations. In contrast, true CXL memory\nhas neither caches nor CPU cores that modify caches, although it\nis exposed as a remote NUMA node. As such, the CPU implements\nan on-chip hardware structure to facilitate fast cache coherence\nchecks for memory accesses to the true CXL memory. Moreover,\ntrue CXL memory features a short intra-chip interconnect within\nthe CXL controller to reach its memory controllers.\nSpeci \uffffcally for DDR5-R and CXL-A, memo provides 76% and 79%\nlower ldlatency values, respectively, than Intel MLC. Although\nboth DDR5-R and CXL-A bene \ufffft from (O1), CXL-A gives a more\nldlatency reduction than DDR5-R. This arises from the following\ndi\ufffferences between memo and Intel MLC. As Intel MLC accesses\nmemory serially, the local CPU performs the aforementioned cache\ncoherence checks one by one. By contrast, memo accesses memory in\nparallel. In such a case, memory accesses to emulated CXL memory\nincur a burst of cache coherence checks that need to go through the\ninter-chip UPI interface, leading to cache coherence tra \uffffc conges-\ntion. This, in turn, increases the time required for cache coherence\nchecks. However, memory accesses to true CXL memory su \uffffer\nnotably less from this overhead because the CPU checks cache\ncoherence through its local on-chip structure described earlier. This\ncontributes to a further reduction in the ldlatency for true CXL\nmemory. Also note that DDR5-R, based on a 40-core Intel SPR CPU,\npresents 4% longer ldlatency than CXL-A due to a higher overhead\nof cache coherence checks.\nThe overhead of cache coherence checks becomes even more\nprominent for stbecause of two reasons. First, the stlatency\nis much higher than the ldlatency in general. For example, the\nlatency of stto DDR5-R is 2.3 \u21e5longer than that of ldfrom DDR5-\nL. This is because of the cache write-allocate policy in Intel CPUs.\nWhen stexperiences an LLC miss, the CPU \uffffrst reads 64-byte data\nfrom memory to a cache line ( i.e., implicitly executing ld), and\nthen writes modi \uffffed data to the cache line [ 43]. This overhead isincreased for both emulated CXL memory and true CXL memory,\nas the overhead of traversing the UPI and CXL interfaces is doubled.\nYet, the latency of stto emulated CXL memory increases more\nthan that of stto emulated CXL memory, when compared to ldor\nnt-ld .", "start_char_idx": 71371, "end_char_idx": 75070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea8b4e8b-002e-4241-95ae-0b649616fc5d": {"__data__": {"id_": "ea8b4e8b-002e-4241-95ae-0b649616fc5d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f5fa7b7-c0fb-47d4-8277-429cc49db8f1", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5fb5c9e4c6122bc7cced5c036273fc8c71eecbf5657c097b9667afe24458ee96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73af9a00-d3f2-40b5-b2cf-5c45fc4b34ae", "node_type": "1", "metadata": {}, "hash": "5900b2c118e9fefca84abfc284e5416ce1a3740442e9d7ed87ecd47dee9067c3", "class_name": "RelatedNodeInfo"}}, "text": "First, the stlatency\nis much higher than the ldlatency in general. For example, the\nlatency of stto DDR5-R is 2.3 \u21e5longer than that of ldfrom DDR5-\nL. This is because of the cache write-allocate policy in Intel CPUs.\nWhen stexperiences an LLC miss, the CPU \uffffrst reads 64-byte data\nfrom memory to a cache line ( i.e., implicitly executing ld), and\nthen writes modi \uffffed data to the cache line [ 43]. This overhead isincreased for both emulated CXL memory and true CXL memory,\nas the overhead of traversing the UPI and CXL interfaces is doubled.\nYet, the latency of stto emulated CXL memory increases more\nthan that of stto emulated CXL memory, when compared to ldor\nnt-ld . This is because the emulated CXL memory incurs a higher\noverhead for cache coherence checks than the true CXL memory,\nas discussed earlier.\nLastly, although both nt-ld andnt-st bypass caches and di-\nrectly access memory, the local CPU accessing emulated CXL mem-\nory still needs to check with the remote CPU for cache coher-\nence [ 39]. This explains why the nt-ld latency values of all the\nmemory devices are similar to those of ldthat needs to be served\nby memory in Figure 3. Unlike st, however, nt-st does not read\n64-byte data from the memory since it does not allocate a cache line\nby its semantics, which eliminates the memory access and cache\ncoherence overheads associated with implicit ld. Therefore, the\nabsolute values of nt-st latency across all the memory devices are\nsmaller than those of stlatency. Furthermore, nt-st can also o \uffffer\nshorter latency than ldandnt-ld because the CPU issuing nt-st\nsends the address and data simultaneously. In contrast, the CPU\nissuing ldornt-ld sends the address \uffffrst and then receive the data\nlater, which makes signals go through the UPI and CXL interfaces\ntwice. With shorter latency for a cache coherency check, nt-st\nto true CXL memory can be shorter than nt-st to emulated CXL\nmemory. For instance, CXL-A exhibits a 25% lower latency than\nDDR5-R for nt-st . Note that nt-st behaves di \ufffferently depending\non whether the allocated memory region is cacheable or not [ 39],\nand we conduct our experiment with a cacheable memory region.\n4.2 Bandwidth\nThe sequential memory access bandwidth represents the maximum\nthroughput of the memory subsystem when all the CPU cores sends\nmemory requests in this paper. Nonetheless, it notably varies across\n(1) CXL controller designs, (2) memory access instruction types, and\n(3) DRAM technologies ( i.e., DDR5-4800, DDR4-3200, and DDR4-\n2400 in this paper). As such, for fair and insightful comparison,\nwe use bandwidth e \uffffciency as a metric, normalizing the mea-\nsured bandwidth to the theoretical maximum bandwidth. Figure 4\npresents the bandwidth e \uffffciency values of DDR5-R, CXL-A, CXL-\nB, and CXL-C for various read/write ratios and di \ufffferent memory\ninstruction types, respectively. Analyzing these values, we make\nthe following Observations.\n(O4) The bandwidth is strongly dependent on the e \uffffciency of\nCXL controllers. The maximum sequential bandwidth values that\ncan be provided by the DDR5-4800, DDR4-3200, and DDR4-2400\nDRAM technologies are 38.4 GB/s, 25.6 GB/s, and 19.2 GB/s per\nchannel, respectively. Nonetheless, Figure 4a shows that DDR5-R,\nCXL-A, CXL-B, and CXL-C provides only 70%, 46%, 47%, and 20%\nof the theoretical maximum bandwidth, respectively, for \u2018All Read\u2019.\nDDR5-R and CXL-A are based on the same DDR5-4800 DRAM tech-\nnology, yet the bandwidth e \uffffciency of DDR5-R is 23 percentage\npoints higher than that of CXL-A. We speculate that the lower e \uffff-\nciency of the CXL-A\u2019s memory controller for memory read accesses\ncontributes to this bandwidth e \uffffciency gap, as both DDR5-R and\nCXL-A exhibit similar ldlatency values.", "start_char_idx": 74399, "end_char_idx": 78103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73af9a00-d3f2-40b5-b2cf-5c45fc4b34ae": {"__data__": {"id_": "73af9a00-d3f2-40b5-b2cf-5c45fc4b34ae", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea8b4e8b-002e-4241-95ae-0b649616fc5d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "69903fead55c731294a91c40b710ead512ab55be1f4b238e53a6de00f3c8d852", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc32b152-847a-49d4-a552-4a5b9853fd6a", "node_type": "1", "metadata": {}, "hash": "063c1143bb3eaad2837325a09cf7705e479061c544b86bf9de543fe2559b3c96", "class_name": "RelatedNodeInfo"}}, "text": "Nonetheless, Figure 4a shows that DDR5-R,\nCXL-A, CXL-B, and CXL-C provides only 70%, 46%, 47%, and 20%\nof the theoretical maximum bandwidth, respectively, for \u2018All Read\u2019.\nDDR5-R and CXL-A are based on the same DDR5-4800 DRAM tech-\nnology, yet the bandwidth e \uffffciency of DDR5-R is 23 percentage\npoints higher than that of CXL-A. We speculate that the lower e \uffff-\nciency of the CXL-A\u2019s memory controller for memory read accesses\ncontributes to this bandwidth e \uffffciency gap, as both DDR5-R and\nCXL-A exhibit similar ldlatency values.\n5(a) MLC with various read and write ratios\n(b)memo with di \ufffferent memory instruction types\nFigure 4: E \uffffciency of maximum sequential memory access\nbandwidth across di \ufffferent memory types.\nAs the write ratio increases, however, CXL-A starts to provide\nhigher bandwidth e \uffffciencies. For example, Figure 4a shows that\nthe bandwidth e \uffffciency of CXL-A for \u20182:1-RW\u2019 is 23 percentage\npoints higher than that of DDR5-R. We speculate that the CXL-\nA\u2019s memory controller is designed to handle interleaved memory\nread and write accesses more e \uffffciently than the DDR5-R\u2019s and\nCXL-B\u2019s memory controllers. This is supported by (1) the fact that\nstinvolves both memory read and write accesses due to implicit\nldwhen it incurs a cache miss ( cf. (O3)), and (2) the bandwidth\ne\uffffciency of CXL-A for all the other memory access instruction\ntypes is lower than that of DDR5-R and CXL-B. This also implies\nthat the higher bandwidth e \uffffciency of CXL-A for stis not solely\nattributed to a unique property of true CXL memory.\nFigure 4b shows that the bandwidth e \uffffciency of CXL-B is higher\nthan that of CXL-A, except for st, although the latency values of\nCXL-B is higher than those of CXL-A. Speci \uffffcally, the bandwidth\ne\uffffciency values of CXL-B for ld,nt-ld , and nt-st are 1, 1, and 6\npercentage points higher than that of CXL-A, respectively. We spec-\nulate that the recently developed third-party DDR5 memory con-\ntrollers may not be as e \uffffcient as the mature and highly optimized\nDDR4 memory controller used in CXL-B for read- or write-only\nmemory accesses. Note that CXL-C is based on DDR4-3200 DRAM\ntechnology, but it generally exhibits poor bandwidth e \uffffciency due\nto the FPGA-based implementation of the CXL controller. The band-\nwidth e \uffffciency values of CXL-C for ld,nt-ld ,st, and nt-st are\n26, 26, 3, and 20 percentage points lower, respectively, than those of\nCXL-B, which is based on the same DDR4 DRAM technology but\nprovides 25% lower theoretical maximum bandwidth per channel.\n(O5) True CXL memory can o \uffffer competitive bandwidth ef-\n\uffffciency for the store compared to emulated CXL memory.\nFigure 4b shows that styields lower bandwidth e \uffffciency values\nthan ldacross all the memory devices due to the overheads of\nimplicit ldand cache coherence checks ( cf. (O3)). Speci \uffffcally, st\nto DDR5-R, CXL-A, CXL-B, and CXL-C o \uffffers 74%, 31%, 59%, and15% lower bandwidth e \uffffciency values, respectively, than ldfrom\nDDR5-R, CXL-A, CXL-B, and CXL-C. This suggests that emulated\nCXL memory experiences a notably more bandwidth e \uffffciency\ndegradation than true CXL memory partly because it su \uffffers more\nfrom the overhead of cache coherence checks. As a result, the band-\nwidth e \uffffciency values of CXL-A and CXL-B for stare 12 and 1\npercentage points higher, respectively, than DDR5-R. For nt-st ,\nthe bandwidth e \uffffciency gap between emulated CXL memory and\ntrue CXL memory is noticeably reduced compared to nt-ld .", "start_char_idx": 77574, "end_char_idx": 80999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc32b152-847a-49d4-a552-4a5b9853fd6a": {"__data__": {"id_": "bc32b152-847a-49d4-a552-4a5b9853fd6a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73af9a00-d3f2-40b5-b2cf-5c45fc4b34ae", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f1eda2e59b8043737f010b4ba90969622de59454a1f08c29fe31e2235b2fd6f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6fc7030-49ed-45fa-a00a-d44b3f90001a", "node_type": "1", "metadata": {}, "hash": "1275e8f73c37d8c3e7136cb9632abb95ba48256fe23a0972c30cc8be82cdada9", "class_name": "RelatedNodeInfo"}}, "text": "This suggests that emulated\nCXL memory experiences a notably more bandwidth e \uffffciency\ndegradation than true CXL memory partly because it su \uffffers more\nfrom the overhead of cache coherence checks. As a result, the band-\nwidth e \uffffciency values of CXL-A and CXL-B for stare 12 and 1\npercentage points higher, respectively, than DDR5-R. For nt-st ,\nthe bandwidth e \uffffciency gap between emulated CXL memory and\ntrue CXL memory is noticeably reduced compared to nt-ld . Specif-\nically, the bandwidth e \uffffciency gap between DDR5-R and CXL-A\nfornt-ld is 26 percentage points, whereas it is reduced to 6 percent-\nage points for nt-st . CXL-B provides almost the same bandwidth\ne\uffffciency as DDR5-R for nt-st .\n4.3 Interaction with Cache Hierarchy\nStarting with the Intel Skylake CPU, Intel has adopted non-inclusive\ncache architecture [ 34]. Suppose that a CPU core with non-inclusive\ncache architecture incurs an LLC miss that needs to be served by\nmemory. Then it loads data from the memory into a cache line in\nthe CPU core\u2019s (private) L2 cache rather than the (shared) LLC, in\ncontrast to a CPU core with inclusive cache architecture. When\nevicting the cache line in the L2 cache, the CPU core will place it\ninto the LLC. That is, the LLC serves as a victim cache. The SNC\nmode (\u00a73.1), however, restricts where the CPU core places evicted\nL2 cache lines within the LLC to provide LLC isolation among SNC\nnodes. The LLC comprises as many slices as the number of CPU\ncores, and L2 cache lines in an SNC node are evicted only to LLC\nslices within the same SNC node when data in the cache lines are\nfrom the local DDR memory of that SNC node (light-green lines in\nFigure 5). In contrast, we notice that L2 cache lines can be evicted\nto any LLC slices within any SNC nodes when the data are from\nremote memory, including both emulated CXL memory and true\nCXL memory (red-dashed lines in Figure 5). As such, CPU cores\naccessing CXL memory break LLC isolation among SNC nodes in\nSNC mode. This makes such CPU cores bene \ufffft from 2\u20134 \u21e5larger\nLLC capacity than the ones accessing local DDR memory, notably\ncompensating for the slower access latency of CXL memory.\nCoreCh 1Mem.ctrl.Ch 1Ch 0Mem.ctrl.SNC-3SNC-1SNC-2SNC-0\nCh 1Ch 0Mem.ctrl.Ch 1Ch 0Mem.ctrl.Ch 0\nDDRDDRDDRDDRCoreCoreCoreCoreCoreCoreCXL memory\nLegendLocal DDR pathCXL.mempathCXL.memdataLocal DDR dataCXL Ctrl.DDRMem. ctrl.Mem. ctrl.LLCLLCLLCLLCLLCLLCLLCLLCCoreCoreCoreCoreCoreCoreLLCLLCLLCLLCLLCLLCLLCCore\nCoreCoreCoreCoreLLCLLCLLCLLCLLCCoreLLCCoreMem.ctrl.CoreCoreCoreCoreCoreLLCLLCLLCLLCLLCLLCCoreLLCCoreLLCCoreDDRDDRCoreCoreLLC\nCoreLLCCoreLLCDDRDDR\nFigure 5: Di \ufffference in L2 cache line eviction paths between\nlocal DDR memory and CXL memory in SNC mode.", "start_char_idx": 80538, "end_char_idx": 83236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6fc7030-49ed-45fa-a00a-d44b3f90001a": {"__data__": {"id_": "e6fc7030-49ed-45fa-a00a-d44b3f90001a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc32b152-847a-49d4-a552-4a5b9853fd6a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "4dfac36c1023d5abfc12a9d1a2d0665eaed7f9fbb0def3b8dc301ec22e884177", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96242707-6156-484e-a363-11d4f6942758", "node_type": "1", "metadata": {}, "hash": "65f08693726a786e07e676af6662cf64aa5d62847e599ba14e573c3c95a941e8", "class_name": "RelatedNodeInfo"}}, "text": "ctrl.Mem. ctrl.LLCLLCLLCLLCLLCLLCLLCLLCCoreCoreCoreCoreCoreCoreLLCLLCLLCLLCLLCLLCLLCCore\nCoreCoreCoreCoreLLCLLCLLCLLCLLCCoreLLCCoreMem.ctrl.CoreCoreCoreCoreCoreLLCLLCLLCLLCLLCLLCCoreLLCCoreLLCCoreDDRDDRCoreCoreLLC\nCoreLLCCoreLLCDDRDDR\nFigure 5: Di \ufffference in L2 cache line eviction paths between\nlocal DDR memory and CXL memory in SNC mode.\n6DDR5:CXL-A = 100:075:2550:5025:750:100\n050100150200\n020406080100p99 Latency (us)Target QPS (Kilo-QPS)(a) [Redis] YCSB-A (RD:UPD = 5:5)\n1.E+01.E+11.E+21.E+31.E+41.E+5\n012345p99 Latency (ms)\nTarget QPS (Kilo-QPS)(b) [DSB] compose posts\n1.E+01.E+11.E+21.E+31.E+41.E+5\n010203040p99 Latency (ms)\nTarget QPS (Kilo-QPS)(c) [DSB] read user timelines\n1.E+01.E+11.E+21.E+31.E+41.E+5\n0481216p99 Latency (ms)\nTarget QPS (Kilo-QPS)(d) [DSB] mixed workloads\nFigure 6: 99th-percentile (p99) latency values of Redis with various percentages of pages allocated to CXL memory and DSB\nwith 100% of only \u2018caching and storage\u2019 pages allocated to either CXL memory or DDR memory.\nTo verify this, we run a single instance of Intel MLC on an idle\nCPU and measure the average latency of randomly accessing 32 MB\nbu\uffffers allocated to DDR5-L and CXL-A, respectively, in the SNC\nmode. The total LLC capacity of the CPU with four SNC nodes\nin our system is \u21e060 MB. A 32 MB bu \uffffer is larger than the total\nLLC capacity of a single SNC node but smaller than that of all\nfour SNC nodes. This shows that accessing the bu \uffffer allocated to\nCXL-A gives an average memory access latency of 41 ns, whereas\naccessing the bu \uffffer allocated to DDR5-L o \uffffers an average memory\naccess latency of 76.8 ns. The shorter latency of accessing the bu \uffffer\nallocated to CXL-A evidently shows that the CPU cores accessing\nCXL memory can bene \ufffft from larger e \uffffective LLC capacity than\nthe CPU cores accessing local DDR memory in the SNC mode.\n(O6) CXL memory interacts with the CPU\u2019s cache hierar-\nchy di \ufffferently compared to local DDR memory. As discussed\nabove, the CPU cores accessing CXL memory are exposed to a\nlarger e \uffffective LLC capacity in the SNC mode. This often signi \uffff-\ncantly impacts LLC hit/miss and interference characteristics that\nthe CPU cores experience, and thus a \uffffecting the performance of ap-\nplications (\u00a75.3). Therefore, we must consider this attribute of CXL\nmemory when analyzing the performance of applications using\nCXL memory, especially in the SNC mode.\n5 IMPACT OF USING CXL MEMORY ON\nAPPLICATION PERFORMANCE\nTo study the impact of using CXL memory on the performance\nof applications (\u00a73.3), we take CXL-A, which provides the most\nbalanced latency and bandwidth characteristics among the three\nCXL memory devices. We use numactl in Linux to allocate memory\npages of a given program, either fully or partially, to CXL memory,\nexploiting the fact that the OS recognizes CXL memory as memory\nin a remote NUMA node by the OS. Speci \uffffcally, numactl allows\nusers to: (1) bind a program to a speci \uffffc memory node ( membind\nmode); (2) allocate memory pages to the local NUMA node \uffffrst,\nand then other remote NUMA nodes only when the local NUMA\nnode runs out of memory space ( preferred mode); or (3) allocate\nmemory pages evenly across a set of nodes ( interleaved mode).", "start_char_idx": 82896, "end_char_idx": 86080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96242707-6156-484e-a363-11d4f6942758": {"__data__": {"id_": "96242707-6156-484e-a363-11d4f6942758", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6fc7030-49ed-45fa-a00a-d44b3f90001a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "db72e9f03b2b0a78c93668ae0bfc8637a46ac3b1bfb758916f2fca5a5c8bd26a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7dc46b2f-7b61-4d28-b26d-e2a3e672f772", "node_type": "1", "metadata": {}, "hash": "64223f81198ec23d539301cbaacd136cb671fcf782775bfa8f160a3ca2e2acf6", "class_name": "RelatedNodeInfo"}}, "text": "We use numactl in Linux to allocate memory\npages of a given program, either fully or partially, to CXL memory,\nexploiting the fact that the OS recognizes CXL memory as memory\nin a remote NUMA node by the OS. Speci \uffffcally, numactl allows\nusers to: (1) bind a program to a speci \uffffc memory node ( membind\nmode); (2) allocate memory pages to the local NUMA node \uffffrst,\nand then other remote NUMA nodes only when the local NUMA\nnode runs out of memory space ( preferred mode); or (3) allocate\nmemory pages evenly across a set of nodes ( interleaved mode).\nA recent Linux kernel patch [ 85] enhances the interleaved mode\nto facilitate \uffffne-grained control over the allocation of a speci \uffffc\npercentage of pages to a chosen NUMA node. For example, we canchange the percentage of pages allocated to CXL memory from\nthe default 50% to 25%. That is, 75% of pages are allocated to local\nDDR5 memory.\nIn this section, we will vary the percentage of pages allocated\nto CXL memory and analyze its impact on performance using\napplication-speci \uffffc performance metrics, setting the stage for our\nCXL memory-aware dynamic page allocation policy (\u00a76). Note that\nwe enable the SNC mode to use only two local DDR5 memory\nchannels along with one CXL memory channel. This is because\nour system can accommodate only one CXL memory device, and it\nneeds to make a meaningful contribution to the total bandwidth\nof the system. In such a setup, the local DDR5 memory with two\nchannels provides \u21e02\u21e5higher bandwidth for stand\u21e03.4\u21e5higher\nbandwidth for ldthan CXL memory. As future platforms accommo-\ndate more CXL memory devices, we may connect up to four CXL\nmemory devices to a single CPU socket with eight DDR5 memory\nchannels, providing the same DDR5 to CXL channel ratio as our\nsetup.\n5.1 Latency\nRedis. Figure 6a shows the 99C\u2318-percentile (p99) latency values\nofRedis with YCSB workload A(50% read and 50% update) while\nvarying the percentage of pages allocated to CXL memory or lo-\ncal DDR5 memory (referred to as DDR memory hereafter). First,\nallocating 100% of pages to CXL memory (CXL 100%) signi \uffffcantly\nincreases the p99 latency compared to allocating 100% of pages\nto DDR memory (DDR 100%), especially at high target QPS val-\nues. For example, CXL 100% results in 10%, 73%, and 105% higher\np99 latency than DDR 100% at 25 K, 45 K, and 85 K target QPS,\nrespectively. Second, as more pages are allocated to CXL memory,\nthe p99 latency increases proportionally. For instance, at 85 K tar-\nget QPS, allocating 25%, 50%, and 75% of pages to CXL memory\nresults in p99 latency increases of 9%, 23%, and 45%, respectively,\ncompared to DDR 100%. Finally, as expected, allocating 100% of\npages to DDR memory results in the lowest and most stable p99\nlatency. Explaining the substantial di \ufffference in the p99 latency\nvalues for various percentages of pages allocated to CXL memory\nand di \ufffferent target QPS values, we note that Redis typically oper-\nates with response time at a `s scale, making it highly sensitive to\nmemory access latency (\u00a73.3). Therefore, allocating more pages to\n7Figure 7: Impact of TPP on latency of Redis compared with\nstatically allocating 25% of (random) pages to CXL memory.\nWe show the distributions up to the p99 latency.\nCXL memory and/or increasing the target QPS makes Redis more\nfrequently access CXL memory with almost 2 \u21e5longer latency than\nDDR memory, resulting in higher p99 latency.\nRedis+TPP. We conduct an experiment to assess whether the\nmost recent transparent page placement (TPP) [ 64] can minimize\nthe impact of using CXL memory on the p99 latency. The most\nrecent publicly available release [ 63] only o \uffffers an enhanced page\nmigration policy, and it does not automatically place pages in CXL\nmemory.", "start_char_idx": 85531, "end_char_idx": 89245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7dc46b2f-7b61-4d28-b26d-e2a3e672f772": {"__data__": {"id_": "7dc46b2f-7b61-4d28-b26d-e2a3e672f772", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96242707-6156-484e-a363-11d4f6942758", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "1d107058be47e8acfa081c7d4257cd66bf64e2febf359b5e6f7488c7662d2e41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41ffe974-2995-4f92-b39c-0abb35266fa2", "node_type": "1", "metadata": {}, "hash": "5f8881dedb8f0332af0a0b7032f508d1824f87116d83dc8ce602540ea19a8dbf", "class_name": "RelatedNodeInfo"}}, "text": "Therefore, allocating more pages to\n7Figure 7: Impact of TPP on latency of Redis compared with\nstatically allocating 25% of (random) pages to CXL memory.\nWe show the distributions up to the p99 latency.\nCXL memory and/or increasing the target QPS makes Redis more\nfrequently access CXL memory with almost 2 \u21e5longer latency than\nDDR memory, resulting in higher p99 latency.\nRedis+TPP. We conduct an experiment to assess whether the\nmost recent transparent page placement (TPP) [ 64] can minimize\nthe impact of using CXL memory on the p99 latency. The most\nrecent publicly available release [ 63] only o \uffffers an enhanced page\nmigration policy, and it does not automatically place pages in CXL\nmemory. Thus, we begin by allocating 100% of pages requested by\nRedis to CXL memory and let TPP automatically migrate pages to\nDDR memory until the percentage of the pages allocated to CXL\nmemory becomes 25%, based on the DDR to CXL bandwidth ratio in\nour setup. Then we measure the latency values of Redis . Figure 7\ncompares two distributions of the measured latency values. The\n\uffffrst one is from using TPP, while the second one is from statically\nallocating 25% of pages to CXL memory.\nTPP migrates a large number of pages to DDR memory in the\nbeginning phase, requiring the CPU to (1) copy pages from one\nmemory device to another and (2) update the OS page table entries\nassociated with the migrated pages [ 89]. Since (1) and (2) incur high\noverheads, we measure the p99 latency only after 75% of pages are\nmigrated to DDR memory. As shown in Figure 7, TPP generally\ngives higher latency, resulting in 174% higher p99 latency than\nstatically allocating 25% of pages to CXL memory. This is because\nTPP constantly migrates a small percentage of pages between DDR\nmemory and CXL memory over time, based on its metric assessing\nhotness/coldness of the pages. Although TPP has a feature that re-\nduces ping-pong behavior ( i.e., pages are constantly being promoted\nand demoted between DDR memory and CXL memory), migrating\npages incurs the overheads from (1) and (2) above. (1) blocks the\nmemory controllers from serving urgent memory read requests\nfrom latency-sensitive applications [ 57], and (2) also requires a\nconsiderable number of CPU cycles and memory accesses.\nDSB. Figure 6b\u20136d present the p99 latency values of (b) compose\nposts , (c)read user timelines , and (d) mixed workloads . Ta-\nble 2 summarizes the components of the benchmarks, their working\nset sizes and characteristics, and allocated memory devices. In our\nexperiment, we allocate 100% of the pages pertinent to the caching\nand storage components with large working sets to either DDR\nmemory (DDR 100%) or CXL memory (CXL 100%). Meanwhile, we\nalways allocate 100% of the pages associated with the remaining\ncomponents, such as nginx front-end and analytic docker images\n(e.g., logic in Table 2), to DDR memory, since these components\nare more sensitive to memory access latency than the caching andTable 2: Components of DSBsocial network benchmark.\nName Working set Intensiveness Allocated mem. type\nFrontend 83 MB Compute DDR memory\nLogic 208 MB Compute DDR memory\nCaching & Storage 628 MB Memory CXL memory\n12345\n110100100010000\n4k8k16k32k64k128k256k512kPercent Increasep99 Latency (us)Block Size (Bytes)DDR5CXL-APercent Increase\nFigure 8: p99 latency values of FIOfor various block sizes,\nand percentage values of increase in p99 latency by allocating\npage cache to CXL.\nstorage components. For example, nginx spends 60% of CPU cycles\non the CPU front-end, which is dominated by fetching instructions\nfrom memory [ 28]. Therefore, pages of such components should\nbe allocated to DDR memory.\nThis experiment shows that all three benchmarks, compose posts ,\nread user timelines , and mixed workloads are not sensitive to\nlong latency of accessing CXL memory as they exhibit little di \uffffer-\nence in p99 latency values between CXL 100% and DDR 100%. This\nis because of two reasons.", "start_char_idx": 88547, "end_char_idx": 92491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41ffe974-2995-4f92-b39c-0abb35266fa2": {"__data__": {"id_": "41ffe974-2995-4f92-b39c-0abb35266fa2", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dc46b2f-7b61-4d28-b26d-e2a3e672f772", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8946cdeb6b8c719628c6402ecb9186e7f0e4aa1e8a3f089122939bc7ef8c6cb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c45d1533-f68e-4420-88f2-81bec47e3ed1", "node_type": "1", "metadata": {}, "hash": "06b0dde6e124e77d0300e78baede49e284a3632f43216ecb8fd2804644da8405", "class_name": "RelatedNodeInfo"}}, "text": "storage components. For example, nginx spends 60% of CPU cycles\non the CPU front-end, which is dominated by fetching instructions\nfrom memory [ 28]. Therefore, pages of such components should\nbe allocated to DDR memory.\nThis experiment shows that all three benchmarks, compose posts ,\nread user timelines , and mixed workloads are not sensitive to\nlong latency of accessing CXL memory as they exhibit little di \uffffer-\nence in p99 latency values between CXL 100% and DDR 100%. This\nis because of two reasons. First, most of the p99 latency in these\nbenchmarks is contributed by the front-end and/or logic compo-\nnents ( i.e., DDR 100%). This makes the latency of accessing CXL\nmemory amortized by the these components, and thus the p99 la-\ntency is much less dependent on the latency of accessing databases\n(i.e., CXL 100%). Second, the p99 latency of DSBis at a <s scale and\ntwo orders of magnitude longer than that of Redis . Therefore, it is\nnot as sensitive to memory access latency as that of Redis .\nNote that CXL 100% provides lower p99 latency values than\nDDR 100% for mixed workloads when the QPS range is between\n5 K and 11 K. This is because mixed workloads is far more memory-\nbandwidth-intensive than compose posts andread user timelines .\nSpeci \uffffcally, when we measure the average bandwidth consump-\ntion by these three benchmarks in the QPS range that saturates the\nthroughput of the benchmarks, we observe that mixed workloads\nconsumes 32 GB/s while compose posts andread user timelines\nconsume only 7 GB/s and 10 GB/s, respectively. When a given ap-\nplication consumes such high bandwidth in our setup, we observe\nthat the application\u2019s throughput, which is inversely proportional\nto its latency, becomes sensitive to the bandwidth available for the\napplication (\u00a75.2). Lastly, as the QPS approaches to 11 K, the com-\npute capability of the CPU cores becomes the dominant bottleneck,\nleading to a decrease in the p99 latency gap between DDR 100%\nand CXL 100%.\nFIO. Figure 8 presents the p99 latency values of FIOwith 4 GB\npage cache allocated to either DDR memory or CXL memory for\nvarious I/O block sizes. We use a Zipfian distribution for FIOto\nevaluate the impact of using page cache on \uffffle-system performance.\nIt shows that allocating the page cache to CXL memory gives only\n\u21e03% longer p99 latency than DDR5 memory for 4 KB block size.\n8This is because the p99 latency for a 4 KB block size is primarily\ndominated by the Linux kernel operations related to page cache\nmanagement, such as context switching, page cache lookup, send-\ning an I/O request through the \uffffle system, block layer, and device\ndriver. However, with a 8 KB block size, the cost of Linux kernel\noperations is amortized, as multiple 4 KB pages are brought from a\nstorage device by a single system call. Consequently, longer access\nlatency of CXL memory a \uffffects the p99 latency of FIOmore no-\ntably, resulting in a \u21e04.5% increase in the p99 latency. Meanwhile,\nas the block size increases beyond 8 KB, the page cache hit rate\ndecreases from 76% for 8 KB to 65% for 128 KB. As lower page cache\nhit rates necessitate more page transfers from the storage device,\nthe storage access latency begins to dominate the p99 latency. In\nsuch a case, the di \ufffference in memory access latency between DDR\nmemory and CXL memory exhibits a lower impact on p99 latency,\nsince Data Direct I/O (DDIO) [ 38] directly injects pages read from\nthe storage device into the LLC [ 3,26,27,93]. Lastly, we observe\nanother trend shift beyond 128 KB block size, which is mainly due\nto the limited read and write bandwidth of CXL memory. As more\npage cache entries are evicted from the LLC to memory as well as\nfrom memory to the storage device, the limited bandwidth of CXL\nmemory increases the e \uffffective latency of I/O requests.\nKey\uffffndings. Based on our analyses above, we present the follow-\ning three key Findings.", "start_char_idx": 91986, "end_char_idx": 95859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c45d1533-f68e-4420-88f2-81bec47e3ed1": {"__data__": {"id_": "c45d1533-f68e-4420-88f2-81bec47e3ed1", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41ffe974-2995-4f92-b39c-0abb35266fa2", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "39d64c3399a829bea023b0aebcec229717d7c90d831f11f6e1a4dd4cb82c96e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd25aaa0-33b5-4274-baf1-ff09840de5c7", "node_type": "1", "metadata": {}, "hash": "5c7d8469e8793b04f240a6e2025fea060604faea70a7fc7a0d87081fbf179aa5", "class_name": "RelatedNodeInfo"}}, "text": "In\nsuch a case, the di \ufffference in memory access latency between DDR\nmemory and CXL memory exhibits a lower impact on p99 latency,\nsince Data Direct I/O (DDIO) [ 38] directly injects pages read from\nthe storage device into the LLC [ 3,26,27,93]. Lastly, we observe\nanother trend shift beyond 128 KB block size, which is mainly due\nto the limited read and write bandwidth of CXL memory. As more\npage cache entries are evicted from the LLC to memory as well as\nfrom memory to the storage device, the limited bandwidth of CXL\nmemory increases the e \uffffective latency of I/O requests.\nKey\uffffndings. Based on our analyses above, we present the follow-\ning three key Findings. (F1)Allocating any percentage of pages to\nCXL memory proportionally increases the p99 latency of simple\nmemory-intensive applications demanding `s-scale latency, since\nsuch applications are highly sensitive to memory access latency.\n(F2)Even an intelligent page migration policy may further increase\nthe p99 latency of such latency-sensitive applications because of\nthe overheads of migrating pages. (F3)Judiciously allocating cer-\ntain pages to CXL memory does not increase the p99 latency of\ncomplex applications exhibiting <s-scale latency. This is because\nthe long latency of accessing CXL memory marginally contributes\nto the end-to-end latency of such applications and it is amortized\nby intermediate operations between accesses to CXL memory.\n5.2 Throughput\nDLRM. Figure 9a shows the throughput of DLRM embedding re-\nduction for various percentages of pages allocated to CXL mem-\nory. As the throughput of DLRM embedding reduction is bounded\nby memory bandwidth [ 50,58,94], it begins to saturate over 20\nthreads when 100% of pages are allocated to DDR memory. In such\na case, we observe that allocating a certain percentage of pagesTable 3: Throughput of DLRM using only 1 SNC node versus all\n4 SNC nodes, normalized to the throughput of DLRM running\non 1 SNC node allocating all the pages to local DDR memory.\n1 SNC node 4 SNC nodes\nDDR 100% CXL 100% DDR 100% CXL 100%\n1 0.947 1 0.504\nto CXL memory can improve the throughput further, as it supple-\nments to the bandwidth of DDR memory, increasing the total band-\nwidth available for DLRM . For instance, when running 32 threads,\nwe observe that allocating 63% of pages to CXL memory can maxi-\nmize the throughput of DLRM embedding reduction, providing 88%\nhigher throughput than DDR 100%. Note that a lower percentage of\npages will be allocated to CXL memory for achieving the maximum\nthroughput if the maximum bandwidth capability of a given CXL\nmemory device is lower ( e.g., CXL-C). This clearly demonstrates\nthe bene \ufffft of CXL memory as a memory bandwidth expander.\nRedis. Although Redis is a latency-sensitive application, its through-\nput is also an important performance metric. Figure 9b shows the\nmaximum sustainable QPS for various percentages of pages allo-\ncated to CXL memory. For example, for YCSB-A , allocating 25%,\n50%, 75%, and 100% of pages to CXL memory provides 8%, 15%, 22%,\nand 30% lower throughput than allocating 100% of pages to DDR\nmemory. As Redis does not fully utilize the memory bandwidth,\nits throughput is bounded by memory access latency. Thus, similar\nto its p99 latency trends (Figure 6a), allocating more pages to CXL\nmemory reduces the throughput of Redis .\nKey\uffffndings. Based on our analyses above, we present the follow-\ning key Finding. (F4)For memory-bandwidth-intensive applica-\ntions, na\u00efvely allocating 50% of pages to CXL memory based on the\nOS default policy may result in lower throughput than allocating\n100% of pages to DDR memory, even with higher total bandwidth\nfrom using both DDR memory and CXL memory together. This\nmotivates us to develop a dynamic page allocation policy that can\nautomatically con \uffffgure the percentage of pages allocated to CXL\nmemory at runtime based on the bandwidth capability of a given\nCXL memory device and bandwidth consumed by co-running ap-\nplications (\u00a76).\n5.3 Interaction with Cache Hierarchy\nPreviously, we discussed that accessing CXL memory breaks the\nLLC isolation among SNC nodes (\u00a74.3).", "start_char_idx": 95194, "end_char_idx": 99297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd25aaa0-33b5-4274-baf1-ff09840de5c7": {"__data__": {"id_": "bd25aaa0-33b5-4274-baf1-ff09840de5c7", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c45d1533-f68e-4420-88f2-81bec47e3ed1", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "892f40e668fea933eb53151fc9c7954b24cf6c053df21270ed36eb913b08d11c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fa95e3c-09bf-48ad-8a3a-cbf000a257bb", "node_type": "1", "metadata": {}, "hash": "a62400bb596c06e7f9b45aa927e58dcddeb2157bf4dffde53ebf68363651e83c", "class_name": "RelatedNodeInfo"}}, "text": "Key\uffffndings. Based on our analyses above, we present the follow-\ning key Finding. (F4)For memory-bandwidth-intensive applica-\ntions, na\u00efvely allocating 50% of pages to CXL memory based on the\nOS default policy may result in lower throughput than allocating\n100% of pages to DDR memory, even with higher total bandwidth\nfrom using both DDR memory and CXL memory together. This\nmotivates us to develop a dynamic page allocation policy that can\nautomatically con \uffffgure the percentage of pages allocated to CXL\nmemory at runtime based on the bandwidth capability of a given\nCXL memory device and bandwidth consumed by co-running ap-\nplications (\u00a76).\n5.3 Interaction with Cache Hierarchy\nPreviously, we discussed that accessing CXL memory breaks the\nLLC isolation among SNC nodes (\u00a74.3). To analyze the impact of\n0246810121416\n48121620242832Inference Throughput (M-queries/sec)Thread CountDDR5:CXL-A = 100:083:1762:3850:5037:6317:830:100\n(a) [DLRM] embedding reduction\n0.00.20.40.60.81.01.2\nABCDFMax QPS Normalized to DDR100%\nWorkloadDDR5:CXL-A = 100:075:2550:5025:750:100(b) [Redis] YCSB-A\nFigure 9: Impact of using CXL memory on throughput of Redis andDLRM for various ratios of page allocation to CXL memory.\n9such an attribute of accessing CXL memory on application perfor-\nmance, we evaluate two cases. In the \uffffrst case (\u20181 SNC node\u2019 in\nTable 3), only one SNC node ( i.e., SNC-0 in Figure 5) runs 8 DLRM\nthreads while the other three SNC nodes idle. In the second case (\u20184\nSNC nodes\u2019 in Table 3), each SNC node runs 8 DLRM threads. Only\nSNC-0 allocates 100% of its pages to either its DDR memory or CXL\nmemory, while the other three SNC nodes ( i.e., SNC-1, SNC-2, and\nSNC-3) allocate 100% of their pages only to their respective local\nDDR memory. The second case is introduced to induce interference\nat the LLC among all the SNC nodes when SNC-0 with CXL 100%.\nTable 3 shows that SNC-0 with CXL 100% in \u20181 SNC node\u2019 o \uffffers\n88% higher throughput than SNC-0 with CXL 100% in \u20184 SNC nodes. \u2019\nThis is because of the other three SNC nodes in \u20184 SNC nodes\u2019\nreduces the e \uffffective LLC capacity of SNC-0 with CXL 100% ( i.e.,\nLLC slices from all the SNC nodes). Speci \uffffcally, while the other\nthree SNC nodes evict LLC lines within their respective LLC slices,\nthey also inevitably evict many LLC lines from SNC-0 with CXL\n100%. Although not shown in Table 3, SNC-0 with DDR 100% in \u20181\nSNC node\u2019 provides 2% higher throughput than each of the other\nthree SNC nodes in \u20184 SNC nodes\u2019 when SNC-0 in \u20184 SNC nodes\u2019 is\nwith CXL 100%. This is because SNC-0 with CXL 100% in \u20184 SNC\nnodes\u2019 pollutes the LLC slices of the other three SNC nodes with\ncache lines evicted from the L2 caches of SNC-0, breaking the LLC\nisolation among the SNC nodes. Lastly, in our previous evaluation\nofDLRM throughput (\u00a75.2), when SNC-0 needs to run more than 8\nthreads of DLRM in the SNC mode, it makes the remaining threads\nrun on the CPU cores in the other three SNC nodes. Nonetheless,\nthe CPU cores in the other three SNC nodes continue to access the\nDDR memory of SNC-0, and cache lines in the L2 caches of these\nCPU cores are still evicted to the LLC slices of SNC-0 since the\ncache lines were from the DDR memory of SNC-0.\n6 CXL-MEMORY-AWARE DYNAMIC\nPAGE ALLOCATION POLICY\nWe have demonstrated a potential of CXL memory as a band-\nwidth expander, which can improve the performance of bandwidth-\nintensive applications (\u00a75.2).", "start_char_idx": 98516, "end_char_idx": 101910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fa95e3c-09bf-48ad-8a3a-cbf000a257bb": {"__data__": {"id_": "4fa95e3c-09bf-48ad-8a3a-cbf000a257bb", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd25aaa0-33b5-4274-baf1-ff09840de5c7", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8db37e8df295dbddac4279b02f7307802629be986c164e4b97a2a78e69f86067", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9074039c-bf1f-40b7-b8ed-e11dd11a54e2", "node_type": "1", "metadata": {}, "hash": "3f3d7ea55a21310768d7efc90627d42184a004e43fe738041d96a7aa504ded49", "class_name": "RelatedNodeInfo"}}, "text": "Lastly, in our previous evaluation\nofDLRM throughput (\u00a75.2), when SNC-0 needs to run more than 8\nthreads of DLRM in the SNC mode, it makes the remaining threads\nrun on the CPU cores in the other three SNC nodes. Nonetheless,\nthe CPU cores in the other three SNC nodes continue to access the\nDDR memory of SNC-0, and cache lines in the L2 caches of these\nCPU cores are still evicted to the LLC slices of SNC-0 since the\ncache lines were from the DDR memory of SNC-0.\n6 CXL-MEMORY-AWARE DYNAMIC\nPAGE ALLOCATION POLICY\nWe have demonstrated a potential of CXL memory as a band-\nwidth expander, which can improve the performance of bandwidth-\nintensive applications (\u00a75.2). If the throughput of a given application\nis limited by the bandwidth, allocating a higher percentage of pages\nto CXL memory may alleviate bandwidth pressure on DDR memory,\nand hence reduce average memory access latency. Intuitively, such\na percentage should be tuned for di \ufffferent CXL memory devices\ngiven their distinct bandwidth capabilities (\u00a75.2). By contrast, if a\ngiven application is not memory-bandwidth-bounded, allocating\na lower percentage of pages to CXL memory may lead to lower\naverage memory access latency and thus higher throughput. That\nstated, to better utilize auxiliary memory bandwidth provided by\nCXL memory, we present Caption , a dynamic page allocation pol-\nicy.Caption automatically tunes the percentage of new pages to\nbe allocated by the OS to CXL memory based on three factors: (1)\nbandwidth capability of CXL memory, (2) memory intensiveness of\nco-running applications, and (3) average memory access latency.\nNote that Caption , focusing on the page allocation ratio between\nDDR memory and CXL memory, is orthogonal and complementary\ntoTPP.\nMonitorEstimatorTuner\nCollect/parsePredict throughputPMU-toolsPCM-*eBPFPerf. metricsBinary searchNext page allocation ratioLinear regression modelMachine learning modelAllocationratioThroughputFigure 10: Overview of Caption . The components in dotted\nboxes can be used for better performance.\n6.1 Policy Design\nCaption consists of three runtime Modules (Figure 10). (M1) peri-\nodically monitors some CPU counters related to memory subsystem\nperformance, and then (M2) estimates memory-subsystem perfor-\nmance based on values of the counters. When a given application\nrequests an allocation of new pages, (M3) tunes the percentage of\nthe new pages allocated to CXL memory, aiming to improve the\nthroughput of the application. Subsequently, mempolicy [85] sets\nthe page allocation ratio between DDR memory and CXL memory\nbased on the percentage guided by (M3), and instructs the OS to\nallocate the new pages based on the ratio.\n(M1) Monitoring CPU counters related to memory subsys-\ntem performance. We use Intel PCM [ 36] to periodically sample\nvarious CPU counters related to memory subsystem performance,\nas listed in Table 4. These CPU counters allow (M2) to estimate\noverall memory-subsystem performance. In Figure 11, we run DLRM ,\nof which the throughput is bounded by memory bandwidth. Then\nwe observe correlations between DLRM throughput and values of\nthose counters, as we vary the percentage of pages allocated to\nCXL memory.\nFigure 11a shows that DLRM throughput is proportional to the\nconsumed memory bandwidth. Yet, as the consumed memory band-\nwidth exceeds a certain amount, the memory access latency rapidly\nincreases due to contention and resulting queuing delay at the mem-\nory controller [ 80], which, in turn, decreases the DLRM throughput.\nMeanwhile, Figure 11b shows that DLRM throughput is inversely\nproportional to L1 miss latency. The L1 miss latency is an impor-\ntant memory-subsystem performance metric that simultaneously\ncaptures both the cache friendliness and bandwidth intensiveness\n(i.e., queuing delay at the memory controller) of given (co-running)\napplications at the same time. At \uffffrst, allocating more pages to CXL\nmemory reduces pressure on the DDR memory controller, thereby\ndecreasing the latency of accessing DDR memory and handling\nL1 misses. However, at some point, the long latency of accessing\nCXL memory begins to dominate that of handling L1 misses, and\nTable 4: CPU counters pertinent to memory-subsystem perf.", "start_char_idx": 101242, "end_char_idx": 105441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9074039c-bf1f-40b7-b8ed-e11dd11a54e2": {"__data__": {"id_": "9074039c-bf1f-40b7-b8ed-e11dd11a54e2", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fa95e3c-09bf-48ad-8a3a-cbf000a257bb", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "2ceef4642811f18d25316623d87f620c00cb90c869525eff9533198ac6f21d96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7a70987-015e-4694-ace1-7f054cce01cb", "node_type": "1", "metadata": {}, "hash": "9d1c412e3ca0bed20ae980d5dcadb4915cba8e7d6bddbde1f748a41778c6af38", "class_name": "RelatedNodeInfo"}}, "text": "Meanwhile, Figure 11b shows that DLRM throughput is inversely\nproportional to L1 miss latency. The L1 miss latency is an impor-\ntant memory-subsystem performance metric that simultaneously\ncaptures both the cache friendliness and bandwidth intensiveness\n(i.e., queuing delay at the memory controller) of given (co-running)\napplications at the same time. At \uffffrst, allocating more pages to CXL\nmemory reduces pressure on the DDR memory controller, thereby\ndecreasing the latency of accessing DDR memory and handling\nL1 misses. However, at some point, the long latency of accessing\nCXL memory begins to dominate that of handling L1 misses, and\nTable 4: CPU counters pertinent to memory-subsystem perf.\nMetric Tool Description\nL1 miss latency pcm-latency Average L1 miss latency (ns)\nDDR read latency pcm-latency DDR read latency (ns)\nIPC pcm Instructions per cycle\n100.40.60.81.01.21.4\n10152025303540Norm. DLRM ThroughputSystem Bandwidth (GB/s)(a) System bandwidth\n0.81.01.21.41.61.82.02.2\n20406080100L1 Miss Latency (ns)(b) L1 miss latency\nFigure 11: Correlations between throughput and various\ncounter values, as we increase the percentage of pages al-\nlocated to CXL memory for DLRM . The system bandwidth is\nthe total consumed memory bandwidth, and The throughput\nis normalized to DDR 100%.\nthe application throughput begins to decrease. Finally, IPC is an-\nother important metric that implicitly measures the e \uffffciency of\nthe memory subsystem for the applications.\n(M2) Estimating system throughput. To build a model that esti-\nmates the system performance, we collect CPU counter values at\nvarious DDR:CXL ratios while running DLRM with 24 threads. We\nthen build a linear-regression model that correlates these counter\nvalues with DLRM throughput. Taking these counter values from\n(M1), Caption periodically estimates (or infers) memory-subsystem\nperformance at runtime. In our current implementation of Caption ,\n(M1) samples the counters every 1 second. To reduce the noise\namong the values, we collect a moving average of the past 5 sam-\nples for each counter. The averaged value is then fed into (M2) for\nperformance estimation. Although we may use a machine-learning\n(ML) model, we use the following simple linear model for the cur-\nrent implementation of Caption :\n.=V0+V1-1+V2-2+... (1)\nwhere .represents the estimated memory-subsystem performance,\n-=represents a counter value listed in Table 4, and V=represents\nthe-=\u2019s weight obtained through multiple linear regression steps.\nThis linear model is simple enough to be used by the OS at a low\nperformance cost, yet e \uffffective enough to estimate the memory-\nsubsystem performance. In the current implementation of Caption ,\nwe\uffffnd that using PCMtoolkit is su \uffffcient. Nonetheless, we may use\nPMU tools and eBPF [ 24] to access more diverse counters, facilitat-\ning a more precise estimation of memory-subsystem performance.\n(M3) Tuning the percentage of pages allocated to CXL mem-\nory. When a given application demands allocation of memory\npages, Caption (Algorithm 1) \uffffrst compares the estimated memory-\nsubsystem performance value from the past period (line: 9\u201311) with\nthe current period (line: 3). If the memory-subsystem performance\nin the current period has increased compared to the previous pe-\nriod, Caption assumes that its previous decision, i.e., increasing\n(or decreasing) the percentage of pages allocated to CXL memory,\nwas correct. Then it continues to incrementally increase (or de-\ncrease) the percentage by a \uffffxed amount (line: 5). Otherwise, it\nwill begin to reverse the step by half (line: 4), which decreases (or\nincreases) the percentage and evaluate the decision in the future\nperiod to determine a favorable percentage of pages allocated to\nCXL memory. Note that the absolute value of the step variable hasAlgorithm 1: Caption tuning algorithm. state ,step and\nratio represent memory subsystem performance, unit of\ntuning page allocation ratio, and ratio of page allocation\nbetween DDR and CXL memory.", "start_char_idx": 104743, "end_char_idx": 108728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7a70987-015e-4694-ace1-7f054cce01cb": {"__data__": {"id_": "e7a70987-015e-4694-ace1-7f054cce01cb", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9074039c-bf1f-40b7-b8ed-e11dd11a54e2", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "751d76d09acada6d8755b43e77031f46f57a281cf767455eeba45ecdc49910c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97faf31f-b25b-4ef8-bc78-7823842030a2", "node_type": "1", "metadata": {}, "hash": "d9eac57f7db078eb8cae48cc840f9590274933aeeb23dc2b30005ada83e702b8", "class_name": "RelatedNodeInfo"}}, "text": "If the memory-subsystem performance\nin the current period has increased compared to the previous pe-\nriod, Caption assumes that its previous decision, i.e., increasing\n(or decreasing) the percentage of pages allocated to CXL memory,\nwas correct. Then it continues to incrementally increase (or de-\ncrease) the percentage by a \uffffxed amount (line: 5). Otherwise, it\nwill begin to reverse the step by half (line: 4), which decreases (or\nincreases) the percentage and evaluate the decision in the future\nperiod to determine a favorable percentage of pages allocated to\nCXL memory. Note that the absolute value of the step variable hasAlgorithm 1: Caption tuning algorithm. state ,step and\nratio represent memory subsystem performance, unit of\ntuning page allocation ratio, and ratio of page allocation\nbetween DDR and CXL memory.\n1while CAD4 do\n2 2DAA _BC0C4  4BC8<0C>A ()\n3 if2DAA _BC0C4 <?A4E _BC0C4 then\n4 2DAA _BC4?  ?A4E _BC4? \u21e5(\u00000.5)//reverse\n5 2DAA _A0C8>  ?A4E _A0C8> +2DAA _BC4?\n6 2\u231842: _A0C8> _1>D=3 ()\n7 B4C_A0C8> (2DAA _A0C8> )\n8 ifnew allocations then\n9 ?A4E _BC0C4  2DAA _BC0C4\n10 ?A4E _BC4?  2DAA _BC4?\n11 ?A4E _A0C8>  2DAA _A0C8>\n12 B;44? (CD=4 _8=C4AE0; )\nthe minimum limit ( e.g., 9% in our evaluation) to prevent it from\nbeing close to zero. Lastly, inspired by conventional control theory,\nCaption implements mechanisms to e \uffffciently handle very small\nor sudden large changes in memory subsystem performance, even\nthough they are not described in Algorithm 1.\n6.2 Evaluation\nWe have developed Caption after analyzing the various perfor-\nmance characteristics and memory subsystem statistics of DLRM .\nHowever, we expect that Caption should work well for other appli-\ncations because the monitored L1 miss latency, DDR read latency,\nand IPC counters are fundamental memory subsystem performance\nmetrics that are strongly correlated with the throughput of memory-\nbandwidth-intensive applications; we believe CXL memory access\nlatency and bandwidth statistics are also useful for estimating mem-\nory subsystem performance, but we currently cannot access the\ncorresponding counters. To demonstrate this, we evaluate the ef-\n\uffffcacy of Caption by co-running (1) SPEC-Mix , various mixes of\nmemory-intensive SPECrate CPU2017 benchmarks, and (2) Redis\nandDLRM without measuring their performance characteristics and\nmemory-subsystem statistics in advance.\nFigure 12 shows normalized measured throughput ( Throughput ),\nnormalized estimated memory-subsystem performance (Eq. (1),\nModel Output ), and Pearson correlation coe \uffffcient values. For DLRM\nwe simply sweep the percentage of pages allocated to CXL memory\nover time. For SPEC-Mix , we let Caption automatically tune the per-\ncentage of pages allocated to CXL memory whenever a benchmark\ncompletes its execution. The Pearson correlation method allows us\nto quantify synchrony between time-series data [ 9]. The coe \uffffcient\nvalue can range from -1 and 1, indicating that both sets of data\ntrend the same direction when it is positive. We calculate the Pear-\nson correlation coe \uffffcient values to assess the e \uffffectiveness of the\nestimation model, as Algorithm 1 depends on precisely determining\nonly the direction of performance changes after tuning the percent-\nage of pages allocated to CXL memory. Figure 12 demonstrates that\nthe Pearson correlation coe \uffffcient values mostly remains positive\nfor both DLRM and SPEC-Mix .", "start_char_idx": 107904, "end_char_idx": 111284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97faf31f-b25b-4ef8-bc78-7823842030a2": {"__data__": {"id_": "97faf31f-b25b-4ef8-bc78-7823842030a2", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7a70987-015e-4694-ace1-7f054cce01cb", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "9765edd818e3f504eae99ce984fecf484f15d646a8e8b8658f929a9aecd0d43c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56c6d236-6baa-4a64-ab27-d608c2c1ebad", "node_type": "1", "metadata": {}, "hash": "eb18fad6f01dfc4c21b997b54235265d6c7a463998cfad23954cbff911bf93d5", "class_name": "RelatedNodeInfo"}}, "text": "For SPEC-Mix , we let Caption automatically tune the per-\ncentage of pages allocated to CXL memory whenever a benchmark\ncompletes its execution. The Pearson correlation method allows us\nto quantify synchrony between time-series data [ 9]. The coe \uffffcient\nvalue can range from -1 and 1, indicating that both sets of data\ntrend the same direction when it is positive. We calculate the Pear-\nson correlation coe \uffffcient values to assess the e \uffffectiveness of the\nestimation model, as Algorithm 1 depends on precisely determining\nonly the direction of performance changes after tuning the percent-\nage of pages allocated to CXL memory. Figure 12 demonstrates that\nthe Pearson correlation coe \uffffcient values mostly remains positive\nfor both DLRM and SPEC-Mix . This indicates that the estimation\n110.01.02.03.04.00.00.51.01.52.01815222936435057647178859299106113120127134141148155162169176183190Model Output/Pearson CorrelationNormalized ThroughputThroughputModel OutputPearson Correlation\n9%23%33%41%47%326496192Time (s)0128160(a) DLRM\n-2.0-1.00.01.02.03.04.05.00.960.981.001.021.041.0619017926835744653562471380289198010691158124713361425151416031692178118701959204821372226231524042493Model Output/Pearson CorrelationNormalized ThroughputThroughputModel OutputPearson Correlation\n29%41%38%33%512102415362560Time (s)020489%\n(b) SPEC-Mix\nFigure 12: Estimated memory-subsystem performance, mea-\nsured application throughput, and Pearson coe \uffffcient values\nover time. The numbers represent the percentage values of\npages allocated to CXL memory.\nmodel is adequate for Algorithm 1 to e \uffffectively tune both DLRM\nandSPEC-Mix . It is important to note that the estimation model is\nbased on the weight values derived by \ufffftting counter values from\nDLRM exclusively in this paper. However, it has the potential for\nfurther improvement by \ufffftting counter values from a more diverse\nrange of applications.\nFigure 13 evaluates the e \uffffcacy of Caption for 16 instances of\nindividual SPEC benchmarks, two di \ufffferent SPEC-Mix , and a mix\nofRedis andDLRM . For all the evaluation cases, Caption outper-\nforms both 100% and 50% allocations to DDR memory while allocat-\ning substantial percentages of pages to CXL memory. Speci \uffffcally,\nCaption o\uffffers 19%, 18%, 8%, and 20% higher throughput values for\nfotonik3d ,mcf,roms , and cactuBSSN , respectively, than the best\nstatic allocation policy ( i.e., 100% or 50% allocation to DDR memory),\nallocating 29%\u201341% of pages to CXL memory in a steady state. For\nthe mixes of mcfandroms ,cactuBSSN androms , and Redis and\nDLRM ,Caption provides 24%, 1%, and 4% higher throughput values\nthan the best static allocation policy, allocating 33%\u201341% of pages\nto CXL memory. Since DLRM andRedis use di \ufffferent throughput\nmetrics, we show a geometric mean value of normalized through-\nput values of DLRM andRedis as a single throughput value. These\ndemonstrate that Caption captures the immense memory pres-\nsure from co-running applications and tunes to the percentage of\npages to CXL memory that yields higher throughput than the static\nallocation policies.\nIn Figure 13, we do not compare Caption with the static al-\nlocation policy for DLRM andRedis individually. This is because\nwe have derived the estimation model after running DLRM (\u00a76.1)\nand demonstrated that allocating all the pages to DDR memory is\nbest for Redis (\u00a75.2). However, our evaluation shows that Caption\npresents 80% higher and 4% lower throughput values than allocat-\ning 100% and 50% of pages to DDR memory, respectively, for DLRM .", "start_char_idx": 110533, "end_char_idx": 114043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56c6d236-6baa-4a64-ab27-d608c2c1ebad": {"__data__": {"id_": "56c6d236-6baa-4a64-ab27-d608c2c1ebad", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97faf31f-b25b-4ef8-bc78-7823842030a2", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "78211abc55a9735eb3c721f604931aa6a89e37ad991d0a0a98684684f37fce7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0013c566-9315-4dc6-a738-5a7574f3c2fc", "node_type": "1", "metadata": {}, "hash": "0d3d0c812a831956381c13e91046660e291edc000d16377930f6b0744ef400fe", "class_name": "RelatedNodeInfo"}}, "text": "Since DLRM andRedis use di \ufffferent throughput\nmetrics, we show a geometric mean value of normalized through-\nput values of DLRM andRedis as a single throughput value. These\ndemonstrate that Caption captures the immense memory pres-\nsure from co-running applications and tunes to the percentage of\npages to CXL memory that yields higher throughput than the static\nallocation policies.\nIn Figure 13, we do not compare Caption with the static al-\nlocation policy for DLRM andRedis individually. This is because\nwe have derived the estimation model after running DLRM (\u00a76.1)\nand demonstrated that allocating all the pages to DDR memory is\nbest for Redis (\u00a75.2). However, our evaluation shows that Caption\npresents 80% higher and 4% lower throughput values than allocat-\ning 100% and 50% of pages to DDR memory, respectively, for DLRM .\n0.00.20.40.60.81.01.21.4\nfotonik3dmcfcactuBSSNromsroms+mcfroms+cactuRedis+DLRMPerformanceNormalized to 50:50DDR5:CXL-A = 100:050:50Caption41%29%33%41%33%41%41%Figure 13: Throughput of each evaluated benchmark or mix,\nnormalized to that with the default static policy allocating\n50% of pages to CXL memory. A number atop each bar is the\npercentage of pages allocated to CXL memory by Caption .\nForRedis ,Caption is able to identify that allocating more mem-\nory to low-latency DDR memory is bene \uffffcial, and thus o \uffffers 3.2%\nhigher throughput than allocating 50% of pages to DDR memory\nbut 8.6% lower throughput than allocating 100% of pages to DDR\nmemory. Albeit not perfect, Caption demonstrates its capability\nof searching near-optimal percentage values of pages allocated to\nCXL memory without any guidance from users and/or applications\nfor several workloads with notably di \ufffferent characteristics. Lastly,\none of our primary goals is to emphasize the need for a dynamic\npage allocation policy and show a potential of such a policy. Hence,\nwe leave further enhancement of Caption as future work.\n7 RELATED WORK\nWith the rapid development of memory technologies, diverse het-\nerogeneous memory devices have been introduced. These memory\ndevices are often di \ufffferent from the standard DDR-based DRAM\ndevices, and each memory device o \uffffers unique characteristics and\ntrade-o \uffffs. These include but are not limited to persistent mem-\nory, such as Intel Optane DIMM [ 84,88,90], remote/disaggregated\nmemory [ 12,21,30,46,54,60,76], and even byte-addressable\nSSD [ 1,8]. These heterogeneous memory devices in the memory\nhierarchy of datacenters have been applied to diverse domains of\napplications. For example, in a tiered memory/ \uffffle system, pages\ncan be dynamically placed, cached, and migrated across di \ufffferent\nmemory devices, based on their hotness and persistency require-\nments [ 2,5,14,22,23,31,48,49,51,61,64,68,78,79,81,82,89,97].\nBesides, database or key-value store can leverage these memory de-\nvices for faster and more scalable data organization and retrieval [ 10,\n16,17,45,47,53,92,95,98]. Solutions similar to Caption were pro-\nposed in the context of HBM [ 18] and storage system [ 87]. While\nthey have been extensively pro \uffffled and studied, CXL memory, as\na new member in the memory tier, still has unclear performance\ncharacteristics and indications, especially its interaction with CPUs.\nThis leads to new challenges and opportunities for applying CXL\nmemory to the aforementioned domains. This paper aims to bridge\nthe gap of CXL memory understanding, and thus enable the wide\nadoption of CXL memory in the community. Lastly, our Caption\nis speci \uffffcally optimized for CXL memory, making the most out of\nthe memory bandwidth available for a given system.\nSince the inception of the concept in 2019, CXL has been heav-\nily discussed and invested by researchers and practitioners. For\ninstance, Meta envisioned using CXL memory for memory tiering\n12and swapping [ 64,86]; Microsoft built a CXL memory prototype sys-\ntem for memory disaggregation exploration [ 11,59].", "start_char_idx": 113213, "end_char_idx": 117126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0013c566-9315-4dc6-a738-5a7574f3c2fc": {"__data__": {"id_": "0013c566-9315-4dc6-a738-5a7574f3c2fc", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56c6d236-6baa-4a64-ab27-d608c2c1ebad", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "83444a0e4758474f400976da348513e50d19bc5f2cd2f7aadcad46782d9af0c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35f42536-787c-4c6f-925e-995f9ee1e05a", "node_type": "1", "metadata": {}, "hash": "fff0b9fc1d94beef1b2955077f9ba2da566e9efb008e060886fa1a5df3758a03", "class_name": "RelatedNodeInfo"}}, "text": "This leads to new challenges and opportunities for applying CXL\nmemory to the aforementioned domains. This paper aims to bridge\nthe gap of CXL memory understanding, and thus enable the wide\nadoption of CXL memory in the community. Lastly, our Caption\nis speci \uffffcally optimized for CXL memory, making the most out of\nthe memory bandwidth available for a given system.\nSince the inception of the concept in 2019, CXL has been heav-\nily discussed and invested by researchers and practitioners. For\ninstance, Meta envisioned using CXL memory for memory tiering\n12and swapping [ 64,86]; Microsoft built a CXL memory prototype sys-\ntem for memory disaggregation exploration [ 11,59]. Most of them\nused NUMA servers to emulate the behavior of CXL memory. There\nare also e \ufffforts in building software-based CXL simulators [ 91,96].\nGouk et al. built a CXL memory prototype on FPGA-based RISC-\nV CPU [ 29]. There are also a body of work focusing on certain\nparticular applications [ 52,71,74]. Di\ufffferent from the prior stud-\nies, this paper presents the \uffffrst comprehensive study on true CXL\nmemory and compares it with emulated CXL memory using the\ncommercial high-performance CPU and CXL devices with both\nmicrobenchmarks and widely-used applications, which can bet-\nter help the design space exploration of both CXL-memory based\nsoftware systems and simulators.\n8 CONCLUSION\nIn this paper, we have taken a \uffffrst step to analyze the device-\nspeci \uffffc characteristics of true CXL memory and compared them\nwith NUMA-based emulations, a common practice in CXL research.\nOur analysis revealed key di \ufffferences between emulated and true\nCXL memory, with important performance implications. Our analy-\nsis also identi \uffffed opportunities to e \uffffectively use CXL memory as a\nmemory bandwidth expander for memory-bandwidth-intensive ap-\nplications, which leads to the development of a CXL-memory-aware\ndynamic page allocation policy and demonstrated its e \uffffcacy.\nACKNOWLEDGMENTS\nWe would like to thank Robert Blankenship, Miao Cai, Bhushan\nChitlur, Pekon Gupta, David Koufaty, Chidamber Kulkami, Henry\nPeng, Andy Rudo \uffff, Deshanand Singh, and Alexander Yu. This work\nwas supported in part by grants from Samsung Electronics, PRISM,\none of the seven centers in JUMP 2.0, a Semiconductor Research\nCorporation (SRC) program sponsored by DARPA, and NRF funded\nby the Korean Government MSIT (NRF-2018R1A5A1059921). Nam\nSung Kim has a \uffffnancial interest in Samsung Electronics and Neu-\nroRealityVision.\nREFERENCES\n[1]Ahmed Abulila, Vikram Sharma Mailthody, Zaid Qureshi, Jian Huang, Nam Sung\nKim, Jinjun Xiong, and Wen-mei Hwu. 2019. FlatFlash: Exploiting the Byte-\nAccessibility of SSDs within a Uni \uffffed Memory-Storage Hierarchy. In Proceedings\nof the 24th ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS\u201919) .\n[2]Neha Agarwal and Thomas F. Wenisch. 2017. Thermostat: Application-\nTransparent Page Management for Two-Tiered Main Memory. In Proceedings of\nthe 22nd ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS\u201917) .\n[3]Mohammad Alian, Yifan Yuan, Jie Zhang, Ren Wang, Myoungsoo Jung, and\nNam Sung Kim. 2020. Data Direct I/O Characterization for Future I/O System\nExploration. In Proceedings of the IEEE International Symposium on Performance\nAnalysis of Systems and Software (ISPASS\u201920) .\n[4]AMD. accessed in 2023. 4th Gen AMD EPYC \u2122Processor Architecture. https:\n//www.amd.com/en/campaigns/epyc-9004-architecture.\n[5]Thomas E. Anderson, Marco Canini, Jongyul Kim, Dejan Kosti \u0107, Youngjin Kwon,\nSimon Peter, Waleed Reda, Henry N. Schuh, and Emmett Witchel. 2020. Assise:\nPerformance and Availability via Client-local NVM in a Distributed File System.", "start_char_idx": 116449, "end_char_idx": 120194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35f42536-787c-4c6f-925e-995f9ee1e05a": {"__data__": {"id_": "35f42536-787c-4c6f-925e-995f9ee1e05a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0013c566-9315-4dc6-a738-5a7574f3c2fc", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "bc532af7f6c8f2c5b16dc61fa1e938e379029a574d71ef303948050304dd61ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f647646d-ed60-48f7-a378-2bb5cd7f479f", "node_type": "1", "metadata": {}, "hash": "07a7d0bd2e59a3164cbfb732684a17fc90dc5f8fc360736b65f605187a8c3a68", "class_name": "RelatedNodeInfo"}}, "text": "[3]Mohammad Alian, Yifan Yuan, Jie Zhang, Ren Wang, Myoungsoo Jung, and\nNam Sung Kim. 2020. Data Direct I/O Characterization for Future I/O System\nExploration. In Proceedings of the IEEE International Symposium on Performance\nAnalysis of Systems and Software (ISPASS\u201920) .\n[4]AMD. accessed in 2023. 4th Gen AMD EPYC \u2122Processor Architecture. https:\n//www.amd.com/en/campaigns/epyc-9004-architecture.\n[5]Thomas E. Anderson, Marco Canini, Jongyul Kim, Dejan Kosti \u0107, Youngjin Kwon,\nSimon Peter, Waleed Reda, Henry N. Schuh, and Emmett Witchel. 2020. Assise:\nPerformance and Availability via Client-local NVM in a Distributed File System.\nInProceedings of the 14th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI\u201920) .\n[6]Moiz Arif, Kevin Assogba, M. Mustafa Ra \uffffque, and Sudharshan Vazhkudai. 2023.\nExploiting CXL-Based Memory for Distributed Deep Learning. In Proceedings of\nthe 51st International Conference on Parallel Processing (ICPP\u201922) .\n[7]Jens Axboe. accessed in 2023. Flexible I/O Tester. https://github.com/axboe/ \uffffo.\n[8]Duck-Ho Bae, Insoon Jo, Youra Adel Choi, Joo-Young Hwang, Sangyeun Cho,\nDong-Gi Lee, and Jaeheon Jeong. 2018. 2B-SSD: The Case for Dual, Byte- andBlock-Addressable Solid-State Drives. In Proceedings of the ACM/IEEE 45th Annual\nInternational Symposium on Computer Architecture (ISCA\u201918) .\n[9]Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Noise\nReduction in Speech Processing . Springer.\n[10] Lawrence Benson, Hendrik Makait, and Tilmann Rabl. 2021. Viper: An E \uffffcient\nHybrid PMem-DRAM Key-Value Store. Proceedings of the VLDB Endowment\n(2021).\n[11] Daniel S. Berger, Daniel Ernst, Huaicheng Li, Pantea Zardoshti, Monish Shah,\nSamir Rajadnya, Scott Lee, Lisa Hsu, Ishwar Agarwal, Mark D. Hill, and Ricardo\nBianchini. 2023. Design Tradeo \uffffs in CXL-Based Memory Pools for Public Cloud\nPlatforms. IEEE Micro (2023).\n[12] Shai Bergman, Priyank Faldu, Boris Grot, Llu\u00eds Vilanova, and Mark Silberstein.\n2022. Reconsidering OS Memory Optimizations in the Presence of Disaggregated\nMemory. In Proceedings of the ACM SIGPLAN International Symposium on Memory\nManagement (ISMM\u201922) .\n[13] James Bucek, Klaus-Dieter Lange, and J\u00f3akim v. Kistowski. 2018. SPEC CPU2017:\nNext-Generation Compute Benchmark. In Companion of the ACM/SPEC Interna-\ntional Conference on Performance Engineering (ICPE\u201918) .\n[14] Irina Calciu, M. Talha Imran, Ivan Puddu, Sanidhya Kashyap, Hasan Al Maruf,\nOnur Mutlu, and Aasheesh Kolli. 2021. Rethinking Software Runtimes for Dis-\naggregated Memory. In Proceedings of the 26th ACM International Conference on\nArchitectural Support for Programming Languages and Operating Systems (ASP-\nLOS\u201921) .\n[15] Daniel W. Chang, Gyungsu Byun, Hoyoung Kim, Minwook Ahn, Soojung Ryu,\nNam S. Kim, and Michael Schulte. 2013. Reevaluating the Latency Claims of\n3D Stacked Memories. In Proceedings of the 18th Asia and South Paci \uffffc Design\nAutomation Conference (ASP-DAC\u201918) .\n[16] Youmin Chen, Youyou Lu, Kedong Fang, Qing Wang, and Jiwu Shu. 2020. uTree:\na Persistent B+-Tree with Low Tail Latency. Proceedings of the VLDB Endowment\n(2020).", "start_char_idx": 119560, "end_char_idx": 122667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f647646d-ed60-48f7-a378-2bb5cd7f479f": {"__data__": {"id_": "f647646d-ed60-48f7-a378-2bb5cd7f479f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35f42536-787c-4c6f-925e-995f9ee1e05a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "52402262e99406a5a6eaa40a26d4d68bd82b3bdc53669c9b5b825ec10c6467ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddcb1049-d906-4483-8f9f-8bfe4084dea8", "node_type": "1", "metadata": {}, "hash": "dbb200b012991284255633cf55e18d6544a3e1a5e92a39cfeed979a2fa8a3646", "class_name": "RelatedNodeInfo"}}, "text": "2021. Rethinking Software Runtimes for Dis-\naggregated Memory. In Proceedings of the 26th ACM International Conference on\nArchitectural Support for Programming Languages and Operating Systems (ASP-\nLOS\u201921) .\n[15] Daniel W. Chang, Gyungsu Byun, Hoyoung Kim, Minwook Ahn, Soojung Ryu,\nNam S. Kim, and Michael Schulte. 2013. Reevaluating the Latency Claims of\n3D Stacked Memories. In Proceedings of the 18th Asia and South Paci \uffffc Design\nAutomation Conference (ASP-DAC\u201918) .\n[16] Youmin Chen, Youyou Lu, Kedong Fang, Qing Wang, and Jiwu Shu. 2020. uTree:\na Persistent B+-Tree with Low Tail Latency. Proceedings of the VLDB Endowment\n(2020).\n[17] Youmin Chen, Youyou Lu, Fan Yang, Qing Wang, Yang Wang, and Jiwu Shu. 2020.\nFlatStore: An E \uffffcient Log-Structured Key-Value Storage Engine for Persistent\nMemory. In Proceedings of the 25th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems (ASPLOS\u201920) .\n[18] Chiachen Chou, Aamer Jaleel, and Moinuddin Qureshi. 2017. BATMAN: Tech-\nniques for Maximizing System Bandwidth of Memory Systems with Stacked-\nDRAM. In Proceedings of the International Symposium on Memory Systems (MEM-\nSYS\u201917) .\n[19] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking Cloud Serving Systems with YCSB. In Proceedings of\nthe 1st ACM Symposium on Cloud Computing (SoCC\u201910) .\n[20] CXL Consortium. accessed in 2023. Compute Express Link (CXL). https://www.\ncomputeexpresslink.org.\n[21] Aleksandar Dragojevi \u0107, Dushyanth Narayanan, Orion Hodson, and Miguel Castro.\n2014. FaRM: Fast Remote Memory. In Proceedings of the 11th USENIX Conference\non Networked Systems Design and Implementation (NSDI\u201914) .\n[22] Zhuohui Duan, Haikun Liu, Xiaofei Liao, Hai Jin, Wenbin Jiang, and Yu Zhang.\n2019. HiNUMA: NUMA-Aware Data Placement and Migration in Hybrid Memory\nSystems. In Proceedings of the IEEE 37th International Conference on Computer\nDesign (ICCD\u201919) .\n[23] Padmapriya Duraisamy, Wei Xu, Scott Hare, Ravi Rajwar, David Culler, Zhiyi\nXu, Jianing Fan, Christopher Kennelly, Bill McCloskey, Danijela Mijailovic, Brian\nMorris, Chiranjit Mukherjee, Jingliang Ren, Greg Thelen, Paul Turner, Carlos\nVillavieja, Parthasarathy Ranganathan, and Amin Vahdat. 2023. Towards an\nAdaptable Systems Architecture for Memory Tiering at Warehouse-Scale. In\nProceedings of the 28th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS\u201923) .\n[24] eBPF.io. accessed in 2023. eBPF Documentation. https://ebpf.io/what-is-ebpf/.\n[25] Samsung Eletronics. accessed in 2023. Scalable Memory Development Kit v1.3.\nhttps://github.com/OpenMPDK/SMDK.\n[26] Alireza Farshin, Amir Roozbeh, Gerald Q. Maguire Jr., and Dejan Kosti \u0107. 2019.\nMake the Most out of Last Level Cache in Intel Processors. In Proceedings of the\n14th European Conference on Computer Systems (EuroSys\u201919) .\n[27] Alireza Farshin, Amir Roozbeh, Gerald Q. Maguire Jr., and Dejan Kosti \u0107. 2020.\nReexamining Direct Cache Access to Optimize I/O Intensive Applications for\nMulti-hundred-gigabit Networks. In Proceedings of the USENIX Annual Technical\nConference (ATC\u201920) .", "start_char_idx": 122030, "end_char_idx": 125204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddcb1049-d906-4483-8f9f-8bfe4084dea8": {"__data__": {"id_": "ddcb1049-d906-4483-8f9f-8bfe4084dea8", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f647646d-ed60-48f7-a378-2bb5cd7f479f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "cabc2a652911fe1305a352821419ef2fe2dccf21943889254ccd2d2658e11d4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "184d9667-8304-41ba-badd-fd153c1ace58", "node_type": "1", "metadata": {}, "hash": "82994b05189897bfc1a9faad6f31cee947a3f89577bc1d483cc761f384c52dc2", "class_name": "RelatedNodeInfo"}}, "text": "https://ebpf.io/what-is-ebpf/.\n[25] Samsung Eletronics. accessed in 2023. Scalable Memory Development Kit v1.3.\nhttps://github.com/OpenMPDK/SMDK.\n[26] Alireza Farshin, Amir Roozbeh, Gerald Q. Maguire Jr., and Dejan Kosti \u0107. 2019.\nMake the Most out of Last Level Cache in Intel Processors. In Proceedings of the\n14th European Conference on Computer Systems (EuroSys\u201919) .\n[27] Alireza Farshin, Amir Roozbeh, Gerald Q. Maguire Jr., and Dejan Kosti \u0107. 2020.\nReexamining Direct Cache Access to Optimize I/O Intensive Applications for\nMulti-hundred-gigabit Networks. In Proceedings of the USENIX Annual Technical\nConference (ATC\u201920) .\n[28] Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha Shetty, Priyal Rathi, Nayan Katarki,\nAriana Bruno, Justin Hu, Brian Ritchken, Brendon Jackson, Kelvin Hu, Meghna\nPancholi, Yuan He, Brett Clancy, Chris Colen, Fukang Wen, Catherine Leung,\nSiyuan Wang, Leon Zaruvinsky, Mateo Espinosa, Rick Lin, Zhongling Liu, Jake\nPadilla, and Christina Delimitrou. 2019. An Open-Source Benchmark Suite for\nMicroservices and Their Hardware-Software Implications for Cloud & Edge\nSystems. In Proceedings of the 24th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems (ASPLOS\u201919) .\n[29] Donghyun Gouk, Miryeong Kwon, Hanyeoreum Bae, Sangwon Lee, and Myoung-\nsoo Jung. 2023. Memory Pooling with CXL. IEEE Micro (2023).\n13[30] Juncheng Gu, Youngmoon Lee, Yiwen Zhang, Mosharaf Chowdhury, and Kang G.\nShin. 2017. E \uffffcient Memory Disaggregation with INFINISWAP. In Proceedings\nof the 14th USENIX Conference on Networked Systems Design and Implementation\n(NSDI\u201917) .\n[31] Manish Gupta, Vilas Sridharan, David Roberts, Andreas Prodromou, Ashish\nVenkat, Dean Tullsen, and Rajesh Gupta. 2018. Reliability-Aware Data Place-\nment for Heterogeneous Memory Architecture. In Proceedings of the 24th IEEE\nInternational Symposium on High Performance Computer Architecture (HPCA\u201918) .\n[32] Rambus Incorporated. accessed in 2023. Memory Interface Chips \u2013 CXL Memory\nInterconnect Initiative. https://www.rambus.com/memory-and-interfaces/cxl-\nmemory-interconnect/.\n[33] Intel Corporation. accessed in 2023. 4th Gen Intel Xeon Processor Scalable Family,\nsapphire rapids. https://www.intel.com/content/www/us/en/developer/articles/\ntechnical/fourth-generation-xeon-scalable-family-overview.html.\n[34] Intel Corporation. accessed in 2023. Di \ufffference of Cache Memory Be-\ntween CPUs for Intel Xeon E5 Processors and Intel Xeon Scalable Proces-\nsors. https://www.intel.com/content/www/us/en/support/articles/000027820/\nprocessors/intel-xeon-processors.html.\n[35] Intel Corporation. accessed in 2023. Intel Launches 4th Gen Xeon Scalable\nProcessors, Max Series CPUs. https://www.intel.com/content/www/us/\nen/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-\ngpus.html#gs.o28z2f.\n[36] Intel Corporation. accessed in 2023. Intel Performance Counter Monitor. https:\n//github.com/intel/pcm.\n[37] Intel Corporation. accessed in 2023. Intel Xeon Gold 6430 Proces-\nsor. https://ark.intel.com/content/www/us/en/ark/products/231737/intel-xeon-\ngold-6430-processor-60m-cache-2-10-ghz.html.\n[38] Intel Corporation. accessed in 2023. Intel \u00aeData Direct I/O (DDIO). https:\n//www.intel.com/content/www/us/en/io/data-direct-i-o-technology.html.\n[39] Intel Corporation. accessed in 2023.", "start_char_idx": 124575, "end_char_idx": 127894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "184d9667-8304-41ba-badd-fd153c1ace58": {"__data__": {"id_": "184d9667-8304-41ba-badd-fd153c1ace58", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddcb1049-d906-4483-8f9f-8bfe4084dea8", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5fdab019381fc57e0f94dc5a6b40fde000c86254acf4b190f6285cd5c170d179", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f11dba25-2128-4c36-ace8-893438bbc078", "node_type": "1", "metadata": {}, "hash": "b6ab1aa3ad0a7df7eb29affd1409f7a838f881af51800032f9c9b3e6a788ce3c", "class_name": "RelatedNodeInfo"}}, "text": "https://www.intel.com/content/www/us/\nen/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-\ngpus.html#gs.o28z2f.\n[36] Intel Corporation. accessed in 2023. Intel Performance Counter Monitor. https:\n//github.com/intel/pcm.\n[37] Intel Corporation. accessed in 2023. Intel Xeon Gold 6430 Proces-\nsor. https://ark.intel.com/content/www/us/en/ark/products/231737/intel-xeon-\ngold-6430-processor-60m-cache-2-10-ghz.html.\n[38] Intel Corporation. accessed in 2023. Intel \u00aeData Direct I/O (DDIO). https:\n//www.intel.com/content/www/us/en/io/data-direct-i-o-technology.html.\n[39] Intel Corporation. accessed in 2023. Intel \u00ae64 and IA-32 Architectures Optimiza-\ntion Reference Manual. https://cdrdv2-public.intel.com/671488/248966-046A-\nsoftware-optimization-manual.pdf.\n[40] Intel Corporation. accessed in 2023. Intel \u00aeAgilex \u21227 FPGA I-Series Devel-\nopment Kit. https://www.intel.com/content/www/us/en/products/details/fpga/\ndevelopment-kits/agilex/i-series/dev-agi027.html.\n[41] Intel Corporation. accessed in 2023. Intel \u00aeFPGA Compute Express Link\n(CXL) IP. https://www.intel.com/content/www/us/en/products/details/fpga/\nintellectual-property/interface-protocols/cxl-ip.html.\n[42] Intel Corporation. accessed in 2023. Intel \u00aeMemory Latency Checker\nv3.10. https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-\nmemory-latency-checker.html.\n[43] Intel Corporation. accessed in 2023. Performance Analysis Guide\nfor Intel \u00aeCore \u2122i7 Processor and Intel \u00aeXeon \u21225500 processors.\nhttps://www.intel.com/content/dam/develop/external/us/en/documents/\nperformance-analysis-guide-181827.pdf.\n[44] JEDEC \u2013 Global Standards for the Microelectronics Industry. accessed in 2023.\nMain Memory: DDR4 & DDR5 SDRAM. https://www.jedec.org/category/\ntechnology-focus-area/main-memory-ddr3-ddr4-sdram.\n[45] Olzhas Kaiyrakhmet, Songyi Lee, Beomseok Nam, Sam H. Noh, and Young-ri\nChoi. 2019. SLM-DB: Single-Level Key-Value Store with Persistent Memory.\nInProceedings of the 17th USENIX Conference on File and Storage Technologies\n(FAST\u201919) .\n[46] Anuj Kalia, David Andersen, and Michael Kaminsky. 2020. Challenges and\nSolutions for Fast Remote Persistent Memory Access. In Proceedings of the 11th\nACM Symposium on Cloud Computing (SoCC\u201920) .\n[47] Sudarsun Kannan, Nitish Bhat, Ada Gavrilovska, Andrea C. Arpaci-Dusseau, and\nRemzi H. Arpaci-Dusseau. 2018. Redesigning LSMs for Nonvolatile Memory with\nNoveLSM. In Proceedings of the USENIX Annual Technical Conference (ATC\u201918) .\n[48] Sudarsun Kannan, Ada Gavrilovska, Vishal Gupta, and Karsten Schwan. 2017.\nHeteroOS: OS Design for Heterogeneous Memory Management in Datacenter. In\nProceedings of the ACM/IEEE 44th Annual International Symposium on Computer\nArchitecture (ISCA\u201917) .\n[49] Sudarsun Kannan, Yujie Ren, and Abhishek Bhattacharjee. 2021. KLOCs: Kernel-\nLevel Object Contexts for Heterogeneous Memory Systems. In Proceedings of\nthe 26th ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS\u201921) .", "start_char_idx": 127276, "end_char_idx": 130281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f11dba25-2128-4c36-ace8-893438bbc078": {"__data__": {"id_": "f11dba25-2128-4c36-ace8-893438bbc078", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "184d9667-8304-41ba-badd-fd153c1ace58", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e42d59952a1361920eacdebfdcc692c8840648eb6787078093f582a385cba6f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06a72cc0-adc8-412e-8104-032a5b7da43b", "node_type": "1", "metadata": {}, "hash": "90587229bfd0ad8462114bf2fde7b89a51aede7cc04b5661c4c0a64196c8be90", "class_name": "RelatedNodeInfo"}}, "text": "2018. Redesigning LSMs for Nonvolatile Memory with\nNoveLSM. In Proceedings of the USENIX Annual Technical Conference (ATC\u201918) .\n[48] Sudarsun Kannan, Ada Gavrilovska, Vishal Gupta, and Karsten Schwan. 2017.\nHeteroOS: OS Design for Heterogeneous Memory Management in Datacenter. In\nProceedings of the ACM/IEEE 44th Annual International Symposium on Computer\nArchitecture (ISCA\u201917) .\n[49] Sudarsun Kannan, Yujie Ren, and Abhishek Bhattacharjee. 2021. KLOCs: Kernel-\nLevel Object Contexts for Heterogeneous Memory Systems. In Proceedings of\nthe 26th ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS\u201921) .\n[50] Liu Ke, Udit Gupta, Benjamin Youngjae Cho, David Brooks, Vikas Chandra, Utku\nDiril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Meng\nLi, Bert Maher, Dheevatsa Mudigere, Maxim Naumov, Martin Schatz, Mikhail\nSmelyanskiy, Xiaodong Wang, Brandon Reagen, Carole-Jean Wu, Mark Hemp-\nstead, and Xuan Zhang. 2020. RecNMP: Accelerating Personalized Recommenda-\ntion with near-Memory Processing. In Proceedings of the ACM/IEEE 47th Annual\nInternational Symposium on Computer Architecture (ISCA\u201920) .\n[51] Jonghyeon Kim, Wonkyo Choe, and Jeongseob Ahn. 2021. Exploring the Design\nSpace of Page Management for Multi-Tiered Memory Systems. In Proceedings of\nthe USENIX Annual Technical Conference (ATC\u201921) .\n[52] Kyungsan Kim, Hyunseok Kim, Jinin So, Wonjae Lee, Junhyuk Im, Sungjoo Park,\nJeonghyeon Cho, and Hoyoung Song. 2023. SMT: Software-De \uffffned MemoryTiering for Heterogeneous Computing Systems With CXL Memory Expander.\nIEEE Micro (2023).\n[53] Wonbae Kim, Chanyeol Park, Dongui Kim, Hyeongjun Park, Young ri Choi, Alan\nSussman, and Beomseok Nam. 2022. ListDB: Union of Write-Ahead Logs and\nPersistent SkipLists for Incremental Checkpointing on Persistent Memory. In\nProceedings of the 16th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI\u201922) .\n[54] Vamsee Reddy Kommareddy, Simon David Hammond, Clayton Hughes, Ahmad\nSamih, and Amro Awad. 2019. Page Migration Support for Disaggregated Non-\nVolatile Memories. In Proceedings of the International Symposium on Memory\nSystems (MEMSYS\u201919) .\n[55] Donghyuk Lee, Mike O\u2019Connor, and Niladrish Chatterjee. 2018. Reducing Data\nTransfer Energy by Exploiting Similarity within a Data Transaction. In Proceed-\nings of the 24th IEEE International Symposium on High Performance Computer\nArchitecture (HPCA\u201918) .\n[56] Sukhan Lee, Hyunyoon Cho, Young Hoon Son, Yuhwan Ro, Nam Sung Kim, and\nJung Ho Ahn. 2018. Leveraging Power-Performance Relationship of Energy-\nE\uffffcient Modern DRAM Devices. IEEE Access (2018).\n[57] Sukhan Lee, Kiwon Lee, Minchul Sung, Mohammad Alian, Chankyung Kim,\nWooyeong Cho, Reum Oh, Seongil O, Jung Ho Ahn, and Nam Sung Kim. 2018.\n3D-Xpath: High-Density Managed DRAM Architecture with Cost-E \uffffective Al-\nternative Paths for Memory Transactions. In Proceedings of the 27th ACM Interna-\ntional Conference on Parallel Architectures and Compilation Techniques (PACT\u201918) .\n[58] Yejin Lee, Seong Hoon Seo, Hyunji Choi, Hyoung Uk Sul, Soosung Kim, Jae W.\nLee, and Tae Jun Ham. 2021. MERCI: E \uffffcient Embedding Reduction on Com-\nmodity Hardware via Sub-Query Memoization. In Proceedings of the 26th ACM\nInternational Conference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS\u201921) .", "start_char_idx": 129619, "end_char_idx": 132986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06a72cc0-adc8-412e-8104-032a5b7da43b": {"__data__": {"id_": "06a72cc0-adc8-412e-8104-032a5b7da43b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f11dba25-2128-4c36-ace8-893438bbc078", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "768d61c93ecee512918f27589da2f1bee5617987717dca712a1b259891ac4237", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75c0f980-7f75-4323-9278-0a5f07aaa926", "node_type": "1", "metadata": {}, "hash": "876b5cf227a9086252e1c4348bd6ae660de5fc745d8a73fbf076cc44581af6d4", "class_name": "RelatedNodeInfo"}}, "text": "2018.\n3D-Xpath: High-Density Managed DRAM Architecture with Cost-E \uffffective Al-\nternative Paths for Memory Transactions. In Proceedings of the 27th ACM Interna-\ntional Conference on Parallel Architectures and Compilation Techniques (PACT\u201918) .\n[58] Yejin Lee, Seong Hoon Seo, Hyunji Choi, Hyoung Uk Sul, Soosung Kim, Jae W.\nLee, and Tae Jun Ham. 2021. MERCI: E \uffffcient Embedding Reduction on Com-\nmodity Hardware via Sub-Query Memoization. In Proceedings of the 26th ACM\nInternational Conference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS\u201921) .\n[59] Huaicheng Li, Daniel S. Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko\nNovakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, Mark D.\nHill, Marcus Fontoura, and Ricardo Bianchini. 2023. Pond: CXL-Based Memory\nPooling Systems for Cloud Platforms. In Proceedings of the 28th ACM International\nConference on Architectural Support for Programming Languages and Operating\nSystems (ASPLOS\u201923) .\n[60] Kevin Lim, Jichuan Chang, Trevor Mudge, Parthasarathy Ranganathan, Steven K.\nReinhardt, and Thomas F. Wenisch. 2009. Disaggregated Memory for Expan-\nsion and Sharing in Blade Servers. In Proceedings of the ACM/IEEE 36th Annual\nInternational Symposium on Computer Architecture (ISCA\u201909) .\n[61] Lei Liu, Shengjie Yang, Lu Peng, and Xinyu Li. 2019. Hierarchical Hybrid Memory\nManagement in OS for Tiered Memory Systems. IEEE Transactions on Parallel\nand Distributed Systems (2019).\n[62] Kevin Loughlin, Stefan Saroiu, Alec Wolman, Yatin A Manerkar, and Baris Kasikci.\n2022. MOESI-prime: Preventing Coherence-Induced Hammering in Commodity\nWorkloads. In Proceedings of the ACM/IEEE 49th Annual International Symposium\non Computer Architecture (ISCA\u201922) .\n[63] Hasan Al Maruf. accessed in 2023. Transparent Page Placement for Tiered-\nMemory. https://lore.kernel.org/all/cover.1637778851.git.hasanalmaruf@fb.\ncom/.\n[64] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agarwal,\nPallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanaujia,\nand Prakash Chauhan. 2023. TPP: Transparent Page Placement for CXL-Enabled\nTiered-Memory. In Proceedings of the 28th ACM International Conference on Archi-\ntectural Support for Programming Languages and Operating Systems (ASPLOS\u201923) .\n[65] Micron. accessed in 2023. CZ120 memory expansion module. https://www.\nmicron.com/solutions/server/cxl.\n[66] Daniel Molka, Daniel Hackenberg, Robert Sch\u00f6ne, and Wolfgang E Nagel. 2015.\nCache Coherence Protocol and Memory Performance of the Intel Haswell-EP Ar-\nchitecture. In Proceedings of the 44th International Conference on Parallel Processing\n(ICPP\u201915) .\n[67] Montage Technology. accessed in 2023. CXL Memory eXpander Controller\n(MXC). https://www.montage-tech.com/MXC.\n[68] Amanda Raybuck, Tim Stamler, Wei Zhang, Mattan Erez, and Simon Peter. 2021.\nHeMem: Scalable Tiered Memory Management for Big Data Applications and\nReal NVM. In Proceedings of the ACM SIGOPS 28th Symposium on Operating\nSystems Principles (SOSP\u201921) .\n[69] Redis Ltd. accessed in 2023. Redis. https://redis.io/.\n[70] Robert Blankenship. 2020. Compute Express Link (CXL): Memory and Cache\nProtocols. https://snia.org/sites/default/ \uffffles/SDC/2020/130-Blankenship-CXL-\n1.1-Protocol-Extensions.pdf.", "start_char_idx": 132406, "end_char_idx": 135684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75c0f980-7f75-4323-9278-0a5f07aaa926": {"__data__": {"id_": "75c0f980-7f75-4323-9278-0a5f07aaa926", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06a72cc0-adc8-412e-8104-032a5b7da43b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "caa005c56a56bd095d4cd3d67de68cbe428ec458b1b745d0fb87da9c33c13a88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bf5bdac-2348-40e0-9b47-c2b450dd21c3", "node_type": "1", "metadata": {}, "hash": "baabd2802c5beec45112600fafb8ebc9aa9270c02de1c35a8b7a5c30d4cbc486", "class_name": "RelatedNodeInfo"}}, "text": "[67] Montage Technology. accessed in 2023. CXL Memory eXpander Controller\n(MXC). https://www.montage-tech.com/MXC.\n[68] Amanda Raybuck, Tim Stamler, Wei Zhang, Mattan Erez, and Simon Peter. 2021.\nHeMem: Scalable Tiered Memory Management for Big Data Applications and\nReal NVM. In Proceedings of the ACM SIGOPS 28th Symposium on Operating\nSystems Principles (SOSP\u201921) .\n[69] Redis Ltd. accessed in 2023. Redis. https://redis.io/.\n[70] Robert Blankenship. 2020. Compute Express Link (CXL): Memory and Cache\nProtocols. https://snia.org/sites/default/ \uffffles/SDC/2020/130-Blankenship-CXL-\n1.1-Protocol-Extensions.pdf.\n[71] Seokhyun Ryu, Sohyun Kim, Jaeyung Jun, Donguk Moon, Kyungsoo Lee, Jungmin\nChoi, Sunwoong Kim, Hyungsoo Kim, Luke Kim, Won Ha Choi, Moohyeon Nam,\nDooyoung Hwang, Hongchan Roh, and Youngpyo Joo. 2023. System Optimization\nof Data Analytics Platforms using Compute Express Link (CXL) Memory. In\nProceedings of the IEEE International Conference on Big Data and Smart Computing\n(BigComp\u201923) .\n[72] Samsung Semiconductor. accessed in 2023. Memory-Semantic SSD. https://\nsamsungmsl.com/ms-ssd/.\n14[73] Debendra Das Sharma. 2022. Compute Express Link (CXL): Enabling Heteroge-\nneous Data-Centric Computing With Heterogeneous Memory Hierarchy. IEEE\nMicro (2022).\n[74] Joonseop Sim, Soohong Ahn, Taeyoung Ahn, Seungyong Lee, Myunghyun Rhee,\nJooyoung Kim, Kwangsik Shin, Donguk Moon, Euiseok Kim, and Kyoung Park.\n2023. Computational CXL-Memory Solution for Accelerating Memory-Intensive\nApplications. IEEE Computer Architecture Letters (2023).\n[75] Tom Simon. 2021. Low Power High Performance PCIe SerDes IP for Sam-\nsung Silicon - SemiWiki. https://semiwiki.com/events/305345-low-power-high-\nperformance-pcie-serdes-ip-for-samsung-silicon/.\n[76] Arjun Singhvi, Aditya Akella, Dan Gibson, Thomas F. Wenisch, Monica Wong-\nChan, Sean Clark, Milo M. K. Martin, Moray McLaren, Prashant Chandra, Rob\nCauble, Hassan M. G. Wassel, Behnam Montazeri, Simon L. Sabato, Joel Scherpelz,\nand Amin Vahdat. 2020. 1RMA: Re-Envisioning Remote Memory Access for\nMulti-Tenant Datacenters. In Proceedings of the Annual Conference of the ACM\nSpecial Interest Group on Data Communication on the Applications, Technologies,\nArchitectures, and Protocols for Computer Communication (SIGCOMM\u201920) .\n[77] SK hynix Inc. 2022. SK hynix Introduces Industry\u2019s First CXL-based\nComputational Memory Solution (CMS) at the OCP Global Summit.\nhttps://news.skhynix.com/sk-hynix-introduces-industrys- \uffffrst-cxl-based-\ncms-at-the-ocp-global-summit/.\n[78] Jingbo Su, Jiahao Li, Luofan Chen, Cheng Li, Kai Zhang, Liang Yang, and Yinlong\nXu. 2023. Revitalizing the Forgotten On-Chip DMA to Expedite Data Movement\nin NVM-based Storage Systems. In Proceedings of the 21st USENIX Conference on\nFile and Storage Technologies (FAST\u201923) .\n[79] Kshitij Sudan, Karthick Rajamani, Wei Huang, and John B. Carter. 2012. Tiered\nMemory: An Iso-Power Memory Architecture to Address the Memory Power\nWall. IEEE Trans. Comput. (2012).\n[80] Amin Tootoonchian, Aurojit Panda, Chang Lan, Melvin Walls, Katerina Argyraki,\nSylvia Ratnasamy, and Scott Shenker. 2018. ResQ: Enabling SLOs in Network\nFunction Virtualization. In Proceedings of 15th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI\u201918) .", "start_char_idx": 135073, "end_char_idx": 138334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bf5bdac-2348-40e0-9b47-c2b450dd21c3": {"__data__": {"id_": "7bf5bdac-2348-40e0-9b47-c2b450dd21c3", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75c0f980-7f75-4323-9278-0a5f07aaa926", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5ff1cfb257e1cbe1e72b12ae7fe459a7187c3ad53708a80b724d99c2a4e4dc14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ccfe094-f374-4f17-aa3d-e3bb886cd1a2", "node_type": "1", "metadata": {}, "hash": "2d751f0188c22cec88b37d8a4cf8f5ea6f84f44d0f103b4975df25796d6c8a32", "class_name": "RelatedNodeInfo"}}, "text": "2023. Revitalizing the Forgotten On-Chip DMA to Expedite Data Movement\nin NVM-based Storage Systems. In Proceedings of the 21st USENIX Conference on\nFile and Storage Technologies (FAST\u201923) .\n[79] Kshitij Sudan, Karthick Rajamani, Wei Huang, and John B. Carter. 2012. Tiered\nMemory: An Iso-Power Memory Architecture to Address the Memory Power\nWall. IEEE Trans. Comput. (2012).\n[80] Amin Tootoonchian, Aurojit Panda, Chang Lan, Melvin Walls, Katerina Argyraki,\nSylvia Ratnasamy, and Scott Shenker. 2018. ResQ: Enabling SLOs in Network\nFunction Virtualization. In Proceedings of 15th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI\u201918) .\n[81] Majed Valad Beigi, Bahareh Pourshirazi, Gokhan Memik, and Zhichun Zhu. 2020.\nDeepSwapper: A Deep Learning Based Page Swap Management Scheme for\nHybrid Memory Systems. In Proceedings of the 29th ACM International Conference\non Parallel Architectures and Compilation Techniques (PACT\u201920) .\n[82] Evangelos Vasilakis, Vassilis Papaefstathiou, Pedro Trancoso, and Ioannis Sourdis.\n2020. Hybrid2: Combining Caching and Migration in Hybrid Memory Systems.\nInProceedings of the 26th IEEE International Symposium on High Performance\nComputer Architecture (HPCA\u201920) .\n[83] Markus Velten, Robert Sch\u00f6ne, Thomas Ilsche, and Daniel Hackenberg. 2022.\nMemory Performance of AMD EPYC Rome and Intel Cascade Lake SP Server\nProcessors. In Proceedings of the ACM/SPEC on International Conference on Per-\nformance Engineering (ICPE\u201922) .\n[84] Zixuan Wang, Xiao Liu, Jian Yang, Theodore Michailidis, Steven Swanson, and\nJishen Zhao. 2020. Characterizing and Modeling Non-Volatile Memory Sys-\ntems. In Proceedings of the 53rd Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO\u201920) .\n[85] Johannes Weiner. accessed in 2023. [PATCH] mm: mempolicy: N:M inter-\nleave policy for tiered memory nodes. https://lore.kernel.org/linux-mm/YqD0%\n2FtzFwXvJ1gK6@cmpxchg.org/T/.\n[86] Johannes Weiner, Niket Agarwal, Dan Schatzberg, Leon Yang, Hao Wang, Blaise\nSanouillet, Bikash Sharma, Tejun Heo, Mayank Jain, Chunqiang Tang, and Dim-\nitrios Skarlatos. 2022. TMO: Transparent Memory O \uffffoading in Datacenters. In\nProceedings of the 27th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS\u201922) .\n[87] Kan Wu, Zhihan Guo, Guanzhou Hu, Kaiwei Tu, Ramnatthan Alagappan, Rathijit\nSen, Kwanghyun Park, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau.\n2021. The Storage Hierarchy is Not a Hierarchy: Optimizing Caching on Modern\nStorage Devices with Orthus. In Proceedings of the 19th USENIX Conference on\nFile and Storage Technologies (FAST\u201921) .\n[88] Lingfeng Xiang, Xingsheng Zhao, Jia Rao, Song Jiang, and Hong Jiang. 2022.\nCharacterizing the Performance of Intel Optane Persistent Memory: A Close\nLook at Its on-DIMM Bu \uffffering. In Proceedings of the 17th European Conference\non Computer Systems (EuroSys\u201922) .\n[89] Zi Yan, Daniel Lustig, David Nellans, and Abhishek Bhattacharjee. 2019. Nimble\nPage Management for Tiered Memory Systems. In Proceedings of the 24th ACM\nInternational Conference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS\u201919) .\n[90] Jian Yang, Juno Kim, Morteza Hoseinzadeh, Joseph Izraelevitz, and Steven Swan-\nson. 2020. An Empirical Guide to the Behavior and Use of Scalable Persistent\nMemory. In Proceedings of the 18th USENIX Conference on File and Storage Tech-\nnologies (FAST\u201920) .", "start_char_idx": 137677, "end_char_idx": 141122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ccfe094-f374-4f17-aa3d-e3bb886cd1a2": {"__data__": {"id_": "6ccfe094-f374-4f17-aa3d-e3bb886cd1a2", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bf5bdac-2348-40e0-9b47-c2b450dd21c3", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "00657adc4f122f9193087f2482938457e684e8210cc8b5e4a81868fe7fa825fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d275945-bc9f-4c98-843f-b54d2c2d8027", "node_type": "1", "metadata": {}, "hash": "27a8a7432fb65f6d58319ba4dc0384d2dadd4b90382348954e70c2573f7572df", "class_name": "RelatedNodeInfo"}}, "text": "2022.\nCharacterizing the Performance of Intel Optane Persistent Memory: A Close\nLook at Its on-DIMM Bu \uffffering. In Proceedings of the 17th European Conference\non Computer Systems (EuroSys\u201922) .\n[89] Zi Yan, Daniel Lustig, David Nellans, and Abhishek Bhattacharjee. 2019. Nimble\nPage Management for Tiered Memory Systems. In Proceedings of the 24th ACM\nInternational Conference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS\u201919) .\n[90] Jian Yang, Juno Kim, Morteza Hoseinzadeh, Joseph Izraelevitz, and Steven Swan-\nson. 2020. An Empirical Guide to the Behavior and Use of Scalable Persistent\nMemory. In Proceedings of the 18th USENIX Conference on File and Storage Tech-\nnologies (FAST\u201920) .\n[91] Yiwei Yang, Pooneh Safayenikoo, Jiacheng Ma, Tanvir Ahmed Khan, and Andrew\nQuinn. 2023. CXLMemSim: A pure software simulated CXL.mem for performance\ncharacterization. arXiv preprint arXiv:2303.06153 (2023).\n[92] Ting Yao, Yiwen Zhang, Jiguang Wan, Qiu Cui, Liu Tang, Hong Jiang, Chang-\nsheng Xie, and Xubin He. 2020. MatrixKV: Reducing Write Stalls and WriteAmpli \uffffcation in LSM-tree Based KV Stores with Matrix Container in NVM. In\nProceeings of the USENIX Annual Technical Conference (ATC\u201920) .\n[93] Yifan Yuan, Mohammad Alian, Yipeng Wang, Ren Wang, Ilia Kurakin, Charlie\nTai, and Nam Sung Kim. 2021. Don\u2019t Forget the I/O When Allocating Your\nLLC. In Proceedings of the IEEE/ACM 48th International Symposium on Computer\nArchitecture (ISCA\u201921) .\n[94] Chaoliang Zeng, Layong Luo, Qingsong Ning, Yaodong Han, Yuhang Jiang, Ding\nTang, Zilong Wang, Kai Chen, and Chuanxiong Guo. 2022. FAERY: An FPGA-\naccelerated Embedding-based Retrieval System. In Proceedings of the 16th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI\u201922) .\n[95] Wenhui Zhang, Xingsheng Zhao, Song Jiang, and Hong Jiang. 2021.\nChameleonDB: A Key-Value Store for Optane Persistent Memory. In Proceedings\nof the 26th European Conference on Computer Systems (EuroSys\u201921) .\n[96] Xu Zhang. accessed in 2023. gem5-CXL. https://github.com/zxhero/gem5-CXL.\n[97] Shengan Zheng, Morteza Hoseinzadeh, and Steven Swanson. 2019. Ziggurat: A\nTiered File System for Non-Volatile Main Memories and Disks. In Proceedings of\nthe 17th USENIX Conference on File and Storage Technologies (FAST\u201919) .\n[98] Xinjing Zhou, Lidan Shou, Ke Chen, Wei Hu, and Gang Chen. 2019. DPTree:\nDi\ufffferential Indexing for Persistent Memory. Proceedings of VLDB Endowment\n(2019).\n15TPP: Transparent Page Placement for CXL-Enabled\nTiered-Memory\nHasan Al Maruf\nUniversity of Michigan\nUSAHao Wang\nNVIDIA\nUSAAbhishek Dhanotia\nMeta Inc.\nUSA\nJohannes Weiner\nMeta Inc.\nUSANiket Agarwal\nNVIDIA\nUSAPallab Bhattacharya\nNVIDIA\nUSA\nChris Petersen\nMeta Inc.\nUSAMosharaf Chowdhury\nUniversity of Michigan\nUSAShobhit Kanaujia\nMeta Inc.\nUSA\nPrakash Chauhan\nMeta Inc.\nUSA\nABSTRACT\nThe increasing demand for memory in hyperscale applications has\nled to memory becoming a large portion of the overall datacen-\nter spend. The emergence of coherent interfaces like CXL enables\nmain memory expansion and o \uffffers an e \uffffcient solution to this prob-\nlem. In such systems, the main memory can constitute di \ufffferent\nmemory technologies with varied characteristics. In this paper, we\ncharacterize memory usage patterns of a wide range of datacenter\napplications across the server \uffffeet of Meta.", "start_char_idx": 140399, "end_char_idx": 143738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d275945-bc9f-4c98-843f-b54d2c2d8027": {"__data__": {"id_": "4d275945-bc9f-4c98-843f-b54d2c2d8027", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ccfe094-f374-4f17-aa3d-e3bb886cd1a2", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "455a575c2e086742360fa157df1391050d85a2207da13cfcfbcf5cfef44cd369", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba3feacb-988e-4add-9fc0-12dbb00113e3", "node_type": "1", "metadata": {}, "hash": "a30f02e971ba2e95f5d1075bc19b11faca0bca22ce893cbcafd84adf6dd8282e", "class_name": "RelatedNodeInfo"}}, "text": "USA\nJohannes Weiner\nMeta Inc.\nUSANiket Agarwal\nNVIDIA\nUSAPallab Bhattacharya\nNVIDIA\nUSA\nChris Petersen\nMeta Inc.\nUSAMosharaf Chowdhury\nUniversity of Michigan\nUSAShobhit Kanaujia\nMeta Inc.\nUSA\nPrakash Chauhan\nMeta Inc.\nUSA\nABSTRACT\nThe increasing demand for memory in hyperscale applications has\nled to memory becoming a large portion of the overall datacen-\nter spend. The emergence of coherent interfaces like CXL enables\nmain memory expansion and o \uffffers an e \uffffcient solution to this prob-\nlem. In such systems, the main memory can constitute di \ufffferent\nmemory technologies with varied characteristics. In this paper, we\ncharacterize memory usage patterns of a wide range of datacenter\napplications across the server \uffffeet of Meta. We, therefore, demon-\nstrate the opportunities to o \uffffoad colder pages to slower memory\ntiers for these applications. Without e \uffffcient memory management,\nhowever, such systems can signi \uffffcantly degrade performance.\nWe propose a novel OS-level application-transparent page place-\nment mechanism (TPP) for CXL-enabled memory. TPP employs a\nlightweight mechanism to identify and place hot/cold pages to ap-\npropriate memory tiers. It enables a proactive page demotion from\nlocal memory to CXL-Memory. This technique ensures a memory\nheadroom for new page allocations that are often related to request\nprocessing and tend to be short-lived and hot. At the same time, TPP\ncan promptly promote performance-critical hot pages trapped in\nthe slow CXL-Memory to the fast local memory, while minimizing\nboth sampling overhead and unnecessary migrations. TPP works\ntransparently without any application-speci \uffffc knowledge and can\nbe deployed globally as a kernel release.\nWe evaluate TPP with diverse memory-sensitive workloads in\nthe production server \uffffeet with early samples of new x86 CPUs with\nCXL 1.1 support. TPP makes a tiered memory system performant as\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro \ufffft or commercial advantage and that copies bear this notice and the full citation\non the \uffffrst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speci \uffffc permission\nand/or a fee. Request permissions from permissions@acm.org.\nASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9918-0/23/03. . . $15.00\nhttps://doi.org/10.1145/3582016.3582063an ideal baseline (<1% gap) that has all the memory in the local tier.\nIt is 18% better than today\u2019s Linux, and 5\u201317% better than existing\nsolutions including NUMA Balancing and AutoTiering. Most of the\nTPP patches have been merged in the Linux v5.18 release while the\nremaining ones are just pending for more discussion.\nCCS CONCEPTS\n\u2022Software and its engineering !Operating systems ;Memory\nmanagement ;\u2022Hardware !Emerging architectures ;Mem-\nory and dense storage .\nKEYWORDS\nDatacenters, Operating Systems, Memory Management, Tiered-\nMemory, CXL-Memory, Heterogeneous System\nACM Reference Format:\nHasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket\nAgarwal, Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shob-\nhit Kanaujia, and Prakash Chauhan. 2023. TPP: Transparent Page Placement\nfor CXL-Enabled Tiered-Memory. In Proceedings of the 28th ACM Interna-\ntional Conference on Architectural Support for Programming Languages and\nOperating Systems, Volume 3 (ASPLOS \u201923), March 25\u201329, 2023, Vancouver,\nBC, Canada. ACM, New York, NY, USA, 14pages.", "start_char_idx": 143008, "end_char_idx": 146764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba3feacb-988e-4add-9fc0-12dbb00113e3": {"__data__": {"id_": "ba3feacb-988e-4add-9fc0-12dbb00113e3", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d275945-bc9f-4c98-843f-b54d2c2d8027", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ae9ce6f1ce6e42300dad38f84d4fcd8026c1f8478d2e8f5864e6732bada598ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd66b23d-bb5b-4842-9e96-a7402bee9c83", "node_type": "1", "metadata": {}, "hash": "2b300bcfa88a34fe6f2431fa592a97beaae933d0c75abf289537260a0ac33a37", "class_name": "RelatedNodeInfo"}}, "text": "KEYWORDS\nDatacenters, Operating Systems, Memory Management, Tiered-\nMemory, CXL-Memory, Heterogeneous System\nACM Reference Format:\nHasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket\nAgarwal, Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shob-\nhit Kanaujia, and Prakash Chauhan. 2023. TPP: Transparent Page Placement\nfor CXL-Enabled Tiered-Memory. In Proceedings of the 28th ACM Interna-\ntional Conference on Architectural Support for Programming Languages and\nOperating Systems, Volume 3 (ASPLOS \u201923), March 25\u201329, 2023, Vancouver,\nBC, Canada. ACM, New York, NY, USA, 14pages. https://doi.org/10.1145/\n3582016.3582063\n1 INTRODUCTION\nThe surge in memory needs for datacenter applications [ 12,61],\ncombined with the increasing DRAM cost and technology scaling\nchallenges [ 49,54] has led to memory becoming a signi \uffffcant infras-\ntructure expense in hyperscale datacenters. Non-DRAM memory\ntechnologies provide an opportunity to alleviate this problem by\nbuilding tiered memory subsystems and adding higher memory\ncapacity at a cheaper $/GB point [ 5,19,38,39,46]. These technolo-\ngies, however, have much higher latency vs. main memory and\ncan signi \uffffcantly degrade performance when data is ine \uffffciently\nplaced in di \ufffferent levels of the memory hierarchy. Additionally,\nprior knowledge of application behavior and careful application\n742\nASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.CPUDRAMDRAMDRAMDRAMDRAMDRAM(a) Without CXLCPUDRAMDRAMNVMDDRLPDDRCXL\u2026.Memory Technology(b) With CXL\nFigure 1: CXL decouples memory from compute.\ntuning is required to e \uffffectively use these technologies. This can be\nprohibitively resource-intensive in hyperscale environments with\nvarieties of rapidly evolving applications.\nCompute Express Link (CXL) [ 7] mitigates this problem by pro-\nviding an intermediate latency operating point with DRAM-like\nbandwidth and cache-line granular access semantics. CXL protocol\nallows a new memory bus interface to attach memory to the CPU\n(Figure 1). From a software perspective, CXL-Memory appears to\na system as a CPU-less NUMA node where its memory character-\nistics (e.g., bandwidth, capacity, generation, technology, etc.) are\nindependent of the memory directly attached to the CPU. This\nallows \uffffexibility in memory subsystem design and \uffffne-grained\ncontrol over the memory bandwidth and capacity [ 9,10,24]. Addi-\ntionally, as CXL-Memory appears like the main memory, it provides\nopportunities for transparent page placement on the appropriate\nmemory tier. However, Linux\u2019s memory management mechanism\nis designed for homogeneous CPU-attached DRAM-only systems\nand performs poorly on CXL-Memory system. In such a system,\nas memory access latency varies across memory tiers (Figure 2),\napplication performance greatly depends on the fraction of memory\nserved from the fast memory.\nTo understand whether memory tiering can be bene \uffffcial, we\nneed to understand the variety of memory access behavior in\nexisting datacenter applications. For each application, we want\nto know how much of its memory remains hot, warm, and cold\nwithin a certain period and what fraction of its memory is short- vs.\nlong-lived. Existing Idle Page Tracking (IPT) based characterization\ntools [ 11,17,69] do not \ufffft the bill as they require kernel modi \uffffca-\ntions that is often not possible in productions. Besides, continuous\naccess bit sampling and pro \uffffling require excessive CPU and mem-\nory overhead. This may not scale with large working sets. More-\nover, applications often have di \ufffferent sensitivity towards di \ufffferent\ntypes of memory pages (e.g., anon page, \uffffle page cache, shared\nmemory, etc.) which existing tools do not account. To this end, we\nbuild Chameleon, a robust and lightweight user-space tool, that\nuses existing CPU\u2019s Precise Event-Based Sampling (PEBS) mecha-\nnism to characterize an application\u2019s memory access behavior (\u00a7 3).", "start_char_idx": 146160, "end_char_idx": 150063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd66b23d-bb5b-4842-9e96-a7402bee9c83": {"__data__": {"id_": "cd66b23d-bb5b-4842-9e96-a7402bee9c83", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba3feacb-988e-4add-9fc0-12dbb00113e3", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d71075d40d6510a5e47d48878549c2134ec3ce9117d2df857e8a770074084338", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a70687a4-4339-4530-8c1d-6692d20abeb4", "node_type": "1", "metadata": {}, "hash": "37fdf9bda34accc273d51023d851fd1d63251d464488773392f71b138bbe0e57", "class_name": "RelatedNodeInfo"}}, "text": "long-lived. Existing Idle Page Tracking (IPT) based characterization\ntools [ 11,17,69] do not \ufffft the bill as they require kernel modi \uffffca-\ntions that is often not possible in productions. Besides, continuous\naccess bit sampling and pro \uffffling require excessive CPU and mem-\nory overhead. This may not scale with large working sets. More-\nover, applications often have di \ufffferent sensitivity towards di \ufffferent\ntypes of memory pages (e.g., anon page, \uffffle page cache, shared\nmemory, etc.) which existing tools do not account. To this end, we\nbuild Chameleon, a robust and lightweight user-space tool, that\nuses existing CPU\u2019s Precise Event-Based Sampling (PEBS) mecha-\nnism to characterize an application\u2019s memory access behavior (\u00a7 3).\nChameleon generates a heat-map of memory usage on di \ufffferent\ntypes of pages and provides insights into an application\u2019s expected\nperformance with multiple temperature tiers.\nWe use Chameleon to pro \uffffle a variety of large memory-bound\napplications across di \ufffferent service domains running in our produc-\ntion and make the following observations. (1)Meaningful portions\nof application working sets can be warm/cold. We can o \uffffoad that\nto a slow tier memory without signi \uffffcant performance impact. (2)\nA large fraction of anon memory (created for a program\u2019s stack,\nheap, and/or calls to mmap) tends to be hotter, while a large fraction\nof\uffffle-backed memory tends to be relatively colder. (3)Page accessCacheMain MemoryCXL-MemoryNVM Disaggregated MemorySSDHDDRegister0.2ns1-40ns80-140ns170-250ns300-400ns2-4\u03bcs10-40\u03bcs3-10msAttached to CPUNetwork AttachedCPU IndependentFigure 2: Latency characteristics of memory technologies.\npatterns remain relatively stable for meaningful time durations\n(minutes to hours). This is enough to observe application behavior\nand make page placement decisions in kernel-space. (4)With new\n(de)allocations, actual physical page addresses can change their\nbehavior from hot to cold and vice versa fairly quickly. Static page\nallocations can signi \uffffcantly degrade performance.\nConsidering the above observations, we design an OS-level trans-\nparent page placement mechanism \u2013 TPP, to e \uffffciently place pages\nin a tiered-memory systems so that relatively hot pages remain in\nfast memory tier and cold pages are moved to the slow memory tier\n(\u00a75). TPP has three prime components: (a)a lightweight reclama-\ntion mechanism to demote colder pages to the slow tier node; (b)\ndecoupling the allocation and reclamation logic for multi-NUMA\nsystems to maintain a headroom of free pages on fast tier node; and\n(c)a reactive page promotion mechanism that e \uffffciently identi \uffffes\nhot pages trapped in the slow memory tier and promote them to\nthe fast memory tier to improve performance. We also introduce\nsupport for page type-aware allocation across the memory tiers \u2013\npreferably allocate sensitive anon pages to fast tier and \uffffle caches\nto slow tier. With this optional application-aware setting, TPP can\nact from a better starting point and converge faster for applications\nwith certain access behaviors.\nWe choose four production workloads that constitute signi \uffffcant\nportion of our server \uffffeet and run them on a system that support\nCXL 1.1 speci \uffffcation (\u00a7 6). We \uffffnd that TPP provides the similar\nperformance behavior of all memory served from the fast memory\ntier. For some workloads, this holds true even when local DRAM is\nonly 20% of the total system memory. TPP moves all the e \uffffective\nhot memory to the fast memory tier and improves default Linux\u2019s\nperformance by up to 18%. We compare TPP against NUMA Bal-\nancing [ 22] and AutoTiering [ 47], two state-of-the-art solutions for\ntiered memory. TPP outperforms both of them by 5\u201317%.\nWe make the following contributions in this paper:\n\u2022We present Chameleon, a lightweight user-space memory char-\nacterization tool. We use it to understand workload\u2019s memory\nconsumption behavior and assess the scope of tiered-memory\nin hyperscale datacenters (\u00a7 3). We open source Chameleon .", "start_char_idx": 149332, "end_char_idx": 153308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a70687a4-4339-4530-8c1d-6692d20abeb4": {"__data__": {"id_": "a70687a4-4339-4530-8c1d-6692d20abeb4", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd66b23d-bb5b-4842-9e96-a7402bee9c83", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a3b5a1e15cceb61b22e5f18390f0462b54060f7b8476c0a6cbaf3a9286ccbe9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceffa9b3-773c-459c-9dfc-5a0b37c914a9", "node_type": "1", "metadata": {}, "hash": "1d89579bd8424a2e74da41681ad05693fa488f9f9a72f8bf019d02a12cbcb9d1", "class_name": "RelatedNodeInfo"}}, "text": "We \uffffnd that TPP provides the similar\nperformance behavior of all memory served from the fast memory\ntier. For some workloads, this holds true even when local DRAM is\nonly 20% of the total system memory. TPP moves all the e \uffffective\nhot memory to the fast memory tier and improves default Linux\u2019s\nperformance by up to 18%. We compare TPP against NUMA Bal-\nancing [ 22] and AutoTiering [ 47], two state-of-the-art solutions for\ntiered memory. TPP outperforms both of them by 5\u201317%.\nWe make the following contributions in this paper:\n\u2022We present Chameleon, a lightweight user-space memory char-\nacterization tool. We use it to understand workload\u2019s memory\nconsumption behavior and assess the scope of tiered-memory\nin hyperscale datacenters (\u00a7 3). We open source Chameleon .\n\u2022We propose TPP for e \uffffcient memory management on a tiered-\nmemory system (\u00a7 5).We publish the source code of TPP .A\nmajor portion of it has been merged to Linux kernel v5.18. Rest\nof it is under an upstream discussion.\n\u2022We evaluate TPP on a CXL-enabled tiered-memory systems\nwith real production workloads (\u00a7 6) for years. TPP makes tiered\nmemory systems as performant as an ideal system with all\nmemory in local tier. For datacenter applications, TPP improves\n743TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada\ndefault Linux\u2019s performance by up to 18%. It also outperforms\nNUMA Balancing and AutoTiering by 5\u201317%.\nTo the best of our knowledge, we are the \uffffrst to characterize\nand evaluate an end-to-end practical CXL-Memory system that can\nbe readily deployed in hyperscale datacenters.\n2 MOTIVATION\nIncreased Memory Demand in Datacenter Applications. To\nbuild low-latency services, in-memory computation has become a\nnorm in datacenter applications. This has led to rapid growth in\nmemory demands across the server \uffffeet. With new generations\nof CPU and DRAM technologies, memory is becoming the more\nprominent portion of rack-level power and total cost of ownership\n(TCO). (Figure 3).14.619.819.720.728.833.315.625.521.621.231.837.1010203040Gen0Gen1Gen2Gen3Gen4Gen5Percentage (%)PowerCostFigure 3: Memory as a percentage of rack TCO and power\nacross di \ufffferent hardware generations of Meta.\nScaling Challenges in Homogeneous Server Designs. In to-\nday\u2019s server architectures, memory subsystem design is completely\ndependent on the underlying memory technology support in the\nCPUs. This has several limitations: (a) memory controllers only\nsupport a single generation of memory technology which limits\nmix-and-match of di \ufffferent technologies with di \ufffferent cost-per-GB\nand bandwidth vs. latency pro \uffffles; (b) memory capacity comes at\npower-of-two granularity which limits \uffffner grain memory capacity\nsizing; (c) there are limited bandwidth vs. capacity points per DRAM\ngeneration (Figure 4). This forces higher memory capacity in order\nto get more bandwidth on the system. Such tight coupling between\nCPU and memory subsystem restricts the \uffffexibility in designing ef-\n\uffffcient memory hierarchies and leads to stranded compute, network,\nand/or memory resources. Prior bus interfaces that allow memory\nexpansion are also proprietary to some extent [ 3,18,44] and not\ncommonly supported across all the CPUs [ 6,14,23]. Besides, high\nlatency characteristics and lack of coherency limit their viability in\nhyperscalers.\nCXL for Designing Tiered-Memory Systems. CXL [ 7] is an\nopen, industry-supported interconnect based on the PCI Express\n(PCIe) interface. It enables high-speed, low latency communication\nbetween the host processor and devices (e.g., accelerators, memory\nbu\uffffers, smart I/O devices, etc.) while expanding memory capac-\nity and bandwidth. CXL provides byte addressable memory in the\nsame physical address space and allows transparent memory allo-\ncation using standard memory allocation APIs. It allows cache-line\ngranularity access to the connected devices and underlying hard-\nware maintains coherency and consistency.", "start_char_idx": 152538, "end_char_idx": 156505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ceffa9b3-773c-459c-9dfc-5a0b37c914a9": {"__data__": {"id_": "ceffa9b3-773c-459c-9dfc-5a0b37c914a9", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a70687a4-4339-4530-8c1d-6692d20abeb4", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "05c19d869215f41adc9939340f28a55915ded346dd80a47561dfcd88691c6f0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87db7a65-c790-4496-ab37-297be4091b39", "node_type": "1", "metadata": {}, "hash": "bc249bc548f1969d4437f7a2e21761314119267db323e41f92a1bfb872885dd9", "class_name": "RelatedNodeInfo"}}, "text": "Besides, high\nlatency characteristics and lack of coherency limit their viability in\nhyperscalers.\nCXL for Designing Tiered-Memory Systems. CXL [ 7] is an\nopen, industry-supported interconnect based on the PCI Express\n(PCIe) interface. It enables high-speed, low latency communication\nbetween the host processor and devices (e.g., accelerators, memory\nbu\uffffers, smart I/O devices, etc.) while expanding memory capac-\nity and bandwidth. CXL provides byte addressable memory in the\nsame physical address space and allows transparent memory allo-\ncation using standard memory allocation APIs. It allows cache-line\ngranularity access to the connected devices and underlying hard-\nware maintains coherency and consistency. With PCIe 5.0, CPU\nto CXL interconnect bandwidth will be similar to the cross-socket\ninterconnects (Figure 5) on a dual-socket machine. CXL-Memory\naccess latency is also similar to the NUMA access latency. CXL adds11448881611.21.41.61.822.23.605101520Gen0Gen1Gen2Gen3Gen4Gen5Gen6Gen7x TimesCapacity (GB)Bandwidth (GB/s)Figure 4: Memory bandwidth and capacity increase over time.\naround 50-100 nanoseconds of extra latency over normal DRAM\naccess. This NUMA-like behavior with main memory-like access\nsemantics makes CXL-Memory a good candidate for the slow-tier\nin datacenter memory hierarchies.\nCXL solutions are being developed and incorporated by leading\nchip providers [ 1,4,9,21,24,25]. All the tools, drivers, and OS\nchanges required to support CXL are open sourced so that any-\none can contribute and bene \ufffft directly without relying on single\nsupplier solutions. CXL relaxes most of the memory subsystem\nlimitations mentioned earlier. It enables \uffffexible memory subsystem\ndesigns with desired memory bandwidth, capacity, and cost-per-GB\nratio based on workload demands. This helps scale compute and\nmemory resources independently and ensure a better utilization of\nstranded resources.\nScope of CXL-Based Tiered-Memory Systems. Datacenter\nworkloads rarely use all of the memory all the time [ 2,15,48,70,73].\nOften an application allocates a large amount of memory but ac-\ncesses it infrequently [ 48,56]. We characterize four popular appli-\ncations in our production server \uffffeet and \uffffnd that 55-80% of an\napplication\u2019s allocated memory remains idle within any two min-\nutes interval (\u00a7 3.2). Moving this cold memory to a slower memory\ntier can create space for more hot memory pages to operate on\nthe fast memory tier and improve application-level performance.\nBesides, it also allows reducing TCO by \uffffexible server design with\nsmaller fast memory tier and larger but cheaper slow memory tier.\nAs CXL-attached slow memory can be of any technology (e.g.,\nDRAM, NVM, LPDRAM, etc.), for the sake of generality, we will\ncall a memory directly attached to a CPU as local memory and a\nCXL-attached memory as CXL-Memory.\nLightweight Characterization of Datacenter Applications.\nIn a hyperscaler environment, di \ufffferent types of rapidly evolving\napplications consume a production server\u2019s resources. Applications\ncan have di \ufffferent requirements, e.g., some can be extremely la-\ntency sensitive, while others can be memory bandwidth sensitive.\nSensitivity towards di \ufffferent memory page types can also vary for\ndi\ufffferent applications. To understand the scope of tiered-memory\nsystem for existing datacenter applications, we need to characterize\ntheir memory usage pattern and quantify the opportunity of mem-\nory o \uffffoading at di \ufffferent memory tiers for di \ufffferent page types.\nThis insight can help system admins decide on the \uffffexible and\noptimized memory con \uffffgurations to support di \ufffferent workloads.\nExisting memory characterization tools fall short of providing\nthese insights. For example, access bit-based mechanism [ 11,17,\n69] cannot track detailed memory access behavior because it tells\nwhether a given page is accessed within a certain period of time.\nEven if a page gets multiple accesses within a tracking cycle, IPT-\nbased tools will count that as a single access.", "start_char_idx": 155790, "end_char_idx": 159777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87db7a65-c790-4496-ab37-297be4091b39": {"__data__": {"id_": "87db7a65-c790-4496-ab37-297be4091b39", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceffa9b3-773c-459c-9dfc-5a0b37c914a9", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6e93868da967a79f7a1306105e43c2621b0afb032244749e4a212ab16d300207", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e737753-ff6d-43e0-9d47-d6e09794c88f", "node_type": "1", "metadata": {}, "hash": "0f9b7d2b8d012311c0cb5b0924d12824665269961b06892eaf89806e5b8a077f", "class_name": "RelatedNodeInfo"}}, "text": "Sensitivity towards di \ufffferent memory page types can also vary for\ndi\ufffferent applications. To understand the scope of tiered-memory\nsystem for existing datacenter applications, we need to characterize\ntheir memory usage pattern and quantify the opportunity of mem-\nory o \uffffoading at di \ufffferent memory tiers for di \ufffferent page types.\nThis insight can help system admins decide on the \uffffexible and\noptimized memory con \uffffgurations to support di \ufffferent workloads.\nExisting memory characterization tools fall short of providing\nthese insights. For example, access bit-based mechanism [ 11,17,\n69] cannot track detailed memory access behavior because it tells\nwhether a given page is accessed within a certain period of time.\nEven if a page gets multiple accesses within a tracking cycle, IPT-\nbased tools will count that as a single access. Moreover, IPT only\n744ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.CPU0CPU1Interconnect32 GB/s perlinkDRAMDRAM38.4 GB/s per channel~100 ns~180 ns(a) Without CXLCPU0CXL64 GB/s per x16 linkDRAMDRAM~100 ns~170-250 ns38.4 GB/s per channel(b) With CXL\nFigure 5: CXL-System compared to a dual-socket server.\nprovides information in the physical address space \u2013 we cannot\ntrack memory allocation/deallocation if a physical page is re-used\nby multiple virtual pages. The overhead of such tools signi \uffffcantly\nincreases with the application\u2019s memory footprint size. Even for\napplication\u2019s with 10s of GB working set size, the overhead of\nIPT-based tool can be 20\u201390% [ 50].Similarly, complex PEBS-based\nuser-space tools [ 35,63,65,74] lead to high CPU overheads (more\nthan 15% per core) and often slow down the application. None of\nthese tools generate page type-aware heat map.\n3 CHARACTERIZING DATACENTER\nAPPLICATIONS\nTo understand the scope of tiered-memory in hyperscale applica-\ntions, we develop Chameleon, a light-weight user-space memory\naccess behavior characterization tool. The objective of developing\nChameleon is to allow users hop on any existing datacenter produc-\ntion machine and readily deploy it without disrupting the running\napplication(s) and modifying the underline kernel. Chameleon\u2019s\noverhead needs to be comparatively lower such that it does not\nnotably a \uffffect a production application\u2019s behavior. Chameleon\u2019s\nprime use case is to understand an application\u2019s memory access\nbehavior, i.e., what fraction of an application\u2019s memory remains\nhot-warm-cold, how long a page survive on a speci \uffffc temperature\ntier, how frequently they get accessed and so on. In practice, to char-\nacterize a certain type of application, we expect to run Chameleon\nfor a few hours on a tiny fraction of servers in the whole \uffffeet.\nConsidering the above objectives, we design Chameleon with\ntwo primary components \u2013 a Collector and a Worker\u2013 running si-\nmultaneously on two di \ufffferent threads. Collector utilizes the PEBS\nmechanism of modern CPUs to collect hardware-level performance\nevents related to memory accesses. Worker uses the sampled infor-\nmation to generate insights.\nCollector. Collector samples last level cache (LLC) misses\nfor demand loads (event MEM_LOAD_RETIRED.L3_MISS )\nand optionally TLB misses for demand stores (event\nMEM_INST_RETIRED.STLB_MISS_STORES ). Sampled records\nprovide us with the PID and virtual memory address for the\nmemory access events. Like any other sampling mechanism,\naccuracy of PEBS depends on the sampling rate \u2013 frequent samples\nprovide higher accuracy. High sampling rate, however, incurs\nhigher performance overhead directly on application threads and\ndemands more CPU resources for Chameleon\u2019s Worker thread . In\nour\uffffeets, sampling rate is con \uffffgured as one sample for every\n200 events, which appears as a good trade-o \uffffbetween overhead\nand accuracy. Note the choice of the sampling event on the store\nside is due to hardware limitations, i.e., there is no precise event\nfor LLC-missed stores, likely because stores are marked completeonce TLB translation is done in modern high-end CPUs. We have\nconveyed the concern on this limitation to major x86 vendors in\nmultiple occasions.", "start_char_idx": 158947, "end_char_idx": 163026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e737753-ff6d-43e0-9d47-d6e09794c88f": {"__data__": {"id_": "2e737753-ff6d-43e0-9d47-d6e09794c88f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87db7a65-c790-4496-ab37-297be4091b39", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "7c699bf4e223d729ac17df83a6645f0ea77328a7db317f54898396dcac227da5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bdb7f1f-8d70-49a3-aa9e-6cf9cb5cb675", "node_type": "1", "metadata": {}, "hash": "8d477e824729d36d65e2ea7520de8a365ddd03350628fbc8489c56ffa4cf719f", "class_name": "RelatedNodeInfo"}}, "text": "Sampled records\nprovide us with the PID and virtual memory address for the\nmemory access events. Like any other sampling mechanism,\naccuracy of PEBS depends on the sampling rate \u2013 frequent samples\nprovide higher accuracy. High sampling rate, however, incurs\nhigher performance overhead directly on application threads and\ndemands more CPU resources for Chameleon\u2019s Worker thread . In\nour\uffffeets, sampling rate is con \uffffgured as one sample for every\n200 events, which appears as a good trade-o \uffffbetween overhead\nand accuracy. Note the choice of the sampling event on the store\nside is due to hardware limitations, i.e., there is no precise event\nfor LLC-missed stores, likely because stores are marked completeonce TLB translation is done in modern high-end CPUs. We have\nconveyed the concern on this limitation to major x86 vendors in\nmultiple occasions.\nTo improve \uffffexibility , the Collector divides all CPU cores into a\nset of groups and enables sampling on one or more group(s) at a time\n(Figure 6a). After each mini_interval (by default, 5 seconds), the\nsampling thread rotates to the next core group. This duty-cycling\nhelps further tune the trade-o \uffffbetween overhead and accuracy.\nIt also allows sampling di \ufffferent events on di \ufffferent core groups.\nFor example, for latency-critical applications, one can choose to\nsample half of the cores at a time. On the other hand, for store-\nheavy applications, one can enable load sampling on half of the\ncores and store sampling on the other half at the same time.\nThe Collector reads the sampling bu \uffffer and writes into one of\nthe two hash tables. After each interval (by default, 1 minute),\nthe Collector wakes up the Worker to process data in current hash\ntable and moves to the other hash table for storing next interval\u2019s\nsampled data.\nWorker. The Worker (Figure 6b) runs on a separate thread to\nread page access information and generate insights on memory\naccess behavior. It considers the address of a sampled record as a\nvirtual page access where the page size is de \uffffned by the OS. This\nmakes it generic to systems with any page granularities (e.g., 4KB\nbase page, 2MB huge page, etc.). To generate statistics on both\nvirtual- and physical-spaces, the Worker \uffffnds the corresponding\nphysical page mapped to the sampled virtual page. This address\ntranslation can cause high overhead if the target process\u2019s working\nset size is extremely large (e.g., terabyte-scale). One can con \uffffgure\nthe Worker to disable the physical address translation and charac-\nterize an application only on its virtual-space access pattern.\nFor each page, a 64-bit bitmap tracks its activeness within an\ninterval. If a page is active within an interval, the corresponding bit\nis set. At the end of each interval, the bitmap is left-shifted one bit to\ntrack for a new interval. One can use multiple bits for one interval\nto capture the page access frequency, at the cost of supporting\nshorter history. After generating the statistics and reporting them,\nthe Worker sleeps (Figure 6c).\nTo characterize an application deployed on hundreds of thou-\nsands of servers, we run Chameleon on a small set of servers only for\na few hours. During our analysis, we chose servers where system-\nwide CPU and memory usage does not go above 80%. In such produc-\ntion environments, we do not notice any service-level performance\nimpact while running Chameleon. CPU overhead is within 3\u20135%\nof a single core. However, on a synthetic workload that is memory\nbandwidth sensitive and uses all the CPU cores so that Chameleon\nneeds to contend for CPU, we lose 7% performance due to pro \uffffling.\n3.1 Production Workload Overview\nWe use Chameleon to characterize most popular memory-bound\napplications running for years across our production server \uffffeet\nserving live tra \uffffc on four diverse service domains. These workloads\nconstitute a signi \uffffcant portion of the server \uffffeet and represent a\nwide variety of our workloads [ 67,68].Web implements a Virtual\nMachine to serve web requests. Web1 is a HipHop Virtual Machine\n(HHVM)-based and Web2 is a Python-based service.", "start_char_idx": 162175, "end_char_idx": 166239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bdb7f1f-8d70-49a3-aa9e-6cf9cb5cb675": {"__data__": {"id_": "2bdb7f1f-8d70-49a3-aa9e-6cf9cb5cb675", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e737753-ff6d-43e0-9d47-d6e09794c88f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e4ca88302f8cea033172ee77b791db3605c3457c97d4d8599ae8977883a475e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04e3b033-ce68-4fa3-85ed-208e76d24aa4", "node_type": "1", "metadata": {}, "hash": "0e69f0e7a4aad8cdeccfecf32ff407fc9832e4cac5529c83a4acadfe8c15e140", "class_name": "RelatedNodeInfo"}}, "text": "CPU overhead is within 3\u20135%\nof a single core. However, on a synthetic workload that is memory\nbandwidth sensitive and uses all the CPU cores so that Chameleon\nneeds to contend for CPU, we lose 7% performance due to pro \uffffling.\n3.1 Production Workload Overview\nWe use Chameleon to characterize most popular memory-bound\napplications running for years across our production server \uffffeet\nserving live tra \uffffc on four diverse service domains. These workloads\nconstitute a signi \uffffcant portion of the server \uffffeet and represent a\nwide variety of our workloads [ 67,68].Web implements a Virtual\nMachine to serve web requests. Web1 is a HipHop Virtual Machine\n(HHVM)-based and Web2 is a Python-based service. Cache is a\nlarge distributed-memory object caching service lying between the\n745TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, CanadaMemory NodeStore SampledLoad SampledUnsampled\nCore Group 1Core Group 2Core Group 3Core Group NSample StoreSample LoadCollectorRotate sampling event collection to next CPU core group after each mini-interval(a) CollectorHistory GeneratorVirtual Address RangePhysical Address000110\u2026010100\u2026110010\u2026StorageReportHotness TrackerVA to PA Translator/proc/$PID/map/proc/$PID/pagemap(b) WorkerInterval NCollectorwrites to hashtable-0WorkerSleepsWorkerRunsreads from hashtable-1Interval N+1Collectorwrites to hashtable-1WorkerSleepsWorkerRunsreads from hashtable-0(c) Work \uffffow within a Cycle\nFigure 6: Overview of Chameleon components (left) and work \uffffow (right)\n020406080100Web1Web2Cache1Cache2WarehouseAds1Ads2Ads3Memory Utilization (%)Cold10 Min Hot5 Min Hot2 Min Hot1 Min Hot\nFigure 7: Application memory usage over last N mins.\nweb and database tiers for low-latency data-retrieval. Data Ware-\nhouse is a uni \uffffed computing engine for parallel data processing\non compute clusters. This service manages and coordinates the ex-\necution of long and complex batch queries on data across a cluster.\nAdsare compute heavy workloads that retrieve in-memory data\nand perform machine learning computations.\n3.2 Page Temperature\nIn datacenter applications, a signi \uffffcant amount of allocated mem-\nory remains cold beyond a few minutes of intervals (Figure 7). Web,\nCache, and Ads use 95\u201398% of the system\u2019s total memory capac-\nity, but within a two-minute interval, they use 22\u201380% of the total\nallocated memory on average.\nData Warehouse is a compute-heavy workload where a speci \uffffc\ncomputation operation can span even terabytes of memory. This\nworkload consumes almost all the available memory within a server.\nEven here, on average, only 20% of the accessed memory is hot\nwithin a two-minute interval.\nObservation: A signi \uffffcant portion of a datacenter application\u2019s\naccessed memory remain cold for minutes. Tiered memory sys-\ntem can be a good \ufffft for such cold memory if page placement\nmechanism can move these cold pages to a lower memory tier.\n3.3 Temperature Across Di \ufffferent Page Types\nApplications consume di \ufffferent types of pages based on applica-\ntion logic and execution demand. However, the fraction of anons\n(anonymous pages) remain hot is higher than the fraction of \uffffles\n(\uffffle pages). For Web, within a two-minute interval, 35\u201360% of the\ntotal allocated anons remain hot; for \uffffles, in contrast, it is only\n3\u201314% of the total allocation (Figure 8).\nCache applications use tmpfs [27] for a fast in-memory lookup.\nAnons are used mostly for processing queries. As a result, \uffffle\npages contribute signi \uffffcantly to the total hot memory. However, for\nCache1, 40% of the anons get accessed within every two minutes,020406080100AnonFileAnonFileAnonFileAnonFileAnonFileAnonFileAnonFileAnonFileWeb1Web2Cache1Cache2W.houseAds1Ads2Ads3% of AllocationCold10 Min Hot2 Min Hot1 Min Hot\nFigure 8: Anon pages tends to be hotter than \uffffle pages.\nwhile the fraction for \uffffle is only 25%.", "start_char_idx": 165543, "end_char_idx": 169394, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04e3b033-ce68-4fa3-85ed-208e76d24aa4": {"__data__": {"id_": "04e3b033-ce68-4fa3-85ed-208e76d24aa4", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bdb7f1f-8d70-49a3-aa9e-6cf9cb5cb675", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "eb313a89f133c5cbd18a93b37eefc35724dd6756224f97734f46a781b213d822", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c16220b3-782a-4dca-a043-d67d87e13ef5", "node_type": "1", "metadata": {}, "hash": "2f03cbf022815c80bf2be0921223de29dd8ab98cddc59af17d3779434bb6c9b8", "class_name": "RelatedNodeInfo"}}, "text": "Cache applications use tmpfs [27] for a fast in-memory lookup.\nAnons are used mostly for processing queries. As a result, \uffffle\npages contribute signi \uffffcantly to the total hot memory. However, for\nCache1, 40% of the anons get accessed within every two minutes,020406080100AnonFileAnonFileAnonFileAnonFileAnonFileAnonFileAnonFileAnonFileWeb1Web2Cache1Cache2W.houseAds1Ads2Ads3% of AllocationCold10 Min Hot2 Min Hot1 Min Hot\nFigure 8: Anon pages tends to be hotter than \uffffle pages.\nwhile the fraction for \uffffle is only 25%. For Cache2, the fraction\nof anon and \uffffle usage is almost equal within a two-minute time\nwindow. However, within a minute interval, even for Cache2, higher\nfraction of anons (43%) remain hot over \uffffle pages (30%).\nData Warehouse and Ads use anon pages for computation. The\n\uffffle pages are used for writing intermediate computation data to the\nstorage device. As expected, almost all of hot memories are anons\nwhere almost all of the \uffffles remain cold.\nObservation: A large fraction of anon pages is hot, while \uffffle\npages are comparatively colder within short intervals.\nDue to space constraints, we focus on a subset of these applica-\ntions. In our analysis, we \uffffnd the similar behavior from the rest of\nthe applications.\n3.4 Usage of Di \ufffferent Page Types Over Time\nWhen the Web service starts, it loads the virtual machine\u2019s binary\nand bytecode \uffffles into memory. As a result, at the beginning, \uffffle\ncaches occupy a signi \uffffcant portion of the memory. Overtime, anon\nusage slowly grows and \uffffle caches get discarded to make space for\nthe anon pages (Figure 9a).\nCache applications mostly use \uffffle caches for in-memory look-\nups. As a result, \uffffle pages consume most of the allocated memory.\nFor Cache1 and Cache2 (Figure 9b-9c), the fraction of \uffffles hov-\ners around 70\u201382%. While the fraction of anon and \uffffle is almost\nsteady, if at any point, anon usage grows, \uffffle pages are discarded\nto accommodate newly allocated anons.\nFor Data Warehouse workload, anon pages consume most of the\nallocated memory \u2013 85% of the total allocated memory are anons\nand rest of the 15% are \uffffle pages (Figure 9d). The usage of anon\nand\uffffle pages mostly remains steady.\nObservation: Although anon and \uffffle usage may vary over time,\napplications mostly maintain a steady usage pattern. Smart page\nplacement mechanisms should be aware of page type when making\nplacement decisions.\n746ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.\n00.20.40.60.81\n050100150200250300Memory Utilization (%)Time (minute)TotalAnonFile(a) Web100.20.40.60.81\n0200400600800Memory Utilization (%)Time (minute)TotalAnonFile(b) Cache100.20.40.60.81\n0200400600800Memory Utilization (%)Time (minute)TotalAnonFile(c) Cache200.20.40.60.81\n0255075100125Memory Utilization(%)Time (minute)TotalAnonFile(d) Data Warehouse\nFigure 9: Memory usage over time for di \ufffferent applications\n00.20.40.60.8100.20.40.60.81Memory Utilization (%)Throughput (%)FileAnon\n(a) Web100.20.40.60.8100.20.40.60.81Memory Utilization (%)Throughput (%)FileAnon\n(b) Cache100.20.40.60.8100.20.40.60.81Memory Utilization (%)Throughput (%)FileAnon\n(c) Cache200.20.40.60.8100.20.40.60.81Memory Utilization (%)Throughput (%)FileAnon\n(d) Data Warehouse\nFigure 10: Workloads\u2019 sensitivity towards anons and \uffffles varies. High memory capacity utilization provides high throughput.\n3.5 Impact of Page Types on Performance\nFigure 10shows what fractions of di \ufffferent page types are used\nto achieve a certain application-level throughput.", "start_char_idx": 168878, "end_char_idx": 172349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c16220b3-782a-4dca-a043-d67d87e13ef5": {"__data__": {"id_": "c16220b3-782a-4dca-a043-d67d87e13ef5", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04e3b033-ce68-4fa3-85ed-208e76d24aa4", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "9c93ae0c9bf3f5ab82e9677de9bd61c472cf86d884f4b03db11957465ba0edb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c22c8b2-917a-4bc4-9b05-ffbc00996994", "node_type": "1", "metadata": {}, "hash": "e8cfdfdd4fcad841eb3ce8f55cb334f37515be99837be99e9a9e1d9e6f8c891e", "class_name": "RelatedNodeInfo"}}, "text": "High memory capacity utilization provides high throughput.\n3.5 Impact of Page Types on Performance\nFigure 10shows what fractions of di \ufffferent page types are used\nto achieve a certain application-level throughput. Memory-bound\napplication\u2019s throughput improves with high memory utilization.\nHowever, workloads have di \ufffferent levels of sensitivity toward\ndi\ufffferent page types. For example, Web\u2019s throughput improves with\nthe higher utilization of anon pages (Figure 10a).\nFor Cache, tmpfs is allocated during initialization period. Be-\nsides, Cache1 uses a \uffffxed amount of anons throughout its life cycle.\nAs a result, we cannot observe any noticeable relation between\nanon or \uffffle usage and the application throughput (Figure 10b).\nHowever, for Cache2, we can see high throughput is achieved with\ncomparatively higher utilization of anons (Figure 10c). Similarly,\nData Warehouse application maintains a \uffffxed amount of \uffffle pages.\nHowever, it consumes di \ufffferent amount of anons at di \ufffferent execu-\ntion period and the highest throughput is achieved when the total\nusage of anons reaches to its highest point (Figure 10d).\nObservation: Workloads have di \ufffferent levels of sensitivity to-\nward di \ufffferent page types that varies over time.\n3.6 Page Re-access Time Granularity\nCold pages often get re-accessed at a later point. Figure 11shows\nthe fraction of pages that become hot after remaining cold for a\ncertain interval. For Web, almost 80% of the pages are re-accessed\nwithin a ten-minute interval. This indicates Web mostly repurposes\npages allocated at an earlier time. Same goes for Cache \u2013 randomly\no\uffffoading cold memory can impact performance as a good chunk\nof colder pages get re-accessed within a ten-minute window.\nHowever, Data Warehouse shows di \ufffferent characteristics. For\nthis workload, anons are mostly newly allocated \u2013 within a ten-\nminute interval, only 20% of the hot \uffffle pages are previously ac-\ncessed. Rest of them are newly allocated.020406080100Web1Cache1Cache2WarehouseMemory Utilization (%)10 Min5 Min2 Min1 MinFigure 11: Fraction of pages re-accessed at di \ufffferent intervals.\nObservation: Cold page re-access time varies for workloads.\nPage placement on a tiered memory system should be aware of this\nand actively move hot pages to lower memory nodes to avoid high\nmemory access latency.\nFrom above observations, tiered memory subsystems can be\na good \ufffft for datacenter applications as there exists signi \uffffcant\namount of cold memory with steady access patterns.\n4 DESIGN PRINCIPLES OF TPP\nWith the advent of CXL technologies, hyperscalers are embracing\nCXL-enabled heterogeneous tiered-memory system where di \uffffer-\nent memory tier has di \ufffferent performance characteristics [ 41,52].\nFor performance optimization in such systems, transparent page\nplacement mechanism (TPP) is needed to handle pages with varied\nhotness characteristics on appropriate temperature tiers. To de-\nsign TPP for next-generation tiered-memory systems, we consider\nfollowing questions:\n\u2022What is an ideal layer to implement TPP functionalities?\n\u2022How to detect page temperature?\n\u2022What abstraction to provide for accessing CXL-Memory?\nIn this section, we discuss the rationale and trade-o \uffffs behind\ndesign choices for TPP.\n747TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada\nImplementation Layer. Application-transparent page place-\nment mechanism can be employed both in the user- and kernel-\nspace. In user-space, a Chameleon-like tool can be used to detect\npage temperatures and perform NUMA migrations using user-space\nAPIs (e.g., move_pages() ). To identify what to migrate, the migra-\ntion tool needs to implement user-space page lists and history\nmanagement. This technique entails overheads due to user-space to\nkernel-space context switching. It also adds processing overheads\ndue to history management in user space. Besides, there are mem-\nory overheads due to page information management in user-space\nthat may not scale with large working sets. While this is acceptable\nfor pro \uffffling tools that run for short intervals on a small sample of\nservers in the \uffffeet, it can be prohibitively expensive when they run\ncontinuously on all production \uffffeet.", "start_char_idx": 172137, "end_char_idx": 176349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c22c8b2-917a-4bc4-9b05-ffbc00996994": {"__data__": {"id_": "1c22c8b2-917a-4bc4-9b05-ffbc00996994", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c16220b3-782a-4dca-a043-d67d87e13ef5", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "efc527b2797a35ff8eaef3c8dcea653432fed8631da0becba7e6319836b4d6e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8121bbc-0bcf-4685-ae53-91cd00fee759", "node_type": "1", "metadata": {}, "hash": "99c1fae87c699fea15e6e2490e50459dddb82ec48f07f54dce5978aeb958c9ee", "class_name": "RelatedNodeInfo"}}, "text": "Application-transparent page place-\nment mechanism can be employed both in the user- and kernel-\nspace. In user-space, a Chameleon-like tool can be used to detect\npage temperatures and perform NUMA migrations using user-space\nAPIs (e.g., move_pages() ). To identify what to migrate, the migra-\ntion tool needs to implement user-space page lists and history\nmanagement. This technique entails overheads due to user-space to\nkernel-space context switching. It also adds processing overheads\ndue to history management in user space. Besides, there are mem-\nory overheads due to page information management in user-space\nthat may not scale with large working sets. While this is acceptable\nfor pro \uffffling tools that run for short intervals on a small sample of\nservers in the \uffffeet, it can be prohibitively expensive when they run\ncontinuously on all production \uffffeet. Considering these, we design\nTPP as a kernel feature as we think it\u2019s less complex to implement\nand more performant over user-space mechanisms.\nPage Temperature Detection. There are several di \ufffferent tech-\nniques that can potentially be used for page temperature detection\nincluding PEBS, Page Poisoning, and NUMA Balancing. PEBS can\nbe utilized in kernel-space to detect page temperature. However, as\nPEBS counters are not standardized across CPU vendors, a generic\nprecise event-based kernel implementation for page temperature\ndetection that works across all hardware platforms is not feasi-\nble. Additionally, limited number of perf counters are supported\nin CPUs and are generally required to be exposed in user-space.\nMore importantly, as mentioned earlier, even with optimizations,\nPEBS-based pro \uffffling is not good enough as an always-running\ncomponent of TPP for high pressure workloads.\nSampling and poisoning a few pages within a memory re-\ngion to track their access events is another well-established ap-\nproach [ 16,22,29,31] for \uffffnding hot/cold pages. To detect a page ac-\ncess, IPT-based [ 16,29] approach needs to clear the page\u2019s accessed\nbitand\uffffush the corresponding TLB entry. This requires monitor-\ningaccessed bits at high frequency which results in unacceptable\nslowdowns [ 29,31]. Thermostat [ 31] solves this problem by sam-\npling at 2MB page granularity which makes it e \uffffective speci \uffffcally\nfor huge-pages. One of our design goals behind TPP is that it should\nbe agnostic to page size. In our production environment, application\nowners generally pre-allocate 2MB and 1GB pages and use them to\nallocate text regions (code), static data structures, and slab pools\nthat serve request allocations. In most cases, these large pages are\nhot and should never get demoted to CXL-Memory.\nNUMA Balancing (also known as AutoNUMA) [ 22] is transpar-\nent to OS page sizes. It generates a minor page fault when the\nsampled page gets accessed. Periodically incurring page faults on\nmost frequently accessed pages can lead to high overheads. To ad-\ndress this, when designing TPP, we chose to only leverage minor\npage fault as a temperature detection mechanism for CXL-Memory.\nAs CXL-Memory is expected to hold warm and cold pages, this will\nkeep the overhead of temperature detection low. For cold page de-\ntection in local memory node, we \uffffnd Linux\u2019s existing LRU-based\nage management mechanism is lightweight and quite e \uffffcient.\nEmpirically, we do not see any potential bene \ufffft to leverage a\nmore sophisticated page temperature detection mechanism. In our\nexperiments (\u00a7 6), using kernel LRUs for on-the- \uffffy pro \uffffling works\nwell. Combining LRUs and NUMA Balancing, we can detect mosthot pages in CXL-Memory at virtually zero overhead as presented\nin Figure 14and Table 1.\nMemory Abstraction for CXL-Memory. One can use CXL-\nMemory as a swap-space to host colder memory using existing in-\nmemory swapping mechanisms [ 8,13,26,30]. TMO [ 73] is one such\nswap-based mechanism that detects cold pages in local memory\nand moves them to swap-space that is referred to as (z)swap pool.\nHowever, we do not plan to use CXL-Memory as an in-memory\nswap device.", "start_char_idx": 175488, "end_char_idx": 179512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8121bbc-0bcf-4685-ae53-91cd00fee759": {"__data__": {"id_": "c8121bbc-0bcf-4685-ae53-91cd00fee759", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c22c8b2-917a-4bc4-9b05-ffbc00996994", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6d67b7fa3de825c5f165c4a5f9de32fa5f962f6eb101853a79e8e816be391b93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdbf5660-da8a-45d5-b71c-48b697ac05a0", "node_type": "1", "metadata": {}, "hash": "19edadf23d42da42d160be054fbc000c14ddd93268289619eb97a23e09670c73", "class_name": "RelatedNodeInfo"}}, "text": "Empirically, we do not see any potential bene \ufffft to leverage a\nmore sophisticated page temperature detection mechanism. In our\nexperiments (\u00a7 6), using kernel LRUs for on-the- \uffffy pro \uffffling works\nwell. Combining LRUs and NUMA Balancing, we can detect mosthot pages in CXL-Memory at virtually zero overhead as presented\nin Figure 14and Table 1.\nMemory Abstraction for CXL-Memory. One can use CXL-\nMemory as a swap-space to host colder memory using existing in-\nmemory swapping mechanisms [ 8,13,26,30]. TMO [ 73] is one such\nswap-based mechanism that detects cold pages in local memory\nand moves them to swap-space that is referred to as (z)swap pool.\nHowever, we do not plan to use CXL-Memory as an in-memory\nswap device. In such a case, we will e \uffffectively lose CXL\u2019s most\nimportant feature, i.e., load/store access semantics at cache-line\ngranularity. With swap abstraction, every access to a (z)swapped\npage will incur a major page fault and we have to read the whole\npage from CXL-Memory. This will signi \uffffcantly increase the ef-\nfective access latency far over 200ns and make CXL-Memory less\nattractive. We chose to build TPP so that applications can lever-\nage load-store semantics when accessing warm or cold data from\nCXL-Memory.\nWhile the swap semantics of TMO are not desirable in CXL-\nMemory page placement context, the memory saving through\nfeedback-driven reclamation is still valuable. We think TMO as\nan orthogonal and complimentary tool to TPP as it operates one\nlayer above TPP (\u00a7 6.3.2). TMO runs in user-space and keeps push-\ning for memory reclamation, while TPP runs in kernel-space and\noptimizes page placement for allocated memory between local and\nCXL-Memory tiers.\n5 TPP FOR CXL-MEMORY\nAn e \uffffective page placement mechanism should e \uffffciently o \uffffoad\ncold pages to slower CXL-Memory while aptly identify trapped hot\npages in CXL-node and promote them to the fast memory tier. As\nCXL-Memory is CPU-less and independent of the CPU-attached\nmemory, it should be \uffffexible enough to support heterogeneous\nmemory technologies with varied characteristics. Page allocation\nto a NUMA node should not frequently halt due to the slower recla-\nmation mechanism to free up spaces. Besides, an e \uffffective policy\nshould be aware of an application\u2019s sensitivity toward di \ufffferent\npage types.\nConsidering the datacenter workload characteristics and our de-\nsign objectives, we propose TPP\u2013 a smart OS-managed mechanism\nfor tiered-memory system. TPP places \u2018hotter\u2019 pages in local mem-\nory and moves \u2018colder\u2019 pages in CXL-Memory. TPP\u2019s design-space\ncan be divided across four main areas \u2013 (a)lightweight demotion\nto CXL-Memory, (b)decoupled allocation and reclamation paths,\n(c)hot-page promotion to local nodes, and (d)page type-aware\nmemory allocation.\n5.1 Migration for Lightweight Reclamation\nLinux tries to allocate a page to the memory node local to a CPU\nwhere the process is running. When a CPU\u2019s local memory node\n\ufffflls up, default reclamation pages-out to swap device. In such a\ncase, in a NUMA system, new allocations to local node halts and\ntakes place on CXL-node until enough pages are freed up. The\nslower the reclamation is, the more pages end up being allocated to\nthe CXL-node. Besides, invoking paging events in the critical path\nworsens the average page access latency and impacts application\nperformance.\n748ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.CPUdemotionwatermarkallocationwatermarkx% of capacitymigrates to CXL-Memoryhigh watermarkLocal NodeCXL Nodeinitiates reclamation21CXLheadroom for allocationFigure 12: TPP decouples the allocation and reclamation log-\nics for local memory node. It uses migration for demotion.\nTo enable a light-weight page reclamation for local nodes, after\n\uffffnding the reclamation-candidates, instead of invoking swapping\nmechanism, we put them in to a separate demotion list and try to\nmigrate them to the CXL-node asynchronously ( 1in Figure 12).", "start_char_idx": 178792, "end_char_idx": 182726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdbf5660-da8a-45d5-b71c-48b697ac05a0": {"__data__": {"id_": "bdbf5660-da8a-45d5-b71c-48b697ac05a0", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8121bbc-0bcf-4685-ae53-91cd00fee759", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a51e78c29d4c29602cfe3dbe2c6a86c9eb041147d35e1933bf138f96e7ff3d64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adf863d0-375c-4ee2-8f06-32abf56f015e", "node_type": "1", "metadata": {}, "hash": "afbba276c0263d700a43a0d88616e01bd72956251d91fa56c38770cdfb330ce4", "class_name": "RelatedNodeInfo"}}, "text": "Besides, invoking paging events in the critical path\nworsens the average page access latency and impacts application\nperformance.\n748ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.CPUdemotionwatermarkallocationwatermarkx% of capacitymigrates to CXL-Memoryhigh watermarkLocal NodeCXL Nodeinitiates reclamation21CXLheadroom for allocationFigure 12: TPP decouples the allocation and reclamation log-\nics for local memory node. It uses migration for demotion.\nTo enable a light-weight page reclamation for local nodes, after\n\uffffnding the reclamation-candidates, instead of invoking swapping\nmechanism, we put them in to a separate demotion list and try to\nmigrate them to the CXL-node asynchronously ( 1in Figure 12).\nMigration to a NUMA node is orders of magnitude faster than\nswapping. We use Linux\u2019s default LRU-based mechanism to select\ndemotion candidates. However, unlike swapping, as demoted pages\nare still available in-memory, along with inactive \uffffle pages, we scan\ninactive anon pages for reclamation candidate selection. As we start\nwith the inactive pages, chances of hot pages being migrated to\nCXL-node during reclamation is very low unless the local node\u2019s\ncapacity is smaller than the hot portion of working set size. If a\nmigration during demotion fails (e.g., due to low memory on the\nCXL-node), we fall back to the default reclamation mechanism for\nthat failed page. As allocation on CXL-node is not performance\ncritical, CXL-nodes use the default reclamation mechanism (e.g.,\npages out to the swap device).\nIf there are multiple CXL-nodes, the demotion target is chosen\nbased on the node distances from the CPU. Although other complex\nalgorithms can be employed to dynamically choose the demotion\ntarget based on a CXL-node\u2019s state, this simple distance-based static\nmechanism turns out to be e \uffffective.\n5.2 Decoupling Allocation and Reclamation\nLinux maintains three watermarks ( min,low,high ) for each mem-\nory zone within a node. If the total number of free pages for a\nnode goes below low_watermark , Linux considers the node is un-\nder memory pressure and initiates page reclamation for that node.\nIn our case, TPP demotes them to CXL-node. New allocation to\nlocal node halts till the reclaimer frees up enough memory to satisfy\nthehigh_watermark . With high allocation rate, reclamation may\nfail to keep up as it is slower than allocation. Memory retrieved by\nthe reclaimer may \uffffll up soon to satisfy allocation requests. As a\nresult, local memory allocations halt frequently, more pages end up\nin CXL-node which eventually degrades application performance.\nIn a multi-NUMA system with severe memory constraint, we\nshould proactively maintain a reasonable amount of free memory\nheadroom on the local node. This helps in two ways. First, new\nallocation bursts (that are often related to request processing and,\ntherefore, both short-lived and hot) can be directly mapped to the\nlocal node. Second, local node can accept promotions of trapped\nhot pages on CXL-nodes.\nTo achieve that, we decouple the logic of \u2018reclamation stop\u2019 and\n\u2018new allocation happen\u2019 mechanism. We continue the asynchronous\nbackground reclamation process on local node until its total number\nof free pages reaches demotion_watermark , while new allocationCPUdemotionwatermarkallocationwatermarkremoteaccesspromotes active LRU pageshigh watermarkLocal NodeCXL NodeCXL123move to active LRU listheadroom for allocationcheck page active stateFigure 13: TPP promotes a page considering its activity state.\ncan happen if the free page count satis \uffffes a di \ufffferent watermark\n\u2013allocation_watermark (2in Figure 12). Note that demotion\nwatermark is always set to a higher value above the allocation and\nlow watermark so that we always reclaim more to maintain the\nfree memory headroom.\nHow aggressively one needs to reclaim often depends on the\napplication behavior and available resources. For example, if an\napplication has high page allocation demand, but a large fraction\nof its memory is infrequently accessed, aggressive reclamation can\nhelp maintain free memory headroom. On the other hand, if the\namount of frequently accessed pages is larger than the local node\u2019s\ncapacity, aggressive reclamation will thrash hot memory across\nNUMA nodes.", "start_char_idx": 181988, "end_char_idx": 186242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adf863d0-375c-4ee2-8f06-32abf56f015e": {"__data__": {"id_": "adf863d0-375c-4ee2-8f06-32abf56f015e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdbf5660-da8a-45d5-b71c-48b697ac05a0", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a0a6c6d1b6baf853a82d1382ab1e1db29e3501d47759fbb1b7862c4135e08d2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fae38f8c-f20d-43f8-8a84-ab06b6ee8e72", "node_type": "1", "metadata": {}, "hash": "cfb8ee364b4bfb5f9cd434bb5311c4b9a6f2b049e4762f208e4de294251f97a9", "class_name": "RelatedNodeInfo"}}, "text": "can happen if the free page count satis \uffffes a di \ufffferent watermark\n\u2013allocation_watermark (2in Figure 12). Note that demotion\nwatermark is always set to a higher value above the allocation and\nlow watermark so that we always reclaim more to maintain the\nfree memory headroom.\nHow aggressively one needs to reclaim often depends on the\napplication behavior and available resources. For example, if an\napplication has high page allocation demand, but a large fraction\nof its memory is infrequently accessed, aggressive reclamation can\nhelp maintain free memory headroom. On the other hand, if the\namount of frequently accessed pages is larger than the local node\u2019s\ncapacity, aggressive reclamation will thrash hot memory across\nNUMA nodes. Considering these, to tune the aggressiveness of\nthe reclamation process on local nodes, we provide a user-space\nsysctl interface ( /proc/sys/vm/demote_scale_factor ) to con-\ntrol the free memory threshold for triggering the reclamation on\nlocal nodes. By default, its value is empirically set to 2% that means\nreclamation starts as soon as only a 2% of the local node\u2019s capacity is\navailable to consume. One can use workload monitoring tools [ 73]\nto dynamically adjust this value.\n5.3 Page Promotion from CXL-Node\nDue to increased memory pressure on local nodes, new pages may\noften get allocated to CXL-nodes. Besides, demoted pages may\nalso become hot later as discussed in \u00a7 3.6. Without any promotion\nmechanism, hot pages will always be trapped in CXL-nodes and\nhurt application performance. To promote such pages, we augment\nLinux\u2019s NUMA Balancing [ 22].\nNUMA Balancing for CXL-Memory. In NUMA Balancing,\na kernel task routinely samples a subset of a process\u2019s memory\n(by default, 256MB of pages) on each memory node. When a CPU\naccesses a sampled page, a minor page-fault is generated (known as\nNUMA hint fault). Pages that are accessed from a remote CPU are\nmigrated to that CPU\u2019s local memory node (known as promotion).\nIn a CXL-System, it is not reasonable to promote a local node\u2019s hot\nmemory to other local or CXL-nodes. Besides, as sampling pages\nto\uffffnd a local node\u2019s hot memory generates unnecessary NUMA\nhint fault overheads, we limit sampling only to CXL-nodes.\nWhile promoting a CXL-node\u2019s trapped hot pages, we ignore\nthe allocation watermark checking for the target local node. This\ncreates more memory pressure to initiate the reclamation of com-\nparatively colder pages on that node. If a system has multiple local\nnodes, we select the node where the task is running. When applica-\ntions share multiple memory nodes, we choose local node with the\nlowest memory pressure.\n749TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada\nPing-Pong due to Opportunistic Promotion. When a NUMA\nhint fault happens on a page, default NUMA balancing instantly\npromotes the page without checking its active state. As a result,\npages with very infrequent accesses can still be promoted to the\nlocal node. Once promoted, these type of pages may shortly become\nthe demotion candidate if the local nodes are always under pres-\nsure. Thus, promotion tra \uffffc generated from infrequently accessed\npages can easily \uffffll up the local node\u2019s free space and generate a\nhigher demotion tra \uffffc for CXL-nodes. This unnecessary tra \uffffc\ncan negatively impact an application\u2019s performance.\nApt Identi \uffffcation of Trapped Hot Pages. To solve this ping-\npong issue, instead of instant promotion, we check a page\u2019s age\nthrough its position in the LRU list maintained by the OS. If the\nfaulted page is in inactive LRU, we do not consider the page for\npromotion instantly as it might be an infrequently accessed page.\nWe consider the faulted page as a promotion candidate only if it\nis found in the active LRUs ( 1in Figure 13). This signi \uffffcantly\nreduces the promotion tra \uffffc.\nHowever, OS uses LRU lists for reclamation. If a memory node is\nnot under pressure and reclamation does not kick in, then pages in\ninactive LRU list do not automatically move to the active LRU list.", "start_char_idx": 185507, "end_char_idx": 189560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fae38f8c-f20d-43f8-8a84-ab06b6ee8e72": {"__data__": {"id_": "fae38f8c-f20d-43f8-8a84-ab06b6ee8e72", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adf863d0-375c-4ee2-8f06-32abf56f015e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "03bef61ce4691f6e33a255203f255d08e558a2ddbda865a8510444f8044145ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf71de70-e0dd-4720-9836-00a3a285651a", "node_type": "1", "metadata": {}, "hash": "530ddf6b1f22ef0970ed04edbe7a3dd5b26c0ce03b6ad375e9f0d5f50a6e786a", "class_name": "RelatedNodeInfo"}}, "text": "This unnecessary tra \uffffc\ncan negatively impact an application\u2019s performance.\nApt Identi \uffffcation of Trapped Hot Pages. To solve this ping-\npong issue, instead of instant promotion, we check a page\u2019s age\nthrough its position in the LRU list maintained by the OS. If the\nfaulted page is in inactive LRU, we do not consider the page for\npromotion instantly as it might be an infrequently accessed page.\nWe consider the faulted page as a promotion candidate only if it\nis found in the active LRUs ( 1in Figure 13). This signi \uffffcantly\nreduces the promotion tra \uffffc.\nHowever, OS uses LRU lists for reclamation. If a memory node is\nnot under pressure and reclamation does not kick in, then pages in\ninactive LRU list do not automatically move to the active LRU list.\nAs CXL-nodes may not always be under pressure, faulted pages\nmay often be found in the inactive LRU list and, therefore, bypass\nthe promotion \ufffflter. To address this, whenever we \uffffnd a faulted\npage on the inactive LRU list, we mark the page as accessed and\nmove it to the active LRU list immediately ( 2in Figure 13). If the\npage still remains hot during the next NUMA hint fault, it will be\nin the active LRU, and promoted to the local node ( 3in Figure 13).\nThis helps TPP add some hysteresis to page promotion. Besides,\nas Linux maintains separate LRU lists for anon and \uffffle pages, dif-\nferent page types have di \ufffferent promotion rates based on their\nrespective LRU sizes and activeness. This speeds up the convergence\nof hot pages across memory nodes.\n5.4 Page Type-Aware Allocation\nThe page placement mechanism we described above is generic\nto all page types. However, some applications can further bene \ufffft\nfrom page type-aware allocation policy. For example, production\napplications often perform lots of \uffffle I/O during the warm up phase\nand generate \uffffle caches that are accessed infrequently. As a result,\ncold\uffffle caches eventually end up on CXL-nodes. Not only that,\nlocal memory node being occupied by the inactive \uffffle caches often\nforces anons to be allocated on the CXL-nodes that may need to be\npromoted back later.\nTo resolve these unnecessary page migrations, we allow an ap-\nplication allocating caches (e.g., \uffffle cache, tmpfs, etc.) to the CXL-\nnodes preferrably, while preserving the allocation policy for anon\npages. When this allocation policy is enabled, page caches generated\nat any point of an application\u2019s life cycle will be initially allocated\nto the CXL-node. If a page cache becomes hot enough to be selected\nas a promotion candidate, it will be eventually promoted to the\nlocal node. This policy helps applications with infrequent cache\naccesses run on a system with a small amount of local memory and\nlarge but cheap CXL-Memory while maintaining the performance.Table 1: TPP is e \uffffective over its counterparts. It reduces mem-\nory access latency and improves application throughput.\nWorkload/Throughput (%)\n(normalized to Baseline)Default\nLinuxTPPNUMA\nBalancingAutoTiering\nWeb1 (2:1) 83.5 99.5 82.8 87.0\nCache1 (2:1) 97.0 99.9 93.7 92.5\nCache1 (1:4) 86.0 99.5 90.0 Fails\nCache2 (2:1) 98.0 99.6 94.2 94.6\nCache2 (1:4) 82.0 95.0 78.0 Fails\nData Warehouse (2:1) 99.3 99.5 \u2013 \u2013\n5.5 Observability into TPP to Assess\nPerformance\nPromotion- and demotion-related statistics can help better under-\nstand the e \uffffectiveness of the page placement mechanism and de-\nbug issues in production environments. To this end, we introduce\nmultiple counters to track di \ufffferent demotion and promotion re-\nlated events and make them available in the user-space through\n/proc/vmstat interface.\nTo characterize the demotion mechanism, we introduce counters\nto track the distribution of successfully demoted anon and \uffffle pages.", "start_char_idx": 188804, "end_char_idx": 192488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf71de70-e0dd-4720-9836-00a3a285651a": {"__data__": {"id_": "bf71de70-e0dd-4720-9836-00a3a285651a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fae38f8c-f20d-43f8-8a84-ab06b6ee8e72", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "352f0314e24f03c0f37381f28f2d7e37a8742f96f2721afd0542b77e1da7a4d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba191825-e457-4f48-8512-4404c8d3d07d", "node_type": "1", "metadata": {}, "hash": "fbab17da3dbe5ec87b2d6d2ae151a55217b53c703e1eaacb8e9ca649ce1ea4e7", "class_name": "RelatedNodeInfo"}}, "text": "To this end, we introduce\nmultiple counters to track di \ufffferent demotion and promotion re-\nlated events and make them available in the user-space through\n/proc/vmstat interface.\nTo characterize the demotion mechanism, we introduce counters\nto track the distribution of successfully demoted anon and \uffffle pages.\nTo understand the promotion behavior, we add new counters to\ncollect information on the numbers of sampled pages, the number\nof promotion attempts, and the number of successfully promoted\npages for each of the memory types.\nTo track the ping-pong issue mentioned in \u00a7 5.3, we utilize the\nunused 0x40 bit in the page \uffffag to introduce PG_demoted \uffffag for\ndemoted pages. Whenever a page is demoted its PG_demoted bit is\nset and gets cleared upon promotion. We also count the number of\ndemoted pages that become promotion candidates. High value of\nthis counter means TPP is thrashing across NUMA nodes. We add\ncounters for each of the promotion failure scenario (e.g., local node\nhaving low memory, abnormal page references, system-wide low\nmemory, etc.) to reason about where and how promotion fails.\n6 EVALUATION\nWe integrate TPP on Linux kernel v5.12. We evaluate TPP with a\nsubset of representable production applications mentioned in \u00a7 3.1\nserving live tra \uffffc on tiered memory systems across our server \uffffeet.\nWe explore the following questions:\n\u2022How e \uffffective TPP is in distributing pages across memory tiers\nand improving application performance? (\u00a7 6.1)\n\u2022What are the impacts of TPP components? (\u00a7 6.2)\n\u2022How it performs against state-of-the-art solutions? (\u00a7 6.3)\nFor each experiment, we use application-level throughput as the\nmetric for performance. In addition, we use the fraction of memory\naccesses served from the local node as the key low-level supporting\nmetric. We compare TPP against default Linux (\u00a7 6.1), default NUMA\nBalancing [ 22], AutoTiering [ 47], and TMO [ 73] (\u00a76.3) (Table 1).\nNone of our experiments involve swapping to disks, the whole\nsystem has enough memory to support the workload. We use the\ndefault local-node \uffffrst, then CXL-node allocation policy for Linux.\nExperimental Setup. We deploy a number of pre-production\nx86 CPUs with FPGA-based CXL-Memory expansion card that\nsupport CXL 1.1 speci \uffffcation. Memory attached to the expansion\ncard shows up to the OS as a CPU-less NUMA node. Although our\n750ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.\n00.20.40.60.81\n0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(a) Web100.20.40.60.81\n0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(b) Cache100.20.40.60.81\n0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(c) Cache200.20.40.60.81\n0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(d) Data Warehouse\nFigure 14: Fast cold page demotion and e \uffffective hot page promotion allow TPP to serve most of the tra \uffffcs from the local node.\ncurrent FPGA-based CXL cards have around 250ns higher latency\nthan our eventual target, we use them for the functional validation.\nWe have con \uffffrmation from two major x86 CPU vendors that the\naccess latency to CXL-Memory is similar to the remote latency on\na dual-socket system. For performance evaluation, we primarily\nuse dual-socket systems and con \uffffgure them to mimic our target\nCXL-enabled system\u2019s characteristics (one memory node with all\nactive CPU cores and one CPU-less memory node) according to the\nguidance of our CPU vendors. For baseline, we disable the memory\nnode and CPU cores on a socket while enabling su \uffffcient memory\non another socket. Here, single memory node serves the whole\nworking set.\nWe use two memory con \uffffgurations where the ratio of local node\nand CXL-Memory capacity is (a)2:1 (\u00a7 6.1.1) and (b)1:4 (\u00a7 6.1.2).", "start_char_idx": 192180, "end_char_idx": 195919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba191825-e457-4f48-8512-4404c8d3d07d": {"__data__": {"id_": "ba191825-e457-4f48-8512-4404c8d3d07d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf71de70-e0dd-4720-9836-00a3a285651a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f2269fa66abe339766d9aac2755675722fadd098cb705d46d14571c864624e90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eed987cc-15d5-45e8-b75f-bcd083e6bd52", "node_type": "1", "metadata": {}, "hash": "88f36e9ac46104ec690dd39a43448cc202fbb99328ed06c99e77fc853172b5f3", "class_name": "RelatedNodeInfo"}}, "text": "We have con \uffffrmation from two major x86 CPU vendors that the\naccess latency to CXL-Memory is similar to the remote latency on\na dual-socket system. For performance evaluation, we primarily\nuse dual-socket systems and con \uffffgure them to mimic our target\nCXL-enabled system\u2019s characteristics (one memory node with all\nactive CPU cores and one CPU-less memory node) according to the\nguidance of our CPU vendors. For baseline, we disable the memory\nnode and CPU cores on a socket while enabling su \uffffcient memory\non another socket. Here, single memory node serves the whole\nworking set.\nWe use two memory con \uffffgurations where the ratio of local node\nand CXL-Memory capacity is (a)2:1 (\u00a7 6.1.1) and (b)1:4 (\u00a7 6.1.2).\nCon\uffffguration (a)is similar to our current CXL-enabled production\nsystems where local node is supposed to serve all the hot working\nset. We use con \uffffguration (b)to stress-test TPP on a constrained\nmemory scenario \u2013 only a fraction of the total hot working set\ncan\ufffft on the local node and hot pages are forced to move to the\nCXL-node.\n6.1 E \uffffectiveness of TPP\n6.1.1 Default Production Environment (2:1 Configuration). Web.\nWeb1 performs lots of \uffffle I/O during initialization and \ufffflls up the\nlocal node. Default Linux is 44\u21e5slower than TPP during freeing up\nthe local node. As a result, new allocation to the local node halts\nand anons get allocated to the CXL-node and stay there forever. In\ndefault Linux, on average, only 22% of total memory accesses are\nserved from the local node (Figure 14a). As a result, throughput\ndrops by 16.5%.\nActive and faster demotion helps TPP move colder \uffffle pages to\nCXL-node and allow more anon pages to be allocated in the local\nnode. Here, 92% of the total anon pages are served from the local\nnode. As a result, local node serves 90% of total memory accesses.\nThroughput drop is only 0.5%.\nCache. Cache applications maintain a steady ratio of anon and\n\uffffle pages throughout their life-cycle. Almost all the anon pages get\nallocated to the local node from the beginning and served from there.\nFor Cache1, with default Linux, only 8% of the total hot pages are\ntrapped in CXL-node. As a result, application performance remains\nvery close to the baseline \u2013 performance regression is only 3%. TPP\ncan even minimize this performance gap (throughput is 99.9% of\nthe baseline) by promoting all the trapped hot \uffffles and improving\nthe fraction of tra \uffffc served from the local node (Figure 14b).00.20.40.60.81\n0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(a) Cache100.20.40.60.81\n0200400600800Local Traffic (%)Time (minute)All LocalTPPDefault(b) Cache2\nFigure 15: E \uffffectiveness of TPP under memory constraint.\nAlthough most of the Cache2\u2019s anon pages reside in the local\nnode on a default Linux kernel, all of them are not always hot \u2013 only\n75% of the total anon pages remain hot within a two-minute interval.\nTPP can e \uffffciently detect the cold anon pages and demote them to\nthe CXL-node. This allows the promotion of more hot \uffffle pages.\nOn default Linux, local node serves 78% of the memory accesses\n(Figure 14c) and cause 2% throughput regression. TPP improves the\nfraction of local tra \uffffc to 91%. Throughput regression is only 0.4%.\nData Warehouse. This workload generates \uffffle caches to store\nthe intermediate processing data. File caches mostly remain cold.\nBesides, only one third of the total anon pages remain hot. Our\ndefault production con \uffffguration is good enough to serve all the\nhot memory from the local node. Default Linux and TPP perform\nthe same (0.5\u20130.7% throughput drop).\nTPP improves the fraction of local tra \uffffc by moving relatively\nhotter anon pages to the local node.", "start_char_idx": 195210, "end_char_idx": 198850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eed987cc-15d5-45e8-b75f-bcd083e6bd52": {"__data__": {"id_": "eed987cc-15d5-45e8-b75f-bcd083e6bd52", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba191825-e457-4f48-8512-4404c8d3d07d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "b1655a24b44cc7a2937ebf13aa426c3e943ecb6af19c41419bec424225035c05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51e4fc95-119f-4c1e-ab3c-dbc6f1d9e016", "node_type": "1", "metadata": {}, "hash": "9bcf97186bbb926a1a5ab8a2759d6d55d0ef04470f32ce1ca5d4bd2d07583674", "class_name": "RelatedNodeInfo"}}, "text": "This allows the promotion of more hot \uffffle pages.\nOn default Linux, local node serves 78% of the memory accesses\n(Figure 14c) and cause 2% throughput regression. TPP improves the\nfraction of local tra \uffffc to 91%. Throughput regression is only 0.4%.\nData Warehouse. This workload generates \uffffle caches to store\nthe intermediate processing data. File caches mostly remain cold.\nBesides, only one third of the total anon pages remain hot. Our\ndefault production con \uffffguration is good enough to serve all the\nhot memory from the local node. Default Linux and TPP perform\nthe same (0.5\u20130.7% throughput drop).\nTPP improves the fraction of local tra \uffffc by moving relatively\nhotter anon pages to the local node. With TPP, 94% of the total\nanon pages reside on the local node, while the default kernel hosts\nonly 67%. To make space for the hot anon pages, cold \uffffle pages\nare demoted to the CXL-node. TPP allows 4% higher local node\nmemory accesses over Linux (Figure 14d).\n6.1.2 Large Memory Expansion with CXL (1:4 Configuration). Ex-\ntreme setups like 1:4 con \uffffguration allow \uffffexible server design with\nDRAM as a small-sized local node and CXL-Memory as a large\nbut cheaper memory. As in our production, such a con \uffffguration is\nimpractical for Web and Data Warehouse, we limit our discussion\nto the Cache applications. Note that TPP is e \uffffective even for Web\nand Data Warehouse in such a setup and performs very close to\nthe baseline.\nCache1. In a 1:4 con \uffffguration, on a default Linux kernel, \uffffle\npages consume almost all the local node\u2019s capacity. 85% of the total\nanon pages get trapped to the CXL-node and throughput drops by\n14%. Because of the apt promotion, TPP can move almost all the\nCXL-node\u2019s hot anon pages (97% of the total hot anon pages within\n751TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada\n01020304050\n220240260280300Increasein Avg. Memory Latency (ns)CXL-Memory Latency (ns)DefaultTPP\n(a) Avg. Memory Access Latency0123456\n220240260280300Throughput Loss (%)CXL-Memory Latency (ns)DefaultTPP\n(b) Impact on Throughput\nFigure 16: TPP bene \uffffts CXL-node with varied latency traits.\n05001000150020002500\n050010001500Allocation Rate (MB/s)Time (minute)w/ Decouplingw/o Decoupling(a) New Allocation0.0010.010.1110100100010000\n050010001500Promotion Rate (KB/s)Time (minute)w/ Decouplingw/o Decoupling\n(b) Promotion to Toptier Node\nFigure 17: Impact of decoupling allocation and reclamation.\na minute) to the local node. This forces less latency-sensitive \uffffle\npages to be demoted to the CXL-node. Eventually, TPP stabilizes\nthe tra \uffffc between the two nodes and local node serves 85% of\nthe total memory accesses. This helps Cache1 achieve the baseline\nperformance \u2013 even though the local node\u2019s capacity is only 20% of\nthe working set size, throughput regression is only 0.5% (Figure 15a).\nCache2. Similar to Cache1, on default Linux, Cache2 experiences\n18% throughput loss. Only 14% of the anon pages remain on the\nlocal node (Figure 15b). TPP can bring back almost all the remote\nhot anon pages (80% of the total anon pages) to local node while\ndemoting the cold ones to the CXL-node. As Cache2 accesses lots\nof caches, and caches are now mostly in CXL-node, 41% of the\nmemory tra \uffffc comes from the CXL-node. Yet, throughput drop is\nonly 5% with TPP.\n6.1.3 TPP with Varied CXL-Memory Latencies. The variation in\nCXL-Memory\u2019s access latency does not impact TPP much.", "start_char_idx": 198150, "end_char_idx": 201585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51e4fc95-119f-4c1e-ab3c-dbc6f1d9e016": {"__data__": {"id_": "51e4fc95-119f-4c1e-ab3c-dbc6f1d9e016", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eed987cc-15d5-45e8-b75f-bcd083e6bd52", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "190b46330d4ab87aa41d80e01abee1456072b85791da09502ffc0b6a64296bad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f30fca24-6fc0-4279-86a4-2dd10e29cc95", "node_type": "1", "metadata": {}, "hash": "0d29e0865832ff82936f285dfe08c0db37820ab5be8411828fd3af302b551f17", "class_name": "RelatedNodeInfo"}}, "text": "Cache2. Similar to Cache1, on default Linux, Cache2 experiences\n18% throughput loss. Only 14% of the anon pages remain on the\nlocal node (Figure 15b). TPP can bring back almost all the remote\nhot anon pages (80% of the total anon pages) to local node while\ndemoting the cold ones to the CXL-node. As Cache2 accesses lots\nof caches, and caches are now mostly in CXL-node, 41% of the\nmemory tra \uffffc comes from the CXL-node. Yet, throughput drop is\nonly 5% with TPP.\n6.1.3 TPP with Varied CXL-Memory Latencies. The variation in\nCXL-Memory\u2019s access latency does not impact TPP much. We run\nCache2 with 2:1 con \uffffguration where CXL-Memory has di \ufffferent\nlatency characteristics (Figure 16). In all cases, due to TPP\u2019s bet-\nter hot page identi \uffffcation and e \uffffective promotion, only a small\nportion (4\u20135%) of hot pages are served from the CXL-node. On the\nother hand, for default Linux, 22\u201325% hot pages remain trapped in\nthe CXL-node. This increases the average memory access latency\nby7\u21e5for default Linux (Figure 16a). This, eventually, impact the\napplication-level performance, default Linux experiences 2.2\u00002.8\u21e5\nhigher throughput loss over TPP (Figure 16b).\n6.2 Impact of TPP Components\nIn this section, we discuss the contribution of TPP components. As\na case study, we use Cache1 with 1:4 con \uffffguration.\nAllocation and Reclamation Decoupling. Without this fea-\nture, reclamation on local node triggers at a later phase. With high\nmemory pressure and delayed reclamation on local node, the bene-\n\ufffft of TPP disappears as it fails to promote CXL-node pages. Besides,00.20.40.60.8101020304050607080Memory Access (%)Time (minute)Local (Active LRU)CXL-Node (Active LRU)Local (Default)CXL-Node (Default)query startsmemory accesstrafficconvergessteepchange in traffic for defaultwarm upFigure 18: Restricting the promotion candidate based on their\nage reduces unnecessary promotion tra \uffffc.\nTable 2: Page-type aware allocation helps applications.\nApplication Con\uffffguration% of Memory Access Tra \uffffcThroughput\nw.r.t Baseline Local Node CXL-node\nWeb1 2:1 97% 3% 99.5%\nCache1 1:4 85% 15% 99.8%\nCache2 1:4 72% 28% 98.5%\n00.20.40.60.81\n0200400600800Local Traffic (%)Time (minute)TPPNUMA BalancingAutoTiering(a) Web1 on 2:1 Con \uffffguration00.20.40.60.81\n0200400600800Local Traffic (%)Time (minute)TPPNuma BalancingAutoTiering (2:1)(b) Cache1 on 1:4 Con \uffffguration\nFigure 19: TPP outperforms existing page placement mecha-\nnism. Note that AutoTiering can\u2019t run on 1:4 con \uffffguration\nFor Cache1, TPP on 1:4 con \uffffguration performs better than\nAutoTiereing on 2:1.\nnewly allocated pages are often short-lived (less than a minute life-\ntime) and de-allocated even before being selected as a promotion\ncandidate. Trapped CXL-node hot pages worsen the performance.\nWithout the decoupling feature, allocation maintains a steady\nrate that is controlled by the rate of reclamation (Figure 17a). As a\nresult, any burst in allocations puts pages to the CXL-node. When\nallocation and reclamation is decoupled, TPP allows more pages on\nthe local node \u2013 allocation rate to local node increases by 1.6\u21e5at\nthe95C\u2318percentile.\nAs local node is always under memory pressure, new allocations\nconsume freed up pages instantly and promotion fails as the target\nnode becomes low on memory after serving allocation requests.\nFor this reason, without the decoupling feature, promotion almost\nhalts most of the time (Figure 17b).", "start_char_idx": 201008, "end_char_idx": 204385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f30fca24-6fc0-4279-86a4-2dd10e29cc95": {"__data__": {"id_": "f30fca24-6fc0-4279-86a4-2dd10e29cc95", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51e4fc95-119f-4c1e-ab3c-dbc6f1d9e016", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "897c3b117d076da78b229da6871e93424e2d6b73699d31ce98050ee87221193b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b845fca-806e-4a2d-a56e-adef1d1a801e", "node_type": "1", "metadata": {}, "hash": "56558a0874ff149cf522e66657ac964cf68efabd4d72675e3441f288411a5079", "class_name": "RelatedNodeInfo"}}, "text": "Trapped CXL-node hot pages worsen the performance.\nWithout the decoupling feature, allocation maintains a steady\nrate that is controlled by the rate of reclamation (Figure 17a). As a\nresult, any burst in allocations puts pages to the CXL-node. When\nallocation and reclamation is decoupled, TPP allows more pages on\nthe local node \u2013 allocation rate to local node increases by 1.6\u21e5at\nthe95C\u2318percentile.\nAs local node is always under memory pressure, new allocations\nconsume freed up pages instantly and promotion fails as the target\nnode becomes low on memory after serving allocation requests.\nFor this reason, without the decoupling feature, promotion almost\nhalts most of the time (Figure 17b). Trapped pages on the CXL-\nnode generates 55% of the memory tra \uffffc which leads to a 12%\nthroughput drop. With the decoupling feature, promotion maintains\na steady rate of 50KB/s on average. During the surge in remote\nmemory usage, the promotion goes as high as 1.2MB/s in the 99C\u2318\npercentile. This helps TPP to maintain the throughput of baseline.\nActive LRU-Based Hot Page Detection. Considering active\nLRU pages as promotion candidates helps TPP add hysteresis to\npage promotion. This reduces the page promotion rate by 11\u21e5. The\nnumber of demoted pages that subsequently get promoted is also\nreduced by 50%. Although the demotion rate drops by 4%, as we are\nnot allowing unnecessary promotion tra \uffffcs to waste local memory\nnode, now there are more e \uffffective free pages in local node. As a\n752ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.\nTable 3: TMO enhances TPP\u2019s memory reclamation process\nand improves page migration by generating more free space.\nWeb1 on 2:1 Con \uffffguration TPP-only TPP with TMO\nMigration Failure Rate (pages/sec) 20 5\nCXL-node\u2019s Memory Tra \uffffc (%) 3.1% 2.7%\nTable 4: TPP improves TMO by e \uffffectively turning the swap\naction into a two-stage demote-then-swap process.\nWeb1 on 2:1 Con \uffffguration TMO-only TMO with TPP\nProcess Stall (Normalized to Threshold) 70% 40%\nMemory Saving (% of Total Capacity) 13.5% 16.5%\nresult, promotion success rate improves by 48%. Thus, reduced but\nsuccessful promotions provide enough free spaces in local node for\nnew allocations and improve the local node\u2019s memory access by 4%.\nThroughput also improves by 2.4%. The time requires to converge\nthe tra \uffffc across memory tiers is almost similar \u2013 to reach the peak\ntra\uffffc on local node, TPP with active LRU-based promotion takes\nextra \uffffve minutes (Figure 18).\nCache Allocation to Remote Node Policy. For Web and Cache,\npreferring the \uffffle cache allocation to CXL-node can provide all-\nlocal performances even with a small-sized local node (Table 2).\nTPP is e \uffffcient enough to keep most of the e \uffffective hot pages on\nthe local node. Throughput drop over the baseline is only 0.2\u20132.5%.\n6.3 Comparison Against Existing Solutions\nWe compare TPP against Linux\u2019s default NUMA Balancing, Au-\ntoTiering, and TMO. We use Web1 and Cache1, two representative\nworkloads of two di \ufffferent service domains, and evaluate them on\ntarget production setup (2:1 con \uffffguration) and memory-expansion\nsetup (1:4 con \uffffguration), respectively. We omit Data Warehouse\nas it does not show any signi \uffffcant performance drop even with\ndefault Linux (\u00a7 6.1).\n6.3.1 TPP against NUMA Balancing and AutoTiering. Web1. NUMA\nBalancing helps when the reclaim mechanism can provide with\nenough free pages for promotion. When the local node is low on\nmemory, NUMA Balancing stops promoting pages and performs\neven worse than default Linux because of its extra scanning and\nfailed promotion tries. Due to 42\u21e5slower reclamation rate than\nTPP, NUMA Balancing experiences 11\u21e5slower promotion rate.", "start_char_idx": 203690, "end_char_idx": 207366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b845fca-806e-4a2d-a56e-adef1d1a801e": {"__data__": {"id_": "6b845fca-806e-4a2d-a56e-adef1d1a801e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f30fca24-6fc0-4279-86a4-2dd10e29cc95", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "83b34114c0f1ce0ee684034fe3abde609f3120e18d0156e1d0361cca91e5c658", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cd57eaf-b137-471c-ad3d-1992d042c545", "node_type": "1", "metadata": {}, "hash": "e761681d6c177ba13663cb0ea02c5d0dcfb6e32c8273751f85a8ab69433c199f", "class_name": "RelatedNodeInfo"}}, "text": "We omit Data Warehouse\nas it does not show any signi \uffffcant performance drop even with\ndefault Linux (\u00a7 6.1).\n6.3.1 TPP against NUMA Balancing and AutoTiering. Web1. NUMA\nBalancing helps when the reclaim mechanism can provide with\nenough free pages for promotion. When the local node is low on\nmemory, NUMA Balancing stops promoting pages and performs\neven worse than default Linux because of its extra scanning and\nfailed promotion tries. Due to 42\u21e5slower reclamation rate than\nTPP, NUMA Balancing experiences 11\u21e5slower promotion rate.\nLocal node can serve only 20% of the memory tra \uffffc (Figure 19a).\nAs a result, throughput drops by 17.2%. Due to the unnecessary\nsampling, its system-wide CPU overhead is 2% higher that TPP.\nAutoTiering has a faster reclamation mechanism \u2013 it migrates\npages with low access frequencies to CXL-node. However, with a\ntightly-coupled allocation-reclamation path, it maintains a \uffffxed-\nsize bu \uffffer to support promotion under pressure. This reserved\nbu\uffffer eventually \ufffflls up during a surge in CXL-node page accesses.\nAt that point, AutoTiering also fails to promote pages and end up\nserving 70% of the tra \uffffc from the CXL-node. Throughput drops\nby 13% from the baseline.\nNote that TPP experiences only a 0.5% throughput drop.\nCache1. In 1:4 con \uffffguration, with more memory pressure on\nlocal node, NUMA Balancing e \uffffectively stops promoting pages.\nOnly 46% of the tra \uffffcs are accessed from the local node (Figure 19b).\nThroughput drops by 10%.We can not setup AutoTiering with 1:4 con \uffffguration. It fre-\nquently crashes right after the warm up phase, when query \uffffres.\nWe run Cache1 with AutoTiering on 2:1 con \uffffguration. TPP out\nperforms AutoTiering even with a 46% smaller local node \u2013 TPP\ncan serve 10% more tra \uffffc from local node and provides 7% better\nthroughput over AutoTiering.\n6.3.2 Comparison between TPP and TMO. TPP vs. TMO. TMO\nmonitors application stalls during execution time because of in-\nsu\uffffcient system resources (CPU, memory, and IO). Based on the\npressure stall information (PSI), it decides on the amount of memory\nthat needs to be o \uffffoaded to the swap space. As mentioned in \u00a7 4,\nusing TMO for CXL-Memory\u2019s (z)swap-based abstraction is beyond\nour design goal. For the sake of argument, if we con \uffffgure CXL-\nMemory as a swap-space for TMO, it will be only populated during\nreclamation. New page allocation can never happen there. Besides,\nwithout any fast promotion mechanism, aggressive reclamation\ncan hurt application\u2019s performance; especially, when reclaimed\npages are re-accessed through costly swap-ins. As a result, TMO\nthrottles and can not populate most of the CXL-Memory capacity.\nFor Web1, Cache1, and Data Warehouse in 2:1 con \uffffguration, TMO\ncan only consume 45%, 61%, and 7% of the CXL-Memory capacity,\nrespectively. On the other hand, TPP can use CXL-Memory for both\nallocation and reclamation purposes. For same applications, TPP\u2019s\nCXL-Memory usage is 83%, 92%, and 87%, respectively.\nTPP with TMO. We run TMO with TPP and observe they are\northogonal and augment each other\u2019s behavior for Web1 on 2:1\ncon\uffffguration. TPP keeps most hot pages in local node; it gets\nslightly better when TMO is enabled (Table 3).TMO creates more\nfree memory space in the system by swapping out cold pages both\nfrom local and CXL-Memory nodes. The presence of some memory\nheadroom in the system makes it easier for TPP to move pages\naround and leads to fewer page migration failures. As TPP-driven\nmigration fails less frequently, page placement is more optimized,\nresulting in even fewer accesses to the CXL-node.", "start_char_idx": 206831, "end_char_idx": 210388, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cd57eaf-b137-471c-ad3d-1992d042c545": {"__data__": {"id_": "9cd57eaf-b137-471c-ad3d-1992d042c545", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b845fca-806e-4a2d-a56e-adef1d1a801e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a1869cae6ed37dd474cff7d0507e7c10ee12b12e6ea4e34d8f7239ccbd1c8d33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f02924f-892d-4188-98d1-f076d9da3d7b", "node_type": "1", "metadata": {}, "hash": "0f560f7ef78db84a0d977c819b628c726617bc113e06a14aa94a980e263cdf9e", "class_name": "RelatedNodeInfo"}}, "text": "On the other hand, TPP can use CXL-Memory for both\nallocation and reclamation purposes. For same applications, TPP\u2019s\nCXL-Memory usage is 83%, 92%, and 87%, respectively.\nTPP with TMO. We run TMO with TPP and observe they are\northogonal and augment each other\u2019s behavior for Web1 on 2:1\ncon\uffffguration. TPP keeps most hot pages in local node; it gets\nslightly better when TMO is enabled (Table 3).TMO creates more\nfree memory space in the system by swapping out cold pages both\nfrom local and CXL-Memory nodes. The presence of some memory\nheadroom in the system makes it easier for TPP to move pages\naround and leads to fewer page migration failures. As TPP-driven\nmigration fails less frequently, page placement is more optimized,\nresulting in even fewer accesses to the CXL-node.\nTMO is still able to save memory without noticeable performance\noverhead, as shown in Table 4.This is because TPP makes (z)swap a\ntwo-stage process \u2013 TMO-driven reclamation in local node will \uffffrst\ndemote victim pages to CXL-Memory before getting (z)swapped out\neventually. This improves victim page selection process \u2013 semi-hot\npages now get a second chance for staying in local memory when\ndrifting down CXL-node\u2019s LRU list. As a result, TPP reduces the\namount of process stall in TMO originated from major page faults\n(i.e., memory and IO pressure in [ 73]) by 30%. As TMO throttles\nitself based on the process stall metric, this improves memory saving\nby 3% (2GB in absolute terms).\n7 DISCUSSION AND FUTURE RESEARCH\nTPP makes us production-ready to onboard our \uffffrst generation of\nCXL-enabled tiered-memory system. We, however, foresee research\nopportunities with technology evolution.\nTiered Memory for Multi-tenant Clouds. In a typical cloud,\nwhen multiple tenants co-exist on a single host machine, TPP can\ne\uffffectively enable them to competitively share di \ufffferent memory\ntiers. When local memory size dominates the total memory capac-\nity of the system, this may not cause much problem. However, if\n753TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada\napplications with di \ufffferent priorities have di \ufffferent QoS require-\nments, TPP may provide sub-optimal performance. Integrating a\nwell-designed QoS-aware memory management mechanism over\nTPP can address this problem.\nAllocation Policy for Memory Bandwidth Expansion. For\nmemory bandwidth-bound applications, CPU to DRAM memory\nbandwidth often becomes the bottleneck. CXL\u2019s additional memory\nbandwidth can help by spreading memory across the top-tier and\nremote node. Instead of only placing cold pages into CXL-Memory,\nwhich draw very low bandwidth consumption, the optimal solu-\ntion should place the right amount of bandwidth-heavy, latency-\ninsensitive pages to CXL-Memory. The methodology to identify\nthe ideal fraction of such working sets may even require hardware\nsupport. We want to explore transparent memory management for\nmemory bandwidth-expansion use case in our future work.\nHardware Support for E \uffffective Page Placement. Hardware\nfeatures can further enhance performance of TPP. A memory-side\ncache and its associated prefetcher on the CXL ASIC might help\nreduce the e \uffffective latency of CXL-Memory. Hardware support\nfor data movement between memory tiers can help reduce page\nmigration overheads. While in our environment we do not see\na high migration overheads, others may chose to put provision\nsystems more aggressively with very small amount of local memory\nand high amount of CXL-Memory. For our use cases, in steady state,\nthe migration bandwidth is 4\u201316 MB/s (1\u20134K pages/second) which\nis far lower than CXL link bandwidth and also unlikely to cause\nany meaningful CPU overhead due to page movement.\n8 RELATED WORK\nTiered Memory System. With the emergence of low-latency non-\nDDR technologies, heterogeneous memory systems are becoming\npopular. There have been signi \uffffcant e \ufffforts in using NVM to ex-\ntend main memory [ 5,31,37,38,45,46,58,60,63].", "start_char_idx": 209610, "end_char_idx": 213579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f02924f-892d-4188-98d1-f076d9da3d7b": {"__data__": {"id_": "8f02924f-892d-4188-98d1-f076d9da3d7b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cd57eaf-b137-471c-ad3d-1992d042c545", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "10348ef0ff7b9157199bcc884eaf729a20847a3d7063601dd6a29852dabbf920", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "677ff985-63dc-4501-92d1-70a7d67bc0da", "node_type": "1", "metadata": {}, "hash": "32aaef09223f88191340d4f5edd8292d3d4b0d8da19ca054643bf53963121b5e", "class_name": "RelatedNodeInfo"}}, "text": "Hardware support\nfor data movement between memory tiers can help reduce page\nmigration overheads. While in our environment we do not see\na high migration overheads, others may chose to put provision\nsystems more aggressively with very small amount of local memory\nand high amount of CXL-Memory. For our use cases, in steady state,\nthe migration bandwidth is 4\u201316 MB/s (1\u20134K pages/second) which\nis far lower than CXL link bandwidth and also unlikely to cause\nany meaningful CPU overhead due to page movement.\n8 RELATED WORK\nTiered Memory System. With the emergence of low-latency non-\nDDR technologies, heterogeneous memory systems are becoming\npopular. There have been signi \uffffcant e \ufffforts in using NVM to ex-\ntend main memory [ 5,31,37,38,45,46,58,60,63]. CXL enables an\nintermediate memory tier with DRAM-like low-latency in the hier-\narchy and brings a paradigm shift in \uffffexible and performant server\ndesign. Industry leaders are embracing CXL-enabled tiered memory\nsystem in their next-generation datacenters [ 1,4,9,10,21,24,25].\nPage Placement for Tiered Memory. Prior work explored\nhardware-assisted [ 57,59,62] and application-guided [ 20,25,38,72]\npage placement for tiered memory systems, which may not often\nscale to datacenter use cases as they require hardware support or\napplication redesign from the ground up.\nApplication-transparent page placement approaches often pro-\n\uffffle an application\u2019s physical [ 29,31,36,45,48,78] or virtual address-\nspace [ 53,63,75,77] to detect page temperature. This causes high\nperformance-overhead because of frequent invocation of TLB in-\nvalidations or interrupts. We \uffffnd existing in-kernel LRU-based\npage temperature detection is good enough for CXL-Memory. Prior\nstudy also explored machine learning directed decisions [ 36,48],\nuser-space APIs [ 53,63], and swapping [ 31,48] to move pages\nacross the hierarchy, which are either resource or latency intensive.\nIn-memory swapping [ 8,13,26,30] can be used to swap-out\ncold pages to CXL-node. In such cases, CXL-node access requires\npage-fault and swapped-out pages are immediately brought back\nto main memory when accessed. This makes in-memory swapping\nine\uffffective for workloads that access pages at varied frequencies.When CXL-Memory is a part of the main memory, less frequently\naccessed pages can be on CXL-node without any page-fault over-\nhead upon access.\nSolutions considering NVM to be the slow memory tier [ 28,43,\n47,76] are conceptually close to our work. Nimble [ 76] is optimized\nfor huge page migrations. During migration, it employs page ex-\nchange between memory tiers. This worsens the performance as a\ndemotion needs to wait for a promotion in the critical path. Sim-\nilar to TPP, AutoTiereing [ 47] and work from Huang et al. [ 28]\nuse background migration for demotion and optimized NUMA bal-\nancing [ 22] for promotion. However, their timer-based hot page\ndetection causes computation overhead and is often ine \uffffcient, es-\npecially when pages are infrequently accessed. Besides, none of\nthem consider decoupling allocation and reclamation paths. Our\nevaluation shows, this is critical for memory-bound applications to\nmaintain their performance under memory pressure.\nDisaggregated Memory. Memory disaggregation exposes ca-\npacity available in remote hosts as a pool of memory shared among\nmany machines. Most recent memory disaggregation e \ufffforts [ 32\u2013\n34,40,42,51,55,56,64,66,71] are speci \uffffcally designed for RDMA\nover In \uffffniBand or Ethernet networks where latency characteris-\ntics are orders-of-magnitude higher than CXL-Memory. Memory\nmanagements of these systems are orthogonal to TPP\u2013 one can\nuse both CXL- and network-enabled memory tiers and apply TPP\nand memory disaggregation solutions to manage memory on the\nrespective tiers.\n9 CONCLUSION\nWe analyze datacenter applications\u2019 memory usage behavior using\nChameleon, a lightweight and robust user-space working set char-\nacterization tool, to \uffffnd the scope of CXL-enabled tiered-memory\nsystem.", "start_char_idx": 212824, "end_char_idx": 216798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "677ff985-63dc-4501-92d1-70a7d67bc0da": {"__data__": {"id_": "677ff985-63dc-4501-92d1-70a7d67bc0da", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f02924f-892d-4188-98d1-f076d9da3d7b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "46d2eeb6cb5b1123cf5862d91ba3256a5e17759a92754182fd1210ce336f1b0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a111e300-1b3a-4824-bcb1-959aa96ef9e2", "node_type": "1", "metadata": {}, "hash": "e4be52ee4860db025a23931ddc6182632c2de283f063c489f7036f53e26c5b2e", "class_name": "RelatedNodeInfo"}}, "text": "Disaggregated Memory. Memory disaggregation exposes ca-\npacity available in remote hosts as a pool of memory shared among\nmany machines. Most recent memory disaggregation e \ufffforts [ 32\u2013\n34,40,42,51,55,56,64,66,71] are speci \uffffcally designed for RDMA\nover In \uffffniBand or Ethernet networks where latency characteris-\ntics are orders-of-magnitude higher than CXL-Memory. Memory\nmanagements of these systems are orthogonal to TPP\u2013 one can\nuse both CXL- and network-enabled memory tiers and apply TPP\nand memory disaggregation solutions to manage memory on the\nrespective tiers.\n9 CONCLUSION\nWe analyze datacenter applications\u2019 memory usage behavior using\nChameleon, a lightweight and robust user-space working set char-\nacterization tool, to \uffffnd the scope of CXL-enabled tiered-memory\nsystem. To realize such a system, we design TPP, an OS-level trans-\nparent page placement mechanism that works without any prior\nknowledge on applications\u2019 memory access behavior. We evaluate\nTPP using diverse production workloads and \uffffnd TPP improves\napplication\u2019s performance on default Linux by 18%. TPP also out-\nperforms NUMA Balancing and AutoTiering, two state-of-the-art\ntiered-memory management mechanisms, by 5\u201317%.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers for insightful feedback that\nhelped improve the paper. Hasan Al Maruf and Mosharaf Chowd-\nhury were partly supported by National Science Foundation grants\n(CNS-1845853, CNS-2104243) and gifts from VMware and Meta.\nREFERENCES\n[1]A Milestone in Moving Data. https://newsroom.intel.com/editorials/milestone-\nmoving-data/ .\n[2]Alibaba Cluster Trace 2018. https://github.com/alibaba/clusterdata/blob/master/\ncluster-trace-v2018/trace_2018.md .\n[3]AMD In \uffffnity Architecture. https://www.amd.com/en/technologies/in \uffffnity-\narchitecture .\n[4]AMD Joins Consortia to Advance CXL. https://community.amd.com/t5/amd-\nbusiness-blog/amd-joins-consortia-to-advance-cxl-a-new-high-speed-\ninterconnect/ba-p/418202 .\n[5]Baidu feed stream services restructures its in-memory database with intel optane\ntechnology. https://www.intel.com/content/www/us/en/customer-spotlight/\nstories/baidu-feed-stream-case-study.html .\n[6]CCIX. https://www.ccixconsortium.com/ .\n[7]Compute Express Link (CXL). https://www.computeexpresslink.org/ .\n754ASPLOS \u201923, March 25\u201329, 2023, Vancouver, BC, Canada Hasan Al Maruf et al.\n[8]Creating in-memory RAM disks. https://cloud.google.com/compute/docs/disks/\nmount-ram-disks .\n[9]CXL and the Tiered-Memory Future of Servers. https://www.lenovoxperience.\ncom/newsDetail/283yi044hzgcdv7snkrmmx9ovpq6aesmy9u9k7ai2648j7or .\n[10] CXL Roadmap Opens Up the Memory Hierarchy. https://www.nextplatform.\ncom/2021/09/07/the-cxl-roadmap-opens-up-the-memory-hierarchy/ .\n[11] DAMON: Data Access MONitoring Framework for Fun and Memory Management\nOptimizations. https://www.linuxplumbersconf.org/event/7/contributions/659/\nattachments/503/1195/damon_ksummit_2020.pdf .\n[12] Facebook and Amazon are causing a memory shortage. https:\n//www.networkworld.com/article/3247775/facebook-and-amazon-are-causing-\na-memory-shortage.html .\n[13] Frontswap. https://www.kernel.org/doc/html/latest/vm/frontswap.html .\n[14] Gen-Z. https://genzconsortium.org/ .\n[15] Google Cluster Trace 2019. https://github.com/google/cluster-data/blob/master/\nClusterData2019.md .\n[16] Idle Memory Tracking. https://www.kernel.org/doc/Documentation/vm/idle_\npage_tracking.txt .\n[17] Idle page tracking-based working set estimation. https://lwn.net/Articles/460762/ .\n[18] Intel \u00aeXeon \u00aeProcessor Scalable Family Technical Overview. https:\n//www.intel.com/content/www/us/en/developer/articles/technical/xeon-\nprocessor-scalable-family-technical-overview.html .", "start_char_idx": 216013, "end_char_idx": 219686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a111e300-1b3a-4824-bcb1-959aa96ef9e2": {"__data__": {"id_": "a111e300-1b3a-4824-bcb1-959aa96ef9e2", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "677ff985-63dc-4501-92d1-70a7d67bc0da", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "af2e9dcb8f86bc9780c925a2810fdc913d67f011a8bec3817ab324c43002ba92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56f47d77-b474-4ad6-b976-9c2c6ecf7241", "node_type": "1", "metadata": {}, "hash": "f7e6ef114d9314dfc91fe49f0b2607cb15743ea3db67a8ba6bf11e6f892f44e6", "class_name": "RelatedNodeInfo"}}, "text": "[12] Facebook and Amazon are causing a memory shortage. https:\n//www.networkworld.com/article/3247775/facebook-and-amazon-are-causing-\na-memory-shortage.html .\n[13] Frontswap. https://www.kernel.org/doc/html/latest/vm/frontswap.html .\n[14] Gen-Z. https://genzconsortium.org/ .\n[15] Google Cluster Trace 2019. https://github.com/google/cluster-data/blob/master/\nClusterData2019.md .\n[16] Idle Memory Tracking. https://www.kernel.org/doc/Documentation/vm/idle_\npage_tracking.txt .\n[17] Idle page tracking-based working set estimation. https://lwn.net/Articles/460762/ .\n[18] Intel \u00aeXeon \u00aeProcessor Scalable Family Technical Overview. https:\n//www.intel.com/content/www/us/en/developer/articles/technical/xeon-\nprocessor-scalable-family-technical-overview.html .\n[19] Introducing new product innovations for SAP HANA, Expanded AI collaboration\nwith SAP and more. https://azure.microsoft.com/en-us/blog/introducing-new-\nproduct-innovations-for-sap-hana-expanded-ai-collaboration-with-sap-and-\nmore/ .\n[20] Memkind. https://memkind.github.io/memkind/ .\n[21] Micron Exits 3DXPoint, Eyes CXL Opportunities. https://www.eetimes.com/\nmicron-exits-3d-xpoint-market-eyes-cxl-opportunities .\n[22] NUMA Balancing (AutoNUMA). https://mirrors.edge.kernel.org/pub/linux/\nkernel/people/andrea/autonuma/autonuma_bench-20120530.pdf .\n[23] OpenCAPI. https://opencapi.org/ .\n[24] Reimagining Memory Expansion for Single Socket Servers with CXL.\nhttps://www.computeexpresslink.org/post/cxl-consortium-upcoming-\nindustry-events .\n[25] Samsung Unveils Industry-First Memory Module Incorporating New CXL Inter-\nconnect Standard. https://news.samsung.com/global/samsung-unveils-industry-\n\uffffrst-memory-module-incorporating-new-cxl-interconnect-standard .\n[26] The zswap compressed swap cache. https://lwn.net/Articles/537422/ .\n[27] Tmpfs. https://www.kernel.org/doc/html/latest/ \ufffflesystems/tmpfs.html .\n[28] Top-tier memory management. https://lwn.net/Articles/857133/ .\n[29] Using DAMON for proactive reclaim. https://lwn.net/Articles/863753/ .\n[30] zram. https://www.kernel.org/doc/Documentation/blockdev/zram.txt .\n[31] N. Agarwal and T. F. Wenisch. Thermostat: Application-transparent page man-\nagement for two-tiered main memory. SIGPLAN , 2017.\n[32] M. K. Aguilera, N. Amit, I. Calciu, X. Deguillard, J. Gandhi, S. Novakovi \u0107, A. Ra-\nmanathan, P. Subrahmanyam, L. Suresh, K. Tati, R. Venkatasubramanian, and\nM. Wei. Remote regions: a simple abstraction for remote memory. In USENIX\nATC, 2018.\n[33] E. Amaro, C. Branner-Augmon, Z. Luo, A. Ousterhout, M. K. Aguilera, A. Panda,\nS. Ratnasamy, and S. Shenker. Can far memory improve job throughput? In\nEuroSys , 2020.\n[34] I. Calciu, M. T. Imran, I. Puddu, S. Kashyap, H. A. Maruf, O. Mutlu, and A. Kolli.\nRethinking software runtimes for disaggregated memory. In ASPLOS , 2021.\n[35] Y. Chen, I. B. Peng, Z. Peng, X. Liu, and B. Ren. ATMem: Adaptive data placement\nin graph applications on heterogeneous memories. In CGO , 2020.\n[36] T. D. Doudali, S. Blagodurov, A. Vishnu, S. Gurumurthi, and A. Gavrilovska. Kleio:\nA hybrid memory page scheduler with machine intelligence. In HPDC , 2019.\n[37] J. Du and Y. Li. Elastify cloud-native spark application with PMEM. Persistent\nMemory Summit, 2019.", "start_char_idx": 218927, "end_char_idx": 222147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56f47d77-b474-4ad6-b976-9c2c6ecf7241": {"__data__": {"id_": "56f47d77-b474-4ad6-b976-9c2c6ecf7241", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a111e300-1b3a-4824-bcb1-959aa96ef9e2", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "18606538ef7978dcd7f6046f6a0a641d86d305ed2e5b4f4f60f361633f76d6e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94081e9e-93f6-410a-9549-c877c44fd91f", "node_type": "1", "metadata": {}, "hash": "d0f629de364455cca3e242e97fd11c3fd72549538448ab052d3d02b56d96824f", "class_name": "RelatedNodeInfo"}}, "text": "Rethinking software runtimes for disaggregated memory. In ASPLOS , 2021.\n[35] Y. Chen, I. B. Peng, Z. Peng, X. Liu, and B. Ren. ATMem: Adaptive data placement\nin graph applications on heterogeneous memories. In CGO , 2020.\n[36] T. D. Doudali, S. Blagodurov, A. Vishnu, S. Gurumurthi, and A. Gavrilovska. Kleio:\nA hybrid memory page scheduler with machine intelligence. In HPDC , 2019.\n[37] J. Du and Y. Li. Elastify cloud-native spark application with PMEM. Persistent\nMemory Summit, 2019.\n[38] S. R. Dulloor, A. Roy, Z. Zhao, N. Sundaram, N. Satish, R. Sankaran, J. Jackson,\nand K. Schwan. Data tiering in heterogeneous memory systems. In EuroSys ,\n2016.\n[39] A. Eisenman, D. Gardner, I. AbdelRahman, J. Axboe, S. Dong, K. Hazelwood,\nC. Petersen, A. Cidon, and S. Katti. Reducing DRAM footprint with NVM in\nFacebook. In EuroSys , 2018.\n[40] Y. Gao, Q. Li, L. Tang, Y. Xi, P. Zhang, W. Peng, B. Li, Y. Wu, S. Liu, L. Yan, F. Feng,\nY. Zhuang, F. Liu, P. Liu, X. Liu, Z. Wu, J. Wu, Z. Cao, C. Tian, J. Wu, J. Zhu,\nH. Wang, D. Cai, and J. Wu. When cloud storage meets RDMA. In NSDI , 2021.\n[41] D. Gouk, S. Lee, M. Kwon, and M. Jung. Direct access, High-Performance memory\ndisaggregation with DirectCXL. In USENIX ATC , 2022.\n[42] J. Gu, Y. Lee, Y. Zhang, M. Chowdhury, and K. G. Shin. E \uffffcient memory disag-\ngregation with In \uffffniswap. In NSDI , 2017.\n[43] D. Hansen. Migrate pages in lieu of discard. https://lwn.net/Articles/860215/ .\n[44] B. Holden, D. Anderson, J. Trodden, and M. Daves. HyperTransport 3.1 Interconnect\nTechnology . 2008.\n[45] S. Kannan, A. Gavrilovska, V. Gupta, and K. Schwan. Heteroos: Os design for\nheterogeneous memory management in datacenter. In ISCA , 2017.[46] H. T. Kassa, J. Akers, M. Ghosh, Z. Cao, V. Gogte, and R. Dreslinski. Improving\nperformance of \uffffash based Key-Value stores using storage class memory as a\nvolatile memory extension. In USENIX ATC , 2021.\n[47] J. Kim, W. Choe, and J. Ahn. Exploring the design space of page management for\nMulti-Tiered memory systems. In USENIX ATC , 2021.\n[48] A. Lagar-Cavilla, J. Ahn, S. Souhlal, N. Agarwal, R. Burny, S. Butt, J. Chang,\nA. Chaugule, N. Deng, J. Shahid, G. Thelen, K. A. Yurtsever, Y. Zhao, and P. Ran-\nganathan. Software-de \uffffned far memory in warehouse-scale computers. In\nASPLOS , 2019.\n[49] S.-H. Lee. Technology scaling challenges and opportunities of memory devices.\nIn2016 IEEE International Electron Devices Meeting (IEDM) , 2016.\n[50] Y. Lee, Y. Kim, and H. Y. Yeom. Lightweight memory tracing for hot data identi \uffff-\ncation. Cluster Computing , 2020.\n[51] Y. Lee, H. A. Maruf, M. Chowdhury, A. Cidon, and K. G. Shin. Hydra : Resilient\nand highly available remote memory. In FAST , 2022.", "start_char_idx": 221658, "end_char_idx": 224342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94081e9e-93f6-410a-9549-c877c44fd91f": {"__data__": {"id_": "94081e9e-93f6-410a-9549-c877c44fd91f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56f47d77-b474-4ad6-b976-9c2c6ecf7241", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a122bced1a3b32e24fe8964b76c0490565f4055c62f0ef3199bd080c34882d5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e159add2-82e8-47b7-9131-723a58681f56", "node_type": "1", "metadata": {}, "hash": "69809e35ebac3a355dee95d52352a1f7ce2fe37722f8d57e1d4c37e88f2d9e87", "class_name": "RelatedNodeInfo"}}, "text": "Software-de \uffffned far memory in warehouse-scale computers. In\nASPLOS , 2019.\n[49] S.-H. Lee. Technology scaling challenges and opportunities of memory devices.\nIn2016 IEEE International Electron Devices Meeting (IEDM) , 2016.\n[50] Y. Lee, Y. Kim, and H. Y. Yeom. Lightweight memory tracing for hot data identi \uffff-\ncation. Cluster Computing , 2020.\n[51] Y. Lee, H. A. Maruf, M. Chowdhury, A. Cidon, and K. G. Shin. Hydra : Resilient\nand highly available remote memory. In FAST , 2022.\n[52] H. Li, D. S. Berger, S. Novakovic, L. Hsu, D. Ernst, P. Zardoshti, M. Shah, S. Ra-\njadnya, S. Lee, I. Agarwal, M. D. Hill, M. Fontoura, and R. Bianchini. Pond:\nCXL-Based Memory Pooling Systems for Cloud Platforms. In ASPLOS , 2023.\n[53] Y. Li, S. Ghose, J. Choi, J. Sun, H. Wang, and O. Mutlu. Utility-based hybrid\nmemory management. In CLUSTER , 2017.\n[54] C. A. Mack. Fifty years of moore\u2019s law. IEEE Transactions on Semiconductor\nManufacturing , 2011.\n[55] H. A. Maruf and M. Chowdhury. E \uffffectively Prefetching Remote Memory with\nLeap. In USENIX ATC , 2020.\n[56] H. A. Maruf, Y. Zhong, H. Wong, M. Chowdhury, A. Cidon, and C. Waldspurger.\nMemtrade: A disaggregated-memory marketplace for public clouds. arXiv\npreprint arXiv:2108.06893 , 2021.\n[57] M. R. Meswani, S. Blagodurov, D. Roberts, J. Slice, M. Ignatowski, and G. H.\nLoh. Heterogeneous memory architectures: A HW/SW approach for mixing\ndie-stacked and o \uffff-package memories. In HPCA , 2015.\n[58] V. Mishra, J. L. Benjamin, and G. Zervas. MONet: heterogeneous memory over\noptical network for large-scale data center resource disaggregation. Journal of\nOptical Communications and Networking , 2021.\n[59] J. C. Mogul, E. Argollo, M. Shah, and P. Faraboschi. Operating system support\nfor nvm+dram hybrid main memory. In HotOS , 2009.\n[60] M. Oskin and G. H. Loh. A software-managed approach to die-stacked dram. In\nPACT , 2015.\n[61] J. Ousterhout, P. Agrawal, D. Erickson, C. Kozyrakis, J. Leverich, D. Mazi\u00e8res,\nS. Mitra, A. Narayanan, G. Parulkar, M. Rosenblum, S. M. Rumble, E. Stratmann,\nand R. Stutsman. The case for RAMClouds: Scalable high-performance storage\nentirely in DRAM. SIGOPS Oper. Syst. Rev. , 2010.\n[62] L. E. Ramos, E. Gorbatov, and R. Bianchini. Page placement in hybrid memory\nsystems. In ICS, 2011.\n[63] A. Raybuck, T. Stamler, W. Zhang, M. Erez, and S. Peter. HeMem: Scalable tiered\nmemory management for big data applications and real nvm. In SOSP , 2021.\n[64] Z. Ruan, M. Schwarzkopf, M. K. Aguilera, and A. Belay. AIFM: High-performance,\napplication-integrated far memory. In OSDI , 2020.\n[65] H. Servat, A. J. Pe\u00f1a, G. Llort, E. Mercadal, H.-C. Hoppe, and J. Labarta. Au-\ntomating the application data placement in hybrid memory systems. In IEEE\nInternational Conference on Cluster Computing (CLUSTER) , 2017.", "start_char_idx": 223861, "end_char_idx": 226640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e159add2-82e8-47b7-9131-723a58681f56": {"__data__": {"id_": "e159add2-82e8-47b7-9131-723a58681f56", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94081e9e-93f6-410a-9549-c877c44fd91f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6abfdba09793776fcc0831690e8c06ad2c791d148ee702d0ac67e507178da341", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e3f37ab-7553-4323-bf4f-44a7fefc570d", "node_type": "1", "metadata": {}, "hash": "d751d67aacbdb8a9d58636ce390eb6c9eb6f55a39056a5879fb1da49179932d4", "class_name": "RelatedNodeInfo"}}, "text": "Page placement in hybrid memory\nsystems. In ICS, 2011.\n[63] A. Raybuck, T. Stamler, W. Zhang, M. Erez, and S. Peter. HeMem: Scalable tiered\nmemory management for big data applications and real nvm. In SOSP , 2021.\n[64] Z. Ruan, M. Schwarzkopf, M. K. Aguilera, and A. Belay. AIFM: High-performance,\napplication-integrated far memory. In OSDI , 2020.\n[65] H. Servat, A. J. Pe\u00f1a, G. Llort, E. Mercadal, H.-C. Hoppe, and J. Labarta. Au-\ntomating the application data placement in hybrid memory systems. In IEEE\nInternational Conference on Cluster Computing (CLUSTER) , 2017.\n[66] Y. Shan, Y. Huang, Y. Chen, and Y. Zhang. LegoOS: A disseminated, distributed\nOS for hardware resource disaggregation. In OSDI , 2018.\n[67] A. Sriraman and A. Dhanotia. Accelerometer: Understanding Acceleration Oppor-\ntunities for Data Center Overheads at Hyperscale . 2020.\n[68] A. Sriraman, A. Dhanotia, and T. F. Wenisch. SoftSKU: Optimizing server archi-\ntectures for microservice diversity @scale. In ISCA , 2019.\n[69] Vladimir Davydov. Idle Memory Tracking. https://lwn.net/Articles/639341/ .\n[70] M. Vuppalapati, J. Miron, R. Agarwal, D. Truong, A. Motivala, and T. Cruanes.\nBuilding an elastic query engine on disaggregated storage. In NSDI , 2020.\n[71] C. Wang, H. Ma, S. Liu, Y. Li, Z. Ruan, K. Nguyen, M. D. Bond, R. Netravali,\nM. Kim, and G. H. Xu. Semeru: A Memory-Disaggregated managed runtime. In\nOSDI , 2020.\n[72] W. Wei, D. Jiang, S. A. McKee, J. Xiong, and M. Chen. Exploiting program\nsemantics to place data in hybrid memory. In PACT , 2015.\n[73] J. Weiner, N. Agarwal, D. Schatzberg, L. Yang, H. Wang, B. Sanouillet, B. Sharma,\nT. Heo, M. Jain, C. Tang, and D. Skarlatos. TMO: Transparent memory o \uffffoading\nin datacenters. In ASPLOS , 2022.\n[74] K. Wu, Y. Huang, and D. Li. Unimem: Runtime data managementon non-volatile\nmemory-based heterogeneous main memory. In SC, 2017.\n[75] K. Wu, J. Ren, and D. Li. Runtime data management on non-volatile memory-\nbased heterogeneous memory for task-parallel programs. In SC, 2018.\n[76] Z. Yan, D. Lustig, D. Nellans, and A. Bhattacharjee. Nimble page management\nfor tiered memory systems. In ASPLOS , 2019.\n[77] L. Zhang, R. Karimi, I. Ahmad, and Y. Vigfusson. Optimal data placement for\nheterogeneous cache, memory, and storage systems. In SIGMETRICS , 2020.\n[78] P. Zhou, V. Pandey, J. Sundaresan, A. Raghuraman, Y. Zhou, and S. Kumar. Dy-\nnamic tracking of page miss ratio curve for memory management. In ASPLOS ,\n2004.", "start_char_idx": 226070, "end_char_idx": 228527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e3f37ab-7553-4323-bf4f-44a7fefc570d": {"__data__": {"id_": "4e3f37ab-7553-4323-bf4f-44a7fefc570d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e159add2-82e8-47b7-9131-723a58681f56", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "b1ae7772b8cfcde66363d7768b729720664682b443b93605e23bfaa12f998c00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "937b97a3-2a00-4f64-afd0-ba687e2576c5", "node_type": "1", "metadata": {}, "hash": "7d75be7ef0462277f15be37b2105e35b7897bb5ebc280df72be4cb9ee82cc095", "class_name": "RelatedNodeInfo"}}, "text": "Runtime data management on non-volatile memory-\nbased heterogeneous memory for task-parallel programs. In SC, 2018.\n[76] Z. Yan, D. Lustig, D. Nellans, and A. Bhattacharjee. Nimble page management\nfor tiered memory systems. In ASPLOS , 2019.\n[77] L. Zhang, R. Karimi, I. Ahmad, and Y. Vigfusson. Optimal data placement for\nheterogeneous cache, memory, and storage systems. In SIGMETRICS , 2020.\n[78] P. Zhou, V. Pandey, J. Sundaresan, A. Raghuraman, Y. Zhou, and S. Kumar. Dy-\nnamic tracking of page miss ratio curve for memory management. In ASPLOS ,\n2004.\n755Enabling CXL Memory Expansion for In-Memory Database\nManagement Systems\nMinseon Ahn\nDonghun Lee\nJungmin Kim\nminseon.ahn@sap.com\ndomg.hun.lee@sap.com\njungmin.kim@sap.com\nSAP Labs Korea\nSeoul, South KoreaOliver Rebholz\noliver.rebholz@sap.com\nSAP SE\nWalldorf, Baden-W\u00fcrttemberg\nGermanyAndrew Chang\nJongmin Gim\nJaemin Jung\nVincent Pham\nKrishna T. Malladi\nYang Seok Ki\nandrew.c1@samsung.com\ngim.jongmin@samsung.com\nj.jaemin@samsung.com\ntung1.pham@samsung.com\nk.tej@samsung.com\nyangseok.ki@samsung.com\nSamsung Semiconductor Inc.\nSan Jose, California, USA\nABSTRACT\nLimited memory volume is always a performance bottleneck in an\nin-memory database management system (IMDBMS) as the data\nsize keeps increasing. To overcome the physical memory limita-\ntion, heterogeneous and disaggregated computing platforms are\nproposed, such as Gen-Z, CCIX, OpenCAPI, and CXL. In this work,\nwe introduce \uffffexible CXL memory expansion using a CXL type 3\nprototype and evaluate its performance in an IMDBMS. Our evalu-\nation shows that CXL memory devices interfaced with PCIe Gen5\nare appropriate for memory expansion with nearly no throughput\ndegradation in OLTP workloads and less than 8% throughput degra-\ndation in OLAP workloads. Thus, CXL memory is a good candidate\nfor memory expansion with lower TCO in IMDBMSs.\nCCS CONCEPTS\n\u2022Hardware !Emerging interfaces ;\u2022Information systems\n!Database management system engines .\nKEYWORDS\nCXL, Compute Express Link, In-Memory Database, DBMS, Data-\nbase Management Systems\nACM Reference Format:\nMinseon Ahn, Donghun Lee, Jungmin Kim, Oliver Rebholz, Andrew Chang,\nJongmin Gim, Jaemin Jung, Vincent Pham, Krishna T. Malladi, and Yang\nSeok Ki. 2022. Enabling CXL Memory Expansion for In-Memory Database\nManagement Systems. In Data Management on New Hardware (DaMoN\u201922),\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro \ufffft or commercial advantage and that copies bear this notice and the full citation\non the \uffffrst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior speci \uffffc permission and/or a\nfee. Request permissions from permissions@acm.org.\nDaMoN\u201922, June 13, 2022, Philadelphia, PA, USA\n\u00a92022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9378-2/22/06. . . $15.00\nhttps://doi.org/10.1145/3533737.3535090June 13, 2022, Philadelphia, PA, USA. ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/10.1145/3533737.3535090\n1 INTRODUCTION\nModern applications\u2019 growing data needs large DRAM capacity, par-\nticularly for in-memory database management systems (IMDBMS).\nFurthermore, considering data growth after initial deployment,\nthe DRAM capacity of an on-premise server is usually overprovi-\nsioned.", "start_char_idx": 227970, "end_char_idx": 231453, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "937b97a3-2a00-4f64-afd0-ba687e2576c5": {"__data__": {"id_": "937b97a3-2a00-4f64-afd0-ba687e2576c5", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e3f37ab-7553-4323-bf4f-44a7fefc570d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "4c1082da7df79deec9faf4c583eb56dbf8c50f190eb43246e98c8133be2a60c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb0df7ac-47f5-44ea-a6a6-6ac85621d23d", "node_type": "1", "metadata": {}, "hash": "26524ea25b1b0521dfbcec506bcfcc145b236c6e6fd67f70e3ece4f42baa440b", "class_name": "RelatedNodeInfo"}}, "text": "Request permissions from permissions@acm.org.\nDaMoN\u201922, June 13, 2022, Philadelphia, PA, USA\n\u00a92022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9378-2/22/06. . . $15.00\nhttps://doi.org/10.1145/3533737.3535090June 13, 2022, Philadelphia, PA, USA. ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/10.1145/3533737.3535090\n1 INTRODUCTION\nModern applications\u2019 growing data needs large DRAM capacity, par-\nticularly for in-memory database management systems (IMDBMS).\nFurthermore, considering data growth after initial deployment,\nthe DRAM capacity of an on-premise server is usually overprovi-\nsioned. This results in an increased total cost of ownership (TCO)\nfor IMDBMS servers. Emerging new technologies like NVMe [ 11],\nand RDMA [ 12,13,17] provide a \uffffexible and integrated memory\nview larger than the physical memory. However, this needs appli-\ncation changes as well as resulting in longer latency. To overcome\nsuch limitations on physical memory expansion and to provide\nmore \uffffexible solutions, heterogeneous and disaggregated comput-\ning platforms, such as Gen-Z [ 2], CCIX [ 4], OpenCAPI [ 1], and most\nrecently CXL (Compute Express Link) [ 5], are proposed. With wide\nadoption across the industry, CXL is the most promising candidate\nto mitigate memory overprovisioning issues.\nCXL is a new class of interconnect for device connectivity, an\nopen industry standard led by Intel \u00ae, and cache coherent interface\nusing PCIe, enabling memory expansion and heterogeneous mem-\nory for disaggregated computing platforms. CXL has an alternate\nprotocol that runs across the standard PCIe 5.0 physical layer, con-\nsisting of three protocols; (1) CXL.io for discovery, con \uffffguration,\nregister access, and interrupt, (2) CXL.cache for device access to pro-\ncessor memory, and (3) CXL.memory for processor access to device\nattached memory. There are three types of CXL devices. Type 1 is a\nCXL device without host-managed device memory like NIC using\nCXL.io and CXL.cache. Type 2 is a CXL device with host-managed\ndevice memory like GPU or external computing units using all 3\nCXL protocols. Type 3 is a CXL device only with host-managed\ndevice memory using CXL.memory. A typical application of type 3\nis memory expansion.\nDaMoN\u201922, June 13, 2022, Philadelphia, PA, USA Minseon Ahn, et al.\nFigure 1: CXL prototype diagram\nIn this work, we introduce CXL type 3 memory expansion for\nIMDBMS [ 10]. First, we propose a CXL-based solution for \uffffexible\nmemory expansion in our IMDBMS to address the overprovisioning\nissue. Second, we introduce a prototype of CXL type 3 memory\ndevices and a complete working system. Third, we evaluate the\nperformance with common database benchmark tests. Our evalua-\ntion shows that CXL memory expansion has nearly no throughput\ndegradation with TPC-C, one of OLTP workloads, and less than\n8% throughput degradation with TPC-DS, one of OLAP workloads.\nThis is a promising solution given the limited capability of the pro-\ntotype. With PCIe Gen5 performance at the \uffffnal product stage, CXL\nmemory expansion is expected to provide competitive solutions to\noptimize TCO in IMDBMSs without large performance penalty.\nThe remainder of this paper is organized as follows: Section 2\ndiscusses memory expansion in IMDBMSs. Section 3 introduces\nCXL memory expansion devices. The performance evaluation is\naddressed in Section 4. Section 5 represents the related work and\nSection 6 concludes the paper.\n2 MEMORY EXPANSION IN IMDBMS\nIMDBMSs widely support hybrid transactional and analytical pro-\ncessing (HTAP) [ 16]. In this work, SAP HANA in-memory database\nplatform [ 10] is used as a base platform. It adopts the columnar\nstorage [ 15], storing the data of each column in the read-optimized\nmain storage and maintaining the separate delta storage for opti-\nmized writes.", "start_char_idx": 230845, "end_char_idx": 234638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb0df7ac-47f5-44ea-a6a6-6ac85621d23d": {"__data__": {"id_": "eb0df7ac-47f5-44ea-a6a6-6ac85621d23d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "937b97a3-2a00-4f64-afd0-ba687e2576c5", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "fa4904bc6a30ef1fec1e7c5883c88f9a0d74285516ee9f7d0b6daf3b4a36671c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e40169f7-be14-405b-b487-c47098368872", "node_type": "1", "metadata": {}, "hash": "1aa40199a3c0b1180f8539764d3e839a33539af49f8c71851f1e78cfcece4113", "class_name": "RelatedNodeInfo"}}, "text": "The remainder of this paper is organized as follows: Section 2\ndiscusses memory expansion in IMDBMSs. Section 3 introduces\nCXL memory expansion devices. The performance evaluation is\naddressed in Section 4. Section 5 represents the related work and\nSection 6 concludes the paper.\n2 MEMORY EXPANSION IN IMDBMS\nIMDBMSs widely support hybrid transactional and analytical pro-\ncessing (HTAP) [ 16]. In this work, SAP HANA in-memory database\nplatform [ 10] is used as a base platform. It adopts the columnar\nstorage [ 15], storing the data of each column in the read-optimized\nmain storage and maintaining the separate delta storage for opti-\nmized writes. Additionally, a portion of memory is allocated for the\noperational data to keep the intermediate results while processing\na query.\nThere are two options to enable the memory expansion using\nCXL memory devices in our IMDBMS. In the \uffffrst, the additional\nmemory space of the CXL device could be uniformly integrated with\nthe host memory space. This allows both operational memory and\nmain storage to be allocated in CXL memory. However, the random\naccesses to the operational data can degrade overall performance\nowing to longer access latency of the CXL device compared to the\nhost DRAMs. In the second option, the CXL memory device is used\nonly for the main storage. The delta storage and the operational\ndata are stored in the host DRAMs. Like the approach used in the\npersistent memory [ 9], a prefetching scheme can e \uffffectively hide\nthe longer latency of the CXL device when the main storage is\nsequentially accessed. In this work, we use the second option in\nour IMDBMS to take advantage of the prefetch.\n3 CXL MEMORY EXPANSION\nThis section introduces a prototype of CXL type 3 memory ex-\npansion devices in E3.S form factor. It implements CXL.mem and\nCXL.io commands de \uffffned in CXL1.1 speci \uffffcation, carrying 128GBDRAM as media and supporting a theoretical bandwidth of 16GB/s\nwith PCIe Gen4x8 as the bottleneck. As shown in Fig. 1, the proto-\ntype consists of a custom FPGA board and a single-channel-based\nDDR3 DIMM module. The FPGA is composed of CXL PHY link\nsupporting the connection to CPU, CXL protocol engine manag-\ning CXL.mem and CXL.io, and memory controller for the DIMM\nmodule. DDR3 DIMM can be replaced with DDR4 or DDR5 DIMMs\nsupporting multiple channels within a single CXL memory device\nin the future. On the FPGA, the SerDes technology in our prototype\nruns at 16 Gbps, same as PCIe Gen4 speed. It can be upgraded to\n32 Gbps, PCIe Gen5 speed, with ASIC implementation. Because\nCXL and PCIe share the same physical layer, this memory device\nconveniently plugs into existing PCIe slots.\nOur CXL device is recognized as a memory-only NUMA node or\na DAX device. When CXL memory appears on the system memory\nmap along with the host DRAMs, CPUs can directly load/store\nfrom and to the device memory through the host CXL interface\nwithout ever touching the host memory. To highlight CXL.mem\nprotocol bene \ufffft of low latency, the translation logic between CXL\nprotocol to DRAM media is kept to a minimum. CXL physical\nand link layers perform con \uffffguration and link negotiation with\nthe PCIe root complex. CXL protocol layer unpacks CXL \uffffits into\ncommand, address, and data \uffffelds for the internal data path. In\nthis prototype, host physical addresses are directly mapped onto\nthe device memory space, removing the need for translation. CXL\nread and write commands are handled by CXL protocol engine and\nthe memory controller performs 64B read and write transactions\nto DRAM. Because the target throughput is 16GB/s, a single DDR\nchannel is su \uffffcient to match the performance. However, to get the\nbest throughput, multiple outstanding transactions are required to\nmitigate the latency to and from the DDR interface. To maximize\ndevice memory bandwidth, CXL.mem read and write are arbitrated\nfairly in the \uffffrst-come \uffffrst-server order. Writes are completed\nwhen data is written into DDR memory. Responses for read and\nwrite are returned to the host in the same order of their completion.", "start_char_idx": 233987, "end_char_idx": 238035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e40169f7-be14-405b-b487-c47098368872": {"__data__": {"id_": "e40169f7-be14-405b-b487-c47098368872", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb0df7ac-47f5-44ea-a6a6-6ac85621d23d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e2b2528cc351835400ebc4cacad5e4a6d1f56d2e390235f0afb721bf9f14c538", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb6a610b-80a9-48fc-818a-9cc1462a6ce5", "node_type": "1", "metadata": {}, "hash": "85f686e25ae8b7b04f4dc1cdea760f66b7149a2b29855b4b5d30cb41641ed49f", "class_name": "RelatedNodeInfo"}}, "text": "In\nthis prototype, host physical addresses are directly mapped onto\nthe device memory space, removing the need for translation. CXL\nread and write commands are handled by CXL protocol engine and\nthe memory controller performs 64B read and write transactions\nto DRAM. Because the target throughput is 16GB/s, a single DDR\nchannel is su \uffffcient to match the performance. However, to get the\nbest throughput, multiple outstanding transactions are required to\nmitigate the latency to and from the DDR interface. To maximize\ndevice memory bandwidth, CXL.mem read and write are arbitrated\nfairly in the \uffffrst-come \uffffrst-server order. Writes are completed\nwhen data is written into DDR memory. Responses for read and\nwrite are returned to the host in the same order of their completion.\n4 PERFORMANCE EVALUATION\n4.1 System Con \uffffguration\nThe evaluation was done on Intel\u2019s Sapphire Rapids (SPR) customer\nreference board with C0 stepping CPUs. For all con \uffffgurations\nexcept CXL emulation in the single-node test and the scale-out\ncon\uffffguration, we enable one socket per node. For CXL emulation\nand the scale-up con \uffffgurations, we enable two sockets. Each socket\nhas 512 GB DDR5 4800 MHz memory, one 64 GB DIMM per chan-\nnel. CXL memory extensions are set up as DAX devices because\nthe current release of our IMDBMS does not support the memory-\nonly NUMA node yet. Thus, the memory space is recognized with\npersistent memory features [ 9] and the main storage is moved to\nCXL memory. The persistent memory features do not add any over-\nhead when reading the main storage because there is no additional\ninstruction required.\nAs the IMDBMS used for our experiments accepts only one DAX\npath per socket due to the limitation of the test version, we stripe\nmultiple CXL memory devices using the device mapper [ 6] to see\nthe impact of increased bandwidth beyond the current 16GB/s limit.\nWe use TPC-C with 100 warehouses for OLTP workloads, increasingEnabling CXL Memory Expansion for In-Memory Database Management Systems DaMoN\u201922, June 13, 2022, Philadelphia, PA, USA\n(a) Baseline\n (b) 1CXL\n (c) 2CXL\n (d) CXL Emulation\nFigure 2: System con \uffffguration for single-node test\n(a) TPC-C\n (b) TPC-DS\nFigure 3: Benchmark performance in single-node test\nthe number of client connections to maximize the performance. The\nnumber of client connections is set to 176, 352, and 704, which are\nrespectively 4, 8, and 16 times of 44 physical cores of a single CPU in\nthe system. We also use TPC-DS with SF=100 for OLAP workloads,\nincreasing the number of parallel requests up to 32 in the client to\nsee the performance scalability.\n4.2 Evaluation of CXL Memory Devices\nSingle-node Test. We test a single-socket machine to evaluate the\nperformance of CXL memory expansion. In this experiment, we\nhave 4 con \uffffgurations as shown in Fig. 2: (1) the baseline without\nCXL memory expansion, (2) with 1 CXL device, (3) with 2 CXL\ndevices striped, and (4) CXL emulation in SPR by setting up the\nmain storage in the memory of the remote socket as a DAX device\nafter CPU a \uffffnity is set to CPU0 only, assuming that the access\nlatency to the remote memory through UPI is similar to the future\nCXL memory expansion. The baseline allocates the main storage in\nthe host DRAM, while the other con \uffffgurations with CXL memory\ndevices have it in the CXL memory expansion area. CXL emulation\nallocates the main storage in the remote memory.\nFig. 3 shows the normalized throughput to the maximum among\nall con \uffffgurations. TPC-C has nearly no performance di \ufffference be-\ntween the baseline and the other CXL con \uffffgurations. As mentioned\nin Section 2, the latency of sequential accesses to the main storage\nis completely hidden by the prefetching scheme. Pro \uffffle results\nusing Intel \u00aeVTune \u2122Pro\uffffler [7] show low memory bandwidth\nbound in TPC-C.", "start_char_idx": 237259, "end_char_idx": 241047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb6a610b-80a9-48fc-818a-9cc1462a6ce5": {"__data__": {"id_": "eb6a610b-80a9-48fc-818a-9cc1462a6ce5", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e40169f7-be14-405b-b487-c47098368872", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "03952b252ec2a4fa5f618bd697152de61432468ac8717d14622c358ef27dec60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5bccb49-caae-496e-83b0-b00a7b7e2a94", "node_type": "1", "metadata": {}, "hash": "97f6f900024311842bf63a97d3b512b803a689ca1966e681818a06a74c3af78d", "class_name": "RelatedNodeInfo"}}, "text": "The baseline allocates the main storage in\nthe host DRAM, while the other con \uffffgurations with CXL memory\ndevices have it in the CXL memory expansion area. CXL emulation\nallocates the main storage in the remote memory.\nFig. 3 shows the normalized throughput to the maximum among\nall con \uffffgurations. TPC-C has nearly no performance di \ufffference be-\ntween the baseline and the other CXL con \uffffgurations. As mentioned\nin Section 2, the latency of sequential accesses to the main storage\nis completely hidden by the prefetching scheme. Pro \uffffle results\nusing Intel \u00aeVTune \u2122Pro\uffffler [7] show low memory bandwidth\nbound in TPC-C. Thus, CXL memory expansion has no performance\ndegradation in OLTP workloads. However, the average throughput\ndegradation in TPC-DS is 27% in 1CXL, 18% in 2CXL, and 8% in CXL\nemulation. Larger performance degradation is mainly caused by the\nlimited bandwidth of the current CXL prototype with PCIe Gen4x8.\nHowever, we expect that the degradation in TPC-DS would be lessthan 8% in the future CXL product as CXL memory bandwidth is\nincreased with PCIe Gen5x16 (64 GB/s), which is more than UPI\nconnections in CXL emulation.\nComparison between scale-up and scale-out. To study further\nbene \uffffts of CXL memory expansion, we compare the performance\nof a scale-up with 2 CPUs and a 2-node scale-out system as shown\nin Fig. 4. First, the scale-up baseline has no CXL memory expansion.\nSecond, scale-up+2CXL has two CXL memory devices, one per\nsocket. Third, scale-up+4CXL has four CXL memory devices, two\nper socket with striping to increase the bandwidth. The scale-up\nbaseline has the main storage in the host DRAM, while the scale-up\nwith the CXL memory has it in the CXL memory. We use the default\nNUMA-aware location to achieve balanced memory usage across\nNUMA nodes in the scale-up con \uffffgurations. In the scale-out, we\nprepare two nodes enabled with 1 CPU in each node to make a fair\ncomparison. Then, we connect them with 10G Ethernet. We use\nhash partitioning on the \uffffrst columns of the primary keys in all\ntables, which are used in all the join conditions.\nAs shown in Fig. 5, TPC-C has no signi \uffffcant performance di \uffffer-\nence between the scale-up baseline and the scale-up CXL con \uffffgu-\nrations because of low memory bandwidth bound. Comparing the\nperformance between the scale-up and the scale-out, the scale-up\ncon\uffffgurations outperform the scale-out before 704 connections\n(16 * 44 physical cores). We observe that the performance of the\nscale-up decreases for 704 connections due to the overhead caused\nby too many client connections in a single machine. The average\nthroughput degradation for TPC-DS compared to the scale-up base-\nline is 39% in scale-up+2CXL and 16% in scale-up+4CXL due to the\nbandwidth limitation of the prototype. However, scale-up+4CXL\nshows slightly better throughput than the scale-out. Once CXL\nmemory bandwidth is increased with PCIe Gen5, the scale-up with\nCXL memory is expected to have much better performance thanDaMoN\u201922, June 13, 2022, Philadelphia, PA, USA Minseon Ahn, et al.\n(a) Scale-up\n (b) Scale-up+2CXL\n (c) Scale-up+4CXL\n (d) Scale-out\nFigure 4: System con \uffffguration for scale-up and scale-out\n(a) TPC-C\n (b) TPC-DS\nFigure 5: Performance comparison between scale-up and scale-out\nthe scale-out. Therefore, CXL memory expansion is a good solution\nto increasing the memory capacity with lower TCO, if the system\nprovides su \uffffcient computing resources.\n5 RELATED WORK\nProviding a \uffffexible and integrated memory view larger than the\nphysical memory is one of the important topics in IMDBMSs to\novercome the capacity limitation. Guz et. al.", "start_char_idx": 240430, "end_char_idx": 244027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5bccb49-caae-496e-83b0-b00a7b7e2a94": {"__data__": {"id_": "a5bccb49-caae-496e-83b0-b00a7b7e2a94", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb6a610b-80a9-48fc-818a-9cc1462a6ce5", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "9ea5ad1ccdf6f04ace7640f9623a2d9b09813fa20d0018ab2e26808d255fc665", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d68afe09-1348-4e69-b047-0b9d33aaebc3", "node_type": "1", "metadata": {}, "hash": "45a1fce97c572ddf7221b16dfad103032560340e9e14da6acbfabc5f88f6112e", "class_name": "RelatedNodeInfo"}}, "text": "(a) Scale-up\n (b) Scale-up+2CXL\n (c) Scale-up+4CXL\n (d) Scale-out\nFigure 4: System con \uffffguration for scale-up and scale-out\n(a) TPC-C\n (b) TPC-DS\nFigure 5: Performance comparison between scale-up and scale-out\nthe scale-out. Therefore, CXL memory expansion is a good solution\nto increasing the memory capacity with lower TCO, if the system\nprovides su \uffffcient computing resources.\n5 RELATED WORK\nProviding a \uffffexible and integrated memory view larger than the\nphysical memory is one of the important topics in IMDBMSs to\novercome the capacity limitation. Guz et. al. [ 11] proposes NVMe-\nSSD disaggregation using NVMf (NVMe-over-fabrics) [ 3]. Koh et.\nal. [12] introduces a disaggregated memory system integrated with\nthe hypervisor for cloud computing. Adding the disaggregated\nmemory support to the memory management in the KVM hyper-\nvisor minimizes the overhead of remote direct memory accesses\n(RDMA). Korolija et. al. [ 13] proposes Farview, a disaggregated\nand network-attached memory using an FPGA-based smart NIC.\nTaranov et. al. [ 17] proposes CoRM, an RDMA-accelerated shared\nmemory system.\nAs more \uffffexible solutions for the heterogeneous and disaggre-\ngated computing platform, several technical standards for cache\ncoherent interconnects have been devised. CCIX (cache coherent\ninterconnect for accelerators) [ 4] is a protocol to enable coherent\ninterconnects widely used in ARM-based System-on-Chips, while\nOpenCAPI [ 1] is an open standard for Cache Accelerator Processor\nInterface developed for IBM Power CPUs. Gen-Z [ 2] was an open\nsystem interconnect to provide cache coherent memory accesses,\nand now it is merged to CXL. NVLink [ 8] is also a cache coherentinterconnect mainly for NVidia GPUs. It is also supported in IBM\nPower CPUs. Lutz et. al. [ 14] shows that fast interconnects like\nNVLink 2.0 can overcome the limits of the current GPUs, such as\non-board memory capacity and interconnect bandwidth, thus re-\nsulting in better performance in CPU-GPU hash joins with a larger\ndata size than the amount of GPU memory.\n6 CONCLUSION\nThis work proposes a \uffffexible CXL-based memory expansion with\npotentially lower TCO in an IMDBMS as one of the signi \uffffcant\nuse cases of CXL memory. The evaluation results using common\ndatabase benchmark tests proved the feasibility of CXL memory\nexpansion in an IMDBMS. OLTP workloads have nearly no through-\nput degradation with CXL memory devices. Even though OLAP\nworkloads have a certain amount of throughput degradation, its\nperformance on the real CXL product can be dramatically improved\nonce CXL devices are operating at PCIe Gen5 speed. Furthermore,\nconsidering that the current experiments were done using the pre-\nproduction SPR (C0 stepping) CPU and slow DDR3 DIMMs in CXL\nmemory expansion, more performance improvement is anticipated\nwith the mass-production SPR and CXL memory.\nREFERENCES\n[1]2014. OpenCAPI Consortium . https://opencapi.org/\n[2]2016. Gen-Z Consortium . https://genzconsortium.org/\n[3]2016. NVM Express over Fabric 1.0 . https://nvmexpress.org/\n[4]2017. CCIX Consortium . https://www.ccixconsortium.com/Enabling CXL Memory Expansion for In-Memory Database Management Systems DaMoN\u201922, June 13, 2022, Philadelphia, PA, USA\n[5]2019. Compute Express Link . https://www.computeexpresslink.org/\n[6]2021. Device Mapper . https://www.kernel.org/doc/html/latest/admin-guide/\ndevice-mapper/index.html\n[7]2021. Intel \u00aeVTune \u2122Pro\uffffler. https://www.intel.com/content/www/us/en/\ndeveloper/tools/oneapi/vtune-pro \uffffler.html\n[8]2022. NVLink .", "start_char_idx": 243463, "end_char_idx": 246971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d68afe09-1348-4e69-b047-0b9d33aaebc3": {"__data__": {"id_": "d68afe09-1348-4e69-b047-0b9d33aaebc3", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5bccb49-caae-496e-83b0-b00a7b7e2a94", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "05a74acb5dab1154fbbba6499d54eeb8a262aa6f714504f086952a1c7790850c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f6e8d4e-22e7-45f2-b464-ada1bc70641f", "node_type": "1", "metadata": {}, "hash": "ef20d861ca74f55cd2987700b9b7881ee4641962db4b5f8522a24e34bcd796bf", "class_name": "RelatedNodeInfo"}}, "text": "Gen-Z Consortium . https://genzconsortium.org/\n[3]2016. NVM Express over Fabric 1.0 . https://nvmexpress.org/\n[4]2017. CCIX Consortium . https://www.ccixconsortium.com/Enabling CXL Memory Expansion for In-Memory Database Management Systems DaMoN\u201922, June 13, 2022, Philadelphia, PA, USA\n[5]2019. Compute Express Link . https://www.computeexpresslink.org/\n[6]2021. Device Mapper . https://www.kernel.org/doc/html/latest/admin-guide/\ndevice-mapper/index.html\n[7]2021. Intel \u00aeVTune \u2122Pro\uffffler. https://www.intel.com/content/www/us/en/\ndeveloper/tools/oneapi/vtune-pro \uffffler.html\n[8]2022. NVLink . https://www.nvidia.com/en-us/data-center/nvlink/\n[9]Mihnea Andrei, Christian Lemke, G\u00fcnter Radestock, Robert Schulze, Carsten\nThiel, Rolando Blanco, Akanksha Meghlan, Muhammad Sharique, Sebastian\nSeifert, Surendra Vishnoi, et al .2017. SAP HANA adoption of non-volatile\nmemory. Proceedings of the VLDB Endowment 10, 12 (2017), 1754\u20131765.\n[10] Franz F\u00e4rber, Norman May, Wolfgang Lehner, Philipp Gro\u00dfe, Ingo M\u00fcller, Hannes\nRauhe, and Jonathan Dees. 2012. The SAP HANA Database\u2013An Architecture\nOverview. IEEE Data Eng. Bull. 35, 1 (2012), 28\u201333.\n[11] Zvika Guz, Harry Li, Anahita Shayesteh, and Vijay Balakrishnan. 2017. NVMe-\nover-fabrics performance characterization and the path to low-overhead \uffffash\ndisaggregation. In Proceedings of the 10th ACM International Systems and Storage\nConference . 1\u20139.\n[12] Kwangwon Koh, Kangho Kim, Seunghyub Jeon, and Jaehyuk Huh. 2018. Disag-\ngregated cloud memory with elastic block management. IEEE Trans. Comput. 68,1 (2018), 39\u201352.\n[13] Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin Taranov,\nDejan Miloji \u010di\u0107, and Gustavo Alonso. 2021. Farview: Disaggregated memory\nwith operator o \uffff-loading for database engines. arXiv preprint arXiv:2106.07102\n(2021).\n[14] Clemens Lutz, Sebastian Bre\u00df, Ste \uffffen Zeuch, Tilmann Rabl, and Volker Markl.\n2020. Pump up the volume: Processing large data on GPUs with fast interconnects.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data . 1633\u20131649.\n[15] Hasso Plattner. 2014. The impact of columnar in-memory databases on enter-\nprise systems: implications of eliminating transaction-maintained aggregates.\nProceedings of the VLDB Endowment 7, 13 (2014), 1722\u20131729.\n[16] Iraklis Psaroudakis, Florian Wolf, Norman May, Thomas Neumann, Alexan-\nder B\u00f6hm, Anastasia Ailamaki, and Kai-Uwe Sattler. 2014. Scaling up mixed\nworkloads: a battle of data freshness, \uffffexibility, and scheduling. In Technology\nConference on Performance Evaluation and Benchmarking . Springer, 97\u2013112.\n[17] Konstantin Taranov, Salvatore Di Girolamo, and Torsten Hoe \uffffer. 2021. CoRM:\nCompactable Remote Memory over RDMA. In Proceedings of the 2021 International\nConference on Management of Data .", "start_char_idx": 246381, "end_char_idx": 249158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f6e8d4e-22e7-45f2-b464-ada1bc70641f": {"__data__": {"id_": "5f6e8d4e-22e7-45f2-b464-ada1bc70641f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d68afe09-1348-4e69-b047-0b9d33aaebc3", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5a5d43528ab91521a0d4f28a888af3128a6aaedc917f941c53c583c8a3a55401", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0790d16d-c003-4978-9ed9-11405ee0326c", "node_type": "1", "metadata": {}, "hash": "91b372428066dedde2d7223a466a71d03ef1352e12667558c692c6d71d8de0f9", "class_name": "RelatedNodeInfo"}}, "text": "The impact of columnar in-memory databases on enter-\nprise systems: implications of eliminating transaction-maintained aggregates.\nProceedings of the VLDB Endowment 7, 13 (2014), 1722\u20131729.\n[16] Iraklis Psaroudakis, Florian Wolf, Norman May, Thomas Neumann, Alexan-\nder B\u00f6hm, Anastasia Ailamaki, and Kai-Uwe Sattler. 2014. Scaling up mixed\nworkloads: a battle of data freshness, \uffffexibility, and scheduling. In Technology\nConference on Performance Evaluation and Benchmarking . Springer, 97\u2013112.\n[17] Konstantin Taranov, Salvatore Di Girolamo, and Torsten Hoe \uffffer. 2021. CoRM:\nCompactable Remote Memory over RDMA. In Proceedings of the 2021 International\nConference on Management of Data . 1811\u20131824.Near to Far: An Evaluation of Disaggregated Memory\nfor In-Memory Data Processing\nAndreas Geyer\nandreas.geyer@tu-dresden.de\nTechnische Universit\u00e4t Dresden\nDresden, GermanyJohannes Pietrzyk\njohannes.pietrzyk@tu-dresden.de\nTechnische Universit\u00e4t Dresden\nDresden, GermanyAlexander Krause\nalexander.krause@tu-dresden.de\nTechnische Universit\u00e4t Dresden\nDresden, Germany\nDirk Habich\ndirk.habich@tu-dresden.de\nTechnische Universit\u00e4t Dresden\nDresden, GermanyWolfgang Lehner\nwolfgang.lehner@tu-dresden.de\nTechnische Universit\u00e4t Dresden\nDresden, GermanyChristian F\u00e4rber\nchristian.faerber@intel.com\nIntel Deutschland GmbH\nFeldkirchen, Germany\nThomas Willhalm\nthomas.willhalm@intel.com\nIntel Deutschland GmbH\nFeldkirchen, Germany\nAbstract\nE\uffffcient in-memory data processing relies on the availabil-\nity of su \uffffcient resources, be it CPU time or available main\nmemory. Traditional approaches are coping with resource\nlimitations by either adding more processors or RAM sticks\nto a single server (scale-up) or by adding multiple servers\nto a network cluster (scale-out). Further, the In \uffffniBand in-\nterconnect enables Remote Direct Memory Access (RDMA)\nand thus enhances the possibilities of resource sharing be-\ntween distinct servers. Resource disaggregation means the\n(dynamic) sharing of available hardware, e. g., through the\nnetwork. This paradigm is now further enhanced by the\nspeci \uffffcation of Compute Express Link (CXL). In this pa-\nper, we systematically evaluate the implications of memory\nexpansion as a form of resource disaggregation from the\nperspective of in-memory data processing through the lo-\ncal Ultrapath Interconnect (UPI), RDMA via In \uffffniBand, and\nPCIe attached memory via CXL. Our results show that CXL\nyields behavior that is comparable to UPI and outperforms\nthe inherently asynchronous RDMA connection. Further, we\nfound that handling UPI-attached memory as a type of disag-\ngregated resource can yield additional performance bene \uffffts.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for pro \ufffft or commercial advantage and that\ncopies bear this notice and the full citation on the \uffffrst page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speci \uffffc\npermission and/or a fee. Request permissions from permissions@acm.org.\nDIMES \u201923, October 23, 2023, Koblenz, Germany\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 979-8-4007-0300-3/23/10. . . $15.00\nh\uffffps://doi.org/10.1145/3609308.3625271CCS Concepts: \u2022Information systems !Main memory\nengines ;Online analytical processing engines ;\u2022Hardware\n!Emerging interfaces .\nKeywords: Memory Disaggregation, CXL, RDMA, UPI\nACM Reference Format:\nAndreas Geyer, Johannes Pietrzyk, Alexander Krause, Dirk Habich,\nWolfgang Lehner, Christian F\u00e4rber, and Thomas Willhalm. 2023.", "start_char_idx": 248470, "end_char_idx": 252236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0790d16d-c003-4978-9ed9-11405ee0326c": {"__data__": {"id_": "0790d16d-c003-4978-9ed9-11405ee0326c", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f6e8d4e-22e7-45f2-b464-ada1bc70641f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d95ca65caf8904b5fed28d2f37d94439c97c53c199133e2e00c5cd308d347642", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0497b5fd-6d94-4a24-bde8-10e8ece24a24", "node_type": "1", "metadata": {}, "hash": "33f6d993d87ad6c2e1e702fa3ced374258d1bd8477df6dee1457b853c4952c4f", "class_name": "RelatedNodeInfo"}}, "text": "Request permissions from permissions@acm.org.\nDIMES \u201923, October 23, 2023, Koblenz, Germany\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 979-8-4007-0300-3/23/10. . . $15.00\nh\uffffps://doi.org/10.1145/3609308.3625271CCS Concepts: \u2022Information systems !Main memory\nengines ;Online analytical processing engines ;\u2022Hardware\n!Emerging interfaces .\nKeywords: Memory Disaggregation, CXL, RDMA, UPI\nACM Reference Format:\nAndreas Geyer, Johannes Pietrzyk, Alexander Krause, Dirk Habich,\nWolfgang Lehner, Christian F\u00e4rber, and Thomas Willhalm. 2023.\nNear to Far: An Evaluation of Disaggregated Memory for In-Memory\nData Processing. In 1st Workshop on Disruptive Memory Systems\n(DIMES \u201923), October 23, 2023, Koblenz, Germany. ACM, New York,\nNY, USA, 7 pages. h\uffffps://doi.org/10.1145/3609308.3625271\n1 Introduction\nThe continuous evolution of hardware is an inherent charac-\nteristic of this technology. Up to now, main memory and CPU\nhave been the most decisive drivers, but now other compo-\nnents like networking and respecting protocols have become\ninnovation drivers as well. Today, we already see several\nhardware architectures, e. g., scale-up or scale-out solutions\nto cope with the ever increasing demand for more compu-\ntational power and storage space. Over time, multi-socket\nsystems have emerged within the scale-up architecture and\nintroduced the now well-known Non-Uniform Memory Ac-\ncess (NUMA) e \uffffect [8,9,13]. However, still all communica-\ntion happens on the same machine and is \u2013 depending on\nthe interconnect and support by speci \uffffc components [ 22]\u2013\nreasonably fast.\nThe dawn of In \uffffniBand as a high speed interconnect en-\nabled, e. g., cloud providers to more e \uffffciently share po-\ntentially unused resources between individual machines.\nIn that \uffffeld, resource disaggregation is becoming increas-\ningly prevalent. That is, instead of hosting multiple servers,\nproviders now stack up on dedicated resource cards, i. e.,\nracks full of network-attached circuit boards holding either\n16\nDIMES \u201923, October 23, 2023, Koblenz, Germany Geyer et al.CPUDRAMCPUDRAMUPICPUDRAMCPUDRAMPCIePCIeRDMACPUDRAM\nDRAMPCIePCIeCXLCU 1CU 2CU 1CU 2CU 1ME(a)(b)(c)Figure 1. Schematic overview of di \ufffferent memory expansion methods using distinct Compute Units and a Memory Expansion.\nCPUs, DRAM, NVMe drives, accelerators like GPUs or FP-\nGAs, and so on. Currently, disaggregation is realized through\nRemote Direct Memory Access (RDMA), where two or more\nservers are connected by, e. g., an In \uffffniBand cable and share\ntheir main memory with each other. Yet, the next big player\nis just around the corner: Compute Express Link (CXL) [ 17].\nThis new PCIe based communication standard was \uffffrst de-\nscribed in 2019, was later fused with Gen-Z [ 18] and is now\njust arriving at the market. CXL features multiple proto-\ncols, such as CXL.io ,CXL.cache andCXL.mem , to provide,\ne. g., cache-coherent memory access for a remotely attached\nmemory block. Through CXL, we are now able to combine\nmultiple resources through the network to a single software\ncomposed system . However, this new architecture comes with\nwell-known problems: Physically distributed memory leads\nto even more severe NUMA e \uffffects. That is because CXL-\nattached memory extends the coherent virtual address space\nby being exposed as just another NUMA node.\nContribution and Outline. In this paper, we systemati-\ncally investigate the implications of resource disaggregation\nthrough both RDMA and CXL from the perspective of in-\nmemory data processing. The most intriguing questions are:\n(i) Can traditional solutions, that worked for scale-up servers,\nbe applied to disaggregated systems as well? (ii) What are\nkey similarities or di \ufffferences?", "start_char_idx": 251656, "end_char_idx": 255386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0497b5fd-6d94-4a24-bde8-10e8ece24a24": {"__data__": {"id_": "0497b5fd-6d94-4a24-bde8-10e8ece24a24", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0790d16d-c003-4978-9ed9-11405ee0326c", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a67d0239a89b919157b6daebf190fb612b1ab83efe612e47813679fd07ee857b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58c3e2d1-9ecd-4012-900b-206e9e1dc999", "node_type": "1", "metadata": {}, "hash": "7f3c3d8d9d7f138d873fd19b5625db31888317e0078e6872ce55143a198fb2d6", "class_name": "RelatedNodeInfo"}}, "text": "Through CXL, we are now able to combine\nmultiple resources through the network to a single software\ncomposed system . However, this new architecture comes with\nwell-known problems: Physically distributed memory leads\nto even more severe NUMA e \uffffects. That is because CXL-\nattached memory extends the coherent virtual address space\nby being exposed as just another NUMA node.\nContribution and Outline. In this paper, we systemati-\ncally investigate the implications of resource disaggregation\nthrough both RDMA and CXL from the perspective of in-\nmemory data processing. The most intriguing questions are:\n(i) Can traditional solutions, that worked for scale-up servers,\nbe applied to disaggregated systems as well? (ii) What are\nkey similarities or di \ufffferences? To answer these questions,\nwe\uffffrst conduct several microbenchmarks of representa-\ntive in-memory processing operators from modern database\nengines [ 4,14] and thus, Section 2 will introduce our exper-\nimental methodology. Sections 3 and 4 show our \uffffndings,\nclustered by operator complexity. Then, we extend our evalu-\nation focus to a recently proposed optimization technique in\nSection 5. After this evaluation, we give an overview about\nrelated work in Section 6. Finally, this paper closes with a\nsummary and a discussion of future work in Section 7.\n2 Evaluation Methodology\nMain memory is an important resource for data processing\nand its availability can be increased through several methods.\nIn this paper, we systematically investigate memory expan-\nsion through (i) scale-up with UPI, (ii) scale-out-like with\nRDMA over In \uffffniBand, and (iii) resource disaggregation\nwith a CXL attached DRAM card. To study the architectural\nimplications, we developed a comprehensive evaluation pro-\ntotype in C++ according to Figure 1. Further, we designed\ndi\ufffferent benchmarks, based on representative in-memorydata processing operators from database engines with both\na trivial (e. g., aggregation) and complex (e. g., hash join)\ninternal state to showcase, how memory distances and syn-\nchronous vs. asynchronous access in \uffffuence the processing\nbehavior. Currently, we do not have access to a single setup\nthat has both CXL and RDMA hardware. Thus, UPI and CXL\nbenchmarks, cf. Figure 1(a,c), are executed on a di \ufffferent\nphysical setup than the RDMA benchmarks, cf. Figure 1b.\nFor every benchmark, we refer to local and remote data,\nwhere local refers to data that is located on the same NUMA\nnode as the processing CPU core.\nHardware Setup. The benchmarks for Figure 1(a,c) are\nexecuted on a single server with 4th-generation Intel\u00aeXeon\u00ae\nScalable processors (code-named \"Sapphire Rapids\"), i. e., two\nIntel\u00aeXeon\u00aePlatinum 8480+ processors, with each having\n56 physical cores with a base frequency of 2.0GHz. Main\nmemory consists of 16 DDR5 memory DIMMs with 16GB\neach, which results in 128GBof memory per processor. Since\nthe processors do not support CXL type 3 devices, the Host\nto Memory Expansion (ME) interface uses a CXL Type 2\nlink (not to be confused with CXL 2.0, cf. [ 1]) but utilizes\nCXL.mem transactions. The experimental ME is realized us-\ning an Intel\u00aeAgilex \u2122I-Series FPGA Development Kit (DK-\nDEV-AGI027R1BES), which supports PCIe 5.0 x16 CXL 1.1\nconnectivity and DDR4 memory. This card hosts 2x 8GBSR\nDDR4-2666 components soldered to it and is hence exposing\n16 GB of memory as a third NUMA-node to the system.\nRDMA benchmarks are executed on a second server setup\nconsisting of two machines. One server is equipped with four\nIntel\u00aeXeon\u00aeGold 6130 with 16 cores each and a base clock of\n2.1GHz , the second server features four Intel\u00aeXeon\u00aeGold\n5120 with 14 cores each and a base clock of 2.2GHz . On both\nmachines, each CPU is directly connected to 96GBof local\nmemory. A Mellanox ConnectX-4 card with up to 100GBit /s\n(4xEDR) is connecting the NUMA socket 0 of both machines.", "start_char_idx": 254625, "end_char_idx": 258473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58c3e2d1-9ecd-4012-900b-206e9e1dc999": {"__data__": {"id_": "58c3e2d1-9ecd-4012-900b-206e9e1dc999", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0497b5fd-6d94-4a24-bde8-10e8ece24a24", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "46a5d8173f7a00a3e4b98cb6593b618e4b394b8df03f5462f623bd42bab27f87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a4b1129-a3a2-42a1-90d6-7731f4589f32", "node_type": "1", "metadata": {}, "hash": "cf9fcb41d0d729f1ef0540d2160603ca125982c1103fd53e0f048e36215d4dd0", "class_name": "RelatedNodeInfo"}}, "text": "This card hosts 2x 8GBSR\nDDR4-2666 components soldered to it and is hence exposing\n16 GB of memory as a third NUMA-node to the system.\nRDMA benchmarks are executed on a second server setup\nconsisting of two machines. One server is equipped with four\nIntel\u00aeXeon\u00aeGold 6130 with 16 cores each and a base clock of\n2.1GHz , the second server features four Intel\u00aeXeon\u00aeGold\n5120 with 14 cores each and a base clock of 2.2GHz . On both\nmachines, each CPU is directly connected to 96GBof local\nmemory. A Mellanox ConnectX-4 card with up to 100GBit /s\n(4xEDR) is connecting the NUMA socket 0 of both machines.\nTherefore, we pin working threads and memory usage solely\nto NUMA-node 0 on both machines when conducting the\nRDMA experiments.\nUltrapath Interconnect Setup. UPI is an interconnect\nbetween processors inside a NUMA machine, i. e., it connects\nNUMA nodes and allows for cache-coherent data transfer\ninside the coherent memory space. Figure 1a illustrates the\ngeneral setup. For these experiments, local data is placed on\nNUMA socket 0 and remote data is placed on another node,\nwhich is one NUMA-hop away, but still inside the very same\n17Near to Far: An Evaluation of Disaggregated Memory for In-Memory Data Processing DIMES \u201923, October 23, 2023, Koblenz, Germany\n\u0007\b\t\n\u000b\f\n\u000e\u000f\u0005\u001e\u0006\u0003\u001a,%(#\u0003\f\u0007\u0004\u0003*$1,% \u001e&\u0003 )+\",\u000f\f\u0010\u0007\u0010\f\b\u0007\u0007\b\u0007\f\n\u0007\b\t\n\u000b\f\n\u000e\u000f\u0005\u001f\u0006\u0003\u001a,%(#\u0003\b\u0007\u0007\u0004\u0003*$1,% \u001e&\u0003 )+\",\u0019\"')-\"\u0003\u0013)&.'(\u0003\u0013).(-\u0019\"&\u001e-%/\"\u0003\u0012\u001e(!0%!-$\u0003\u001c\u0004\u001d\u001a\u0018\u0015\u0013\u001b\u0016\u001a\u0018\u0015\t\u0019\u0014\u0017\u0011\nFigure 2. Relative behavior of UPI, CXL and RDMA with additional remote bu \uffffers for the aggregate kernel. Threads for\nremote bu \uffffers are \uffffrst assigned to physical cores if available, otherwise to hyperthreads.\nphysical machine. This placement requires the processing\nthread to access the data synchronously through UPI and to\ntransfer it into the local processor cache to process it.\nRemote Direct Memory Access Setup. Figure 1b illus-\ntrates the connection of two servers via PCIe-attached In \uffffni-\nBand (IB) adapters. In this setup, remote data is placed on a\nphysically di \ufffferent machine than the local data. The PCIe\nslot is directly attached to the NUMA socket, where the local\ndata is stored and accessing the remote data is performed\nasynchronously through the IB verbs read/write.\nCompute Express Link Setup. Our CXL setup involves\nthe experimental hardware prototype, which allows us to em-\nulate the basic CXL behavior. Local data is stored on the host\nand remote data is placed on NUMA node 3, which is directly\nconnected to both NUMA sockets of the host system. Allocat-\ning memory on this speci \uffffc node is performed through the\nlibnuma callnuma_alloc_on_node() and data access is as\nsimple as dereferencing a pointer. The compute capabilities\nof the FPGA are not used in the conducted experiments.\nOperator Types. To gain better insights into the e \uffffects\nof the hardware from the perspective of in-memory data pro-\ncessing, we selected typical query operators from modern\ndatabase engines [ 4,14] \u2013 as the prime representative for\nin-memory data processing. We divide our operators into\ntwo categories: (i) operators with trivial internal state and (ii)\noperators with complex internal state. For the trivial internal\nstate operators, we decided to employ the classes of aggre-\ngation and\ufffflteroperations. Both operations exhibit similar\nproperties, like a completely sequential data access on a sin-\ngle column and therefore, only the results of the aggregation\nkernel are reported. As an example for the aggregation, the\nsum is chosen as it mostly depends on the access speed for\nthe data. The operators were equally executed in the fol-\nlowing manner. A number of threads is spawned; for each\nthread, exactly one bu \uffffer\ufffflled with integer values between\n0 and 100 is created.", "start_char_idx": 257874, "end_char_idx": 261570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a4b1129-a3a2-42a1-90d6-7731f4589f32": {"__data__": {"id_": "5a4b1129-a3a2-42a1-90d6-7731f4589f32", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58c3e2d1-9ecd-4012-900b-206e9e1dc999", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "acd1a388ea6c1efec27e8c4a81822e8afb92233e9fedd4343ce64b029e149704", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7def49cb-991a-4787-854d-e651725ba458", "node_type": "1", "metadata": {}, "hash": "d0fb633f5a1e6c9f8691a1af14f362f55b2eb2c09c2f5a51382f36dbad0053c0", "class_name": "RelatedNodeInfo"}}, "text": "We divide our operators into\ntwo categories: (i) operators with trivial internal state and (ii)\noperators with complex internal state. For the trivial internal\nstate operators, we decided to employ the classes of aggre-\ngation and\ufffflteroperations. Both operations exhibit similar\nproperties, like a completely sequential data access on a sin-\ngle column and therefore, only the results of the aggregation\nkernel are reported. As an example for the aggregation, the\nsum is chosen as it mostly depends on the access speed for\nthe data. The operators were equally executed in the fol-\nlowing manner. A number of threads is spawned; for each\nthread, exactly one bu \uffffer\ufffflled with integer values between\n0 and 100 is created. Every bu \uffffer exceeds the cache size to\navoid caching-e \uffffects. All threads execute their respective\noperator on their own bu \uffffer. While the local bu \uffffer holds\n100 % of the data, the remote bu \uffffers are sized according to\nthe remote-to-local-bandwidth ratio. Generally, we reportrelative performance derived from the system bandwidth\ncalculated as the sum of all data sizes of the used bu \uffffers\ndivided by the time taken from starting all threads until the\nlast one is \uffffnished, i. e., the synchronized wallclock time.\nContrary to the trivial state operators, we also imple-\nmented the hash join as a computationally heavier operator\nto represent the category of complex state operators. This\noperator works on two columns and involves random access\nto the hash table. Again, a number of threads is spawned,\nbut this time for each thread there is one large table in local\nmemory \u2013 the probe-table \u2013 and up to 10 smaller tables for\ncreating the lookup-tables, on remote memory. Therefore,\neach thread has to perform a sequence of joins where the\nlarger local table is joined against the smaller remote table.\n3 Evaluation of Trivial State Operators\nWe start our evaluation by looking at using either 50 % (cf.\nFigure 2a) or 100 % (cf. Figure 2b) of the available physical\ncores for local work and up to 8 threads for remote work.\nThese experiments highlight the scaling behavior with an\nincreasingly larger amount of o \uffffoaded data for a given\ninterconnect type. Henceforth UPI denotes the link between\nthe NUMA nodes on the CXL hardware prototype and UPI2\nrefers to the link on the RDMA-supporting setup.\nFor each of the four experimental setups UPI, CXL, UPI2\nand RDMA, the results are normalized to their respective\nvalue at full local work and therefore, at 0 remote bu \uffffers.\nThus, 100 % for UPI and CXL is a di \ufffferent absolute value\nthan 100 % for UPI2 and RDMA. This is mainly due to the fact\nthat the experiments for UPI2/RDMA had to be executed on\na di\ufffferent machine than the ones for UPI/CXL. In all setups,\nthe remote bu \uffffer size that \uffffts their respective bandwidth\nbest is applied. Due to this setup, one line being above or\nbeneath another does not mean the absolute performance of\nthis technique is better or worse. In Figure 2, the results for\nthe usage of 50 % and100 % of the available physical cores\nfor the work on local bu \uffffers are shown. For 50 % (cf. Figure\n2a) the overall system bandwidth for UPI increases to a cer-\ntain degree, when more workers are added to process remote\nbu\uffffers. RDMA on the other hand decreases while UPI2 holds\n18DIMES \u201923, October 23, 2023, Koblenz, Germany Geyer et al.", "start_char_idx": 260852, "end_char_idx": 264181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7def49cb-991a-4787-854d-e651725ba458": {"__data__": {"id_": "7def49cb-991a-4787-854d-e651725ba458", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a4b1129-a3a2-42a1-90d6-7731f4589f32", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "9d42c526f58269ba82d5ad342abc3d89593041abfb843c7f744cde09d56f6f64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d4cd4f1-ed22-4aba-b0ce-1a8cbee31620", "node_type": "1", "metadata": {}, "hash": "ef6d7efd93312ecac7656fba0b19fcdc1c678029900f6721c4ec87ce8a4b830f", "class_name": "RelatedNodeInfo"}}, "text": "In all setups,\nthe remote bu \uffffer size that \uffffts their respective bandwidth\nbest is applied. Due to this setup, one line being above or\nbeneath another does not mean the absolute performance of\nthis technique is better or worse. In Figure 2, the results for\nthe usage of 50 % and100 % of the available physical cores\nfor the work on local bu \uffffers are shown. For 50 % (cf. Figure\n2a) the overall system bandwidth for UPI increases to a cer-\ntain degree, when more workers are added to process remote\nbu\uffffers. RDMA on the other hand decreases while UPI2 holds\n18DIMES \u201923, October 23, 2023, Koblenz, Germany Geyer et al.\n\u0005\u0006\u0007\b\t\n\u000b\f\n\u000e\u0006\u0005\u0006\u0006\u0006\u0007\u0006\b\u0006\t\u0006\n\u0006\u000b\u0006\u0007\b\t\n\u000b\f\n\u000e\u0006\u0005\u0006\u0006\u0006\u0007\u0006\b\u0006\t\u0006\n\u0006\u000b\n\u0005\u0006\u0007\b\t\n\u000b\f\n\u000e\u0006\u0005\u0006\u0006\u0006\u0007\u0006\b\u0006\t\u0006\n\u0006\u000b\u0006\u0005\n\u0005\u0004\n\u0005\u0004\u0006\u0005\u0005\u0004\n\u0012\u0015\u0018\u001a\u001c\u0015\u0003\u000f\u001d\u0016\u0016\u0015\u001b\u0003\u0010\u001a\u001d\u0019\u001c\u0011\u001a\u0014\u0013\u0017\u0003\u000f\u001d\u0016\u0016\u0015\u001b\u0003\u0010\u001a\u001d\u0019\u001c(a)RDMA - scaling remote data with 5%and10 % of local data.\u0005\u0007\t\u000b\n\u0006\u0005\u0006\u0007\u0006\t\u0006\u000b\u0006\n\u0007\u0005\u0007\u0007\u0007\t\u0007\u000b\u0007\n\u0006\t\f\u0006\u0005\u0006\b\u0006\u000b\u0006\u000e\u0007\u0007\u0007\n\u0007\n\b\u0006\b\t\b\f\t\u0005\t\b\t\u000b\t\u000e\n\u0007\n\n\u0006\u0005\n\u0005\u0007\t\u000b\n\u0006\u0005\u0006\u0007\u0006\t\u0006\u000b\u0006\n\u0007\u0005\u0007\u0007\u0007\t\u0007\u000b\u0007\n\u0006\n\u0005\u0004\n\u0005\u0004\u0006\u0005\u0005\u0004\n\u0012\u0015\u0018\u001a\u001c\u0015\u0003\u000f\u001d\u0016\u0016\u0015\u001b\u0003\u0010\u001a\u001d\u0019\u001c\u0011\u001a\u0014\u0013\u0017\u0003\u000f\u001d\u0016\u0016\u0015\u001b\u0003\u0010\u001a\u001d\u0019\u001c(b)CXL - scaling remote data with 10 % and15 % of local data.\nFigure 3. Comparing the relative aggregation bandwidth trend for di \ufffferent interconnect types with varying amounts of local\nand remote data bu \uffffers and adjusted remote data size in percent.\nthe performance of full local work. For CXL, the curve looks\nsimilar to the one for UPI and thus, implies a similar scaling\nbehavior, which is signi \uffffcantly better than the one shown by\nRDMA. For 100 % (cf. Figure 2b) UPI, CXL and UPI2 behave\nsimilarly, while being able to hold the performance of full\nlocal work. Contrarily, RDMA is dropping fast throughout\nthe experiment because of hyperthreading e \uffffects. At this\npoint, some of the processing threads and RDMA infrastruc-\nture threads are scheduled on the same physical core and\nthus induce triple usage of these cores and therefore, a per-\nformance decrease. This is highlighted even more by the\nmostly constant curve of UPI2 on the same machine. These\n\uffffgures show that while RDMA is capable of mostly holding\nits performance if there are resources available, it does not\nmake sense to use it in high-workload-scenarios. UPI and\nCXL on the other hand scale well and o \uffffer the possibility to\nincrease the system performance.\nAs these \uffffgures only show a very selected view of the\nwhole search space, we o \uffffer a more complete overview for\nRDMA (cf. Figure 3a) and CXL (cf. Figure 3b) in the form of\nheatmaps. These show the impact of quantity and size of re-\nmote bu \uffffers by reporting the relative bandwidth, compared\nto the local maximum per heatmap. Figure 3 shows each\ncombination for local (rows) and remote bu \uffffers (columns)\nfor RDMA, i. e., up to 16 each, and from 1 to 56 local and 0 to\n28 remote bu \uffffers for CXL, with varying remote data sizes.\nThe heatmaps of Figure 3a show the relative performance\ngains on the RDMA hardware for 5%and10 % remote bu \uffffer\nsize compared to the \uffffxed size local bu \uffffer. With the right\nfraction of remote data, it is possible to stay roughly at the\nsame level of full local work.", "start_char_idx": 263566, "end_char_idx": 266522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d4cd4f1-ed22-4aba-b0ce-1a8cbee31620": {"__data__": {"id_": "3d4cd4f1-ed22-4aba-b0ce-1a8cbee31620", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7def49cb-991a-4787-854d-e651725ba458", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "32a1bcb0eb30f53f898560116c8859ae89f9797b410e0f8af53de664472b2fdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4779ff58-fafa-48b0-87f6-f5ceba14be18", "node_type": "1", "metadata": {}, "hash": "6c63b22343f42492ea4e81306fb36f6eda3bb5d3b0fc27e05d919ce546080736", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3b) in the form of\nheatmaps. These show the impact of quantity and size of re-\nmote bu \uffffers by reporting the relative bandwidth, compared\nto the local maximum per heatmap. Figure 3 shows each\ncombination for local (rows) and remote bu \uffffers (columns)\nfor RDMA, i. e., up to 16 each, and from 1 to 56 local and 0 to\n28 remote bu \uffffers for CXL, with varying remote data sizes.\nThe heatmaps of Figure 3a show the relative performance\ngains on the RDMA hardware for 5%and10 % remote bu \uffffer\nsize compared to the \uffffxed size local bu \uffffer. With the right\nfraction of remote data, it is possible to stay roughly at the\nsame level of full local work. But when exceeding the opti-\nmal amount of remote data, the overall performance drops\nsigni \uffffcantly as can be seen in the right heatmap of Figure 3a\nthat has an overall lighter color. As the threads that work\non the remote bu \uffffers are mostly waiting, their in \uffffuence\nby hyperthreading is not that large. The very light spot in\nthe lower right of each heatmap is a remarkable occurrence,\nwhich clearly indicates the triple usage of one core, i. e., the\nRDMA-communication-threads, local and remote working\u0006\u0007\b\t\u0005\u0004\u0012!\u001a\u001f$\u0003\u000b\u001e!\u001d \u0006\u0007\b\t\u0013\u001a\u001c\u0017 \u001b\"\u001a\u0003\u0014\u001c\u001e#\u0019\u001e#\u001d\u0003\n\u0017\u0018 \u001e\u001f\u0015\u0011\u000e\u000b\u0016\u000f\u0015\u0011\u000e\u0006\u0013\f\u0010\nFigure 4. Relative behavior of UPI, CXL, UPI2 and RDMA\nwith additional remote bu \uffffers for the hash-join kernel. Using\n100 % of the available physical cores.\nthreads. This leads to the conclusion that RDMA can be used\nfor o \uffffoading parts of the data, but its characteristics limit\nits scalability in this use-case.\nFigure 3b in comparison shows the same experiment for\nCXL but di \ufffferent results. The results clearly show that the\nfollowing two conclusions can be drawn: (i) As long as there\nare physical cores available for processing remote bu \uffffers\n(all rows except 56), we can either gain relative system band-\nwidth or at least do not lose out on any. (ii) The staircase\npattern shows that a combination of remote and local bu \uffffer\nprocessing yields approximately the same performance as\nthe next step. Therefore, we can conclude that it is possible\nto work on data located on CXL memory without sacri \uffffcing\nperformance while gaining memory capacities. The lower\nright corner of each heatmap staircase pattern exhibits an\noverall lower bandwidth, which is because there are no phys-\nical cores left for processing and thus, hyperthreads are used,\nmeaning double work for the physical core on which this\nhyperthread is scheduled. This e \uffffect becomes increasingly\nprevalent with larger remote bu \uffffers, as there is also more\nwork to do for the hyperthread. This observation is indepen-\ndent of the actual data placement. Additionally, there is a\nshift to the left of the dark part of each heatmap visible with\n19Near to Far: An Evaluation of Disaggregated Memory for In-Memory Data Processing DIMES \u201923, October 23, 2023, Koblenz, Germany\n\b\n\f\u000e\u0010\t\b\t\n\t\f\t\u000e\t\u0010\n\b\u0005$\u0006\u0003 2+.)\u0003\n\b\u0004\u00030*72+&$,\u0003&/1(2\b\t\b\n\b\u000b\b\f\b\n\b\u000e\b\u000f\b\u0010\b\u0011\b\n\b\n\f\u000e\u0010\t\b\t\n\t\f\t\u000e\t\u0010\n\b\u0005%\u0006\u0003 2+.)\u0003\t\b\b\u0004\u00030*72+&$,\u0003&/1(2\u001d4(17\u0003\u0015/4.3\u0014$.'5+'3*\u0003\"\u0018+\u0014\u00072#\u001f70(\u0012 \u001c\u0019\u0015!\u001a \u001c\u0019\n\u001e\u0016\u001b\u00138\u0003\u001760(1+-(.3\u00122-$,,\u00031(-,$1)(\u00031(-\nFigure 5. Absolute behavior of UPI, CXL and RDMA with additional remote bu \uffffers for the hash-join kernel applying pipeline\ngrouping and prefetching strategies.\nincreasing remote data size, which is caused by the band-\nwidth of the CXL device.", "start_char_idx": 265878, "end_char_idx": 269155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4779ff58-fafa-48b0-87f6-f5ceba14be18": {"__data__": {"id_": "4779ff58-fafa-48b0-87f6-f5ceba14be18", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d4cd4f1-ed22-4aba-b0ce-1a8cbee31620", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "af1f79375e479c4b20ac3586f258665c2aa07f00650be12c2a3f5ddecb23682a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea7b9ea5-a5ff-4798-9593-b5ef1e2b349d", "node_type": "1", "metadata": {}, "hash": "9df89f453571edbe13d0794b4bae1510b51b36812d89fadcadfe64c6866a7544", "class_name": "RelatedNodeInfo"}}, "text": ")\u0003\t\b\b\u0004\u00030*72+&$,\u0003&/1(2\u001d4(17\u0003\u0015/4.3\u0014$.'5+'3*\u0003\"\u0018+\u0014\u00072#\u001f70(\u0012 \u001c\u0019\u0015!\u001a \u001c\u0019\n\u001e\u0016\u001b\u00138\u0003\u001760(1+-(.3\u00122-$,,\u00031(-,$1)(\u00031(-\nFigure 5. Absolute behavior of UPI, CXL and RDMA with additional remote bu \uffffers for the hash-join kernel applying pipeline\ngrouping and prefetching strategies.\nincreasing remote data size, which is caused by the band-\nwidth of the CXL device. Thus, it is clear that the o \uffffoaded\namount of data is a key performance factor. Moreover, it\nindicates that CXL scales with its available bandwidth and\ntherefore, increases the portion of e \uffffciently o \uffffoadable data.\n4 Evaluation of Complex State Operators\nAs already introduced (cf. Section 2), we also evaluated the\nhash join as a complex state operator and show the results in\nFigure 4, again with adjusting the remote table size according\nto the relative bandwidth discrepancy. The relative system\nbandwidth for the other experiments does not make sense\nfor the hash-join because the amount of work of each thread\nvaries according to the number of joins. Thus, we report the\nrelative slowdown compared to a single hash-join with in-\ncreasing number of joins that are executed sequentially. The\nvery na\u00efve assumption would be a constant factor following\nthe \"twice the work, twice the time\" principle (depicted as\ndashed, grey line), but this is not quite the case. In Figure 4,\nagain the results for 100 % of the physical cores working\nin parallel on individual bu \uffffers is displayed. Even for the\ncomplex state operator, we observe that UPI and CXL be-\nhave similarly and RDMA cannot keep up with their scaling\nbehavior. Counter-intuitively, UPI2 seems to perform worse\nthan RDMA, which is due to the normalization. Each curve is\nagain normalized to its performance for only one join. While\nall approaches with direct synchronous data access can start\nimmediately, RDMA has to wait on the \uffffrst data package\nand thus, the \uffffrst join for RDMA is slow and the subsequent\njoins can pro \ufffft from explicit prefetching. Remarkable is the\nfact that all approaches are below the assumed slowdown.\nThis happens because with an increasing amount of executed\njoins, the actual compute portion and local random access\ninto the hash table becomes the limiting factor and hardware\nacceleration mechanisms like prefetching are able to hide a\nlot of the introduced latency for retrieving the data from the\nremote source. Due to the asynchronous character of RDMA,it cannot bene \ufffft from these mechanisms that well. The plots\nfor50 % physical core usage show similar tendencies and are\nthus omitted.\n5 Evaluation of Advanced Techniques\nOur initial memory expansion experiments with na\u00efvely\nusing RDMA and CXL showed, that expectedly the network\nis indeed a performance bottleneck. While CXL is able to\nmatch the scaling performance of UPI, especially RDMA does\nnot scale well in the shown benchmarks. To overcome that,\nwe have developed a technique called pipeline grouping [ 5],\nwhich aims to e \uffffciently overlap processing and network\ntransfer. We want to build upon this principle and investigate\nthe implications of explicit data access grouping for data\nthat resides in a coherent address space. To facilitate that,\nwe changed the access pattern for our UPI/UPI2 and CXL\nexperiments from a synchronous to an asynchronous pattern.\nMeaning the data is explicitly copied from the remote source\nto local memory for further usage. This allows us to group\nthe execution of multiple hash joins together and trigger data\ntransfer much earlier, allowing for an e \uffffcient prefetching\nwhile reducing the network tra \uffffc by reducing redundant\ndata transfers. However, this comes with the cost of pinning\nsome threads - 5 in our case - for executing the copying\noperation from the remote source to local memory.\nFor a better overall impression, the following benchmark\nfeatures up to 20 sequential hash joins, as introduced before.\nThe used small columns are distinct to each other, while one\nlarge column is used for all of the 20 joins. Therefore, it can\nbe seen as up to 20 individual pipelines on the same base\ncolumn.", "start_char_idx": 268813, "end_char_idx": 272849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea7b9ea5-a5ff-4798-9593-b5ef1e2b349d": {"__data__": {"id_": "ea7b9ea5-a5ff-4798-9593-b5ef1e2b349d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4779ff58-fafa-48b0-87f6-f5ceba14be18", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "921a00385e4ec4b6958df204d54081fed03356f793df2d678d7dc751ec7165fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48ae52fc-9f5f-449c-8859-76fcffb3dbe7", "node_type": "1", "metadata": {}, "hash": "2953bc108d23e118aefc7a7201c8f5de3ab41ba1929c6be171d1712d8b2ec4fa", "class_name": "RelatedNodeInfo"}}, "text": "Meaning the data is explicitly copied from the remote source\nto local memory for further usage. This allows us to group\nthe execution of multiple hash joins together and trigger data\ntransfer much earlier, allowing for an e \uffffcient prefetching\nwhile reducing the network tra \uffffc by reducing redundant\ndata transfers. However, this comes with the cost of pinning\nsome threads - 5 in our case - for executing the copying\noperation from the remote source to local memory.\nFor a better overall impression, the following benchmark\nfeatures up to 20 sequential hash joins, as introduced before.\nThe used small columns are distinct to each other, while one\nlarge column is used for all of the 20 joins. Therefore, it can\nbe seen as up to 20 individual pipelines on the same base\ncolumn. This benchmark is executed in two variations: (i)\nWe place the smaller columns remote and the large column is\nplaced in local memory and (ii) the placements are inverted.\nThe results of this benchmark are reported in Figure 5. For\nthe usage of 50 % of the available physical cores as depicted\nin Figure 5a, it is visible that again UPI and CXL behave\nsimilarly. The throughput di \ufffference between the UPI and\n20DIMES \u201923, October 23, 2023, Koblenz, Germany Geyer et al.\nCXL curves is easily explained with the limited bandwidth of\nour prototypical CXL hardware setup. We are con \uffffdent that\nthis gap will shrink dramatically, when \uffffnal CXL products\nare hitting the market. It is also visible that with a higher\nnumber of queries the gap between all 4 curves grows smaller.\nThis is because more data can be reused and the absolute\ndata transfer is reduced and therefore, the limited bandwidth\nhas a lesser impact. RDMA and UPI2 on the other hand,\nperform signi \uffffcantly worse than UPI and CXL. At \uffffrst sight,\nthis seems counter-intuitive to our evaluation of pipeline\ngroups [ 5], but going more into detail, instead of reducing\nthe redundant data transfer in parallel executions, we applied\nit to sequential joins. Therefore, RDMA cannot bene \ufffft that\nwell, as the amount of simultaneously requested data is not\nreduced and thus, its lower bandwidth and higher latency is\na more dominating factor than for UPI and CXL. In addition\nto that, the local memory bandwidth of the UPI2/RDMA\nmachine is signi \uffffcantly lower than the one of the UPI/CXL\nmachine, which \uffffattens the UPI2/RDMA curves even more.\nFurthermore, while all benchmarks need additional threads\nfor the explicit data transfer (i. e., to force the asynchronous\nprefetching according to [ 5]), the utilization of the RDMA\ncommunication threads is much higher compared to UPI and\nCXL and thus, in \uffffuences the performance of the executing\nphysical cores much stronger. For the usage of 100 % of the\navailable physical cores, shown in Figure 5b, the behavior is\nvery similar, only the absolute values are larger. It is visible\nthat the bandwidth limitation, especially for RDMA, has a\nhigher in \uffffuence for lesser query counts.\nOverall, this benchmark has shown that, even though\ndirect access to the CXL memory is possible, it can be bene \uffff-\ncial to have an explicit asynchronous transfer from the CXL\nmemory to local memory. If data access grouping is applied,\nit is less important which data is placed on what memory\nregion, but rather if synchronous or asynchronous access\nshall be leveraged.\n6 Related Work\nHardware disaggregation is by no means trivial and also\na hot contemporary topic as presented in [ 20]. In a lot of\ncases, the na\u00efve usage of disaggregation-strategies such as\nRDMA or CXL provide usually a higher latency and less\nthroughput than state-of-the-art server architectures due\nto a longer physical distance. From our point of view, the\nmost promising approaches for using RDMA are operator\npush-down as implemented in Farview [ 10] and pipeline\ngrouping as introduced by us in [ 5] and applied in this paper.\nBoth approaches o \uffffer appropriate techniques to reduce the\nnetwork transfer signi \uffffcantly by either \uffffltering data before\ntransferring or avoiding redundant data transfers priorly.\nHowever, as comprehensively evaluated in this paper and\ndiscussed in [ 7], CXL is the better solution from the perspec-\ntive of in-memory data processing.", "start_char_idx": 272072, "end_char_idx": 276267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48ae52fc-9f5f-449c-8859-76fcffb3dbe7": {"__data__": {"id_": "48ae52fc-9f5f-449c-8859-76fcffb3dbe7", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea7b9ea5-a5ff-4798-9593-b5ef1e2b349d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "2c1c87af658e478704c8b0a2a2c4decd277fd5b337002255f907e19b9b399800", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4575b79e-aa35-4cc0-bcc4-ce7b079d113f", "node_type": "1", "metadata": {}, "hash": "1df2764d60ed8c4a142cb0de05b52783669b1269acde22456396684d6e98cea6", "class_name": "RelatedNodeInfo"}}, "text": "In a lot of\ncases, the na\u00efve usage of disaggregation-strategies such as\nRDMA or CXL provide usually a higher latency and less\nthroughput than state-of-the-art server architectures due\nto a longer physical distance. From our point of view, the\nmost promising approaches for using RDMA are operator\npush-down as implemented in Farview [ 10] and pipeline\ngrouping as introduced by us in [ 5] and applied in this paper.\nBoth approaches o \uffffer appropriate techniques to reduce the\nnetwork transfer signi \uffffcantly by either \uffffltering data before\ntransferring or avoiding redundant data transfers priorly.\nHowever, as comprehensively evaluated in this paper and\ndiscussed in [ 7], CXL is the better solution from the perspec-\ntive of in-memory data processing. Because of this, there is alot of research ongoing with hardware disaggregation using\nCXL. On the one hand, there are several papers [ 2,6,12,19,\n21] discussing the possibilities CXL o \uffffers, from simple mem-\nory pooling to memory sharing and communication through\nshared memory and how to use it to bypass limitations of\nstate-of-the-art-server architectures. Where [ 21] for example\no\uffffers an approach to combine CXL-enabled memory and\nSSDs at virtually no performance cost to reduce the TCO fur-\nther. On the other hand, papers like [ 7,11,16] laid their focus\non the evaluation of early CXL devices in di \ufffferent scenarios,\nsimilar to this paper. With [ 16] providing the \uffffrst absolute\nnumbers of a CXL protoype device in microbenchmarks as\nwell as real-world scenarios and [ 11] using CXL as expan-\nsion for the in-memory database system SAP HANA and\napplying typical database benchmarks as TPC-C and TPC-H,\nthere are studies of CXL as memory expansion. While [ 3,15]\npropose an approach for pushing down some computation to\nthe CXL memory device to reduce the network-bottleneck;\nhowever, to the best our knowledge, none of them tried an\nexplicit data-transfer model as we did in this paper. There-\nfore, it remains a future \uffffeld of research to combine these\napproaches which should be very interesting, especially with\nCXL-enabled FPGAs.\n7 Conclusion\nE\uffffcient in-memory data processing relies on the availability\nof su \uffffcient resources, be it CPU time or available main mem-\nory. In order to meet the increasing resource demands, hard-\nware disaggregation is a promising concept for the future.\nIn line with ongoing research in this direction, we presented\na comprehensive evaluation of the implications of di \ufffferent\nmemory expansion methods (UPI, RDMA, and CXL) from\nthe perspective of in-memory data processing in this paper.\nWhile recent work on RDMA has seemingly been a promis-\ning way out of over-provisioning of memory resources and\nhigh costs, in this paper we reproduced its limitations re-\ngarding scalability for memory expansion. In addition, we\nwere able to show that \u2013 for representative operators from\nin-memory database engines and despite having an early\nCXL prototype hardware \u2013 (i) CXL yields comparable behav-\nior to UPI and outperforms RDMA and (ii) CXL scales well.\nHence, we see CXL as the most promising memory expan-\nsion technique for real world applications in the domain of\nin-memory data processing.\nAcknowledgments\nThis work was partly funded by (1) the German Research\nFoundation (DFG) via a Reinhart Koselleck-Project (LE-1416/28-\n1) and (2) the European Union\u2019s Horizon 2020 research and in-\nnovation program under grant agreement No 957407 (DAPHNE).\nWe would like to thank our colleagues Oleg Struk, Otto\nBruggeman and Suprasad Mutalik Desai from Intel for their\nsupport with the CXL machine setup.\n21Near to Far: An Evaluation of Disaggregated Memory for In-Memory Data Processing DIMES \u201923, October 23, 2023, Koblenz, Germany\nReferences\n[1]Minseon Ahn, Andrew Chang, Donghun Lee, Jongmin Gim, Jungmin\nKim, Jaemin Jung, Oliver Rebholz, Vincent Pham, Krishna T. Malladi,\nand Yang-Seok Ki. 2022. Enabling CXL Memory Expansion for In-\nMemory Database Management Systems. In DaMoN@SIGDMO . 8:1\u2013\n8:5.", "start_char_idx": 275517, "end_char_idx": 279501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4575b79e-aa35-4cc0-bcc4-ce7b079d113f": {"__data__": {"id_": "4575b79e-aa35-4cc0-bcc4-ce7b079d113f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48ae52fc-9f5f-449c-8859-76fcffb3dbe7", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "84c56e7f03d85509d608ba664cdd074991dc21ff19424163dc2b1c3a4da0997a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cf51d9e-9b84-4b14-8b4b-bc3ee0310262", "node_type": "1", "metadata": {}, "hash": "1992849625bd74f0e2fc581b87392dfb9fcdf9625d6c5214811a7c7a34004d16", "class_name": "RelatedNodeInfo"}}, "text": "We would like to thank our colleagues Oleg Struk, Otto\nBruggeman and Suprasad Mutalik Desai from Intel for their\nsupport with the CXL machine setup.\n21Near to Far: An Evaluation of Disaggregated Memory for In-Memory Data Processing DIMES \u201923, October 23, 2023, Koblenz, Germany\nReferences\n[1]Minseon Ahn, Andrew Chang, Donghun Lee, Jongmin Gim, Jungmin\nKim, Jaemin Jung, Oliver Rebholz, Vincent Pham, Krishna T. Malladi,\nand Yang-Seok Ki. 2022. Enabling CXL Memory Expansion for In-\nMemory Database Management Systems. In DaMoN@SIGDMO . 8:1\u2013\n8:5.\n[2]Daniel S. Berger, Daniel Ernst, Huaicheng Li, Pantea Zardoshti, Monish\nShah, Samir Rajadnya, Scott Lee, Lisa Hsu, Ishwar Agarwal, Mark D.\nHill, and Ricardo Bianchini. 2023. Design Tradeo \uffffs in CXL-Based\nMemory Pools for Public Cloud Platforms. IEEE Micro 43, 2 (2023),\n30\u201338. h\uffffps://doi.org/10.1109/MM.2023.3241586\n[3]David Boles, Daniel Waddington, and David A. Roberts. 2023. CXL-\nEnabled Enhanced Memory Functions. IEEE Micro 43, 2 (2023), 58\u201365.\nh\uffffps://doi.org/10.1109/MM.2022.3229627\n[4]Peter A. Boncz, Martin L. Kersten, and Stefan Manegold. 2008. Breaking\nthe memory wall in MonetDB. Commun. ACM 51, 12 (2008), 77\u201385.\n[5]Andreas Geyer, Alexander Krause, Dirk Habich, and Wolfgang Lehner.\n2023. Pipeline Group Optimization on Disaggregated Systems. In\nCIDR .\n[6]Andreas Geyer, Daniel Ritter, Dong Hun Lee, Minseon Ahn, Johannes\nPietrzyk, Alexander Krause, Dirk Habich, and Wolfgang Lehner. 2023.\nWorking with Disaggregated Systems. What are the Challenges and\nOpportunities of RDMA and CXL?. In BTW , Vol. P-331. 751\u2013755.\n[7]Donghyun Gouk, Miryeong Kwon, Hanyeoreum Bae, Sangwon Lee,\nand Myoungsoo Jung. 2023. Memory Pooling With CXL. IEEE Micro\n43, 2 (2023), 48\u201357. h\uffffps://doi.org/10.1109/MM.2023.3237491\n[8]Tim Kiefer, Benjamin Schlegel, and Wolfgang Lehner. 2013. BTW,\nVol. P-214. 185\u2013204.\n[9]Thomas Kissinger, Tim Kiefer, Benjamin Schlegel, Dirk Habich, Daniel\nMolka, and Wolfgang Lehner. 2014. ERIS: A NUMA-Aware In-Memory\nStorage Engine for Analytical Workload. In ADMS@VLDB . 74\u201385.\n[10] Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin\nTaranov, Dejan S. Milojicic, and Gustavo Alonso. 2022. Farview: Dis-\naggregated Memory with Operator O \uffff-loading for Database Engines.\nInCIDR .\n[11] Donghun Lee, Thomas Willhalm, Minseon Ahn, Suprasad Mutalik De-\nsai, Daniel Booss, Navneet Singh, Daniel Ritter, Jungmin Kim, and\nOliver Rebholz. 2023. Elastic Use of Far Memory for In-Memory Data-\nbase Management Systems. In DaMoN@SIGMOD . 35\u201343.\n[12] Huaicheng Li, Daniel S. Berger, Lisa Hsu, Daniel Ernst, Pantea Zar-\ndoshti, Stanko Novakovic, Monish Shah, Samir Rajadnya, Scott Lee,\nIshwar Agarwal, Mark D. Hill, Marcus Fontoura, and Ricardo Bian-\nchini. 2023. Pond: CXL-Based Memory Pooling Systems for CloudPlatforms.", "start_char_idx": 278955, "end_char_idx": 281744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cf51d9e-9b84-4b14-8b4b-bc3ee0310262": {"__data__": {"id_": "0cf51d9e-9b84-4b14-8b4b-bc3ee0310262", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4575b79e-aa35-4cc0-bcc4-ce7b079d113f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "44a4283ab43018a6b77f3c06dafd045525555b09ffa0ebf275c3a99f1424c5da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1da21a9-4a9b-469e-9def-821dc49ed7fe", "node_type": "1", "metadata": {}, "hash": "cc876055aadf87c93ccbf8752ff741e964eec270169810f64585081e2400dea7", "class_name": "RelatedNodeInfo"}}, "text": "InCIDR .\n[11] Donghun Lee, Thomas Willhalm, Minseon Ahn, Suprasad Mutalik De-\nsai, Daniel Booss, Navneet Singh, Daniel Ritter, Jungmin Kim, and\nOliver Rebholz. 2023. Elastic Use of Far Memory for In-Memory Data-\nbase Management Systems. In DaMoN@SIGMOD . 35\u201343.\n[12] Huaicheng Li, Daniel S. Berger, Lisa Hsu, Daniel Ernst, Pantea Zar-\ndoshti, Stanko Novakovic, Monish Shah, Samir Rajadnya, Scott Lee,\nIshwar Agarwal, Mark D. Hill, Marcus Fontoura, and Ricardo Bian-\nchini. 2023. Pond: CXL-Based Memory Pooling Systems for CloudPlatforms. In Proceedings of the 28th ACM International Conference\non Architectural Support for Programming Languages and Operating\nSystems, Volume 2 (Vancouver, BC, Canada) (ASPLOS 2023) . Asso-\nciation for Computing Machinery, New York, NY, USA, 574\u2013587.\nh\uffffps://doi.org/10.1145/3575693.3578835\n[13] Iraklis Psaroudakis, Tobias Scheuer, Norman May, Abdelkader Sellami,\nand Anastasia Ailamaki. 2016. Adaptive NUMA-aware data placement\nand task scheduling for analytical workloads in main-memory column-\nstores. PVLDB 10, 2 (2016), 37\u201348.\n[14] Mark Raasveldt and Hannes M\u00fchleisen. 2019. DuckDB: an Embeddable\nAnalytical Database. In SIGMOD . 1981\u20131984.\n[15] Joonseop Sim, Soohong Ahn, Taeyoung Ahn, Seungyong Lee,\nMyunghyun Rhee, Jooyoung Kim, Kwangsik Shin, Donguk Moon,\nEuiseok Kim, and Kyoung Park. 2023. Computational CXL-Memory\nSolution for Accelerating Memory-Intensive Applications. IEEE Com-\nputer Architecture Letters 22, 1 (2023), 5\u20138. h\uffffps://doi.org/10.1109/\nLCA.2022.3226482\n[16] Yan Sun, Yifan Yuan, Zeduo Yu, Reese Kuper, Ipoom Jeong, Ren Wang,\nand Nam Sung Kim. 2023. Demystifying CXL Memory with Genuine\nCXL-Ready Systems and Devices. arXiv preprint arXiv:2303.15375\n(2023).\n[17] The CXL Consortium. 2019. Compute Express Link. h\uffffps://www.\ncomputeexpresslink.org/ . [Online; accessed 14-December-2022].\n[18] The CXL Consortium. 2019. Compute Express Link. h\uffffps://www.\ncomputeexpresslink.org/projects-3 . [Online; accessed 14-December-\n2022].\n[19] Jacob Wahlgren, Maya Gokhale, and Ivy B. Peng. 2022. Evaluating\nEmerging CXL-enabled Memory Pooling for HPC Systems. In 2022\nIEEE/ACM Workshop on Memory Centric High Performance Computing\n(MCHPC) . 11\u201320. h\uffffps://doi.org/10.1109/MCHPC56545.2022.00007\n[20] Ruihong Wang, Jianguo Wang, Stratos Idreos, M. Tamer \u00d6zsu, and\nWalid G. Aref. 2022. The Case for Distributed Shared-Memory\nDatabases with RDMA-Enabled Memory Disaggregation. PVLDB 16, 1\n(2022), 15\u201322.\n[21] Qirui Yang, Runyu Jin, Bridget Davis, Devasena Inupakutika, and\nMing Zhao. 2022. Performance Evaluation on CXL-enabled Hybrid\nMemory Pool. In 2022 IEEE International Conference on Networking,\nArchitecture and Storage (NAS) . 1\u20135. h\uffffps://doi.org/10.1109/NAS55553.\n2022.9925356\n[22] Hao Yu, Jos\u00e9 E. Moreira, Parijat Dube, I-Hsin Chung, and Li Zhang.\n2007. Performance Studies of a WebSphere Application, Trade, in\nScale-out and Scale-up Environments.", "start_char_idx": 281207, "end_char_idx": 284105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1da21a9-4a9b-469e-9def-821dc49ed7fe": {"__data__": {"id_": "b1da21a9-4a9b-469e-9def-821dc49ed7fe", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cf51d9e-9b84-4b14-8b4b-bc3ee0310262", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e665a07d661078718106cb17c5eb7b1409a121ebfca2c603e2430b63c5ed30e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4021aae7-49d7-4a21-80fc-3d35da841496", "node_type": "1", "metadata": {}, "hash": "c1d8b6c1d9fc6145544fe6b393084519e4e46fd478115c3f0f8bf44bda361a86", "class_name": "RelatedNodeInfo"}}, "text": "2022. The Case for Distributed Shared-Memory\nDatabases with RDMA-Enabled Memory Disaggregation. PVLDB 16, 1\n(2022), 15\u201322.\n[21] Qirui Yang, Runyu Jin, Bridget Davis, Devasena Inupakutika, and\nMing Zhao. 2022. Performance Evaluation on CXL-enabled Hybrid\nMemory Pool. In 2022 IEEE International Conference on Networking,\nArchitecture and Storage (NAS) . 1\u20135. h\uffffps://doi.org/10.1109/NAS55553.\n2022.9925356\n[22] Hao Yu, Jos\u00e9 E. Moreira, Parijat Dube, I-Hsin Chung, and Li Zhang.\n2007. Performance Studies of a WebSphere Application, Trade, in\nScale-out and Scale-up Environments. In IPDPS . 1\u20138.\n22CXL Memory as Persistent Memory for Disaggregated HPC:\nA Practical Approach\nYehonatan Fridman\nBen-Gurion University, NRCN, Israel\nfridyeh@post.bgu.ac.ilSuprasad Mutalik Desai, Navneet Singh\nIntel, India\n{suprasad.desai,navneet.singh}@intel.com\nThomas Willhalm\nIntel, Germany\nthomas.willhalm@intel.comGal Oren\nTechnion, NRCN, Israel\ngaloren@cs.technion.ac.il\nABSTRACT\nIn the landscape of High-Performance Computing (HPC), the quest\nfor e \uffffcient and scalable memory solutions remains paramount.\nThe advent of Compute Express Link (CXL) introduces a promis-\ning avenue with its potential to function as a Persistent Memory\n(PMem) solution in the context of disaggregated HPC systems. This\npaper presents a comprehensive exploration of CXL memory\u2019s\nviability as a candidate for PMem, supported by physical experi-\nments conducted on cutting-edge multi-NUMA nodes equipped\nwith CXL-attached memory prototypes. Our study not only bench-\nmarks the performance of CXL memory but also illustrates the\nseamless transition from traditional PMem programming models\nto CXL, reinforcing its practicality.\nTo substantiate our claims, we establish a tangible CXL prototype\nusing an FPGA card embodying CXL 1.1/2.0 compliant endpoint\ndesigns (Intel FPGA CXL IP). Performance evaluations, executed\nthrough the STREAM and STREAM-PMem benchmarks, showcase\nCXL memory\u2019s ability to mirror PMem characteristics in App-Direct\nandMemory Mode while achieving impressive bandwidth metrics\nwith Intel 4th generation Xeon (Sapphire Rapids) processors.\nThe results elucidate the feasibility of CXL memory as a per-\nsistent memory solution, outperforming previously established\nbenchmarks. In contrast to published DCPMM results, our CXL-\nDDR4 memory module o \uffffers comparable bandwidth to local DDR4\nmemory con \uffffgurations, albeit with a moderate decrease in per-\nformance. The modi \uffffed STREAM-PMem application underscores\nthe ease of transitioning programming models from PMem to CXL,\nthus underscoring the practicality of adopting CXL memory.\nThe sources of this work are available at: https://github.com/\nScienti \uffffc-Computing-Lab-NRCN/STREAMer .\nKEYWORDS\nCXL, Memory disaggregation, Persistent Memory (PMem), Intel\nOptane DCPMM, HPC, STREAM, STREAM-PMem, STREAMer\n1 INTRODUCTION\n1.1 Current HPC Memory Solutions Limitations\nAs the era of Exa-Scale computing unfolds, the demand for ana-\nlyzing, manipulating, and storing massive amounts of data inten-\nsi\uffffes [60]. Exascale systems are designed to meet these demands\nand enable the execution of a broad spectrum of computations,\nranging from loosely to tightly coupled tasks, including CFD simu-\nlations and deep learning optimizations [ 11]. Memory and storageresources play a crucial role in the performance and scalability of\nthese computations [ 32]. Memory factors such as capacity, latency,\nand bandwidth are responsible for successfully handling extensive\ntasks and delivering data to processing units promptly [ 45]. In sci-\nenti\uffffc computing, storage devices hold signi \uffffcance for preserving\ndiagnostics throughout computations [ 37]. Notably, the growing\nfrequency of failures in exascale machines emphasizes the signi \uffff-\ncance of storing vast data volumes to support recovery and bolster\nfault tolerance [ 8,14].", "start_char_idx": 283529, "end_char_idx": 287366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4021aae7-49d7-4a21-80fc-3d35da841496": {"__data__": {"id_": "4021aae7-49d7-4a21-80fc-3d35da841496", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1da21a9-4a9b-469e-9def-821dc49ed7fe", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "de5214d5ed8b6bba9009161659ec9abeb8ef0df6682000d5c65585c53a280ee0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a430a03-545c-4596-ab32-2cdac57973b7", "node_type": "1", "metadata": {}, "hash": "5338ab06c0f429152525a7c66960433d214998b501e97029e51631c2f1cb98bc", "class_name": "RelatedNodeInfo"}}, "text": "Exascale systems are designed to meet these demands\nand enable the execution of a broad spectrum of computations,\nranging from loosely to tightly coupled tasks, including CFD simu-\nlations and deep learning optimizations [ 11]. Memory and storageresources play a crucial role in the performance and scalability of\nthese computations [ 32]. Memory factors such as capacity, latency,\nand bandwidth are responsible for successfully handling extensive\ntasks and delivering data to processing units promptly [ 45]. In sci-\nenti\uffffc computing, storage devices hold signi \uffffcance for preserving\ndiagnostics throughout computations [ 37]. Notably, the growing\nfrequency of failures in exascale machines emphasizes the signi \uffff-\ncance of storing vast data volumes to support recovery and bolster\nfault tolerance [ 8,14].\nHowever, the traditional memory and storage hierarchy in HPC\nsystems reveals notable gaps that impose critical constraints on\nscienti \uffffc computations [ 32]. From the vantage point of memory\narchitecture, DRAM has inherent limitations of bandwidth and\ncapacity that impact performance and prevent the processing of\nlarge-scale problems [ 56,58]. From the storage perspective, tradi-\ntional devices (such as HDDs and SSDs) provide large capacities\nbut exhibit very slow access times, leading to signi \uffffcant overheads\nfor I/O-bound applications and fault tolerance mechanisms [ 37].\nThese gaps and limitations of traditional hardware highlight the\nongoing endeavors to expand the memory-storage hierarchy and\ndevelop novel memory architectures and solutions. A notable ex-\nample is Non-Volatile RAM [ 59,71] (on which we elaborate in\nsubsection 1.2 ).\nWhile High-Bandwidth Memory (HBM) [ 28] has been intro-\nduced as a solution to enhance memory performance, it doesn\u2019t\nentirely alleviate the problem [ 29]. HBM memory modules are\nstacked vertically, allowing for higher memory bandwidth due to\ntheir increased parallelism. However, even with HBM, the memory\ncapacity remains limited compared to conventional DDR (Double\nData Rate) memory modules [ 67]. This limitation can still lead to\nconstraints in memory-intensive applications that require larger\nmemory spaces [ 44]. Moreover, while HBM addresses the band-\nwidth issue to some extent, it doesn\u2019t eliminate the underlying\nproblem of memory hierarchy [ 44]. The processor still needs to\naccess di \ufffferent memory levels, and the latency of transferring data\nbetween these levels can impact performance [ 44]. HBM improves\nbandwidth between the processor and certain memory modules,\nbut the need to access di \ufffferent levels of memory introduces latency\nthat can a \uffffect the execution of various tasks [ 44].\nIn general, it is possible to proclaim that the conventional ap-\nproach of locating memory modules directly on the board poses\nsigni \uffffcant challenges in the context of HPC systems [ 10]. This ar-\nrangement restricts memory bandwidth due to the limited number\nof connections between the processor and these modules [ 10]. As\n1arXiv:2308.10714v1  [cs.DC]  21 Aug 2023,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren\na result, the data transfer rate between the processor and board-\nmounted memory becomes a bottleneck, hindering the overall per-\nformance of the system [ 10].\nFor an increase of memory capacity outside of the node, ad-\nvanced communication technologies such as the Remote Direct\nMemory Access (RDMA) based Message Passing Interface (MPI)\noptimize inter-node communication [ 35]. However, these sophis-\nticated frameworks are not devoid of challenges [ 17]: MPI, a cor-\nnerstone for distributed computing communication, contends with\nlatency and overhead issues during message transmission, dispro-\nportionately a \uffffecting e \uffffciency for applications requiring frequent\ncommunication. Furthermore, the management complexity esca-\nlates with the cluster\u2019s scale due to heightened contention for net-\nwork resources among a larger node count [ 7].\n1.2 Persistent Memory in HPC\nA proposed solution aimed at bridging the gap between memory\nand storage is Persistent Memory (PMem) [ 33,49]. PMem imple-\nmentations such as BBU (battery backed up) DIMM or Non-Volatile\nRAM (NVRAM) aim to deliver rapid byte-addressable data access\nalongside persistent data retention across power cycles. PMem tech-\nnologies establish a new tier within the memory-storage hierarchy\nby combining memory and storage characteristics [ 59,71].", "start_char_idx": 286559, "end_char_idx": 290991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a430a03-545c-4596-ab32-2cdac57973b7": {"__data__": {"id_": "3a430a03-545c-4596-ab32-2cdac57973b7", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4021aae7-49d7-4a21-80fc-3d35da841496", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "fb1485165b6b1667fddb88ecfbeec522d262f13ca03be364bb5b3fb5b075d9bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc9d39b9-60a0-4e3e-b480-9da132b25203", "node_type": "1", "metadata": {}, "hash": "f2f6f5974308c9d93a0a2c241c24184a1d02ea85297cb2418419b186eada7803", "class_name": "RelatedNodeInfo"}}, "text": "Furthermore, the management complexity esca-\nlates with the cluster\u2019s scale due to heightened contention for net-\nwork resources among a larger node count [ 7].\n1.2 Persistent Memory in HPC\nA proposed solution aimed at bridging the gap between memory\nand storage is Persistent Memory (PMem) [ 33,49]. PMem imple-\nmentations such as BBU (battery backed up) DIMM or Non-Volatile\nRAM (NVRAM) aim to deliver rapid byte-addressable data access\nalongside persistent data retention across power cycles. PMem tech-\nnologies establish a new tier within the memory-storage hierarchy\nby combining memory and storage characteristics [ 59,71]. Basic\nsolutions include battery-backed DRAM and have been accessible\nfrom diverse vendors over a signi \uffffcant timeframe, representing an\nestablished concept [ 30,39,50,63]. However, these solutions face\nchallenges due to limited scalability and potential data loss risks.\nThe reliance on batteries introduces concerns regarding power fail-\nures, leading to potential data corruption or loss if batteries deplete.\nMoreover, the approach\u2019s scalability is hampered by the need for\nindividual batteries for each module, impacting cost-e \uffffectiveness\nand overall system performance.\nYet, in recent years new PMem technologies have emerged,\nwith 3D-Xpoint [ 19] being the main technology and Intel Optane\nDCPMM [ 21,72] the prominent product on the market. These mod-\nern PMem technologies o \uffffer byte-addressable memory in larger\ncapacities compared to DRAM while maintaining comparable ac-\ncess times [ 27]. Moreover, as these technologies are non-volatile\nin nature, they enable data retrieval even in instances of power\nfailures. Moreover, PMem o \uffffers two con \uffffguration options based\non these characteristics: (1) It can be utilized as main memory ex-\npansion, providing additional volatile memory, and (2) it can serve\nas a persistent memory pool that can be accessed by applications\nvia a PMem-aware \uffffle system [ 71] or be managed and accessed\ndirectly by applications [ 27]. To simplify and streamline PMem pro-\ngramming and management, the Persistent Memory Development\nKit (PMDK) was created [ 64].\nDuring recent years, PMem has gained signi \uffffcant traction in\nHPC applications [ 15,48,55,62], with two direct use cases of PMem\nfor scienti \uffffc applications that require no (or minimal) changes to\napplications. The \uffffrst use-case involves PMem as memory expan-\nsion to support the execution of large scienti \uffffc problems [ 48]. The\nsecond use case involves leveraging PMem as a fast storage device\naccessed by a PMem-aware \uffffle system (mainly based on the POSIX\nAPI), primarily for application diagnostics and checkpoint restart(C/R) mechanisms [ 38], but also for increasing the performance and\ninherent fault tolerance of scienti \uffffc applications [ 14].\nIn addition to the direct use cases of PMem in scienti \uffffc applica-\ntions, various frameworks and algorithms were developed to access\nand manage data structures on PMem [ 4]. Among these are pri-\nmary methods that are built on top of the PMDK library [ 14,31].\nFor example, persistent memory object storage frameworks such\nas MOSIQS [ 31] and the NVM-ESR recovery model for exact state\nreconstruction of linear iterative solvers using PMem [ 14].\nNevertheless, as HPC workloads advance, computing units\nevolve, and onboard processing elements increase, the demand\nfor heightened memory bandwidth becomes essential [ 32]. Exist-\ning PMem solutions demonstrate notable shortcomings in meeting\nthese requirements, showing limitations in scalability beyond a\ncertain threshold [ 15]. Speci \uffffcally, PMem devices exhibit limited\nbandwidth. For instance, the bandwidth of Optane DCPMM for\nreading and writing is multi-factor lower than that of DRAM [ 27].\nThis, in part, is connected with the hybrid and in-between proper-\nties of a PMem module [ 18], as schematically described in Table 1 .\nAdding to these challenges, a signi \uffffcant limitation arises from\nthe physical attachment of most PMem devices, like Optane\nDCPMM, to the CPU board through memory DIMMs.", "start_char_idx": 290361, "end_char_idx": 294394, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc9d39b9-60a0-4e3e-b480-9da132b25203": {"__data__": {"id_": "bc9d39b9-60a0-4e3e-b480-9da132b25203", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a430a03-545c-4596-ab32-2cdac57973b7", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8db9ef6ff10c721e69a37c61a6d3a03fc37db02218d5083042ab227c8707347a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8896edc-6004-4c68-9228-f16cfeaff75e", "node_type": "1", "metadata": {}, "hash": "6eb057dd45726f0700bf09113cd62839069471e4992db0b6be0ec11db8c3db2c", "class_name": "RelatedNodeInfo"}}, "text": "Nevertheless, as HPC workloads advance, computing units\nevolve, and onboard processing elements increase, the demand\nfor heightened memory bandwidth becomes essential [ 32]. Exist-\ning PMem solutions demonstrate notable shortcomings in meeting\nthese requirements, showing limitations in scalability beyond a\ncertain threshold [ 15]. Speci \uffffcally, PMem devices exhibit limited\nbandwidth. For instance, the bandwidth of Optane DCPMM for\nreading and writing is multi-factor lower than that of DRAM [ 27].\nThis, in part, is connected with the hybrid and in-between proper-\nties of a PMem module [ 18], as schematically described in Table 1 .\nAdding to these challenges, a signi \uffffcant limitation arises from\nthe physical attachment of most PMem devices, like Optane\nDCPMM, to the CPU board through memory DIMMs. This con \uffffg-\nuration restricts the potential for memory expansion, as PMem\ncontends for DIMM slots alongside conventional DRAM cards,\npresenting a bottleneck to achieving optimal memory con \uffffgura-\ntions [ 51,69]. The HPC community as a whole \u2014 both the super\nand cloud computing [ 61] \u2014 recognizes the drawbacks associated\nwith tight integrating memory and compute resources, particularly\nin relation to capacity, bandwidth, elasticity, and overall system\nutilization [ 10,57]. PMem technologies that are tightly coupled\nwith the CPU inherit these limitations. Now, as prominent PMem\ntechnologies are phased out (Optane DCPMM, for example, as an-\nnounced in 2022 [ 20,22]), there is an active and prominent pursuit\nfor the adoption of novel memory solutions in particular, and a\nstrive to achieve more disaggregated computing in general [ 36].\n1.3 Dissagregated Memory with CXL\nThe emergence of discrete memory nodes housing DRAM and\nnetwork interface controllers (NICs) is anticipated to revolutionize\nconventional memory paradigms, facilitating distributed and shared\nmemory access and reshaping HPC landscapes [ 10]. This shift aligns\nwith the concept of disaggregation, where compute resources and\nmemory units are decoupled for optimized resource utilization,\nscalability, and adaptability.\nThe concept of memory disaggregation has been facilitated re-\ncently by the development of advanced interconnect technologies,\nexempli \uffffed by Compute Express Link (CXL) [ 66]. CXL is an open\nstandard to support cache-coherent interconnect between a va-\nriety of devices [ 66]. After its introduction in 2019, the standard\nhas evolved and continues to be enhanced. CXL 1.1 de \uffffnes the\nprotocol for three major device types [ 66]: Accelerators with cache-\nonly (type 1), cache with attached memory (type 2), and memory\nexpansion (type 3). CXL 2.0 expands the speci \uffffcation \u2013 among\nother capabilities \u2013 to memory pools using CXL switches on a de-\nvice level. CXL 3.0 introduces fabric capabilities and management,\n2CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,\nProperty As a main memory extension As a direct access to persistent memory\nVolatility Volatile in memory extension mode Non-volatile in direct access mode\nAccess Cache-coherent memory expansion Transactional byte-addressable object store\nCapacity Higher than main memory volume Lower than storage volume\nCost Cheaper than the main memory More expansive than storage\nPerformance Several factors below main memory bandwidth High bandwidth compared to storage\nTable 1: Properties of PMem modules, either as a memory extension ( Memory Mode) or as a direct access PMem ( App-Direct ).\nimproved memory sharing and pooling with dynamic capacity ca-\npability, enhanced coherency, and peer-to-peer communication.\nBandwidth-wise, CXL 1.1 and 2.0 employ PCIe 5.0, achieving 32\nGT/s for transfers up to 64 GB/s in each direction via a 16-lane link.\nOn the other hand, CXL 3.0 utilizes PCIe 6.0, doubling the speed to\n64 GT/s, supporting 128 GB/s bi-directional communication via an\nx16 link.\nSince the market of CXL memory modules is emerging, several\nvendors have announced products using the CXL protocol. For\nexample, Samsung [ 52] and SK Hynix [ 53] introduce CXL DDR5\nmodules, AsteraLabs [ 3] announced a CXL memory accelerator,\nand Montage Technology [ 68] will o \uffffer a CXL memory expander\ncontroller.", "start_char_idx": 293588, "end_char_idx": 297779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8896edc-6004-4c68-9228-f16cfeaff75e": {"__data__": {"id_": "d8896edc-6004-4c68-9228-f16cfeaff75e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc9d39b9-60a0-4e3e-b480-9da132b25203", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3860bf5b881ef7ae8be2c9fbf85cd676dd4f77c8a444d1caad9adfc3cd626fcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67db5ecd-6e7b-4aee-9e82-48f02b2e03b8", "node_type": "1", "metadata": {}, "hash": "8cf9aaaad7d53a09bd359fe3dacd21888460dcf0eb9ddbd5d113b9510c22d61b", "class_name": "RelatedNodeInfo"}}, "text": "Bandwidth-wise, CXL 1.1 and 2.0 employ PCIe 5.0, achieving 32\nGT/s for transfers up to 64 GB/s in each direction via a 16-lane link.\nOn the other hand, CXL 3.0 utilizes PCIe 6.0, doubling the speed to\n64 GT/s, supporting 128 GB/s bi-directional communication via an\nx16 link.\nSince the market of CXL memory modules is emerging, several\nvendors have announced products using the CXL protocol. For\nexample, Samsung [ 52] and SK Hynix [ 53] introduce CXL DDR5\nmodules, AsteraLabs [ 3] announced a CXL memory accelerator,\nand Montage Technology [ 68] will o \uffffer a CXL memory expander\ncontroller.\nLeveraging CXL, memory nodes will be interconnected through\nhigh-speed links, enabling adaptive memory provisioning to com-\npute nodes in real time [ 70]. The practice of intra-rack disaggrega-\ntion holds the potential to e \uffffectively address the memory demands\nof applications while concurrently ensuring an adequate supply\nof e\uffffcient remote memory bandwidth [ 46,47].Figure 1 demon-\nstrates the expected phase change from the processor\u2019s point of\nview, from previous years\u2019 DDR4+PMem memory access, equipped\nwith NVMe SSDs via the PCIe Gen4, to the upcoming future of\nDDR5 local memory equipped with local or remote NVMe SSDs\nand CXL memory for memory expansion or persistency over the\nnew generations of PCIe.\nNative DDR4Native DDR4Native DDR4Native DDR4PCIe Gen4ProcessorPMemPMemPMemPMemNative DDR5Native DDR5Native DDR5Native DDR5PCIe Gen5ProcessorNative DDR5Native DDR5Native DDR5Native DDR5NVMe SSDsCXL memory as PMemNVMe SSDsTodayCXL Future\nFigure 1: The migration from PMem as hardware to CXL\nmemory as PMem in future systems.\nNevertheless, while the concept of memory disaggregation with\ntechnologies like CXL holds signi \uffffcant promise, it is important to\nacknowledge that there are still challenges and considerations that\nneed to be addressed [ 2,16]; challenges and considerations that\nresemble the ones of persistent memory integration in HPC [ 5].\nFor example, software and programming models need to evolve\nto take advantage of disaggregated memory fully; Applications\nand algorithms must be designed or adapted to work seamlessly\nacross distributed memory nodes; and e \uffffcient data placement\nand movement strategies are crucial to minimize the impact ofnetwork latencies and ensure that data-intensive workloads can\ne\uffffectively utilize CXL-based disaggregated memory resources, es-\npecially when cache-coherence or direct access is enabled. Notwith-\nstanding, when comparing CXL memory aspects to the ones of\nPMem as non-volatile RAM (NVRAM), in general, it can be observed\n(Table 2 ) that from the disaggregated HPC usage perspective, there\nshould be a prevalence to CXL over NVRAM considering band-\nwidth, data transfer, and scalability, but also considering memory\ncoherency, integration, pooling and sharing.\n1.4 Contribution\nIn this work, based on actual physical experiments with multi-\nNUMA nodes and multi-core high-performance SOTA hardware\n(subsection 2.1 ) and CXL-remote memory ( subsection 2.2 ), we claim\nthat it is not only possible to exemplify most persistent memory\nmodules characteristics (as described in Table 1 ) with CXL memory\nfully but also that in terms of performance, we can achieve much\nbetter bandwidth than previously published Optane DCPMM ones\n(such in [ 26], which, for a single Optane DCPMM, discovers that\nits max read bandwidth is 6.6 GB/s, whereas its max write band-\nwidth is 2.3 GB/s). In fact, we show ( Figure 4 ) that by approaching\nour CXL-DDR4 memory module \u2013 much cheaper than DDR5 \u2013 we\nachieve comparable results to the local DDR4 module and exhibit\nperformance degradation of only about 60% in bandwidth in com-\nparison to local DDR5 module access (noting that DDR4 has about\n50% bandwidth of DDR5). Our tests were made in multiple con \uffffg-\nurations ( subsection 3.2 ) in relation to the memory distance from\nthe working threads using the well-known STREAM benchmark\n(subsection 3.1 ).", "start_char_idx": 297188, "end_char_idx": 301126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67db5ecd-6e7b-4aee-9e82-48f02b2e03b8": {"__data__": {"id_": "67db5ecd-6e7b-4aee-9e82-48f02b2e03b8", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8896edc-6004-4c68-9228-f16cfeaff75e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "074e2ec1a3672b6b07d708e70363d1d98835228abc96a4c55e5e9f019735830a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37b81762-f267-4f8d-9017-9db8d509e975", "node_type": "1", "metadata": {}, "hash": "3a9befe2d433d1dcd9b738c8737bfcd00565de56cad2bd9fd06244e100ff92a0", "class_name": "RelatedNodeInfo"}}, "text": "In fact, we show ( Figure 4 ) that by approaching\nour CXL-DDR4 memory module \u2013 much cheaper than DDR5 \u2013 we\nachieve comparable results to the local DDR4 module and exhibit\nperformance degradation of only about 60% in bandwidth in com-\nparison to local DDR5 module access (noting that DDR4 has about\n50% bandwidth of DDR5). Our tests were made in multiple con \uffffg-\nurations ( subsection 3.2 ) in relation to the memory distance from\nthe working threads using the well-known STREAM benchmark\n(subsection 3.1 ).\nIn order to demonstrate the non-volatile properties of the mem-\nory as PMem, the CXL memory was located outside of the node, in\nan FPGA device ( subsection 2.2 ), potentially backed by battery, like\nprevious battery-backed DIMMs. As many nodes can approach the\ndevice, the battery-backed consideration is no longer considered\nby us as a major overhead since it will be applied only once for the\nmemory modules and not in each compute node.\nMoreover, besides the cache-coherent performance benchmarks\nwith STREAM [ 40,41], we retested the memory bandwidth in an\nequivalent of the App-Direct approach with a modi \uffffed STREAM ap-\nplication, named STREAM-PMem [ 12] when all of the main arrays\nwere allocated as a PMDK\u2019s pmemobj and manipulated accordingly\n[65].pmemobj provides an assurance that the condition of objects\nwill remain internally consistent regardless of when the program\nconcludes. Additionally, it o \uffffers a transaction function that can\nencompass various modi \uffffcations made to persistent objects. This\nfunction ensures that either all of the modi \uffffcations are successfully\napplied or none of them take e \uffffect.\n3,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren\nAspect CXL Memory NVRAM\nBandwidth &\nData TransferSigni \uffffcantly higher bandwidth enabling fast data trans-\nfers between processors and memory devices.Non-volatile storage with potential data transfer rate\nlimitations due to underlying interface and technology.\nMemory\nCoherencyProvides memory-coherent links, ensuring consistent\ndata across di \ufffferent memory tiers.Requires additional mechanisms for memory coherency,\nexcept with local RAM, when integrated with other\nmemory technologies.\nHeterogeneous\nMemory\nIntegrationAllows seamless integration of various memory tech-\nnologies within a uni \uffffed architecture.E\uffffective for extending memory capacity, but inte-\ngration may require additional considerations due to\nunique characteristics.\nMemory Pooling\nand SharingFacilitates memory pooling and sharing, enabling e \uffff-\ncient resource utilization and dynamic allocation based\non workload requirements.Extends memory capacity, but inherent \uffffexibility in\nmemory sharing and pooling may be limited.\nIndustry\nStandardizationOpen industry standard supported by major technology\nplayers, ensuring compatibility, interoperability, and\nbroader adoption.Solutions may vary, potentially leading to compatibility\nchallenges and limited integration options.\nScalability Architecture designed for scalability with multiple lanes\nand protocols, catering to evolving data center needs.Scalability may be constrained by underlying technol-\nogy characteristics, such as DIMM count and RAM/N-\nVRAM tradeo \uffff.\nRelevance to HPC Higher bandwidth, memory coherency, and memory\npooling capabilities enhance HPC workload perfor-\nmance. Standardization compatibility in heterogeneous\nenvironments and scalability cater to evolving demands.O\uffffers non-volatility but is constrained by limitations in\nbandwidth, coherency management, and scalability, af-\nfecting its applicability to complex HPC memory needs.\nTable 2: General comparison between common aspects of CXL memory and NVRAM for disaggregated HPC.\nWe stress that as our CXL memory module is located outside\nof the node and can be backed by a battery, the ability to transac-\ntionally and directly access the memory, exactly as previously done\nwith Optane DCPMM, while achieving even better performances,\nis a key to our practical approach, which consider CXL memory as\na persistent memory for the future of disaggregated HPC.\nFinally, we open-sourced the entire benchmarking methodology\nas an easy-to-use and automated tool named STREAMer for future\nCXL memory device evaluations for HPC purposes.", "start_char_idx": 300620, "end_char_idx": 304865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37b81762-f267-4f8d-9017-9db8d509e975": {"__data__": {"id_": "37b81762-f267-4f8d-9017-9db8d509e975", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67db5ecd-6e7b-4aee-9e82-48f02b2e03b8", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "fb1d05212b3cc1b947dac3fb0302507f5ee6a1fdafe3905aaddca3b70fd61480", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e779ca7-4861-4d33-855b-b3b21063742a", "node_type": "1", "metadata": {}, "hash": "96ca5b97e608db4f667b75ffae483e45a824f08c08114746f7a17a14e528dac2", "class_name": "RelatedNodeInfo"}}, "text": "Standardization compatibility in heterogeneous\nenvironments and scalability cater to evolving demands.O\uffffers non-volatility but is constrained by limitations in\nbandwidth, coherency management, and scalability, af-\nfecting its applicability to complex HPC memory needs.\nTable 2: General comparison between common aspects of CXL memory and NVRAM for disaggregated HPC.\nWe stress that as our CXL memory module is located outside\nof the node and can be backed by a battery, the ability to transac-\ntionally and directly access the memory, exactly as previously done\nwith Optane DCPMM, while achieving even better performances,\nis a key to our practical approach, which consider CXL memory as\na persistent memory for the future of disaggregated HPC.\nFinally, we open-sourced the entire benchmarking methodology\nas an easy-to-use and automated tool named STREAMer for future\nCXL memory device evaluations for HPC purposes.\n2 PHYSICAL EXPERIMENTAL SETUP\n2.1 HPC hardware\nOur HPC hardware experimental environment is based on 2 setups:\n(1)Node equipped with two Intel 4C\u2318generation Xeon (Sapphire\nRapids) processors with a base frequency of 2.1GHz and 48 cores\neach, plus Hyper-Threading. BIOS was updated to support only 10\ncores per socket. Each processor has one memory DIMM (64GB\nDDR5 4800MHz DIMM). The system is equipped with a CXL pro-\ntotype device, implemented as DDR4 memory on a PCIe-attached\nFPGA (see Figure 2).\n(2)Node equipped with two Intel Xeon Gold 5215 processors with a\nbase frequency of 2.5GHz and 10 cores each, plus Hyper-Threading.\nEach processor has total 96GB DRAM in 6 channels, 16GB DDR4\n2666MHz DIMM per channel. (see Figure 3).\n2.2 CXL prototype\nWe provide an in-depth overview of our CXL prototype\u2019s imple-\nmentation on an FPGA card [ 25].Figure 2 andFigure 4 grant a more\ndetailed view into the implementation of our CXL memory pool on\nthe FPGA card (while Figure 3 show the reference system, withoutany CXL attachment, with DDR4 main memory). The prototype\naims to harness the capabilities of the R-Tile Intel FPGA IP for CXL,\nencompassing critical functionalities for CXL link establishment\nand transaction layer management. This comprehensive solution\nfacilitates the construction of FPGA-based CXL 1.1/2.0 compliant\nendpoint designs, including Type 1, Type 2, and Type 3 con \uffffgura-\ntions. It\u2019s built upon a previously proven prototype, with necessary\nslight modi \uffffcations for PMem activity [ 34].\nThe architecture of our CXL implementation revolves around\na synergistic pairing of protocol Soft IP within the FPGA main\nfabric die and the Hard IP counterpart, the R-Tile. This cohesive\narrangement ensures e \uffffective management of CXL link functions,\nwhich are pivotal for seamless communication. Speci \uffffcally, the\nR-Tile interfaces with a CPU host via a PCIe Gen5x16 connection,\ndelivering a theoretical bandwidth of up to 64GB/s. As a key facet\nof our implementation, the FPGA device is duly enumerated as a\nCXL endpoint within the host system.\nComplementing this link management, the Soft IP assumes the\nmantle of transaction layer functions, vital for the successful execu-\ntion of di \ufffferent CXL endpoint types. For Type 3 con \uffffgurations, the\nCXL.mem transaction layer adeptly handles incoming CXL.mem\nrequests originating from the CPU host. It orchestrates the gener-\nation of host-managed device memory (HDM) requests directed\ntoward an HDM subsystem. Simultaneously, the CXL.io transaction\nlayer undertakes the responsibility of processing CXL.io requests.\nThese requests encompass both con \uffffguration and memory space\ninquiries initiated from the CPU host, seamlessly forwarding them\nto their designated control and status registers. A noteworthy aug-\nmentation is the User Streaming Interface, o \uffffering a conduit for\ncustom CXL.io features that can be seamlessly integrated into the\nuser design.", "start_char_idx": 303949, "end_char_idx": 307778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e779ca7-4861-4d33-855b-b3b21063742a": {"__data__": {"id_": "0e779ca7-4861-4d33-855b-b3b21063742a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37b81762-f267-4f8d-9017-9db8d509e975", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a6ea767b19cf411ecf142dab1607e6063b9206fc86df51dd3a077ffa4a75bad8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6641cf14-a7b1-4e2b-807c-99393c7e8ac0", "node_type": "1", "metadata": {}, "hash": "11a8bb4c89316773f54bbac4befdbceb4185b9404a2e4833469ab849c7bc96f8", "class_name": "RelatedNodeInfo"}}, "text": "Complementing this link management, the Soft IP assumes the\nmantle of transaction layer functions, vital for the successful execu-\ntion of di \ufffferent CXL endpoint types. For Type 3 con \uffffgurations, the\nCXL.mem transaction layer adeptly handles incoming CXL.mem\nrequests originating from the CPU host. It orchestrates the gener-\nation of host-managed device memory (HDM) requests directed\ntoward an HDM subsystem. Simultaneously, the CXL.io transaction\nlayer undertakes the responsibility of processing CXL.io requests.\nThese requests encompass both con \uffffguration and memory space\ninquiries initiated from the CPU host, seamlessly forwarding them\nto their designated control and status registers. A noteworthy aug-\nmentation is the User Streaming Interface, o \uffffering a conduit for\ncustom CXL.io features that can be seamlessly integrated into the\nuser design.\n4CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,\n64GB 4800 MHzMemory\nUPIDDR5Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)CXL-attached memory\nMemoryHostProcessorSocket0HostProcessorSocket164GB 4800 MHznumactl --membind=0/mnt/pmem0 numactl --membind=1/mnt/pmem1 numactl --membind=2/mnt/pmem2 PCIe5x16cores 0-9cores 10-19CXLDDR5\nFigure 2: Setup #1 with DDR5 on-node memory and DDR4\nCXL-attached memory.\n16GB 2933 MHzDDR4MemoryMemoryHost Processor Socket0Host Processor Socket116GB 2933 MHznumactl --membind=0numactl --membind=1cores 0-9cores 10-19UPIDDR4Figure 3: Setup #2 with DDR4 on-node memory.\n Figure 4: Overview of CXL IP for Intel \u00aeAgilex \u00ae7 I-Series\nFPGA [ 24], demonstrated in Figure 2 (setup #1).\nIntegral to our FPGA card is the inclusion of two onboard DDR4\nmemory modules, each boasting a capacity of 8GB and operating at\na clock frequency of 1333 MHz. These modules are accessible from\nthe host system as conventional memory resources. It is imperative\nto highlight a distinctive attribute of this prototype con \uffffguration:\nthe CXL link facilitates access to an identical memory volume. In\nessence, this means that the same far memory segment can be made\navailable to two distinct NUMA nodes, eliminating any concerns\nof address overlap. However, due to the absence of a uni \uffffed cache-\ncoherent domain, the onus of maintaining coherency between the\ntwo NUMA nodes assigned to the shared far memory rests with\nthe applications leveraging this con \uffffguration.\nNotably, the bandwidth attainable from this prototype con \uffffgura-\ntion is subject to current implementation constraints and does not\nre\uffffect an intrinsic limitation of the CXL standard. Potential avenues\nfor enhancing bandwidth include several considerations. First, tran-\nsitioning to a higher-speed FPGA, supporting DDR4 speeds of 3200\nMbps or even embracing the capabilities of DDR5 at 5600 Mbps,\ncould appreciably enhance throughput. Additionally, scaling the\nresources allocated to the CXL IP by increasing the number of slices\nis a viable strategy. Furthermore, expanding the FPGA\u2019s capacity to\naccommodate multiple independent DDR channels, possibly transi-\ntioning from one channel to four, holds promise in augmenting the\nprototype\u2019s bandwidth potential.\nIn our discussion, the fact that the CXL memory device is DDR4\nand not DDR5 is key, as usually, PMem is slower and cheaper than\nthe main memory. By using DDR4 CXL memory and not DDR5,\nwhile main memory is DDR5, we keep on this important relation.\n3 PERFORMANCE EVALUATION\n3.1 STREAM and STREAM-PMem Benchmarks\nThe STREAM benchmark [ 42] is a synthetic benchmark program\nthat measures sustainable memory bandwidth for simple vectorkernels in high-performance computers. STREAM was developed\nas a proxy for the basic computational kernels in scienti \uffffc com-\nputations [ 43] and includes Copy, Scale, Sum, and Triad kernels.\nSTREAM has a dedicated version to benchmark PMem modules by\nallocating and accessing PMem via PMDK (STREAM-PMem [ 12]).\nThe excerpt presented in Listing 1 constitutes a portion of the\ninitial codebase that has since been extracted from the current\nversion of the code.\nListing 1: Original STREAM benchmark code at line 175-181.", "start_char_idx": 306922, "end_char_idx": 310988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6641cf14-a7b1-4e2b-807c-99393c7e8ac0": {"__data__": {"id_": "6641cf14-a7b1-4e2b-807c-99393c7e8ac0", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e779ca7-4861-4d33-855b-b3b21063742a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "4c596d7e639961b46eafafcca4ddf47a82dcd2a2cf6fe1a9453df1259213263e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d17977e2-d35b-462f-ade4-b269e29460e4", "node_type": "1", "metadata": {}, "hash": "ba412421a846701a644054d5d37dbe0a237793d5a17b0309a6dc0c9751e6bd68", "class_name": "RelatedNodeInfo"}}, "text": "By using DDR4 CXL memory and not DDR5,\nwhile main memory is DDR5, we keep on this important relation.\n3 PERFORMANCE EVALUATION\n3.1 STREAM and STREAM-PMem Benchmarks\nThe STREAM benchmark [ 42] is a synthetic benchmark program\nthat measures sustainable memory bandwidth for simple vectorkernels in high-performance computers. STREAM was developed\nas a proxy for the basic computational kernels in scienti \uffffc com-\nputations [ 43] and includes Copy, Scale, Sum, and Triad kernels.\nSTREAM has a dedicated version to benchmark PMem modules by\nallocating and accessing PMem via PMDK (STREAM-PMem [ 12]).\nThe excerpt presented in Listing 1 constitutes a portion of the\ninitial codebase that has since been extracted from the current\nversion of the code.\nListing 1: Original STREAM benchmark code at line 175-181.\n1#ifndef STREAM_TYPE\n2#define STREAM_TYPE double\n3#endif\n4static STREAM_TYPE a[STREAM_ARRAY_SIZE+OFFSET],\n5 b[STREAM_ARRAY_SIZE+OFFSET],\n6 c[STREAM_ARRAY_SIZE+OFFSET];\nThe content represented in Listing 1 has been substituted in\nSTREAM-PMem [ 12] with the code demonstrated in Listing 2 . The\ncode commences by accessing the memory pool. Furthermore, a\nfunction named initiate is employed to initialize the three arrays.\nFollowing this initialization, the code proceeds to execute the re-\nmaining segments of the STREAM benchmark code, mirroring the\nstructure of the original STREAM benchmark code.\nListing 2: Code that has replaced original code.\n1PMEMobjpool *pop;\n2POBJ_LAYOUT_BEGIN(array);\n3POBJ_LAYOUT_TOID(array, double );\n4POBJ_LAYOUT_END(array); //Declearing the arrays\n5TOID( double ) a, b, c;\n6void initiate() { //Initiating the arrays .\n7 POBJ_ALLOC(pop, &a, double ,\n(STREAM_ARRAY_SIZE+OFFSET)*sizeof(STREAM_TYPE),\nNULL, NULL); //Same for band c.\n8int main(){\n5,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren\n9 const char path[] = \u0000.../ pool .obj\u0000;\n10 pop = pmemobj_create(path, LAYOUT_NAME, 10737418240,\n0666);\n11 if(pop == NULL)\n12 pop = pmemobj_open(path, LAYOUT_NAME);\n13 if(pop == NULL) {\n14 perror(path);\n15 exit(1); }\n16 initiate();\n17 //The rest ofthe STREAM benchmark after this .\n18 }\nIn this work, we employ STREAM in those two versions to show-\ncase the shift from PMem to CXL. Throughout this demonstration,\nwe illustrate how programs designed for PMem can seamlessly\noperate on CXL-enabled devices. Furthermore, we provide perfor-\nmance assessments to anticipate the impact of CXL on performance\nin relation to local RAM (DDR4 and DDR5) and local PMem-like\ndevices (emulation of remote sockets either for memory expansion\nor as a direct access device, as done in [ 6,13]).\nIn contrast to previous research that primarily emphasizes\ndemonstrating the use of CXL memory for in-memory database\nqueries or \uffffle system operations [ 1,34], STREAM memory access\ninvolves accessing and manipulating large arrays, making it par-\nticularly applicable and signi \uffffcant for scienti \uffffc computations in\nHPC systems. Moreover, STREAM is implemented with OpenMP\nthreads, which is the common shared-memory paradigm in scien-\nti\uffffc computing for parallelism [ 9].\n3.2 Test Con \uffffgurations\nThe methodology of this work is to employ STREAM and STREAM-\nPMem in various CPU and memory con \uffffgurations, taking into\naccount the availability of DRAM and CXL memory available on\nthe HPC setups, as will be described next. The results presented\ninFigure 5 ,Figure 6 ,Figure 7 ,Figure 8 refer to STREAM execu-\ntions with 100M array elements for Scale, Add, Copy, and Triad\noperations correspondingly. For each STREAM method, the results\nof our tests are presented in 2 classes (and a total of 5 groups), di-\nvided conceptually for unique comparisons. We sub-divide those 5\ngroups into two classes.", "start_char_idx": 310184, "end_char_idx": 313922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d17977e2-d35b-462f-ade4-b269e29460e4": {"__data__": {"id_": "d17977e2-d35b-462f-ade4-b269e29460e4", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6641cf14-a7b1-4e2b-807c-99393c7e8ac0", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "45185d99e7aab1576e9592bed6213caaa2ffcc96e07a70ead0e1d09d90ab0b32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "602cd835-3c9c-4857-8f70-c7edb7a955df", "node_type": "1", "metadata": {}, "hash": "43541d525642884023046db8ec1720a5cdbf1672c2c627db1db8791776c50772", "class_name": "RelatedNodeInfo"}}, "text": "Moreover, STREAM is implemented with OpenMP\nthreads, which is the common shared-memory paradigm in scien-\nti\uffffc computing for parallelism [ 9].\n3.2 Test Con \uffffgurations\nThe methodology of this work is to employ STREAM and STREAM-\nPMem in various CPU and memory con \uffffgurations, taking into\naccount the availability of DRAM and CXL memory available on\nthe HPC setups, as will be described next. The results presented\ninFigure 5 ,Figure 6 ,Figure 7 ,Figure 8 refer to STREAM execu-\ntions with 100M array elements for Scale, Add, Copy, and Triad\noperations correspondingly. For each STREAM method, the results\nof our tests are presented in 2 classes (and a total of 5 groups), di-\nvided conceptually for unique comparisons. We sub-divide those 5\ngroups into two classes. The \uffffrst class (Class 1, (a)-(c)) refers to the\nequivalent of the App-Direct mode in PMem in which we directly\naccess the local or remote memory (either in the alternative socket\nor in the CXL memory), and the second class (Class 2, (a)-(b)) refers\nto the Memory Mode in PMem, in which we increase the available\nmemory using other CC-NUMA nodes:\nClass 1 \u2014 App-Direct :\n(a)Local memory access as PMem: Con\uffffgurations within this\ngroup involve accessing local memory (on-socket memory) in App-\nDirect mode (thus benchmarking STREAM-PMem).\n(b)Remote memory access as PMem: Con\uffffgurations within\nthis group involve computing cores on a single socket that access\nremote memory in App-Direct mode (thus benchmarking STREAM-\nPMem). The term \"remote memory\" in this context encompasses\nboth CXL-attached memory and on-node memory accessed from\nthe alternative CPU socket (i.e., memory accessed through the UPI).\n(c)Remote memory as PMem (thread a \uffffnity): Con\uffffgurations\nwithin this group involve computing cores in both sockets thataccess remote memory in App-Direct mode (thus benchmarking\nSTREAM-PMem) using two distinct thread a \uffffnity methods: close\nandspread . The close method populates an entire socket \uffffrst and\nthen adds cores from the second socket. The spread method, on the\nopposite, adds cores alternately from both sockets.\nClass 2 \u2014 Memory Mode :\n(a)Remote CC-NUMA: Con\uffffgurations within this group involve\ncomputing cores on a single socket that access remote memory as\nCC-NUMA.\n(b)Remote CC-NUMA (all cores): Con\uffffgurations within this\ngroup involve cores on both CPU sockets accessing remote memory\nas CC-NUMA. This includes con \uffffgurations where both sockets\noperate and access memory on one of them since these workloads\ninclude remote accesses.\nFor better clarity, the data \uffffow for each test con \uffffguration is\ndemonstrated in Figure 9. Each row in Figure 9contains the data\n\uffffow examinations of the test groups of the two classes. Thus, in each\nof our test groups, for each of the STREAM operations (Figures 5,6,\n7,8), the way to understand each trend, and its correspondence to\nthe relevant data \uffffow, is given in the trend itself by a combination\nof three: symbol, color and memory annotation. The symbol is used\nto distinguish between accessing on-node DDR4 ( s), on-node DDR5\n(l) or CXL-attached DDR4 ( \u21e5). The color implies the active com-\npute cores \u2014- either in socket0, socket1, or both. The annotations\npmem #{0,1,2}ornuma #{0,1,2}accompanying each trend provide\nan explanation of the accesses memory location: 0 for socket0; 1 for\nsocket1; and 2 for CXL memory. numa signi \uffffes STREAM accessing\nmemory as NUMA memory expansion, while pmem represents\nSTREAM-PMem accessing memory using PMDK.\n4 RESULTS AND ANALYSIS\nFigure 5 ,Figure 6 ,Figure 7 andFigure 8 present STREAM results\nfor the Scale, Add, Copy, and Triad operations correspondingly,\nand for the test con \uffffgurations de \uffffned in subsection 3.2 as will\nbe described next.", "start_char_idx": 313158, "end_char_idx": 316860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "602cd835-3c9c-4857-8f70-c7edb7a955df": {"__data__": {"id_": "602cd835-3c9c-4857-8f70-c7edb7a955df", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d17977e2-d35b-462f-ade4-b269e29460e4", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "72a03d9ea5d5f0bf199e7ac2eed20437d301dcee276120dc574b8181cb36c028", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "deeedbde-9e19-4aba-a3f7-ddbf39e73d2d", "node_type": "1", "metadata": {}, "hash": "d2b06650a02dc1e03a1903bad9390865f5b7b64ada99d25485351b873b9d6547", "class_name": "RelatedNodeInfo"}}, "text": "The color implies the active com-\npute cores \u2014- either in socket0, socket1, or both. The annotations\npmem #{0,1,2}ornuma #{0,1,2}accompanying each trend provide\nan explanation of the accesses memory location: 0 for socket0; 1 for\nsocket1; and 2 for CXL memory. numa signi \uffffes STREAM accessing\nmemory as NUMA memory expansion, while pmem represents\nSTREAM-PMem accessing memory using PMDK.\n4 RESULTS AND ANALYSIS\nFigure 5 ,Figure 6 ,Figure 7 andFigure 8 present STREAM results\nfor the Scale, Add, Copy, and Triad operations correspondingly,\nand for the test con \uffffgurations de \uffffned in subsection 3.2 as will\nbe described next. Figure 5a ,Figure 6a ,Figure 7a andFigure 8a\nthrough Figure 5e ,Figure 6e ,Figure 7e andFigure 8e present results\nfor Class 1. (a)group though Class 2. (b)group correspondingly.\nThe results explain the costs associated with memory access\nacross varied con \uffffgurations distinguished by parameters such as\nmemory type (on-node or CXL-attached), memory placement (local\nto the socket, on the alternate CPU socket, or the CXL-attached\nmemory), access mode ( App-Direct vs.Memory Mode ), and thread\na\uffffnity (Close or Spread).\nNext, we will examine and analyze the achieved results in rela-\ntion to the con \uffffguration classes and groups presented in subsec-\ntion 3.2 :\nClass 1 \u2014 App-Direct :\n(a)Local memory access as PMem: It is possible to observe that\namong all of the STREAM actions, the App-Direct access using\nPMDK to the local DDR5 memory is saturated around 20-22 GB/s.\nThis test is a reference for the remote access presented in the follow-\ning group, either to a nearby remote socket or to the CXL memory\n(with PMDK).\n(b)Remote memory access as PMem: App-Direct access to the\nemulated remote PMem (DDR5 on the alternate socket) results\nin a decrease of 30% ( \u21e015 GB/s) of performance on average for\n6CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,\n(a) Class 1.a: Local memory access as PMem\n(b) Class 1.b: Remote memory access as PMem\n(c) Class 1.c: Remote memory as PMem (thread\na\uffffnity)\n(d) Class 2.a: Remote CC-NUMA\n(e) Class 2.b: Remote CC-NUMA (all cores)\nFigure 5: SCALE \u2014 Various STREAM test con \uffffgurations. Refer to Section 3.2for de \uffffnition of test groups 1. (a), 1.(b), 1.(c), 2.(a),\n2.(b)and legend clari \uffffcations.\n(a) Class 1.a: Local memory access as PMem\n(b) Class 1.b: Remote memory access as PMem\n(c) Class 1.c: Remote memory as PMem (thread\na\uffffnity)\n(d) Class 2.a: Remote CC-NUMA\n(e) Class 2.b: Remote CC-NUMA (all cores)\nFigure 6: ADD \u2014 Various STREAM test con \uffffgurations. Refer to Section 3.2for de \uffffnition of test groups 1. (a), 1.(b), 1.(c), 2.(a),\n2.(b)and legend clari \uffffcations.\n7,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren\n(a) Class 1.a: Local memory access as PMem\n(b) Class 1.b: Remote memory access as PMem\n(c) Class 1.c: Remote memory as PMem (thread\na\uffffnity)\n(d) Class 2.a: Remote CC-NUMA\n(e) Class 2.b: Remote CC-NUMA (all cores)\nFigure 7: COPY \u2014 Various STREAM test con \uffffgurations. Refer to Section 3.2for de \uffffnition of test groups 1. (a), 1.(b), 1.(c), 2.(a),\n2.(b)and legend clari \uffffcations.", "start_char_idx": 316236, "end_char_idx": 319360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "deeedbde-9e19-4aba-a3f7-ddbf39e73d2d": {"__data__": {"id_": "deeedbde-9e19-4aba-a3f7-ddbf39e73d2d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "602cd835-3c9c-4857-8f70-c7edb7a955df", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "aa5f8bc874ea8334b9e03dd4aefb66a6781e2a739c64feb4b4399d380dfb8d27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0efb634c-c11e-4c57-8645-4c5a7ca6d9a9", "node_type": "1", "metadata": {}, "hash": "9d7cc76e88098541c5372fb46cb883fe5a2cbfc59b9e3c630e30fb1bf3474a71", "class_name": "RelatedNodeInfo"}}, "text": "(a),\n2.(b)and legend clari \uffffcations.\n7,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren\n(a) Class 1.a: Local memory access as PMem\n(b) Class 1.b: Remote memory access as PMem\n(c) Class 1.c: Remote memory as PMem (thread\na\uffffnity)\n(d) Class 2.a: Remote CC-NUMA\n(e) Class 2.b: Remote CC-NUMA (all cores)\nFigure 7: COPY \u2014 Various STREAM test con \uffffgurations. Refer to Section 3.2for de \uffffnition of test groups 1. (a), 1.(b), 1.(c), 2.(a),\n2.(b)and legend clari \uffffcations.\n(a) Class 1.a: Local memory access as PMem\n(b) Class 1.b: Remote memory access as PMem\n(c) Class 1.c: Remote memory as PMem (thread\na\uffffnity)\n(d) Class 2.a: Remote CC-NUMA\n(e) Class 2.b: Remote CC-NUMA (all cores)\nFigure 8: TRIAD \u2014 Various STREAM test con \uffffgurations. Refer to Section 3.2for de \uffffnition of test groups 1. (a), 1.(b), 1.(c), 2.(a),\n2.(b)and legend clari \uffffcations.", "start_char_idx": 318856, "end_char_idx": 319737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0efb634c-c11e-4c57-8645-4c5a7ca6d9a9": {"__data__": {"id_": "0efb634c-c11e-4c57-8645-4c5a7ca6d9a9", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "deeedbde-9e19-4aba-a3f7-ddbf39e73d2d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "2e3ae57cd5e42f7a96b6ceab141792cd98ec8b6c6fe28cd5829176ecd9099349", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37e79361-c5f8-4833-b9a5-794209ac3182", "node_type": "1", "metadata": {}, "hash": "e7a8b80f53dc12d4cb24808224f1128ae37fe1ec1556ba26b4bb1a5ba78fac94", "class_name": "RelatedNodeInfo"}}, "text": "Refer to Section 3.2for de \uffffnition of test groups 1. (a), 1.(b), 1.(c), 2.(a),\n2.(b)and legend clari \uffffcations.\n(a) Class 1.a: Local memory access as PMem\n(b) Class 1.b: Remote memory access as PMem\n(c) Class 1.c: Remote memory as PMem (thread\na\uffffnity)\n(d) Class 2.a: Remote CC-NUMA\n(e) Class 2.b: Remote CC-NUMA (all cores)\nFigure 8: TRIAD \u2014 Various STREAM test con \uffffgurations. Refer to Section 3.2for de \uffffnition of test groups 1. (a), 1.(b), 1.(c), 2.(a),\n2.(b)and legend clari \uffffcations.\n8CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,(Class 1.a)\n64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5Host Processor Socket0Host Processor Socket1/mnt/pmem0 \n(Class 1.b)\n64GB 4800 MHzUPIDDR5MemoryCXL-attached memory\nMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)/mnt/pmem2 PCIe5x16CXL\n64GB 4800 MHzUPIDDR5MemoryCXL-attached memory\nMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)/mnt/pmem2 PCIe5x16CXL\n64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHz/mnt/pmem1 cores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory\n(Class 1.c)\n64GB 4800 MHzUPIDDR5MemoryCXL-attached memory\nMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)PCIe5x16CXL/mnt/pmem2 \n64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5Host Processor Socket0Host Processor Socket1CXL-a\ufffdached memory\n/mnt/pmem0 \n64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory\n/mnt/pmem1 \n(Class 2.a)\n64GB 4800 MHzUPIDDR5MemoryCXL-attached memory\nMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)PCIe5x16CXLnumactl --membind=2\n64GB 4800 MHzUPIDDR5MemoryCXL-attached memory\nMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)PCIe5x16CXLnumactl --membind=2\n64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory\nnumactl --membind=116GB 2933 MHzDDR4MemoryMemoryHostProcessorSocket0HostProcessorSocket116GB 2933 MHznumactl --membind=1cores 0-9cores 10-19UPIDDR4\n(Class 2.b)\n64GB 4800 MHzUPIDDR5MemoryCXL-attached memory\nMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1Memory16GB 1333 MHzDDR4Intel Agilex 7 (FPGA)PCIe5x16CXLnumactl --membind=2\n64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory16GB 2933 MHzDDR4MemoryMemoryHostProcessorSocket0HostProcessorSocket116GB 2933 MHzcores 0-9cores 10-19UPIDDR4\nnumactl --membind=0numactl --membind=0\n64GB 4800 MHzUPIDDR5MemoryMemory64GB 4800 MHzcores 0-9cores 10-19DDR5HostProcessorSocket0HostProcessorSocket1CXL-attached memory\nnumactl --membind=116GB 2933 MHzDDR4MemoryMemoryHostProcessorSocket0HostProcessorSocket116GB 2933 MHznumactl --membind=1cores 0-9cores 10-19UPIDDR4Figure 9: Data \uffffow demonstrations for the two classes ( App Direct andMemory Mode ).", "start_char_idx": 319250, "end_char_idx": 322541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37e79361-c5f8-4833-b9a5-794209ac3182": {"__data__": {"id_": "37e79361-c5f8-4833-b9a5-794209ac3182", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0efb634c-c11e-4c57-8645-4c5a7ca6d9a9", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "4e366fab10b1eb04d5989a0ecb7de8d0827d84187922fbedf80c8e1df604c7c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45199acd-ee0e-4343-aac1-1cd58e81fd8d", "node_type": "1", "metadata": {}, "hash": "1cd9d66b791562b164ad5bd06aa9be12a3d8b1c898684b20feb59d1d04a66261", "class_name": "RelatedNodeInfo"}}, "text": "Each test group is evaluated in corre-\nsponding to sub \uffffgures of Figure 5 ,Figure 6 ,Figure 7 ,Figure 8 . Each row corresponds to a test group.9,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren\nall STREAM operations, in comparison to local App-Direct access.\nIn the case of App-Direct access to remote CXL memory (DDR4),\nwe experience 50% decrease in performance in comparison to the\nemulated PMem on DDR5. However, we note that DDR5 inherently\nhas about 50% higher bandwidth than DDR4, meaning that the\nrest of the overhead \u2013 about 2-3 GB/s loss in bandwidth \u2013 can be\nattributed to the CXL fabric.\n(c)Remote memory as PMem (thread a \uffffnity): As observed in\nprevious groups, local App-Direct accesses result in higher band-\nwidth than remote accesses. In the case of close thread a \uffffnity, after\npopulating the entire socket, adding remote accesses of compute\ncores to the workload negatively impacts the bandwidth, whereas\nadding local accesses contributes positively. With spread a\uffffnity,\nthe performance demonstrates an average between local and remote\naccesses due to the inclusion of alternating accesses. Eventually,\nwhen both sockets are operating with the entire core count, the\nresults converge for on-node DDR5 and remote CXL memory, sep-\narately. Notably, accessing remote CXL memory (DDR4) leads to a\n50% observed degradation compared to on-node DDR5.\nClass 2 \u2014 Memory Mode :\n(a)Remote CC-NUMA: Evaluating DDR4 CC-NUMA, whether\non the remote socket or CXL-attached memory, yields comparable\n\uffffgures (with average gaps of up to 2-5 GB/s). However, following a\nsmall number of threads, a slight advantage is observed for access-\ning CXL memory. This advantage can be attributed to the larger\ncaches in Setup #1 utilizing CXL (Shappire Rapids), as opposed to\nSetup #2 (Xeon Gold) with on-node DDR4 ( subsection 2.1 ). This\nindicates that the CXL fabric overhead is constrained by the per-\nformance reduction when transitioning back from Sapphire Rapids\nto Xeon Gold. Moreover, the gap between the CC-NUMA to DDR5\nand DDR4 (on-node or CXL-attached) stands on a factor of two, as\nalready observed in 1. (b)and 1. (c). In addition, in comparison to\nthe results of the App-Direct tests in 1. (b), it is observed that PMDK\noverheads over CC-NUMA are 10%-15% (in all STREAM methods).\n(b)Remote CC-NUMA (all cores): The observed gap between\nDDR4 and DDR5 repeats here. Moreover, accessing on-node DDR4\nusing all cores converges to the same results as accessing DDR4\nCXL memory.\nTo conclude, the analysis reveals that direct access to local DDR5\nmemory using PMDK saturates at 20-22 GB/s, while direct remote\naccess to emulated PMem and CXL memory results in 30% and\n50% performance decreases, respectively, with about 2-3 GB/s band-\nwidth loss attributed to CXL fabric. In terms of memory expansion,\naccessing remote DDR4 CC-NUMA and DDR4 CXL-attached mem-\nory exhibit similar performance gaps of 2-3 GB/s, while DDR5\nCC-NUMA maintains an advantage gap of a factor of 1.5 compared\nto DDR4, and on-node DDR4 access converges with o \uffff-node DDR4\naccess under varying thread a \uffffnities.\n5 CONCLUSIONS\nIn this study, we embarked on a comprehensive exploration of the\npotential of CXL memory as a promising candidate for serving as a\npersistent memory solution in the context of disaggregated HPC\nsystems. By conducting physical experiments on state-of-the-art\nmulti-NUMA nodes equipped with high-performance processors\nand CXL-attached memory prototypes, we have provided empiricalevidence that supports the feasibility of using CXL memory to\nexhibit all the characteristics of persistent memory modules while\nachieving impressive performance metrics.\nOur \uffffndings demonstrate that CXL memory has the capabil-\nity to outperform previously published benchmarks for Optane\nDCPMM in terms of bandwidth.", "start_char_idx": 322542, "end_char_idx": 326373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45199acd-ee0e-4343-aac1-1cd58e81fd8d": {"__data__": {"id_": "45199acd-ee0e-4343-aac1-1cd58e81fd8d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37e79361-c5f8-4833-b9a5-794209ac3182", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "c37840327c7716f9280a01b3fec53aad66d6e14a73ddf4da60cb98bc515747e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3d8d233-48d7-4a03-936e-388edd575300", "node_type": "1", "metadata": {}, "hash": "6229ce5e0f80c3d3e6b9e262c04b16a1f3890045e7a73934690517b96d860053", "class_name": "RelatedNodeInfo"}}, "text": "5 CONCLUSIONS\nIn this study, we embarked on a comprehensive exploration of the\npotential of CXL memory as a promising candidate for serving as a\npersistent memory solution in the context of disaggregated HPC\nsystems. By conducting physical experiments on state-of-the-art\nmulti-NUMA nodes equipped with high-performance processors\nand CXL-attached memory prototypes, we have provided empiricalevidence that supports the feasibility of using CXL memory to\nexhibit all the characteristics of persistent memory modules while\nachieving impressive performance metrics.\nOur \uffffndings demonstrate that CXL memory has the capabil-\nity to outperform previously published benchmarks for Optane\nDCPMM in terms of bandwidth. Speci \uffffcally, by employing a CXL-\nDDR4 memory module, which is a cost-e \uffffective alternative to\nDDR5 memory, we achieved bandwidth results comparable to lo-\ncal DDR4 memory con \uffffgurations, with only a marginal decrease\nof around 50% when compared to local DDR5 memory con \uffffgura-\ntions. These results, attained across various memory distances from\nthe working threads, were assessed through the well-established\nSTREAM benchmark underscoring the reliability and versatility of\nCXL memory in the HPC landscape.\nThe shift from PMem to CXL was not only demonstrated through\nperformance evaluations but was also highlighted through the\nmodi \uffffcation of the STREAM application into STREAM-PMem. We\nshowcased the seamless transition of programming models from\nPMem to CXL, leveraging the PMDK\u2019s pmemobj to ensure trans-\nactional integrity and consistency of operations on persistent ob-\njects. Furthermore, the ability to access CXL memory directly and\ntransactionally, akin to Optane DCPMM, was underscored as a key\nadvantage for practical implementation.\nOur study extends beyond theoretical considerations by imple-\nmenting a practical CXL prototype on an FPGA card. This prototype\nembodies CXL 1.1/2.0 compliant endpoint designs, demonstrating\ne\uffffective link establishment and transaction layer management\nthrough a combination of Soft and Hard IP components. The proto-\ntype\u2019s performance, while constrained by current implementation\nlimitations, stands as a testament to the extensibility of this solu-\ntion and o \uffffers a blueprint for potential enhancements, including\nhigher-speed FPGAs and increased resources.\n6 FUTURE WORK\nWhile this study provides valuable insights into the feasibility and\npotential bene \uffffts of using CXL-enabled memory in HPC systems,\nseveral avenues for future research and exploration remain:\n\u2022Scalability and Performance Optimization : Further investiga-\ntion is warranted to explore the scalability of CXL-enabled memory\nin larger HPC clusters, with more than one node accessing the CXL\nmemory. Optimizing communication protocols and memory access\npatterns can help maximize memory disaggregation bene \uffffts.\n\u2022Hybrid Architectures : Combining di \ufffferent memory technolo-\ngies, such as DDR, PMem, and CXL memory, in a hybrid mem-\nory architecture could o \uffffer a balanced solution that leverages the\nstrengths of each technology. Also, the CXL memory could also use\nDDR5 and even Optane DCPMM, and as such, revisiting the results\nwith those CXL memories would be bene \uffffcial.\n\u2022Real-World Applications : Extending the evaluation to real-\nworld HPC applications beyond benchmarks can provide a clearer\nunderstanding of how CXL memory performs in practical scenarios.\n\u2022Fault Tolerance and Reliability : Investigating fault tolerance\nmechanisms and data reliability in the context of CXL-enabled\nmemory is crucial, especially in large-scale distributed environ-\nments. Speci \uffffcally, code systems that have previously been built\nupon PMDK and Optane DCPMM presence in the HPC system.\n10CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,\nACKNOWLEDGMENTS\nThis work was supported by Pazy grant 226/20, the Lynn and\nWilliam Frankel Center for Computer Science, and Intel Corpora-\ntion (oneAPI Center of Excellence program). Computational support\nwas provided by the NegevHPC project [ 54] and Intel Developer\nCloud [ 23]. The authors would like to thank Gabi Dadush, Israel\nHen, and Emil Malka for their hardware support on NegevHPC.\nThe authors also want to thank Jay Mahalingam and Guy Tamir of\nIntel for their great help in forming this collaboration.", "start_char_idx": 325663, "end_char_idx": 329964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3d8d233-48d7-4a03-936e-388edd575300": {"__data__": {"id_": "b3d8d233-48d7-4a03-936e-388edd575300", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45199acd-ee0e-4343-aac1-1cd58e81fd8d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "9867c0c5107db64fe758c3430e05d13c30d7313fb352bbb028ec251a8064cc12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4ca2cdf-0048-46af-85f5-fc2137dabd26", "node_type": "1", "metadata": {}, "hash": "aec5969f1aec750031801a78e739d0953b653cadc96c04fcf714b9519e0d2d7b", "class_name": "RelatedNodeInfo"}}, "text": "Speci \uffffcally, code systems that have previously been built\nupon PMDK and Optane DCPMM presence in the HPC system.\n10CXL Memory as Persistent Memory for Disaggregated HPC: A Practical Approach ,,\nACKNOWLEDGMENTS\nThis work was supported by Pazy grant 226/20, the Lynn and\nWilliam Frankel Center for Computer Science, and Intel Corpora-\ntion (oneAPI Center of Excellence program). Computational support\nwas provided by the NegevHPC project [ 54] and Intel Developer\nCloud [ 23]. The authors would like to thank Gabi Dadush, Israel\nHen, and Emil Malka for their hardware support on NegevHPC.\nThe authors also want to thank Jay Mahalingam and Guy Tamir of\nIntel for their great help in forming this collaboration.\nREFERENCES\n[1]Minseon Ahn, Andrew Chang, Donghun Lee, Jongmin Gim, Jungmin Kim, Jaemin\nJung, Oliver Rebholz, Vincent Pham, Krishna Malladi, and Yang Seok Ki. 2022.\nEnabling CXL memory expansion for in-memory database management systems.\nInProceedings of the 18th International Workshop on Data Management on New\nHardware . 1\u20135.\n[2]Hasan Al Maruf and Mosharaf Chowdhury. 2023. Memory Disaggregation: Open\nChallenges in the Era of CXL. In Workshop on HotTopics in System Infrastructure ,\nVol. 18.\n[3]AsteraLabs. 2022. CXL Memory Accelerators .https://www.asteralabs.com/\nproducts/cxl-memory-platform/\n[4]Alexandro Baldassin, Joao Barreto, Daniel Castro, and Paolo Romano. 2021. Per-\nsistent memory: A survey of programming support and implementations. ACM\nComputing Surveys (CSUR) 54, 7 (2021), 1\u201337.\n[5]Lawrence Benson, Marcel Weisgut, and Tilmann Rabl. 2023. What We Can Learn\nfrom Persistent Memory for CXL. BTW 2023 (2023).\n[6]Lars Bergstrom. 2011. Measuring NUMA e \uffffects with the STREAM benchmark.\narXiv preprint arXiv:1103.3225 (2011).\n[7]David E Bernholdt, Swen Boehm, George Bosilca, Manjunath Gorentla Venkata,\nRyan E Grant, Thomas Naughton, Howard P Pritchard, Martin Schulz, and Geof-\nfroy R Vallee. 2020. A survey of MPI usage in the US exascale computing project.\nConcurrency and Computation: Practice and Experience 32, 3 (2020), e4851.\n[8]Andr\u00e9s Bustos, Antonio Juan Rubio-Montero, Roberto M\u00e9ndez, Sergio Rivera,\nFrancisco Gonz\u00e1lez, Xandra Campo, Hern\u00e1n Asorey, and Rafael Mayo-Garc\u00eda.\n2023. Response of HPC hardware to neutron radiation at the dawn of exascale.\nThe Journal of Supercomputing (2023), 1\u201322.\n[9]Leonardo Dagum and Ramesh Menon. 1998. OpenMP: An industry-standard API\nfor shared-memory programming. Computing in Science & Engineering 1 (1998),\n46\u201355.\n[10] Nan Ding, Pieter Maris, Hai Ah Nam, Taylor Groves, Muaaz Gul Awan, LeAnn\nLindsey, Christopher Daley, Oguz Selvitopi, Leonid Oliker, and Nicholas Wright.\n2023. Evaluating the Potential of Disaggregated Memory Systems for HPC\napplications. arXiv preprint arXiv:2306.04014 (2023).\n[11] Thomas M Evans, Andrew Siegel, Erik W Draeger, Jack Deslippe, Marianne M\nFrancois, Timothy C Germann, William E Hart, and Daniel F Martin. 2022. A\nsurvey of software implementations used by application codes in the Exascale\nComputing Project. The International Journal of High Performance Computing\nApplications 36, 1 (2022), 5\u201312.\n[12] Svein Gunnar Fagerheim. 2021. Benchmarking Persistent Memory with Respect to\nPerformance and Programmability . Master\u2019s thesis.\n[13] Cl\u00e9ment Foyer, Brice Goglin, and Andr\u00e8s Rubio Proa\u00f1o. 2023. A survey of soft-\nware techniques to emulate heterogeneous memory systems in high-performance\ncomputing.", "start_char_idx": 329256, "end_char_idx": 332665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4ca2cdf-0048-46af-85f5-fc2137dabd26": {"__data__": {"id_": "a4ca2cdf-0048-46af-85f5-fc2137dabd26", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3d8d233-48d7-4a03-936e-388edd575300", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "72e20a9cfb1ce418054a2be832808fc9b28fe51c95fed9755d3d9645fc8e1bf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c993fdc-0b5f-4dd6-947f-b8fcd33a290f", "node_type": "1", "metadata": {}, "hash": "f01ce7710c63e2eb79026e6c0fdbd439d79cf1d71d530dc19401694d60fa543d", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2306.04014 (2023).\n[11] Thomas M Evans, Andrew Siegel, Erik W Draeger, Jack Deslippe, Marianne M\nFrancois, Timothy C Germann, William E Hart, and Daniel F Martin. 2022. A\nsurvey of software implementations used by application codes in the Exascale\nComputing Project. The International Journal of High Performance Computing\nApplications 36, 1 (2022), 5\u201312.\n[12] Svein Gunnar Fagerheim. 2021. Benchmarking Persistent Memory with Respect to\nPerformance and Programmability . Master\u2019s thesis.\n[13] Cl\u00e9ment Foyer, Brice Goglin, and Andr\u00e8s Rubio Proa\u00f1o. 2023. A survey of soft-\nware techniques to emulate heterogeneous memory systems in high-performance\ncomputing. Parallel Comput. (2023), 103023.\n[14] Yehonatan Fridman, Yaniv Snir, Harel Levin, Danny Hendler, Hagit Attiya, and\nGal Oren. 2022. Recovery of Distributed Iterative Solvers for Linear Systems\nUsing Non-Volatile RAM. In 2022 IEEE/ACM 12th Workshop on Fault Tolerance for\nHPC at eXtreme Scale (FTXS) . IEEE, 11\u201323.\n[15] Yehonatan Fridman, Yaniv Snir, Matan Rusanovsky, K \uffffr Zvi, Harel Levin, Danny\nHendler, Hagit Attiya, and Gal Oren. 2021. Assessing the use cases of persis-\ntent memory in high-performance scienti \uffffc computing. In 2021 IEEE/ACM 11th\nWorkshop on Fault Tolerance for HPC at eXtreme Scale (FTXS) . IEEE, 11\u201320.\n[16] Andreas Geyer, Daniel Ritter, Dong Hun Lee, Minseon Ahn, Johannes Pietrzyk,\nAlexander Krause, Dirk Habich, and Wolfgang Lehner. 2023. Working with\nDisaggregated Systems. What are the Challenges and Opportunities of RDMA\nand CXL? BTW 2023 (2023).\n[17] William Gropp. 2012. MPI 3 and beyond: why MPI is successful and what\nchallenges it faces. In European MPI Users\u2019 Group Meeting . Springer, 1\u20139.\n[18] Shashank Gugnani, Arjun Kashyap, and Xiaoyi Lu. 2020. Understanding the\nidiosyncrasies of real persistent memory. Proceedings of the VLDB Endowment\n14, 4 (2020), 626\u2013639.\n[19] Frank T Hady, Annie Foong, Bryan Veal, and Dan Williams. 2017. Platform\nstorage performance with 3D XPoint technology. Proc. IEEE 105, 9 (2017), 1822\u2013\n1833.[20] Jim Handy and Tom Coughlin. 2023. Optane\u2019s Dead: Now What? Computer 56, 3\n(2023), 125\u2013130.\n[21] Takahiro Hirofuchi and Ryousei Takano. 2020. A prompt report on the perfor-\nmance of intel optane dc persistent memory module. IEICE TRANSACTIONS on\nInformation and Systems 103, 5 (2020), 1168\u20131172.\n[22] Intel. 2022. Migration from Direct-Attached Intel Optane Persistent Memory\nto CXL-Attached Memory. https://www.intel.com/content/dam/www/central-\nlibraries/us/en/documents/2022-11/optane-pmem-to-cxl-tech-brief.pdf . [On-\nline].\n[23] Intel. 2023. Intel Developer Cloud. https://www.intel.com/content/www/us/en/\ndeveloper/tools/devcloud/overview.html . [Online].\n[24] Intel. 2023. Intel \u00aeFPGA Compute Express Link (CXL) IP. https:\n//www.intel.com/content/www/us/en/products/details/fpga/intellectual-\nproperty/interface-protocols/cxl-ip.html . [Online].\n[25] Intel. 2023. Intel \u00aeFPGA Compute Express Link (CXL) IP. https:\n//www.intel.com/content/www/us/en/products/details/fpga/intellectual-\nproperty/interface-protocols/cxl-ip.html . [Online].\n[26] Joseph Izraelevitz, Hammurabi Mendes, and Michael L Scott.", "start_char_idx": 331986, "end_char_idx": 335140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c993fdc-0b5f-4dd6-947f-b8fcd33a290f": {"__data__": {"id_": "5c993fdc-0b5f-4dd6-947f-b8fcd33a290f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4ca2cdf-0048-46af-85f5-fc2137dabd26", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "c53a1706cc63f855276d2d007342e525b20a06c3bff50d357f8c07b4b47b9348", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71c1feb2-4a79-4fa8-8250-d7b035288089", "node_type": "1", "metadata": {}, "hash": "f4df2c8b2d11d5bf08b812979839e3948ab9e4eebfd864dd98d7322f5bdb504d", "class_name": "RelatedNodeInfo"}}, "text": "[On-\nline].\n[23] Intel. 2023. Intel Developer Cloud. https://www.intel.com/content/www/us/en/\ndeveloper/tools/devcloud/overview.html . [Online].\n[24] Intel. 2023. Intel \u00aeFPGA Compute Express Link (CXL) IP. https:\n//www.intel.com/content/www/us/en/products/details/fpga/intellectual-\nproperty/interface-protocols/cxl-ip.html . [Online].\n[25] Intel. 2023. Intel \u00aeFPGA Compute Express Link (CXL) IP. https:\n//www.intel.com/content/www/us/en/products/details/fpga/intellectual-\nproperty/interface-protocols/cxl-ip.html . [Online].\n[26] Joseph Izraelevitz, Hammurabi Mendes, and Michael L Scott. 2016. Lineariz-\nability of persistent memory objects under a full-system-crash failure model. In\nInternational Symposium on Distributed Computing . Springer, 313\u2013327.\n[27] Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amirsaman\nMemaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R Dulloor, et al .\n2019. Basic performance measurements of the intel optane DC persistent memory\nmodule. arXiv preprint arXiv:1903.05714 (2019).\n[28] Hongshin Jun, Jinhee Cho, Kangseol Lee, Ho-Young Son, Kwiwook Kim, Hanho\nJin, and Keith Kim. 2017. Hbm (high bandwidth memory) dram technology and\narchitecture. In 2017 IEEE International Memory Workshop (IMW) . IEEE, 1\u20134.\n[29] Hongshin Jun, Sangkyun Nam, Hanho Jin, Jong-Chern Lee, Yong Jae Park, and\nJae Jin Lee. 2016. High-bandwidth memory (HBM) test challenges and solutions.\nIEEE Design & Test 34, 1 (2016), 16\u201325.\n[30] Rajat Kateja, Anirudh Badam, Sriram Govindan, Bikash Sharma, and Greg Ganger.\n2017. Viyojit: Decoupling battery and DRAM capacities for battery-backed DRAM.\nACM SIGARCH Computer Architecture News 45, 2 (2017), 613\u2013626.\n[31] Awais Khan, Hyogi Sim, Sudharshan S Vazhkudai, Jinsuk Ma, Myeong-Hoon\nOh, and Youngjae Kim. 2020. Persistent memory object storage and indexing\nfor scienti \uffffc computing. In 2020 IEEE/ACM Workshop on Memory Centric High\nPerformance Computing (MCHPC) . IEEE, 1\u20139.\n[32] Peter M Kogge and William J Dally. 2022. Frontier vs the Exascale Report: Why\nso long? and Are We Really There Yet?. In 2022 IEEE/ACM International Workshop\non Performance Modeling, Benchmarking and Simulation of High Performance\nComputer Systems (PMBS) . IEEE, 26\u201335.\n[33] Benjamin C Lee, Engin Ipek, Onur Mutlu, and Doug Burger. 2010. Phase change\nmemory architecture and the quest for scalability. Commun. ACM 53, 7 (2010),\n99\u2013106.\n[34] Donghun Lee, Thomas Willhalm, Minseon Ahn, Suprasad Mutalik Desai, Daniel\nBooss, Navneet Singh, Daniel Ritter, Jungmin Kim, and Oliver Rebholz. 2023.\nElastic Use of Far Memory for In-Memory Database Management Systems. In\nProceedings of the 19th International Workshop on Data Management on New\nHardware . 35\u201343.\n[35] Jiuxing Liu, Jiesheng Wu, Sushmitha P Kini, Pete Wycko \uffff, and Dhabaleswar K\nPanda. 2003. High performance RDMA-based MPI implementation over In \uffffni-\nBand. In Proceedings of the 17th annual international conference on Supercomputing .\n295\u2013304.\n[36] Ming Liu. 2023. Fabric-Centric Computing. In Proceedings of the 19th Workshop\non Hot Topics in Operating Systems . 118\u2013126.", "start_char_idx": 334550, "end_char_idx": 337636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71c1feb2-4a79-4fa8-8250-d7b035288089": {"__data__": {"id_": "71c1feb2-4a79-4fa8-8250-d7b035288089", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c993fdc-0b5f-4dd6-947f-b8fcd33a290f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d96b48d0ab3cd68539ee645744b40b00790e7157d2d13ac43271e564c231b0f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e234e9ff-705f-460d-9cae-c30e20f143d2", "node_type": "1", "metadata": {}, "hash": "5ee43e78a7b2aacf4b094d6289fc18997ffc622c36a9b284bb03b85f4abced18", "class_name": "RelatedNodeInfo"}}, "text": "2023.\nElastic Use of Far Memory for In-Memory Database Management Systems. In\nProceedings of the 19th International Workshop on Data Management on New\nHardware . 35\u201343.\n[35] Jiuxing Liu, Jiesheng Wu, Sushmitha P Kini, Pete Wycko \uffff, and Dhabaleswar K\nPanda. 2003. High performance RDMA-based MPI implementation over In \uffffni-\nBand. In Proceedings of the 17th annual international conference on Supercomputing .\n295\u2013304.\n[36] Ming Liu. 2023. Fabric-Centric Computing. In Proceedings of the 19th Workshop\non Hot Topics in Operating Systems . 118\u2013126.\n[37] Glenn K Lockwood, Damian Hazen, Quincey Koziol, R Shane Canon, Katie Anty-\npas, Jan Balewski, Nicholas Balthaser, Wahid Bhimji, James Botts, Je \uffffBroughton,\net al. 2023. Storage 2020: A vision for the future of hpc storage. (2023).\n[38] Luke Logan, Jay Lofstead, Xian-He Sun, and Anthony Kougkas. 2023. An Evalua-\ntion of DAOS for Simulation and Deep Learning HPC Workloads. In Proceedings\nof the 3rd Workshop on Challenges and Opportunities of E \uffffcient and Performant\nStorage Systems . 9\u201316.\n[39] Krishna T Malladi, Manu Awasthi, and Hongzhong Zheng. 2016. DRAMPersist:\nMaking DRAM Systems Persistent. In Proceedings of the Second International\nSymposium on Memory Systems . 94\u201395.\n[40] John D. McCalpin. 1991-2007. STREAM: Sustainable Memory Bandwidth in High\nPerformance Computers . Technical Report. University of Virginia, Charlottesville,\nVirginia. http://www.cs.virginia.edu/stream/ A continually updated technical\nreport. http://www.cs.virginia.edu/stream/.\n[41] John D. McCalpin. 1995. Memory Bandwidth and Machine Balance in Current\nHigh Performance Computers. IEEE Computer Society Technical Committee on\nComputer Architecture (TCCA) Newsletter (Dec. 1995), 19\u201325.\n[42] John D. McCalpin. 1995. STREAM Benchmark. https://www.cs.virginia.edu/\nstream/ . [Online].\n[43] John D McCalpin. 1995. Stream benchmark. Link: www. cs. virginia. edu/stream/ref.\nhtml# what 22, 7 (1995).\n[44] John D. McCalpin. 2023. Bandwidth Limits in the Intel Xeon Max (Sapphire\nRapids with HBM) Processors. In ISC 2023 IXPUG Workshop . 1\u201324.\n11,, Yehonatan Fridman, Suprasad Mutalik Desai, Navneet Singh, Thomas Willhalm, and Gal Oren\n[45] Sally A McKee. 2004. Re \uffffections on the memory wall. In Proceedings of the 1st\nconference on Computing frontiers . 162.\n[46] George Michelogiannakis, Yehia Arafa, Brandon Cook, Liang Yuan Dai, Ab-\ndel Hameed Badawy, Madeleine Glick, Yuyang Wang, Keren Bergman, and John\nShalf. 2023. E \uffffcient Intra-Rack Resource Disaggregation for HPC Using Co-\nPackaged DWDM Photonics. arXiv preprint arXiv:2301.03592 (2023).\n[47] George Michelogiannakis, Benjamin Klenk, Brandon Cook, Min Yee Teh,\nMadeleine Glick, Larry Dennison, Keren Bergman, and John Shalf. 2022. A case\nfor intra-rack resource disaggregation in HPC. ACM Transactions on Architecture\nand Code Optimization (TACO) 19, 2 (2022), 1\u201326.\n[48] Vladimir Mironov, Igor Chernykh, Igor Kulikov, Alexander Moskovsky, Evgeny\nEpifanovsky, and Andrey Kudryavtsev. 2019. Performance evaluation of the intel\noptane dc memory with scienti \uffffc benchmarks. In 2019 IEEE/ACM Workshop on\nMemory Centric High Performance Computing (MCHPC) . IEEE, 1\u20136.", "start_char_idx": 337091, "end_char_idx": 340249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e234e9ff-705f-460d-9cae-c30e20f143d2": {"__data__": {"id_": "e234e9ff-705f-460d-9cae-c30e20f143d2", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71c1feb2-4a79-4fa8-8250-d7b035288089", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8309a91738ddc09cfd3183022267c884184558488d3083f585921611f03c2745", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "423a9b60-d907-4785-998a-de5a66e423f6", "node_type": "1", "metadata": {}, "hash": "a4be5c838b0dca93d60de72b2399be89f73fa809f68f69e248b79bba742851ce", "class_name": "RelatedNodeInfo"}}, "text": "[47] George Michelogiannakis, Benjamin Klenk, Brandon Cook, Min Yee Teh,\nMadeleine Glick, Larry Dennison, Keren Bergman, and John Shalf. 2022. A case\nfor intra-rack resource disaggregation in HPC. ACM Transactions on Architecture\nand Code Optimization (TACO) 19, 2 (2022), 1\u201326.\n[48] Vladimir Mironov, Igor Chernykh, Igor Kulikov, Alexander Moskovsky, Evgeny\nEpifanovsky, and Andrey Kudryavtsev. 2019. Performance evaluation of the intel\noptane dc memory with scienti \uffffc benchmarks. In 2019 IEEE/ACM Workshop on\nMemory Centric High Performance Computing (MCHPC) . IEEE, 1\u20136.\n[49] Onur Mutlu. 2013. Memory scaling: A systems architecture perspective. In 2013\n5th IEEE International Memory Workshop . IEEE, 21\u201325.\n[50] Dushyanth Narayanan and Orion Hodson. 2012. Whole-system persistence. In\nProceedings of the seventeenth international conference on Architectural Support\nfor Programming Languages and Operating Systems . 401\u2013410.\n[51] Kevin Huang Nathan Pham. 2019. Analyzing the Performance of Intel Optane Per-\nsistent Memory 200 Series in Memory Mode with Lenovo ThinkSystem Servers.\n[52] Samsung Newsroom. 2022. Samsung Electronics Introduces Industry\u2019s First 512GB\nCXL Memory Module .https://news.samsung.com/global/samsung-electronics-\nintroduces-industrys- \uffffrst-512gb-cxl-memory-module\n[53] SK Hynix Newsroom. 2022. SK hynix Develops DDR5 DRAM CXLTM Memory\nto Expand the CXL Memory Ecosystem .https://news.skhynix.com/sk-hynix-\ndevelops-ddr5-dram-cxltm-memory-to-expand-the-cxl-memory-ecosystem/\n[54] Rotem Industrial Park. 2019. NegevHPC Project. https://www.negevhpc.com .\n[Online].\n[55] Onkar Patil, Latchesar Ionkov, Jason Lee, Frank Mueller, and Michael Lang. 2019.\nPerformance characterization of a dram-nvm hybrid memory architecture for\nhpc applications using intel optane dc persistent memory modules. In Proceedings\nof the International Symposium on Memory Systems . 288\u2013303.\n[56] Ivy Peng, Ian Karlin, Maya Gokhale, Kathleen Shoga, Matthew Legendre, and\nTodd Gamblin. 2021. A holistic view of memory utilization on HPC systems:\nCurrent and future trends. In The International Symposium on Memory Systems .\n1\u201311.\n[57] Ivy Peng, Roger Pearce, and Maya Gokhale. 2020. On the memory underuti-\nlization: Exploring disaggregated memory on hpc systems. In 2020 IEEE 32nd\nInternational Symposium on Computer Architecture and High Performance Com-\nputing (SBAC-PAD) . IEEE, 183\u2013190.\n[58] Milan Radulovic, Darko Zivanovic, Daniel Ruiz, Bronis R de Supinski, Sally A\nMcKee, Petar Radojkovi \u0107, and Eduard Ayguad\u00e9. 2015. Another trip to the wall:\nHow much will stacked dram bene \ufffft hpc?. In Proceedings of the 2015 International\nSymposium on Memory Systems . 31\u201336.\n[59] Sadhana Rai and Basavaraj Talawar. 2023. Nonvolatile Memory Technologies:\nCharacteristics, Deployment, and Research Challenges. Frontiers of Quality\nElectronic Design (QED) AI, IoT and Hardware Security (2023), 137\u2013173.\n[60] Daniel Reed, Dennis Gannon, and Jack Dongarra. 2022. Reinventing high perfor-\nmance computing: challenges and opportunities. arXiv preprint arXiv:2203.02544\n(2022).\n[61] Chaoyi Ruan, Yingqiang Zhang, Chao Bi, Xiaosong Ma, Hao Chen, Feifei Li,\nXinjun Yang, Cheng Li, Ashraf Aboulnaga, and Yinlong Xu. 2023. Persistent\nMemory Disaggregation for Cloud-Native Relational Databases. In Proceedings of\nthe 28th ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 3 . 498\u2013512.", "start_char_idx": 339675, "end_char_idx": 343103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "423a9b60-d907-4785-998a-de5a66e423f6": {"__data__": {"id_": "423a9b60-d907-4785-998a-de5a66e423f6", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e234e9ff-705f-460d-9cae-c30e20f143d2", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6f4c91fa162d5091f4328c3a58a6ffa50e0519a2996e7329ceeea3072c0af933", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3950ad46-d5e6-4104-8928-d0389d63a5a0", "node_type": "1", "metadata": {}, "hash": "8de5b19fb8a21215ed0d80af2d160b020260aa489d55899381a141c79dd31246", "class_name": "RelatedNodeInfo"}}, "text": "Nonvolatile Memory Technologies:\nCharacteristics, Deployment, and Research Challenges. Frontiers of Quality\nElectronic Design (QED) AI, IoT and Hardware Security (2023), 137\u2013173.\n[60] Daniel Reed, Dennis Gannon, and Jack Dongarra. 2022. Reinventing high perfor-\nmance computing: challenges and opportunities. arXiv preprint arXiv:2203.02544\n(2022).\n[61] Chaoyi Ruan, Yingqiang Zhang, Chao Bi, Xiaosong Ma, Hao Chen, Feifei Li,\nXinjun Yang, Cheng Li, Ashraf Aboulnaga, and Yinlong Xu. 2023. Persistent\nMemory Disaggregation for Cloud-Native Relational Databases. In Proceedings of\nthe 28th ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 3 . 498\u2013512.\n[62] Andy Rudo \uffff. 2017. Persistent memory: The value to hpc and the challenges. In\nProceedings of the Workshop on memory centric programming for hpc . 7\u201310.\n[63] Arthur Sainio et al .2016. NVDIMM: changes are here so what\u2019s next. Memory\nComputing Summit (2016).\n[64] Steve Scargall. 2020. Programming Persistent Memory: A Comprehensive Guide\nfor Developers. (2020).\n[65] Steve Scargall and Steve Scargall. 2020. libpmemobj: A Native Transactional Ob-\nject Store. Programming Persistent Memory: A Comprehensive Guide for Developers\n(2020), 81\u2013109.\n[66] Debendra Das Sharma, Robert Blankenship, and Daniel S Berger. 2023. An\nIntroduction to the Compute Express Link (CXL) Interconnect. arXiv preprint\narXiv:2306.11227 (2023).\n[67] Galen M Shipman, Sriram Swaminarayan, Gary Grider, Jim Lujan, and R Joseph\nZerr. 2022. Early Performance Results on 4th Gen Intel (R) Xeon (R) Scalable\nProcessors with DDR and Intel (R) Xeon (R) processors, codenamed Sapphire\nRapids with HBM. arXiv preprint arXiv:2211.05712 (2022).\n[68] Montage Technology. 2022. Montage Technology Delivers the World\u2019s First CXL \u2122\nMemory eXpander Controller .https://www.montage-tech.com/Press_Releases/\n20220506\n[69] TB Tristian and L Travis. 2019. Analyzing the performance of Intel Optane DC\npersistent memory in app direct mode in Lenovo ThinkSystem servers.[70] Jacob Wahlgren, Maya Gokhale, and Ivy B Peng. 2022. Evaluating Emerging\nCXL-enabled Memory Pooling for HPC Systems. In 2022 IEEE/ACM Workshop on\nMemory Centric High Performance Computing (MCHPC) . IEEE, 11\u201320.\n[71] Ying Wang, Wen-Qing Jia, De-Jun Jiang, and Jin Xiong. 2023. A Survey of Non-\nVolatile Main Memory File Systems. Journal of Computer Science and Technology\n38, 2 (2023), 348\u2013372.\n[72] Mich\u00e8le Weiland, Holger Brunst, Tiago Quintino, Nick Johnson, Olivier I \uffffrig,\nSimon Smart, Christian Herold, Antonino Bonanni, Adrian Jackson, and Mark\nParsons. 2019. An early evaluation of intel\u2019s optane dc persistent memory module\nand its impact on high-performance scienti \uffffc applications. In Proceedings of the\ninternational conference for high performance computing, networking, storage and\nanalysis . 1\u201319.\n12A Case Against CXL Memory Pooling\nPhilip Levis\nGoogle\nplevis@google .comKun Lin\nGoogle\nlinkun@google .comAmy Tai\nGoogle\namytai@google .com\nAbstract\nCompute Express Link (CXL) is a replacement for PCIe. With\nmuch lower latency than PCIe and hardware support for cache\ncoherence, programs can ef\ufb01ciently access remote memory\nover CXL. These capabilities have opened the possibility of\nCXL memory pools in datacenter and cloud networks, consist-\ning of a large pool of memory that multiple machines share.\nRecent work argues memory pools could reduce memory\nneeds and datacenter costs.", "start_char_idx": 342392, "end_char_idx": 345830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3950ad46-d5e6-4104-8928-d0389d63a5a0": {"__data__": {"id_": "3950ad46-d5e6-4104-8928-d0389d63a5a0", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "423a9b60-d907-4785-998a-de5a66e423f6", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "b39375632cab66d00d86bcb25e52ff9f932206f316d6506d08988243ac8c8ed8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8598ad6f-8201-461a-b638-280ae196404b", "node_type": "1", "metadata": {}, "hash": "b136ad931379147ae0fd85cabd9ffcc146527f78a8b61d97a1adcfc792c5c14e", "class_name": "RelatedNodeInfo"}}, "text": "2019. An early evaluation of intel\u2019s optane dc persistent memory module\nand its impact on high-performance scienti \uffffc applications. In Proceedings of the\ninternational conference for high performance computing, networking, storage and\nanalysis . 1\u201319.\n12A Case Against CXL Memory Pooling\nPhilip Levis\nGoogle\nplevis@google .comKun Lin\nGoogle\nlinkun@google .comAmy Tai\nGoogle\namytai@google .com\nAbstract\nCompute Express Link (CXL) is a replacement for PCIe. With\nmuch lower latency than PCIe and hardware support for cache\ncoherence, programs can ef\ufb01ciently access remote memory\nover CXL. These capabilities have opened the possibility of\nCXL memory pools in datacenter and cloud networks, consist-\ning of a large pool of memory that multiple machines share.\nRecent work argues memory pools could reduce memory\nneeds and datacenter costs.\nIn this paper, we argue that three problems preclude CXL\nmemory pools from being useful or promising: cost, complex-\nity, and utility. The cost of a CXL pool will outweigh any\nsavings from reducing RAM. CXL has substantially higher\nlatency than main memory, enough so that using it will re-\nquire substantial rewriting of network applications in complex\nways. Finally, from analyzing two production traces from\nGoogle and Azure Cloud, we \ufb01nd that modern servers are\nlarge relative to most VMs; even simple VM packing algo-\nrithms strand little memory, undermining the main incentive\nbehind pooling.\nDespite recent research interest, as long as these three\nproperties hold, CXL memory pools are unlikely to be a\nuseful technology for datacenter or cloud systems.\nCCS Concepts\n\u2022Networks !Data center networks ;\u2022Information sys-\ntems !Enterprise resource planning .\nKeywords\ndatacenter networking, CXL memory pooling\nACM Reference Format:\nPhilip Levis, Kun Lin, and Amy Tai. 2023. A Case Against CXL\nMemory Pooling. In The 22nd ACM Workshop on Hot Topics in\nNetworks (HotNets \u201923), November 28\u201329, 2023, Cambridge, MA,\nUSA. ACM, New York, NY, USA, 7 pages. https://doi .org/10 .1145/\n3626111 .3628195\nPermission to make digital or hard copies of part or all of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for pro\ufb01t or commercial advantage and that copies bear\nthis notice and the full citation on the \ufb01rst page. Copyrights for third-party\ncomponents of this work must be honored. For all other uses, contact the\nowner/author(s).\nHotNets \u201923, November 28\u201329, 2023, Cambridge, MA, USA\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0415-4/23/11.\nhttps://doi .org/10 .1145/3626111 .36281951Introduction\nMemory is an expensive component of datacenter and cloud\nservers: recent papers report its fraction of a server\u2019s cost is\n40% for Meta [ 14] and 50% for Azure [ 21]. Google faces\nsimilar pressures [ 6]. The pressure to reduce RAM needs\nand costs has motivated work in far memory [ 18], memory\ncompression [ 12], and Intel Optane memory, which trades off\nperformance for lower cost [ 17]. If a server has insuf\ufb01cient\nmemory, it can have free cores but no available memory\n(stranded cores); if it has too much memory it can have free\nmemory that cores do not use (stranded memory).\nOne approach to reduce RAM costs is to disaggregate mem-\nory through a shared pool. In this model, servers have their\nown local RAM, which is suf\ufb01cient for average or expected\nuse. If a server needs more memory or has stranded cores,\nit can allocate from a pool shared among several servers. A\nmemory pool needs to solve two major problems: latency and\ncache coherence. Main memory in a larger server CPU has\na latency of 120-140ns; if a memory pool\u2019s latency is much\nhigher, application performance will suffer.\nThe Compute Express Link (CXL) protocol promises to\nprovide low-latency, cache coherent access to remote mem-\nory. With claimed latencies in the hundreds of nanoseconds,\nCXL can build a large memory pool shared across several\nservers.", "start_char_idx": 344994, "end_char_idx": 348942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8598ad6f-8201-461a-b638-280ae196404b": {"__data__": {"id_": "8598ad6f-8201-461a-b638-280ae196404b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3950ad46-d5e6-4104-8928-d0389d63a5a0", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "c0f9c8c45cd33b8e05dfed88b0834893acb77a33d53cce2f1646199bd1546971", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3241294-0319-40f5-9621-552eed1e6f31", "node_type": "1", "metadata": {}, "hash": "5b4988c534e4430adc86075a0e3d99e26240d757130b4525a7cdaf56f185475b", "class_name": "RelatedNodeInfo"}}, "text": "One approach to reduce RAM costs is to disaggregate mem-\nory through a shared pool. In this model, servers have their\nown local RAM, which is suf\ufb01cient for average or expected\nuse. If a server needs more memory or has stranded cores,\nit can allocate from a pool shared among several servers. A\nmemory pool needs to solve two major problems: latency and\ncache coherence. Main memory in a larger server CPU has\na latency of 120-140ns; if a memory pool\u2019s latency is much\nhigher, application performance will suffer.\nThe Compute Express Link (CXL) protocol promises to\nprovide low-latency, cache coherent access to remote mem-\nory. With claimed latencies in the hundreds of nanoseconds,\nCXL can build a large memory pool shared across several\nservers. Disaggregating storage from compute led to much\nmore ef\ufb01cient and scalable datacenter storage [ 7]; disaggre-\ngating memory from compute could have a similar impact,\nenabling more ef\ufb01cient and lower cost computing.\nUnfortunately, this paper argues that CXL memory pool-\ning faces three major problems. Each of these problems, in\nisolation, might limit potential use cases but is surmountable.\nTogether, however, they mean that CXL memory pools cost\nmore, require rewriting software, and do not reduce resource\nstranding (e.g., unused memory).\nThe \ufb01rst problem is cost. The primary bene\ufb01t of a CXL\nmemory pool is reducing the aggregate RAM needs of data-\ncenter and cloud systems. Today, servers are provisioned so\nthey can keep all of their VMs or containers in memory even\nwhen all of them maximize their footprint simultaneously (a\n\u201csum-of-max\u201d approach). Using a CXL pool can allow servers\nto instead provision for expected use, and when VMs uses\ntheir entire footprint the system can store cold data in a CXL\npool. This cost calculation, however, ignores infrastructure\ncosts. CXL requires a completely parallel network infras-\ntructure to Ethernet, consisting of a top-of-rack (or top-of-N\nserver) CXL appliance, with direct, alternative cabling to all\nof its servers.\nThe second problem is software complexity . Recent ex-\nperimental results from real CXL hardware \ufb01nd that many of\n18\nHotNets \u201923, November 28\u201329, 2023, Cambridge, MA, USA Philip Levis, Kun Lin, and Amy Tai\nCXL\u2019s latency claims are best-case estimates. For example,\nestimates in the Pond system are that CXL will add 70-90ns\nover same-NUMA-node DRAM. Recent experimental results,\nhowever, are that CXL adds 140ns for pointer chasing work-\nloads [ 19]. This slowdown is for a directly-connected CXL\nmemory device, not a shared pool, which adds switching,\nre-timers, and queueing. While loads and stores to a CXL de-\nvice will be much slower than DRAM, hardware-accelerated\ncopies of 8kB blocks are close to DRAM speed [ 20]. There-\nfore, achieving good performance involves rewriting software\nto explicitly manage CXL memory, copying blocks into lo-\ncal DRAM; this explicit, conditional, and pervasive mem-\nory management increases software complexity. Furthermore,\nmaintaining multiple copies reduces CXL\u2019s memory savings.\nThe third problem is limited utility . The primary argument\nfor CXL memory is that memory that would otherwise be\nstranded, i.e. memory that cannot be allocated to a VM be-\ncause there are no more compute resources to support a VM,\ncan now be pooled and used by other servers. However, after\nanalyzing common server and VM shapes in a 2019 Google\ncluster trace [ 22] and 2020 Azure Cloud trace [ 9] using a\nmethodology we developed to evaluate the conditions when\nmemory pooling can improve stranding, we conclude pooling\nis rarely helpful. Modern servers are large (hundreds of cores\nand terabytes of RAM), and VMs are small enough, that VMs\ncan be placed on a single server with little stranding . For the\ntraces we examined, the ratio of VM to server sizes must in-\ncrease by 32x (Google) and 8x (Azure) before pooling yields\neven modest ef\ufb01ciency gains. To the best of our knowledge,\nthis is the \ufb01rst methodology for estimating the potential of\nmemory pooling for resource packing.\nIn summary, as long as the cost, software complexity, and\nlack of utility properties hold, sharing a large DRAM bank\nbetween servers with CXL is a losing proposition.", "start_char_idx": 348195, "end_char_idx": 352391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3241294-0319-40f5-9621-552eed1e6f31": {"__data__": {"id_": "c3241294-0319-40f5-9621-552eed1e6f31", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8598ad6f-8201-461a-b638-280ae196404b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e2c64d9c1b5aeb9b6dc8267587064e1245482037d7124a1117ff8870c2633782", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4fc44dd-3527-468d-a425-d38d53a4dc60", "node_type": "1", "metadata": {}, "hash": "23f9177bf0e26b81578c78441699e1a1145af11063e488e227f0bb075703cebd", "class_name": "RelatedNodeInfo"}}, "text": "Modern servers are large (hundreds of cores\nand terabytes of RAM), and VMs are small enough, that VMs\ncan be placed on a single server with little stranding . For the\ntraces we examined, the ratio of VM to server sizes must in-\ncrease by 32x (Google) and 8x (Azure) before pooling yields\neven modest ef\ufb01ciency gains. To the best of our knowledge,\nthis is the \ufb01rst methodology for estimating the potential of\nmemory pooling for resource packing.\nIn summary, as long as the cost, software complexity, and\nlack of utility properties hold, sharing a large DRAM bank\nbetween servers with CXL is a losing proposition. If one of\nthese issues goes away \u2013 CXL is cheap, CXL is nearly as fast\nas main memory, or VM shapes become dif\ufb01cult to pack into\nservers \u2013 then CXL memory pools might prove to be useful.\n2CXL Memory Pools\nThis section explains Compute Express Link (CXL) and how\nCXL memory pools work. Readers familiar with CXL can\nskip this section. Because many of the details of CXL are\nextraneous to this paper, we gloss over them; an interested\nreader can consult the speci\ufb01cations [3\u20135].\n2.1 CXL\nIn this paper, we focus on CXL 1.1, the \ufb01rst productized ver-\nsion. Samsung [ 15], Intel [ 10], and Astera Labs [ 1] produce\nCXL 1.1 devices, but none of them are generally available;\nthey are only for pilot use and commercial evaluation. Version\n3.0 was published in August, 2022 [5].\nCXL 1.1 uses the same physical layer (connections and\nsignaling) as PCIe. PCIe connections consist of one or more\nparallel \u201clanes\u201d. CXL 1.1 uses PCIe Gen5 signaling, which\nprovides 3.9GB/s per lane. CXL devices can have 1-32 lanes.\nFigure 1: Processing path of a memory load (into a CXL\nRequest) in a dual-socket server to a memory pool con-\nnected through. Queueing is possible at almost every\ntransition. The response goes through the same path;\nlookups become updates.\nA Samsung 128GB CXL memory device, for example, uses\n8 lanes to support a maximum throughput of 35GB/s. [ 16]\nCXL 3.0 uses PCIe Gen6 to double per-lane throughput. PCIe\nGen7 is expected to double throughput again (to 15GB/s),\nbut this approaches the practical limit for a differential pair\n(224Gbps) due to gate switching latencies.\nCXL differs from PCIe in two major ways: lower latency\nand cache coherence. CXL strips out many of the protocol\noverheads of PCIe to reduce latency. While PCIe Gen5 de-\nvices have best-case round-trip-time latencies of over 500ns,\nCXL devices can be as low as 150ns. This is the minimum\nsignaling latency: it does not include the time for a device to\ngenerate a response (e.g., read from DRAM), any protocol\nprocessing, or queueing delays. While CXL 3.0 increases the\nthroughput of CXL, it will not have lower latency [ 11] as\npacketization delay is not signi\ufb01cant.1\nCXL\u2019s second improvement is hardware cache coherence.\nThis is useful for devices such as NICs or GPUs, but it is less\nimportant for memory pools, which typically do not allow\nservers to share memory.\n2.2 Inside a CXL Pool\nCXL memory pools can take many forms; in this section, we\nfocus on the cloud use case of multiple servers connecting to a\nsingle device through separate physical links (called a \u201cmulti-\nheaded device\u201d), as proposed by Pond. [ 13] We assume the\nbest-case use of a CXL pool, in which effectively all memory\naccesses are to memory that is exclusive to a single server,\nsuch that there are no cache coherence overheads.\nFigure 1 decomposes the sources of latency when reading\nfrom a CXL memory pool. First, there are the standard mem-\nory latency costs: a core must detect that the memory is not in\n1For a 16-lane device at 62GB/s, a 256 byte CXL \ufb02it takes 4ns.\n19A Case Against CXL Memory Pooling HotNets \u201923, November 28\u201329, 2023, Cambridge, MA, USA\nany cache or local DRAM.", "start_char_idx": 351780, "end_char_idx": 355529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4fc44dd-3527-468d-a425-d38d53a4dc60": {"__data__": {"id_": "f4fc44dd-3527-468d-a425-d38d53a4dc60", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3241294-0319-40f5-9621-552eed1e6f31", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f5b74623978e103946a0a9a08f15b82be7845f9cfe11b224d21111ed5c1f0cb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "044588df-9bf0-4ec1-ad00-fc7beb27d862", "node_type": "1", "metadata": {}, "hash": "8ee45806677cf423a962e0f773efddbd2e807b0ba8c7cb0163696db436539505", "class_name": "RelatedNodeInfo"}}, "text": "[ 13] We assume the\nbest-case use of a CXL pool, in which effectively all memory\naccesses are to memory that is exclusive to a single server,\nsuch that there are no cache coherence overheads.\nFigure 1 decomposes the sources of latency when reading\nfrom a CXL memory pool. First, there are the standard mem-\nory latency costs: a core must detect that the memory is not in\n1For a 16-lane device at 62GB/s, a 256 byte CXL \ufb02it takes 4ns.\n19A Case Against CXL Memory Pooling HotNets \u201923, November 28\u201329, 2023, Cambridge, MA, USA\nany cache or local DRAM. In a dual-socket system (common\nin cloud servers today), the CXL device might be connected\nto either socket\u2019s PCIe/CXL lanes, so there is potentially the\nlatency overhead of the CPU interconnect from one socket to\nthe other. The processor\u2019s memory management unit (MMU)\nmust transform a memory request into a CXL request. This\nenters the CXL root port, which dispatches it to the virtual\nswitch (VCS) and virtual PCI-to-PCI bridge (vPPB) of the\ndevice; this dispatch is necessary because a port\u2019s many lanes\ncan be allocated to many devices (e.g., 16 lanes can be al-\nlocated to 4 different 4-lane M.2 SSDs). The read request\nis packetized, encoded, and modulated onto the CXL link,\nadding packetization and propagation delays.\nOn reception, the CXL read request has to be reassem-\nbled from the parallel lanes, decoded and passed to the CXL\nmemory controller. After protocol processing. the controller\ntranslates the request into DDR memory read requests. DDR\nreads are striped across multiple DDR sockets to maximize\nbandwidth. Once the data is assembled, the CXL device re-\nsponds with a data response, which goes through a similar\nswitching, processing, and encoding as the request did.\nAt every step of this process, there can be queueing. E.g.,\nCXL read requests can queue at the client, memory read\nrequests can queue at the device, DDR read requests can\nqueue in the DDR memory controller, etc.\n2.3 Pond, an example CXL Pool\nPond is a recent proposal for using a CXL memory pool to\nreduce RAM spending in cloud/VM systems [ 13]. Through\nextensive analysis of Azure workloads, the paper \ufb01nds that\na sizeable fraction of Azure memory is stranded: some VMs\nrequest a low RAM-to-CPU ratio, such that some servers\nhave unused RAM but every core is in use. Pond argues\nthat by moving a fraction of every server\u2019s memory to a\nCXL pool and statistically multiplexing the memory, a pool\ncan reduce the total memory needed: memory that is unused\ntoday can instead be used by another server. The tradeoff is\nperformance: since some VM memory is in the CXL pool, it\nis slower. Through careful prediction of which applications\nare latency sensitive and which pages are untouched or rarely\ntouched, Pond can reduce overall DRAM requirements by\n7\u20139% with only 2% of VMs seeing performance degrade by\nmore than 5%.\n2.4 The Case Against Memory Pools\nWe argue that, despite recent interest, CXL memory pools\nare not an effective way to provision networked servers. They\nseem like an exciting possibility and fruitful area of research,\nbut this rosy picture is built on three mistaken and simpli\ufb01ed\nassumptions: cost, complexity, and utility. The next three\nsections examine each issue in detail.\n3Cost\nThe \ufb01rst obstacle for CXL memory pools is their cost. On\none hand, RAM is a large fraction of server cost, so a sharedpool to meet peak needs while reducing per-server memory\nwould reduce costs. We argue that such an analysis makes two\nassumptions that do not hold in practice. First, it assumes that\nmemory is fungible and it is possible to cut a server\u2019s RAM by\na small fraction (e.g., 7-9% in the Pond paper [ 13]). Second,\nit ignores the cost of an additional cabling and networking\ninfrastructure.\n3.1 Memory Provisioning\nCloud and datacenter servers are limited to discrete steps in\nDRAM capacity; small reductions in memory do not necessar-\nily translate to cost savings. Modern server CPUs have 8 (In-\ntel) or 12 (AMD) DDR channels.", "start_char_idx": 354981, "end_char_idx": 358962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "044588df-9bf0-4ec1-ad00-fc7beb27d862": {"__data__": {"id_": "044588df-9bf0-4ec1-ad00-fc7beb27d862", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4fc44dd-3527-468d-a425-d38d53a4dc60", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f0fa6ad4465ecd1ab7ed0d3ff921f95d37371e452ea8122830d99707d1c51c8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faf07e91-0dfd-40f7-92dc-9e297004be7a", "node_type": "1", "metadata": {}, "hash": "3526d241fa1abd135455fea94c54a4a3ef98f582b153c447e2f2772c9189e890", "class_name": "RelatedNodeInfo"}}, "text": "3Cost\nThe \ufb01rst obstacle for CXL memory pools is their cost. On\none hand, RAM is a large fraction of server cost, so a sharedpool to meet peak needs while reducing per-server memory\nwould reduce costs. We argue that such an analysis makes two\nassumptions that do not hold in practice. First, it assumes that\nmemory is fungible and it is possible to cut a server\u2019s RAM by\na small fraction (e.g., 7-9% in the Pond paper [ 13]). Second,\nit ignores the cost of an additional cabling and networking\ninfrastructure.\n3.1 Memory Provisioning\nCloud and datacenter servers are limited to discrete steps in\nDRAM capacity; small reductions in memory do not necessar-\nily translate to cost savings. Modern server CPUs have 8 (In-\ntel) or 12 (AMD) DDR channels. Because many applications\nare memory bandwidth bound, servers always populate ev-\nery channel. DIMMs, however, only come in certain discrete\nsizes (e.g., 32GB, 48GB, 64GB)2, and every channel must\nhave the same sized DIMM. For example, a modern AMD\nGenoa CPU has 12 channels and 192 cores (384 vCPUs). A\nGenoa server can be con\ufb01gured with 750GB (64GB DIMMS),\n1.15TB (96GB DIMMS), or 1.5TB (128GB DIMMS), but no\nintervening values.\nPond \ufb01nds that allocating 25% of VM memory (on aver-\nage) to a shared pool leads to only small slowdowns (1%\nof VMs slow down by more than 5%). This is achievable,\ne.g., by replacing 128GB DIMMs with 96GB DIMMS. While\nachievable, allocating 25% of memory to a pool does not\nreduce the amount of memory. The servers still need the same\namount of memory, just some of it is in a CXL-connected\nmemory appliance.\nMore importantly, Pond also \ufb01nds that a CXL memory pool\ncan reduce total RAM by 7-9% without signi\ufb01cantly harming\nperformance. Some VM memory is unused, and by clustering\nmany servers worth of VMs together, Pond can aggregate\nthese savings. However, as one cannot shrink server RAM\nby 7% or 9%, servers must cut their RAM by 25%, and the\nmemory pool takes the 7\u20139% out of this 25%. This, in turn,\nrequires targeting a very speci\ufb01c amount of memory in the\nCXL pool device, which is dif\ufb01cult given the need to populate\nevery socket to maximize throughput and the large jumps in\nDIMM size. There are some speci\ufb01c con\ufb01gurations where\nthis can work out, but using them constrains the rest of the\nsystem to speci\ufb01c amounts of memory, numbers of cores, and\ndegree of CXL pooling.\n3.2 CXL Infrastructure\nCXL memory pools are not free. We \ufb01nd that their costs\nexceed any savings from reducing RAM. When considering\ncost tradeoffs, we consider consumer (MSRP) prices. Cloud\nproviders and hyperscalers receive steep discounts, but as we\nare considering relative costs and tradeoffs between compo-\nnents, we make a simplifying assumption that hyperscaler\ndiscounts are similar across high-volume parts.\n2Today, sizes that are not a power of two, such as 48GB, are rare; we assume\nvendors would produce large numbers for a cloud provider if asked.\n20HotNets \u201923, November 28\u201329, 2023, Cambridge, MA, USA Philip Levis, Kun Lin, and Amy Tai\nFigure 2: Minimum pool size for RAM cost savings to\nequal switch cost as pool RAM cost decreases relative to\nserver RAM. Even if pool RAM is free, for a standard\n4GB/core memory shape, the pool must be 24 nodes to\nbreak even with just the switch cost.\nBecause there are no CXL memory pool devices today,\nwe do not know how much one costs. However, given the\nspeeds and processing involved, we propose that an Ethernet\nswitch is a good approximation. A CXL memory pool device\nis effectively a high-speed switch, processing CXL packets,\nmanaging cacheline state, reading and writing memory, and\nsending responses back to servers. A standard CXL memory\ndevice (e.g., a Astera Leo [ 1] or Intel device [ 10]) uses 16\nlanes.", "start_char_idx": 358216, "end_char_idx": 361948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "faf07e91-0dfd-40f7-92dc-9e297004be7a": {"__data__": {"id_": "faf07e91-0dfd-40f7-92dc-9e297004be7a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "044588df-9bf0-4ec1-ad00-fc7beb27d862", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "239c2e95d01de5d6ce331b2000571773c66ef217c400ba70323474a670a2af43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c138416-d3d9-4e8c-8d99-45e5a27ec7ef", "node_type": "1", "metadata": {}, "hash": "76e30443d5e96eb77edcfe54764690b4ec37f3ae192e493b5eb02d7f0cb487bf", "class_name": "RelatedNodeInfo"}}, "text": "Even if pool RAM is free, for a standard\n4GB/core memory shape, the pool must be 24 nodes to\nbreak even with just the switch cost.\nBecause there are no CXL memory pool devices today,\nwe do not know how much one costs. However, given the\nspeeds and processing involved, we propose that an Ethernet\nswitch is a good approximation. A CXL memory pool device\nis effectively a high-speed switch, processing CXL packets,\nmanaging cacheline state, reading and writing memory, and\nsending responses back to servers. A standard CXL memory\ndevice (e.g., a Astera Leo [ 1] or Intel device [ 10]) uses 16\nlanes. At PCIe Gen5 speeds this is 480Gbps. A 16-server pool\ntherefore processes data at 7.6Tbps.\nA modern, low-end, 32-port 200Gbps Ethernet switch such\nas the Mellanox MSN3700-VS2F0 costs $38,500. [ 2] DDR5\nRAM today is \u21e13$/GB. For the CXL pool device to break\neven with its RAM savings, it must save 12.6TB of RAM.\nAssuming Pond\u2019s optimistic 9% reduction, to break even with\njust the switch, the servers must have12.6)\u232b\n0.09= 140TB of RAM\nin aggregate (using Pond would reduce this to 127TB). For a\n32-node pool, 127TB, means 4TB per server. A dual-socket\nAMD Genoa server, the standard next-generation system for\ncloud providers, has 384 vCPUs. At 4TB/server, there is\n>10GB of RAM per Genoa vCPU, more than high-RAM\nVMs provide. You have to buy considerably more RAM for\nPond\u2019s RAM savings to pay for themselves: you are better\noff just buying less RAM.\nWhat if pool RAM is cheaper than server RAM? E.g.,\nit could be slower, more cost-ef\ufb01cient DIMMs, or DDR4.\nFigure 2 shows how pool RAM cost affects the minimum\npool size to break even. These results assume the Genoa setup\ndescribed above, reducing server RAM by 25% of RAM, and\nbeing able to reduce aggregate RAM by 9%. Even if pool\nRAM is completely free, for a standard 4GB/core memory\nshape, the pool must be 24 nodes to break even. For memory-\noptimized VMs (8GB/core), if the pool memory is half the\ncost of server memory (50%), a pool size of 20 could break\neven with the switch cost.\nThis accounting is only the capital expenditure of the pool\ndevice: it doesn\u2019t include the cost of the cabling, assembly,and maintenance to connect the servers to the pool, the cost\nof the interface cards that connect to the cables, the space\ncosts of giving up rack slots to pools, or the energy costs of\nthe pool devices. It also does not consider any operational ex-\npenditures for maintaining or managing this parallel network\ninfrastructure. We conclude that the costs of introducing CXL\ndevices into a datacenter network eclipse any cost savings of\nreducing RAM.\n4Software Complexity\nThe second major problem with a CXL memory pool is that\nit will signi\ufb01cantly add to software complexity. Experimental\nresults from real hardware show that, for random accesses,\nCXL devices are signi\ufb01cantly slower than the best case num-\nbers suggested in standards documents. While CXL has high\nlatency, its high throughput means that transferring larger\nblocks of memory (a few kB) can be competitive with DRAM.\nThis requires explicitly copying the remote memory into local\nmemory; the CXL pool stops being memory accessed directly\nand instead becomes a far memory cache.\nToday, CXL devices are not commercially available, and\nNDAs preclude publishing results without prior approval.\nThe only CXL experimental results we are aware of are from\na series of versions of a paper by authors from UIUC and\nIntel [ 19] (the Pond paper [ 13] assumes values reported in\nstandards). Because we do not have an agreement with CXL\ndevice vendors, we base our conclusions on these published\nexperimental results.3\n4.1 CXL Performance\nCXL memory devices are high throughput. They are typically\n16-lane CXL devices; at PCIe Gen5 speeds, this is 480Gbps.\nA single DDR5-4800 DIMM (standard on new servers today)\nis 300Gbps.", "start_char_idx": 361350, "end_char_idx": 365187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c138416-d3d9-4e8c-8d99-45e5a27ec7ef": {"__data__": {"id_": "9c138416-d3d9-4e8c-8d99-45e5a27ec7ef", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faf07e91-0dfd-40f7-92dc-9e297004be7a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3e7cc416cef4dc8728958e9cf87ae3ce64ee81d12f93d698d1dadce5a450921f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1296a4eb-0be6-49ee-90c6-e3cdf452a49f", "node_type": "1", "metadata": {}, "hash": "8e86e56d0469c943f33e683a35ac057a11f80676b01f6f1ea31bc987ec52248c", "class_name": "RelatedNodeInfo"}}, "text": "This requires explicitly copying the remote memory into local\nmemory; the CXL pool stops being memory accessed directly\nand instead becomes a far memory cache.\nToday, CXL devices are not commercially available, and\nNDAs preclude publishing results without prior approval.\nThe only CXL experimental results we are aware of are from\na series of versions of a paper by authors from UIUC and\nIntel [ 19] (the Pond paper [ 13] assumes values reported in\nstandards). Because we do not have an agreement with CXL\ndevice vendors, we base our conclusions on these published\nexperimental results.3\n4.1 CXL Performance\nCXL memory devices are high throughput. They are typically\n16-lane CXL devices; at PCIe Gen5 speeds, this is 480Gbps.\nA single DDR5-4800 DIMM (standard on new servers today)\nis 300Gbps. Server CPUs have many DIMMs, but 480Gbps\nis fast and it can support reasonable copy performance of\nlarger objects. For example, a copy from CXL memory to\nlocal DDR by a single core can use 80% of the bandwidth of\ntwo DIMMs.\nHowever, CXL memory devices are also high latency. The\nUIUC measurement results of a directly connected (no switch-\ning) CXL device show CXL loads have best case latencies\nof 2x of local memory, substantially slower than a remote\nNUMA node (1.5x) [ 19]. For a modern server CPU (e.g.,\nSapphire Rapids, as used in the paper), this means a memory\naccess jumps from 140ns for local memory to 280ns for CXL\nmemory. At 2.0-3.0 GHz in a multiple-issue superscalar pro-\ncessor, this latency stalls the CPU for over 500 instructions.\nSwitched system with re-timers will have higher latency.\n3The revision of the UIUC/Intel paper accepted to MICRO [ 19] reports\nresults from multiple CXL devices, which vary greatly in performance. We\nfocus on latencies from the highest performance device measured, CXL-A,\nwhich is an ASIC.\n21A Case Against CXL Memory Pooling HotNets \u201923, November 28\u201329, 2023, Cambridge, MA, USA\n4.2 Instructions or Transfers?\nWhile CXL will perform poorly as a transparent far memory,\nits throughput means it can read or write larger blocks with\ngood performance. For example, early Intel/UIUC results\nshow that synchronously copying 8kB from DRAM to CXL\nmemory can have 80% the throughput of DRAM-to-DRAM\ncopies when using the DSA memory copy accelerator. [20]\nThis involves explicitly copying CXL memory into local\nmemory. In this model, CXL memory is a far memory cache,\nwhich processors can access faster than remote memory or\nstorage, but which programs must explicitly copy from. This\ngets at a fundamental question with using CXL memory: how\ndoes a program access it?\nInstruction-level loads and stores operate on a cache line\ngranularity. Reasonable CXL performance, however, requires\n8kB transfers. Unless an application can take an enormous\nperformance hit when accessing CXL memory, it cannot do\nso transparently. Instead, it must do so explicitly, or the device\nmust act as a page-level cache (e.g., a far memory RAMdisk\npartition).\nExplicit copies require invasive changes to applications.\nFor example, suppose a program calculates the maximum\nvalue over an array (tens of kB), and this array is in CXL\nmemory. The loop is fast, consisting of only 4 instructions. At\n2 instructions per clock, it can process 2 bytes every clock. At\n3GHz, 280ns is 840 ticks, and the loops processes 1680 bytes\nin 280ns. A cache line is 64 bytes, so the processor must have\nover 26 prefetches in \ufb02ight in order to keep the pipeline busy.\nProcessors do not prefetch so deeply, so this loop will spend\nmost of its time stalled on CXL reads.\nIn contrast, a program that explicitly copies from CXL\nmemory into local memory will perform much faster, because\nit pays the 280ns latency once then operates on local, in-cache\nmemory. However, the problem is that this requires an explicit\nmemory copy to local memory. It requires rewriting programs\nto conditionally copy if data is in CXL; CXL memory is not\ntransparent and requires pervasive changes to software. Fur-\nthermore, it requires making copies of data, which increases\napplication memory use.", "start_char_idx": 364394, "end_char_idx": 368462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1296a4eb-0be6-49ee-90c6-e3cdf452a49f": {"__data__": {"id_": "1296a4eb-0be6-49ee-90c6-e3cdf452a49f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c138416-d3d9-4e8c-8d99-45e5a27ec7ef", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f6e8fe8f17c87331ec00859e15f53fa3e03cd9afdedd1ea21e5f1d7638b94b74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90249a5e-78b3-4491-9e0d-44026ffa83b1", "node_type": "1", "metadata": {}, "hash": "95da00e25ec52e8658ee296702769fb49ce90f505f8ba3eee43dd71f943c6480", "class_name": "RelatedNodeInfo"}}, "text": "At\n3GHz, 280ns is 840 ticks, and the loops processes 1680 bytes\nin 280ns. A cache line is 64 bytes, so the processor must have\nover 26 prefetches in \ufb02ight in order to keep the pipeline busy.\nProcessors do not prefetch so deeply, so this loop will spend\nmost of its time stalled on CXL reads.\nIn contrast, a program that explicitly copies from CXL\nmemory into local memory will perform much faster, because\nit pays the 280ns latency once then operates on local, in-cache\nmemory. However, the problem is that this requires an explicit\nmemory copy to local memory. It requires rewriting programs\nto conditionally copy if data is in CXL; CXL memory is not\ntransparent and requires pervasive changes to software. Fur-\nthermore, it requires making copies of data, which increases\napplication memory use.\n5Limited Utility\nIn this section we develop a methodology for estimating the\nef\ufb01ciency gains that can be recouped with memory pool-\ning. Our primary ef\ufb01ciency metric is utilization , de\ufb01ned as\nused capacity\ntotal capacity. The main argument for memory pooling is that it\ncan improve utilization by reducing the amount of stranded\nmemory, in other words reducing \u2018total capacity\u2019.\nMethodology. To approximate utilization improvement\nfrom memory pooling, we need to estimate a cluster\u2019s uti-\nlization. Utilization is a multi-dimensional bin-packing prob-\nlem [ 8,9,23,24], and to optimize ef\ufb01ciency, a datacenter\ncluster scheduler should pack VMs onto physical machines\nas tightly as possible. Of course, there are performance and\nisolation considerations when packing workloads as tightly\nas possible, so realistically, operators leave some fraction ofheadroom on each machine. Despite this, the optimal bin\npacking utilization of a machine is still a good proxy for the\nactual utilization an operator can achieve in a real deployment;\nlater in this section, we validate that the optimal packing ap-\nproximates an event-driven packer within reasonable error\nbounds. Crucially, using the optimal utilization does not pin\nef\ufb01ciency gains to a speci\ufb01c cluster scheduler implementation\nand enables faster analysis than re-running a full cluster trace.\nTherefore, we de\ufb01ne a cluster\u2019s utilization as its utilization\nunder the optimal packing of a VM workload on the cluster.\nTo determine the ef\ufb01ciency gain of pooling, we calculate\nthe utilization improvement from adding a pool to a cluster.\nWe model a cluster as a set of machines. We model pooling\nas a large machine that is #-times the size of a machine.\n#re\ufb02ects the number of machines that share a CXL pool.\nNote that modelling pooling as a large machine overestimates\nthe utilization gain from pooling, because the large machine\nelides allocation boundaries. In a true pooling setup compute\nresources must still be allocated to the machine boundary, and\nmemory resources must respect a machine and pool boundary.\nTherefore any improvement due to pooling in this section\nis a generous upper bound on what pooling can realistically\nachieve.\nDatacenter traces. For VM workloads and machine sizes\nthat are representative of real deployments, we analyze two\ncluster traces, the 2019 Google cluster trace and 2020 Azure\nTrace for Packing [ 9]. From each trace we derive a distribution\nof VM demand and realistic machine sizes [ 22]. In the Google\ntrace, we use VMs and machines from Cell A and only include\nVMs with production priority. In the Azure trace, we only\ninclude VMs with high priority. Note that the Google trace\nis a trace of internal workloads, and the Azure trace is for\ncloud, or customer, VMs; we analyze both traces to see if the\ntwo different settings affect the impact of pooling. We model\nboth machines and VMs as two-dimensional vectors of CPU\nand memory.\nOptimal packing. We use a vector bin-packing library\nto calculate the optimal packing of each set of VMs. The\nlibrary takes a set of VMs and a machine size, and returns\ntheminimum number of machines it takes to pack the entire\nset. Therefore, from a utilization perspective, \u2018used capacity\u2019\nis always \ufb01xed, because we must land all VMs, but \u2018total\ncapacity\u2019 is variable, because it depends how optimally the\nVMs can be packed on the given machine sizes.", "start_char_idx": 367665, "end_char_idx": 371838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90249a5e-78b3-4491-9e0d-44026ffa83b1": {"__data__": {"id_": "90249a5e-78b3-4491-9e0d-44026ffa83b1", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1296a4eb-0be6-49ee-90c6-e3cdf452a49f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6514e7f2f6529e639f294f5a6aa4163df2556152d8f382d1f08bdbd445b93318", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1ef47a4-90ea-4539-91cf-381e6cc9ecfc", "node_type": "1", "metadata": {}, "hash": "8d722d7adb5e5078f0772b01920da1d44286841f4a73439185c3ec522260ae0d", "class_name": "RelatedNodeInfo"}}, "text": "In the Azure trace, we only\ninclude VMs with high priority. Note that the Google trace\nis a trace of internal workloads, and the Azure trace is for\ncloud, or customer, VMs; we analyze both traces to see if the\ntwo different settings affect the impact of pooling. We model\nboth machines and VMs as two-dimensional vectors of CPU\nand memory.\nOptimal packing. We use a vector bin-packing library\nto calculate the optimal packing of each set of VMs. The\nlibrary takes a set of VMs and a machine size, and returns\ntheminimum number of machines it takes to pack the entire\nset. Therefore, from a utilization perspective, \u2018used capacity\u2019\nis always \ufb01xed, because we must land all VMs, but \u2018total\ncapacity\u2019 is variable, because it depends how optimally the\nVMs can be packed on the given machine sizes.\nOur optimal packing packs a snapshot of the cluster, which\nis a simpler problem than the packing problem in a real cluster\nscheduler, because the snapshot is not bound to previous\nplacement decisions and does not take VM departures and\narrivals into consideration. We packed many snapshots and\nstudied the result distribution; all snapshots had trends similar\nto Figures 3 and 4.\nValidation. We validate that a cluster\u2019s utilization under\noptimal packing is close to the utilization under a reasonable,\nlive cluster scheduler. We implement a greedy bin packer that\nreplays a cluster trace and compare the cluster\u2019s utilization\n22HotNets \u201923, November 28\u201329, 2023, Cambridge, MA, USA Philip Levis, Kun Lin, and Amy Tai\nFigure 3: Google cluster trace: Pooling resources across\nup to 16 machines yields does not yield utilization im-\nprovements until VMs are at least 32x larger.\nFigure 4: Azure trace: Pooling resources across up to 16\nmachines does not yield utilization improvements until\nVMs are at least 8x larger. These (cloud) VMs are much\nlarger than the VMs in the Google trace, but still not\nlarge enough for pooling to matter.\nafter running the trace to the utilization of the optimal pack-\ning, at the snapshot of the cluster after the trace has been\nplayed. We do this comparison for subsets of traces across all\n8 cells available in the 2019 Google trace and \ufb01nd that opti-\nmal packing utilization is 0-17% better than the utilization of\nthe greedy bin packer, with a median difference of 5%. This\ngreedy bin packer is likely worse at packing workloads than\nproduction-grade cluster schedulers, which means that the\noptimal packing can even more closely approximate the uti-\nlization of production-grade schedulers. These error bounds\nsuggest that the optimal packing is a reliable proxy for cluster\nutilization in the following results.\nResults. Figures 3 and 4 show the results, where utilization\nis normalized to a 1x machine pooling factor and 1x VM sizes.\nTaking the unmodi\ufb01ed VM demands from the trace resulted\nin no utilization gain from pooling of any size (\ufb02at blue line)\nin both datasets.\nTo see how sensitive this result is and how much packing\n\ufb02exibility there is, we in\ufb02ate the sizes of VMs. For example,\nfor 8x we increase the core count and memory size of every\nVM by a factor of 8. For the Google trace, we \ufb01nd that pooling\nhas at most 1% utilization improvement, even when VMs are\nin\ufb02ated up to 16x. Weighed against the additional costs and\ncomplexity of pooling outlined earlier in the paper, this smallimprovement renders pooling out of the question. In Figure 3,\npooling begins to have a bene\ufb01t with a 32x in\ufb02ation factor,\nand even then, it is modest, less than 5%. VMs must be 64x\nlarger in order for there to be signi\ufb01cant resource stranding at\na single server, which is what reduces stranding with pooling.\nOn the other hand, the Azure VMs (Figure 4) begin to see\nutilization improve at around 8x VM sizes, suggesting that the\ncloud VMs are larger than the internal Google VMs to begin\nwith, and that the pooling calculus may differ for internal\nworkloads versus public cloud workloads.\nIn general, if VMs can reach a certain size with respect to\nphysical machine size, pooling can help with the resulting\nresource stranding.", "start_char_idx": 371045, "end_char_idx": 375098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1ef47a4-90ea-4539-91cf-381e6cc9ecfc": {"__data__": {"id_": "f1ef47a4-90ea-4539-91cf-381e6cc9ecfc", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90249a5e-78b3-4491-9e0d-44026ffa83b1", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "093633a6955248b575e0a0f5ac8068a5117b333857b26e385dd05e8494449a99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1eb8427b-71a5-4afc-b935-7c7fdd2381ab", "node_type": "1", "metadata": {}, "hash": "eaef8b773978cf9db5c213836c5217a5f7a959c778845e10eb07a1794e32b651", "class_name": "RelatedNodeInfo"}}, "text": "In Figure 3,\npooling begins to have a bene\ufb01t with a 32x in\ufb02ation factor,\nand even then, it is modest, less than 5%. VMs must be 64x\nlarger in order for there to be signi\ufb01cant resource stranding at\na single server, which is what reduces stranding with pooling.\nOn the other hand, the Azure VMs (Figure 4) begin to see\nutilization improve at around 8x VM sizes, suggesting that the\ncloud VMs are larger than the internal Google VMs to begin\nwith, and that the pooling calculus may differ for internal\nworkloads versus public cloud workloads.\nIn general, if VMs can reach a certain size with respect to\nphysical machine size, pooling can help with the resulting\nresource stranding. However, based on the analyzed traces,\nmost VMs are small and servers are large: while bin-packing\nis an NP-hard problem, if the bins are large and almost all of\nthe objects are small, there is little leftover space in any bin.\nAs long as VM sizes remain small, pooling is untenable.\n6Discussion\nCompute Express Link provides numerous improvements to\nPCIe, notably lower latency and cache coherence. For com-\nplex, latency-sensitive peripherals such as NICs and GPUs,\nCXL will allow much faster and more \ufb01ne-grained coordina-\ntion between the CPU and these processors.\nThis paper examines another potential use of CXL, dis-\naggregation memory across servers through a shared CXL\nmemory pool. While there has been a lot of excitement and\ninterest in such an approach, there has been almost no experi-\nmental data to verify its feasibility. Furthermore, analyses of\nits potential bene\ufb01ts ignore many of the practical deployment\nissues and costs.\nDisaggregation in datacenter and cloud systems was \ufb01rst\nproposed for hard disk drive storage, where seek times of\nmilliseconds outweigh any additional system or network la-\ntency. Memory, however, is at the opposite of this spectrum.\nArchitectures move memory closer to compute because lo-\ncality is key to performance. GPUs, TPUs, and IPUs all have\ntheir own memory. For example, a 6 foot cable adds 12ns of\npropagation delay in each direction, and at memory speeds\nevery such little increase matters.\nIn this paper, we described three reasons why CXL mem-\nory pools will not be useful in cloud and datacenter systems:\ncost, software complexity, and a lack of utility. Each reason is\nbased on the best information we could \ufb01nd and is grounded\nin computing today. Future advances or marketplace shifts\nmay invalidate our assumptions and change the calculus to\nmake CXL pools attractive; that they could play a role as\nfar memory RAMdisks, which an OS copies into local mem-\nory. We look forward to and encourage research on such a\nfuture, but at the same time do not want to mistake hopeful\npossibilities for technical reality.\nReferences\n[1]Astera Labs. Leo cxl memory connectivity platform. https:\n/ / www .asteralabs .com / products / cxl - memory - platform / leo - cxl -\nmemory-connectivity-platform/, 2023.\n23A Case Against CXL Memory Pooling HotNets \u201923, November 28\u201329, 2023, Cambridge, MA, USA\n[2]CDW Corporation. Mellanox Spectrum-2 MSN3700 switch 32\nports. https://www .cdw.com/product/mellanox-spectrum-2-msn3700-\nswitch-32-ports-managed-rack-mountable/6415759, 2023.\n[3]Compute Express Link Consortium, Inc. Compute Express Link (CXL)\nSpeci\ufb01cation, Revision 1.1, 2019.\n[4]Compute Express Link Consortium, Inc. Compute Express Link (CXL)\nSpeci\ufb01cation, Revision 2.0, 2020.\n[5]Compute Express Link Consortium, Inc. Compute Express Link (CXL)\nSpeci\ufb01cation, Revision 3.0, 2022.\n[6]P. Duraisamy, W. Xu, S. Hare, R. Rajwar, D. Culler, Z. Xu, J. Fan,\nC. Kennelly, B. McCloskey, D. Mijailovic, B. Morris, C. Mukher-\njee, J. Ren, G. Thelen, P. Turner, C. Villavieja, P. Ranganathan, and\nA. Vahdat. Towards an adaptable systems architecture for memory tier-\ning at warehouse-scale.", "start_char_idx": 374420, "end_char_idx": 378226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1eb8427b-71a5-4afc-b935-7c7fdd2381ab": {"__data__": {"id_": "1eb8427b-71a5-4afc-b935-7c7fdd2381ab", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1ef47a4-90ea-4539-91cf-381e6cc9ecfc", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "2c463089f600e0d68031b71ef94a2bc0120beecbb795c4303cbb230a925926ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "687a1876-f339-459e-bae0-5720824202fd", "node_type": "1", "metadata": {}, "hash": "42c8efd0503d36e2d6ba5173b12fa8c1db92a56cad955f7381127380747831dc", "class_name": "RelatedNodeInfo"}}, "text": "[4]Compute Express Link Consortium, Inc. Compute Express Link (CXL)\nSpeci\ufb01cation, Revision 2.0, 2020.\n[5]Compute Express Link Consortium, Inc. Compute Express Link (CXL)\nSpeci\ufb01cation, Revision 3.0, 2022.\n[6]P. Duraisamy, W. Xu, S. Hare, R. Rajwar, D. Culler, Z. Xu, J. Fan,\nC. Kennelly, B. McCloskey, D. Mijailovic, B. Morris, C. Mukher-\njee, J. Ren, G. Thelen, P. Turner, C. Villavieja, P. Ranganathan, and\nA. Vahdat. Towards an adaptable systems architecture for memory tier-\ning at warehouse-scale. In Proceedings ofthe28th ACM International\nConference onArchitectural Support forProgramming Languages and\nOperating Systems, Volume 3, ASPLOS 2023, page 727\u2013741, New\nYork, NY, USA, 2023. Association for Computing Machinery.\n[7]P. X. Gao, A. Narayan, S. Karandikar, J. Carreira, S. Han, R. Agar-\nwal, S. Ratnasamy, and S. Shenker. Network requirements for re-\nsource disaggregation. In Proceedings ofthe12th USENIX Conference\nonOperating Systems Design andImplementation , OSDI\u201916, page\n249\u2013264, USA, 2016. USENIX Association.\n[8]R. Grandl, G. Ananthanarayanan, S. Kandula, S. Rao, and A. Akella.\nMulti-resource packing for cluster schedulers. ACM SIGCOMM\nComputer Communication Review, 44(4):455\u2013466, 2014.\n[9]O. Hadary, L. Marshall, I. Menache, A. Pan, E. E. Greeff, D. Dion,\nS. Dorminey, S. Joshi, Y . Chen, M. Russinovich, et al. Protean: Vm allo-\ncation service at scale. In Proceedings ofthe14th USENIX Conference\nonOperating Systems Design andImplementation , pages 845\u2013861,\n2020.\n[10] I. Intel. Intel FPGA Compute Express Link (CXL) IP. https://\nwww .intel.com/content/www/us/en/products/details/fpga/intellectual-\nproperty/interface-protocols/cxl-ip .html, 2023.\n[11] Ishwar Agarwal. CXL overview and evolution. In\nProceedings of HotChips 34, 2022.\n[12] A. Lagar-Cavilla, J. Ahn, S. Souhlal, N. Agarwal, R. Burny, S. Butt,\nJ. Chang, A. Chaugule, N. Deng, J. Shahid, G. Thelen, K. A. Yurt-\nsever, Y. Zhao, and P. Ranganathan. Software-de\ufb01ned far memory\nin warehouse-scale computers. In Proceedings oftheTwenty-Fourth\nInternational Conference onArchitectural Support forProgramming\nLanguages andOperating Systems , ASPLOS \u201919, page 317\u2013330, New\nYork, NY, USA, 2019. Association for Computing Machinery.\n[13] H. Li, D. S. Berger, L. Hsu, D. Ernst, P. Zardoshti, S. Novakovic,\nM. Shah, S. Rajadnya, S. Lee, I. Agarwal, M. D. Hill, M. Fontoura,\nand R. Bianchini. Pond: Cxl-based memory pooling systems for cloud\nplatforms. In Proceedings ofthe28th ACM International Conference\nonArchitectural Support forProgramming Languages andOperating\nSystems, Volume 2, ASPLOS 2023, page 574\u2013587, New York, NY,\nUSA, 2023. Association for Computing Machinery.\n[14] H. A. Maruf, H. Wang, A. Dhanotia, J. Weiner, N. Agarwal,\nP. Bhattacharya, C. Petersen, M. Chowdhury, S. Kanaujia, and\nP. Chauhan. Tpp: Transparent page placement for cxl-enabled tiered-\nmemory.", "start_char_idx": 377725, "end_char_idx": 380577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "687a1876-f339-459e-bae0-5720824202fd": {"__data__": {"id_": "687a1876-f339-459e-bae0-5720824202fd", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1eb8427b-71a5-4afc-b935-7c7fdd2381ab", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "118dd94f213b6f6167095b919204839a409c88888eac0f4c749dd2775dbdfef5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2a7991e-013d-40df-92bb-55b47af7f667", "node_type": "1", "metadata": {}, "hash": "8945e98e33b0bb0dbea6c08b1b9880ce348b02e6693d377595a655ade90910d3", "class_name": "RelatedNodeInfo"}}, "text": "Pond: Cxl-based memory pooling systems for cloud\nplatforms. In Proceedings ofthe28th ACM International Conference\nonArchitectural Support forProgramming Languages andOperating\nSystems, Volume 2, ASPLOS 2023, page 574\u2013587, New York, NY,\nUSA, 2023. Association for Computing Machinery.\n[14] H. A. Maruf, H. Wang, A. Dhanotia, J. Weiner, N. Agarwal,\nP. Bhattacharya, C. Petersen, M. Chowdhury, S. Kanaujia, and\nP. Chauhan. Tpp: Transparent page placement for cxl-enabled tiered-\nmemory. In Proceedings ofthe28th ACM International Conference\nonArchitectural Support forProgramming Languages andOperating\nSystems, Volume 3, ASPLOS 2023, page 742\u2013755, New York, NY,\nUSA, 2023. Association for Computing Machinery.\n[15] S. Newsroom. Samsung Electronics Introduces Industry\u2019s First 512GB\nCXL Memory Module. https://news .samsung .com/global/samsung-\nelectronics-introduces-industrys-\ufb01rst-512gb-cxl-memory-module,\n2022.\n[16] S. Newsroom. Samsung Develops Industry\u2019s First CXL DRAM Sup-\nporting CXL 2.0. https://news .samsung .com/global/samsung-develops-\nindustrys-\ufb01rst-cxl-dram-supporting-cxl-2-0, 2023.\n[17] I. B. Peng, M. B. Gokhale, and E. W. Green. System Evaluation\nof the Intel Optane Byte-Addressable NVM. In Proceedings ofthe\nInternational Symposium onMemory Systems , MEMSYS \u201919, page304\u2013315, New York, NY, USA, 2019. Association for Computing Ma-\nchinery.\n[18] Z. Ruan, M. Schwarzkopf, M. K. Aguilera, and A. Belay. Aifm:\nHigh-performance, application-integrated far memory. In Proceedings\nofthe14th USENIX Conference onOperating Systems Design and\nImplementation, OSDI\u201920, USA, 2020. USENIX Association.\n[19] Y. Sun, Y. Yuan, Z. Yu, R. Kuper, I. Jeong, R. Wang, and N. S. Kim.\nDemystifying cxl memory with genuine cxl-ready systems and devices,\nOctober 2023.\n[20] Y. Sun, Y. Yuan, Z. Yu, R. Kuper, I. Jeong, R. Wang, and N. S. Kim.\nDemystifying cxl memory with genuine cxl-ready systems and devices,\nv1, March 2023.\n[21] The Next Platform. CXL And Gen-Z Iron Out A Coherent Interconnect\nStrategy. https://www .nextplatform .com/2020/04/03/cxl-and-gen-z-\niron-out-a-coherent-interconnect-strategy/, 2020.\n[22] M. Tirmazi, A. Barker, N. Deng, M. E. Haque, Z. G. Qin, S. Hand,\nM. Harchol-Balter, and J. Wilkes. Borg: the Next Generation.\nInProceedings oftheFifteenth European Conference onComputer\nSystems (EuroSys\u201920), Heraklion, Greece, 2020. ACM.\n[23] A. Verma, M. Korupolu, and J. Wilkes. Evaluating job packing in\nwarehouse-scale computing. In 2014 IEEE International Conference\nonCluster Computing (CLUSTER), pages 48\u201356. IEEE, 2014.\n[24] A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and\nJ. Wilkes. Large-scale cluster management at google with borg. In\nProceedings oftheTenth European Conference onComputer Systems ,\npages 1\u201317, 2015.\n24Cache or Direct Access?\nRevitalizing Cache in Heterogeneous Memory File System\nYubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei Hu\nHuawei, China\nAbstract\nThis paper revisits the value of cache in DRAM-PM hetero-\ngeneous memory /f_ile systems.", "start_char_idx": 380094, "end_char_idx": 383109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2a7991e-013d-40df-92bb-55b47af7f667": {"__data__": {"id_": "d2a7991e-013d-40df-92bb-55b47af7f667", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "687a1876-f339-459e-bae0-5720824202fd", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "001791e478f99d7fae0c21ac6e962e6cc970e625c80921cc532ee7be2d73f7de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "639b4890-c430-4b46-80ea-217d668af544", "node_type": "1", "metadata": {}, "hash": "e6d5c4452b6576d66dfdff9ce241cb83266e56cf2b414f53742e0de9f7cbf694", "class_name": "RelatedNodeInfo"}}, "text": "Evaluating job packing in\nwarehouse-scale computing. In 2014 IEEE International Conference\nonCluster Computing (CLUSTER), pages 48\u201356. IEEE, 2014.\n[24] A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and\nJ. Wilkes. Large-scale cluster management at google with borg. In\nProceedings oftheTenth European Conference onComputer Systems ,\npages 1\u201317, 2015.\n24Cache or Direct Access?\nRevitalizing Cache in Heterogeneous Memory File System\nYubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei Hu\nHuawei, China\nAbstract\nThis paper revisits the value of cache in DRAM-PM hetero-\ngeneous memory /f_ile systems. The /f_irst contribution is a\ncomprehensive analysis of the existing /f_ile systems on het-\nerogeneous memory, including cache-based and DAX-based\n(direct access). We /f_ind that the DRAM cache still plays an im-\nportant role in heterogeneous memory. The second contribu-\ntion is a cache framework for heterogeneous memory, called\nFLAC .FLAC integrates the cache with the virtual memory\nmanagement and proposes two technologies of zero-copy\ncaching and parallel-optimized cache management, which\ndeliver the bene /f_its of fast application-storage data trans-\nfer and e \ufb03cient DRAM-PM data synchronization/migration.\nWe further implement a library /f_ile system upon FLAC . Mi-\ncrobenchmarks show that FLAC provides a performance\nincrease of up to two orders of magnitude over existing /f_ile\nsystems in /f_ileread/write . With a real-world application,\nFLAC achieves up to 77.4% and 89.3% better performance\nthan NOVA and EXT4, respectively.\nCCS Concepts: \u2022Software and its engineering \u2192File\nsystems management ;\u2022Information systems \u2192Stor-\nage class memory .\nKeywords: Heterogeneous Memory, File Systems, Page Cache\nACM Reference Format:\nYubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei\nHu. 2023. Cache or Direct Access? Revitalizing Cache in Heteroge-\nneous Memory File System. In 1st Workshop on Disruptive Memory\nSystems (DIMES \u201923), October 23, 2023, Koblenz, Germany. ACM, New\nYork, NY, USA, 7 pages. h/t_tps://doi.org/10.1145/3609308.3625272\n1 Introduction\nNew persistent memory techniques ( e.g., 3DXPoint [ 12,20]\nand CXL-based SSD [ 15]) and connection techniques ( e.g.,\nCXL [ 4] and GenZ [ 5]) promise high performance, larger\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for pro /f_it or commercial advantage and that\ncopies bear this notice and the full citation on the /f_irst page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speci /f_ic\npermission and/or a fee. Request permissions from permissions@acm.org.\nDIMES \u201923, October 23, 2023, Koblenz, Germany\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 979-8-4007-0300-3/23/10. . . $15.00\nh/t_tps://doi.org/10.1145/3609308.3625272capacity, and energy e \ufb03ciency. These bring a new trend\nof heterogeneous memory architecture which consists of\na volatile memory layer (DRAM) and a persistent memory\nlayer (PM, it can be the 3DXPoint or emerging CXL-based\nmemory storage devices). This work investigates an impor-\ntant question: What kind of storage framework can maximize\nthe potential of heterogeneous memory? Currently, using\nDRAM as cache and direct access (DAX) are two mainstream\nsolutions for heterogeneous memory /f_ile systems.", "start_char_idx": 382487, "end_char_idx": 386073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "639b4890-c430-4b46-80ea-217d668af544": {"__data__": {"id_": "639b4890-c430-4b46-80ea-217d668af544", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2a7991e-013d-40df-92bb-55b47af7f667", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "862882a27c5c046a956db6f755a58dec0536219831fd8b1fb1418029c007b8ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ccb9eb1-172d-4e68-99db-dc6a1d19c336", "node_type": "1", "metadata": {}, "hash": "54412b410a524d5cb8a945db8a2e0854832cb1f5126d04d168d9437acca976b0", "class_name": "RelatedNodeInfo"}}, "text": "Publication rights licensed\nto ACM.\nACM ISBN 979-8-4007-0300-3/23/10. . . $15.00\nh/t_tps://doi.org/10.1145/3609308.3625272capacity, and energy e \ufb03ciency. These bring a new trend\nof heterogeneous memory architecture which consists of\na volatile memory layer (DRAM) and a persistent memory\nlayer (PM, it can be the 3DXPoint or emerging CXL-based\nmemory storage devices). This work investigates an impor-\ntant question: What kind of storage framework can maximize\nthe potential of heterogeneous memory? Currently, using\nDRAM as cache and direct access (DAX) are two mainstream\nsolutions for heterogeneous memory /f_ile systems.\nCaching pages in DRAM, such as VFS page cache, is a\ncommon design in traditional /f_ile systems ( e.g., EXT4 and\nXFS) to bridge the performance gap between fast DRAM\nand slow persistent storage devices ( e.g., HDD and SSD).\nHowever, many previous studies [ 8] in the past decade argue\nthat the DRAM cache incurs signi /f_icant software overhead\nunder the fast, full-memory architecture. Therefore, most\nexisting systems ( e.g., NOVA [ 31], SplitFS [ 17], ctFS [ 21], and\nKucoFS [2]) resort to the DAX method, which bypasses the\nDRAM cache and performs I/Os on PM directly.\nHowever, DAX is still suboptimal for heterogeneous mem-\nory/f_ile systems. First, the DAX method potentially loses the\nperformance bene /f_it of data locality provided by the DRAM\ncache and incurs mandatory data copy across DRAM and\nPM. Even worse, the performance upper bound of the DAX\nis limited by the PM hardware which is inevitably slower\nthan DRAM. As we show in \u00a72, the performance of DAX-\nbased systems is inferior to that of cache-based systems in\nscenarios with high concurrency and strong data locality,\neven though the VFS page cache framework introduces high\nsoftware overheads. Last but not least, instant persistence is\nthe best scene of DAX; but it is overkill in many real-world\nscenarios [30].\nMoreover, new characteristics in emerging hardware and\ncustomer demand motivate us to revisit the value of DRAM\ncache in heterogeneous memory architecture. 1)The per-\nformance gap between PM and DRAM cannot be ignored,\nand multiple PMs will have di \ufb00erent performances in the\nfuture (latency ranges from 170ns to 3000ns [ 14,15]).2)Data\nlocality exists in various real scenarios, and keeping the hot\ndata in DRAM can undoubtedly improve I/O performance.\n3)Instant persistence in DAX is overkill in many real-world\nscenarios [30].\nWhile the DRAM cache still plays an important role in the\nfuture heterogeneous memory architecture, simply reusing\nthe current implementation, such as the VFS page cache,\nis insu \ufb03cient. According to our quantitative analysis (\u00a72),\n38\nDIMES \u201923, October 23, 2023, Koblenz, Germany Yubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei Hu\nwe conclude two challenges of building an e \ufb03cient cache\nframework on heterogeneous memory:\n1)Reduce the data transfer overhead between application\nbu\ufb00er and cache. Transferring data between the application\nand cache is the most critical fast-path operation, but ex-\nisting cache frameworks use memory copy that introduces\nsubstantial performance overhead. Our experiments show\nthat data copying occupies up to 84% of the overhead in the\n/f_ile system with the VFS page cache.\n2)Reduce the impact of the \u201ccache tax\u201d. In addition to\ndata transfer, existing cache frameworks spend lots of ef-\nfort to synchronize ( /f_lushing dirty data) and migrate data\n(evicting data into/out of cache) across DRAM cache and PM.\nCurrently, these operations are implemented in a synchro-\nnous and serial way and signi /f_icantly increase performance\npenalty (more than 30%). For instance, upon a cache miss,\ncurrent systems block I/O operation and fetch data from\nlower-level storage synchronously.\nThis paper proposes FLAC (FLAtCache), a novel cache\nframework for heterogeneous memory. The key idea of FLAC\nis to integrate the cache with the virtual memory manage-\nment subsystem.", "start_char_idx": 385449, "end_char_idx": 389401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ccb9eb1-172d-4e68-99db-dc6a1d19c336": {"__data__": {"id_": "4ccb9eb1-172d-4e68-99db-dc6a1d19c336", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "639b4890-c430-4b46-80ea-217d668af544", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6841bf5b9048540ae66630ae22ab33eaebcd21f623fbce9c4341afb01d21de53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b971bea-5642-4838-9732-9c5331254a9c", "node_type": "1", "metadata": {}, "hash": "51824a6546503219846573188d63395117eff9a98452a10a19fc43fb30c3c2e1", "class_name": "RelatedNodeInfo"}}, "text": "Our experiments show\nthat data copying occupies up to 84% of the overhead in the\n/f_ile system with the VFS page cache.\n2)Reduce the impact of the \u201ccache tax\u201d. In addition to\ndata transfer, existing cache frameworks spend lots of ef-\nfort to synchronize ( /f_lushing dirty data) and migrate data\n(evicting data into/out of cache) across DRAM cache and PM.\nCurrently, these operations are implemented in a synchro-\nnous and serial way and signi /f_icantly increase performance\npenalty (more than 30%). For instance, upon a cache miss,\ncurrent systems block I/O operation and fetch data from\nlower-level storage synchronously.\nThis paper proposes FLAC (FLAtCache), a novel cache\nframework for heterogeneous memory. The key idea of FLAC\nis to integrate the cache with the virtual memory manage-\nment subsystem. FLAC provides a single-level view of het-\nerogeneous memory and enables a transparent and e \ufb03cient\nDRAM cache in the data I/O path. FLAC leverages two novel\ntechniques to deal with the two challenges outlined above:\n1) Zero-Copy Caching. FLAC proposes the heterogeneous\npage table that uni /f_ies heterogeneous memory into a single\nlevel. Virtual pages within FLAC can be dynamically mapped\nto physical pages on DRAM or PM according to their states\n(i.e.cached or evicted). We then design the page attaching\nmechanism, a set of tightly coupled management operations\non the heterogeneous page table, which optimize the data\ntransfer between applications and cache in a zero-copy man-\nner. The core idea of page attaching is to map pages between\nsource and destination addresses with enforced copy on write\n(COW). As a result, data read/write to/from FLAC is executed\nby page attaching to realize e \ufb03cient and safe data transfer.\nWhile page remapping optimizations are also used in some\nsystems to reduce the overhead of data copy [ 7,10,24,25], us-\ning a similar idea in the /f_ile system cache faces some unique\nchallenges that will be discussed in \u00a73.1.\n2) Parallel-Optimized Cache Management. The cache\nmanagement mechanism of FLAC must ensure a low \u201ccache\ntax\u201d impact. Leveraging the multi-version feature that is\nbrought by the zero-copy caching, FLAC can fully exploit\nthe parallelism of data synchronization and migration with\ncritical I/O paths. FLAC proposes a 2-Phase /f_lushing mech-\nanism that allows the expensive persistence phase in dirty\ndata synchronization to be lock-free, and proposes asyn-\nchronous cache miss handling to amortize the overhead of\nloading data to cache in the background.\nFurthermore, we leverage FLAC to implement a prototype\nof/f_ile system read andwrite operations. The evaluation\nshows that FLAC provides a maximum performance increase\n 0 0.7 1.4 2.1 2.8 3.5\nEXT4-nfEXT4-fEXT4-daxNOVAExecution Time (s)(a) Random Write 0 0.7 1.4 2.1 2.8 3.5\nEXT4EXT4-daxNOVAEXT4-cc(b) Random ReadCopyOthersFigure 1. Traditional Cache vs. DAX (Lower is better). \u201c-f/-\nnf\u201d: with/without background /f_lushing; \u201c-cc\u201d: cold cache.\nof more than an order of magnitude over existing systems.\nWith a command line application ( tar),FLAC outperforms\nNOVA and EXT4 by up to 77.4% and 89.3%, respectively.\n2 Cache? or DAX?\nWith the emergence of new interconnection technologies\n(e.g., CXL [ 4], GenZ [ 5]) and persistent storage medias ( e.g.,\n3DXPoint [ 12], CXL-based PM/SSD [ 15]), the storage ar-\nchitecture evolves from memory-block to all-memory. A\ntypical heterogeneous memory architecture consists of a\nfast, volatile, small capacity layer (DRAM), and a slow, non-\nvolatile, large capacity layer (PM). Di \ufb00erent types of memo-\nries present heterogeneity in multiple aspects. The latency\nof DRAM is about 80ns to 140ns, while the latency of low\ntier memory ranges from 170ns to 3000ns [ 14,15]. The PM\nlayer also has lower bandwidth and concurrency than the\nDRAM layer [9, 17].", "start_char_idx": 388594, "end_char_idx": 392398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b971bea-5642-4838-9732-9c5331254a9c": {"__data__": {"id_": "3b971bea-5642-4838-9732-9c5331254a9c", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ccb9eb1-172d-4e68-99db-dc6a1d19c336", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a10e250ef5979c4234662610c936b301956b2f038f39454dc8d7c9c3421e3997", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77524f49-007e-4937-b8f0-dd689bdd8b87", "node_type": "1", "metadata": {}, "hash": "718a8a99ca2e7c9934a84a30712f1c75dc7d2a3f99ab7bc2d7323d1780cc60a4", "class_name": "RelatedNodeInfo"}}, "text": "2 Cache? or DAX?\nWith the emergence of new interconnection technologies\n(e.g., CXL [ 4], GenZ [ 5]) and persistent storage medias ( e.g.,\n3DXPoint [ 12], CXL-based PM/SSD [ 15]), the storage ar-\nchitecture evolves from memory-block to all-memory. A\ntypical heterogeneous memory architecture consists of a\nfast, volatile, small capacity layer (DRAM), and a slow, non-\nvolatile, large capacity layer (PM). Di \ufb00erent types of memo-\nries present heterogeneity in multiple aspects. The latency\nof DRAM is about 80ns to 140ns, while the latency of low\ntier memory ranges from 170ns to 3000ns [ 14,15]. The PM\nlayer also has lower bandwidth and concurrency than the\nDRAM layer [9, 17].\nWe deeply analyze the overhead of three typical /f_ile sys-\ntems with cache (EXT4) and DAX (EXT4-DAX, NOVA) and\ndiscuss the way to e \ufb03ciently utilize heterogeneous memory.\nWe use a single test thread to randomly write /read a 10GB\n/f_ile with 2MB I/O (the testbed is introduced in \u00a75) and come\nup with three observations from the experiments.\nObservation 1 : Existing DAX and cache frameworks are sub-\noptimal, but DRAM cache still has great value for heteroge-\nneous memory /f_ile systems.\nThe VFS page cache is a typical cache framework that\nis designed to bridge the performance gap between DRAM\nand block devices. However, the VFS page cache has a heavy\nsoftware stack, which makes it unsuitable for the heteroge-\nneous memory structure. Therefore, many heterogeneous\nmemory /f_ile systems proposed in the past decade resort to\nthe DAX method, i.e., bypassing the DRAM cache in the data\nI/O path. However, we think DRAM cache still has a lot of\nvalue in heterogeneous memory /f_ile systems. First, PMs with\ndi\ufb00erent performances will emerge in the future, and the\nperformance gap between PM and DRAM cannot be ignored.\nFigure 1 and 3 show that VFS page cache still has better per-\nformance than DAX in some cases ( e.g., read and concurrent\nwrite). Second, taking advantage of data locality is still the\nmain method of performance optimization. Third, POSIX is\nstill a mainstream semantics and it can tolerate cached I/Os,\nwhich makes instant persistence in DAX an overkill in many\nreal-world scenarios [30].\n39Cache or Direct Access? DIMES \u201923, October 23, 2023, Koblenz, Germany\nP0v1 P1v1 P0v0 P1v0P2v0 P3v0\nPersistent \nPTEsLog P0v0 P1v0 P2v0\nDRAM PMVirtual \nPages\nPhysical \nPagesWrite Buf in APP0 \n(Userspace )Read Buf in APP1\n(Userspace )FLAC Space \n(File System Data Area inKernel)\nZero -Copy Write Zero -Copy Read\n2-Phase flushing, \ndoes no block writeAsync cache \nmiss handlingHeterogeneous Page Table\nFigure 2. Architecture of FLAC . File data management is run\non top of FLAC . The /f_ile read/write operation is converted to\nthe zero-copy transfer API of FLAC . The pages are /f_lushed\nto/loaded from the PM by the parallel-optimized mechanism.\nObservation 2 : Data transfer overhead between the /f_ile system\nand application bu \ufb00er is signi /f_icant but often overlooked, and\nit is one of the keys to unlocking the potential of the cache in\nheterogeneous memory.\nFile data I/Os ( read /write ) need to transfer data between\nthe application bu \ufb00er and the storage system ( i.e.cache or\npersistent data area in DAX). Memory copy is the main-\nstream method to transfer data and it takes up more than 23%\nand96% of the total overhead in cache-based and DAX-based\n/f_ile systems, respectively (Figure 1). Since the latency of PM\nis much smaller than block devices, the performance bottle-\nneck of data copy between cache and application is more\nobvious. This observation leads us to consider a zero-copy\napproach to transfer data between the cache and applica-\ntion bu \ufb00er. Thanks to the byte-addressability, it is feasible to\nbuild a single-level address space of heterogeneous memory\nto achieve zero-copy.", "start_char_idx": 391720, "end_char_idx": 395516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77524f49-007e-4937-b8f0-dd689bdd8b87": {"__data__": {"id_": "77524f49-007e-4937-b8f0-dd689bdd8b87", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b971bea-5642-4838-9732-9c5331254a9c", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "2db17f12eeb7c02e69d13c2fadb19c76c24d0bdfa3b33c9711843a92cc64d7d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "259d203e-0612-44ee-931f-5824178bcbf0", "node_type": "1", "metadata": {}, "hash": "b4337d9f64e3bdb4865dfe3218b1f499c0962954b6b03a088712f52541a20ec9", "class_name": "RelatedNodeInfo"}}, "text": "File data I/Os ( read /write ) need to transfer data between\nthe application bu \ufb00er and the storage system ( i.e.cache or\npersistent data area in DAX). Memory copy is the main-\nstream method to transfer data and it takes up more than 23%\nand96% of the total overhead in cache-based and DAX-based\n/f_ile systems, respectively (Figure 1). Since the latency of PM\nis much smaller than block devices, the performance bottle-\nneck of data copy between cache and application is more\nobvious. This observation leads us to consider a zero-copy\napproach to transfer data between the cache and applica-\ntion bu \ufb00er. Thanks to the byte-addressability, it is feasible to\nbuild a single-level address space of heterogeneous memory\nto achieve zero-copy.\nObservation 3 : \u201cCache Tax\u201d in traditional cache frameworks\nis heavy, and it mainly includes the overhead of data synchro-\nnization and migration.\nCaching increases storage levels and brings extra data\nmanagement overhead. Figure 1 shows that the \u201ccache tax\u201d\n(denoted as \u201cother\u201d) takes up to 77% of the execution time\nin EXT4. Furthermore, the experiments reveal the composi-\ntion of the \u201ccache tax\u201d. The background dirty /f_lushing (data\nsynchronization) and cache miss (data migration) lead to\n37%and65% performance declines, respectively. In general,\nthe \u201ccache tax\u201d is di \ufb03cult to eliminate, but we can reduce its\nimpact on the critical I/O paths by improving the parallelism\nbetween them and front-end I/Os.\n3 Flat Cache\nWe propose FLAC ,aFLAtCache framework integrated with\nthe virtual memory subsystem to deeply explore the potential\nof cache in heterogeneous memory. FLAC is designed for\nany heterogeneous memory architecture, while persistent\nmemory can be existing ( e.g., 3DXPoint) or future ( e.g., CXL-\nbased PM/SSD) devices.Table 1. Main APIs of FLAC (for/f_ile system developer)\nAPI Main Para. Description\ninit_flac pm_pathCreate/Recover\ntheFLAC space\nzcopy_from_flac\nzcopy_to_flacfrom_addr\nto_addr\nsizeTransfer data between\napplication bu\ufb00er\nand FLAC with the\nzero-copy approach\npflush_addp/f_lush_handle\naddr\nsizeAttach (map) pages to a\n/f_lushing bu\ufb00er and\nadd to the handle\npflush_commit p/f_lush_handleAtomically /f_lush\ndirty pages to PM\npfreeaddr\nsizeAtomically reclaim\nPMpages\nAs shown in Figure 2, FLAC maintains a range of con-\ntiguous virtual memory addresses, called FLAC space, which\nis as large as the available PM space used to store /f_ile data.\nFLAC space is indexed by the heterogeneous page table and\nit makes the physical location of the page transparent and\nexposes a single-level memory space to /f_ile system develop-\ners. Pages are cached in DRAM when they are accessed (the\ncache size can be adjusted). Data are transferred between the\napplication and FLAC space with the zero-copy approach\n(\u00a73.1) and synchronized/migrated between DRAM and PM\nwith the parallel-optimized mechanism (\u00a73.2).\nFLAC is a development framework for heterogeneous\nmemory architecture that allows the /f_ile system developers\nto customize the data management ( e.g., read/write logic) on\nFLAC space. The other modules of the /f_ile system ( e.g., meta-\ndata management) are independent of FLAC and they can be\n/f_lexibly implemented. Table 1 shows the APIs of FLAC . The\n/f_ile system developers initialize FLAC by calling init_flac ,\nwhich creates FLAC space and binds the PM to it. The /f_ile sys-\ntem internally uses zcopy_to/from_flac to transfer data\nand support read andwrite operations. Data accesses on\nFLAC space are transparent to applications, so applications\nuse standard read andwrite operations to access /f_iles. The\n/f_ile system developers are asked to explicitly /f_lush dirty data\nfrom DRAM to PM, which gives them the /f_lexibility to cus-\ntomize /f_lushing policies. A pair of APIs, pflush_add and\npflush_commit , provide a high concurrency and atomic\nmanner to /f_lush data. At the same time, FLAC space can be\natomically reclaimed by calling pfree .\n3.1 Zero-Copy Caching\nHeterogeneous page table.", "start_char_idx": 394777, "end_char_idx": 398752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "259d203e-0612-44ee-931f-5824178bcbf0": {"__data__": {"id_": "259d203e-0612-44ee-931f-5824178bcbf0", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77524f49-007e-4937-b8f0-dd689bdd8b87", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "65e155af3bd16ca6aaeae0399a40a408e31046f7acde2d5afd36796a335a4e65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1a8719f-215b-40bf-9f98-6974070adfc9", "node_type": "1", "metadata": {}, "hash": "15fb74d40baacfc0168996a68d7387a99a28f496f73f430c70af3b0ea2136fcf", "class_name": "RelatedNodeInfo"}}, "text": "The\n/f_ile system developers initialize FLAC by calling init_flac ,\nwhich creates FLAC space and binds the PM to it. The /f_ile sys-\ntem internally uses zcopy_to/from_flac to transfer data\nand support read andwrite operations. Data accesses on\nFLAC space are transparent to applications, so applications\nuse standard read andwrite operations to access /f_iles. The\n/f_ile system developers are asked to explicitly /f_lush dirty data\nfrom DRAM to PM, which gives them the /f_lexibility to cus-\ntomize /f_lushing policies. A pair of APIs, pflush_add and\npflush_commit , provide a high concurrency and atomic\nmanner to /f_lush data. At the same time, FLAC space can be\natomically reclaimed by calling pfree .\n3.1 Zero-Copy Caching\nHeterogeneous page table. As Figure 2 shows, FLAC uses\nthe heterogeneous page table, a customized sub-level table\n(e.g., one or multiple PUDs) of the kernel page table, to main-\ntainFLAC space. The heterogeneity of the page table has two\nmeanings. 1)Page table entries (PTEs) belonging to the page\ntable are replicated in PM for fault recovery. 2)The address\nindexed in the page table is dynamically mapped to DRAM\n40DIMES \u201923, October 23, 2023, Koblenz, Germany Yubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei Hu\nor PM as the page is cached or evicted, and a bit in the PTE is\nused to indicate the location of the page. The heterogeneous\npage table uni /f_ies the page indexes of cache and persistent\nstorage and simpli /f_ies cache access and management.\nPage attaching. The core technique of optimizing the data\ntransfer in FLAC is a new mechanism for virtual memory\nmanagement \u2013 page attaching. Given a piece of data, the\npage attaching maps its source address (in either DRAM\ncache or PM) to the destination address in the application\nbu\ufb00er without copying the data. Page attaching allows users\nto set permissions ( e.g., read-only) on source and destination\naddresses after remapping. Read/write operations can then\nbe implemented using page attach between the cache and\napplication bu \ufb00er without copying. FLAC guarantees secure\ndata mapping and sharing under concurrent accesses by\nenforcing read-only mapping and copy-on-write (COW).\nIn contrast to existing works that utilize page remapping to\nrealize zero-copy, as far as we know, FLAC is the /f_irst work to\nuse attaching to optimize data transfer between application\nand/f_ile system cache. Furthermore, there are some unique\nchallenges in the /f_ile system cache to employ page remapping.\nFirst, the remapping-based data transfer makes the page\nhave multiple versions, which needs to work with a new\ncache management mechanism to ensure data consistency\nand high concurrency (detailed in \u00a73.2). Second, FLAC uses\nCOW page fault to ensure security and isolation when pages\nare attached from/to FLAC space, which may bring extra\noverhead in some workloads. Third, zero-copy introduces\nthe limitation of page alignment, which may reduce the\napplicability of the /f_ile system implemented based on FLAC .\nThe second and third challenges are discussed in \u00a76.\n3.2 Parallel-Optimized Cache Management\nFLAC requires a cache management mechanism for its multi-\nversion feature, while ensuring low \u201ccache tax\u201d impact. Exist-\ning cache frameworks execute cache /f_lushing and cache miss\nhandling with large synchronization overhead: Cache /f_lush-\ning locks the dirty pages until they are completely /f_lushed,\nwhich blocks the foreground writes and reduces the perfor-\nmance; cache miss handling blocks the foreground I/O until\nthe pages are loaded to DRAM cache. The multi-version fea-\nture and the heterogeneous page table design of FLAC give\nus an opportunity to fully exploit the parallelism of these op-\nerations with critical I/O paths. Figure 2 shows the examples\nof cache /f_lushing and cache miss handling in FLAC .\n2-Phase /f_lushing. FLAC splits the dirty pages /f_lushing into\ntwo phases: collection phase ( pflush_add ) and persistence\nphase ( pflush_commit ). The collection phase allocates a\nfresh VA area as a temporary /f_lush bu \ufb00er and attaches the\ndirty pages to it. This phase requires a lock to prevent concur-\nrent writes from modifying the mapping.", "start_char_idx": 397999, "end_char_idx": 402168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1a8719f-215b-40bf-9f98-6974070adfc9": {"__data__": {"id_": "e1a8719f-215b-40bf-9f98-6974070adfc9", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "259d203e-0612-44ee-931f-5824178bcbf0", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f68e413015a664dfef7cc544b12a6504d21278904640b725247093596f137ecd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5e732c1-b84d-466a-95ba-baf106746856", "node_type": "1", "metadata": {}, "hash": "e3a8633b0672c032567d52457323170914a1432ae80a5f894292226001610c89", "class_name": "RelatedNodeInfo"}}, "text": "The multi-version fea-\nture and the heterogeneous page table design of FLAC give\nus an opportunity to fully exploit the parallelism of these op-\nerations with critical I/O paths. Figure 2 shows the examples\nof cache /f_lushing and cache miss handling in FLAC .\n2-Phase /f_lushing. FLAC splits the dirty pages /f_lushing into\ntwo phases: collection phase ( pflush_add ) and persistence\nphase ( pflush_commit ). The collection phase allocates a\nfresh VA area as a temporary /f_lush bu \ufb00er and attaches the\ndirty pages to it. This phase requires a lock to prevent concur-\nrent writes from modifying the mapping. Then, the persis-\ntence phase stores the dirty pages to PM, which is lock-free\nsince there are no concurrent accesses to the temporarybu\ufb00er. Because the page mapping in the collection phase\nis much faster than cross-memory copy, FLAC signi /f_icantly\nreduces the blocking time on foreground writes. In addition,\nthe persistence phase is atomic \u2013 the consistency of pages\nand persistent PTEs are protected by log-structured /f_lushing\nand logging, respectively.\nAsynchronous cache miss handling. Cache miss has less\nimpact on write operation because it does not require pages\nto be loaded into the cache (except in the case of page mis-\nalignment). Bene /f_iting from the heterogeneous page table,\nFLAC can directly attach the PM pages to the read bu \ufb00er\n(and returns immediately) and handle the cache miss asyn-\nchronously. A background thread in FLAC is responsible for\nloading the cache missed pages to DRAM and remapping\npage tables pointing to those PM pages to the cached pages\non DRAM. As a result, the overhead of handling cache misses\ncan be amortized in the background.\n4 Case Study: File Read/Write on FLAC\nWe introduce a simple library /f_ile system based on FLAC to\nshow its usage and bene /f_its. As FLAC focuses on optimizing\ndata I/O, the prototype only implements read/write and a\nfew necessary metadata operations ( e.g., open and close).\nArchitecture. File data are stored in the FLAC space to\nget bene /f_its from the /f_lat cache design, while metadata are\nstored in the hash table on the normal shared memory space.\nThe inode table is stored on both the DRAM and PM. The\nprototype borrows the page index design from ctFS [ 21],i.e.,\nit allocates a segment of consecutive VAs (virtual memory\naddress) on FLAC space for each /f_ile, and the start VA and\nsize of the /f_ile are recorded in the corresponding metadata.\nRead and Write. The read/write interface is similar\nto that in the traditional POSIX /f_ile. The main parameters\ninclude /f_ile handle, /f_ile o\ufb00set, access length, and read/write\nbu\ufb00er provided by the application. Currently, our prototype\nassumes that the /f_ile o\ufb00set and read/write bu \ufb00er are 4KB\npage aligned (the applicability is discussed in \u00a76). After open-\ning the /f_ile,read/write gets the start VA on FLAC space\nfrom the /f_ile\u2019s metadata and performs zcopy_from_flac\n(read) / zcopy_to_flac (write) to transfer data. In addition,\nthe/f_ile system launches a background thread to periodically\n/f_lush the dirty pages to PM by using the 2-Phase /f_lushing\nmechanism (by calling pflush_add andpflush_commit ).\nCompared to Related Works. FLAC allows /f_ile systems\nbased on it to bene /f_it from the DRAM cache while reducing\nthe e \ufb00ects of \u201ccache tax\u201d as much as possible We compare\nFLAC to a wide range of existing works (shown in Table 2).\nvs. Cache-based File Systems/mmap. There are many\n/f_ile systems designed based on VFS. Although VFS page\ncache can improve the performance in some scenarios in\nheterogeneous memory /f_ile systems, these systems su \ufb00er\nfrom heavy \u201ccache tax\u201d and fail to optimize the application-\nFS data transfer. In addition, some existing works optimize\n41Cache or Direct Access? DIMES \u201923, October 23, 2023, Koblenz, Germany\nTable 2.", "start_char_idx": 401560, "end_char_idx": 405375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5e732c1-b84d-466a-95ba-baf106746856": {"__data__": {"id_": "c5e732c1-b84d-466a-95ba-baf106746856", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1a8719f-215b-40bf-9f98-6974070adfc9", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f8222d0c7f280bd8510573042627ff6df86d85f015ff00e8bf3382ae5142464f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "503456f6-e46d-47f5-bbf9-efbea49c4860", "node_type": "1", "metadata": {}, "hash": "7febf45f71d585572e84f51ff8ce8c60f58717b8e1594fed7c55316c1959407d", "class_name": "RelatedNodeInfo"}}, "text": "Compared to Related Works. FLAC allows /f_ile systems\nbased on it to bene /f_it from the DRAM cache while reducing\nthe e \ufb00ects of \u201ccache tax\u201d as much as possible We compare\nFLAC to a wide range of existing works (shown in Table 2).\nvs. Cache-based File Systems/mmap. There are many\n/f_ile systems designed based on VFS. Although VFS page\ncache can improve the performance in some scenarios in\nheterogeneous memory /f_ile systems, these systems su \ufb00er\nfrom heavy \u201ccache tax\u201d and fail to optimize the application-\nFS data transfer. In addition, some existing works optimize\n41Cache or Direct Access? DIMES \u201923, October 23, 2023, Koblenz, Germany\nTable 2. Comparison with Related Works\nType Typical SystemData\nCacheLow/Non Cache\nTax ImpactApp-Storage\nZero-CopyApp-Storage\nDecouple\nCache-based FS VFS page cache FSes (ETX4, SPFS [30], etc.) /enc-34 /enc-37 /enc-37 /enc-34\nCache-based mmap mmap in VFS page cache FSes (EXT4, etc.) /enc-34 /enc-37 /enc-34 /enc-37\nDAX-based FSNOVA [31], Strata [19], SplitFS [17], WineFS [16], ctFS [21],\nKucoFS [2], PMFS [8], libnvmmio [3], EXT4-DAX [6], XFS-DAX, HTMFS [32]/enc-37 /enc-34 /enc-37 /enc-34\nDAX-based RuntimeTwizzler [1], Mnemosyne [28], PMDK [13]\nzIO [27], SubZero [18]/enc-37 /enc-34 /enc-34 /enc-37\nFlat Cache FLAC-based FS /enc-34 /enc-34 /enc-34 /enc-34\nDRAM page cache [ 22,23] for PM, but they are built on top\nof the virtual memory subsystem and therefore fail to exploit\nthe full potential of cache. Although some cache-based /f_ile\nsystems also provide the mmap method to avoid the data\ntransfer overhead, it makes application design and storage\nbackend to be coupled, which is complementary to the /f_ile\nsemantics.\nvs. DAX-based File Systems. DAX-based /f_ile systems by-\npass the DRAM cache in data I/O, making them su \ufb00er from\nhigh application-storage transfer overhead. Also, the latency\nand concurrency of PM hardware greatly limit their per-\nformance. In particular, some DAX-based /f_ile systems also\nuse remapping: SplitFS [ 17] proposes relink, an operation\nto atomically move a contiguous extent from one /f_ile to an-\nother, which is used to accelerate appends and atomic data\noperations; ctFS [ 21] proposes pswap to swap the page map-\nping of two same-sized contiguous virtual addresses, which\nis used to reduce the overhead of maintaining /f_ile data in\ncontiguous virtual addresses. However, neither SplitFS nor\nctFS uses remapping to optimize data copying between appli-\ncations and /f_ile systems, and FLAC optimizes this part with\nthe zero-copy caching technique.\nvs. DAX-based Runtime. This type of work usually pro-\nvides a memory management library or programming frame-\nwork for applications. Although the overhead of data transfer\nbetween the application and storage system can be avoided,\nthey require the application to be co-designed with the stor-\nage backend ( e.g., use customized interfaces or object ab-\nstraction). Some of these works provide zero-copy PM I/O\nlibraries [ 18,27]. However, they require applications to allo-\ncate read/write bu \ufb00ers on PM to avoid data copy, and thus\nforce to ship the data processing from DRAM to PM, which\nis not friendly for some cases [ 29]. DAX-based runtime fo-\ncuses on programming directly on PM and can be seen as\ncomplementary to the /f_ile system.\n5 Preliminary Results\nWe compare the read/write performance in FLAC with\ncache-based ( FLAC and EXT4) and DAX-based (EXT4-DAX\nand NOVA) /f_ile systems. FLAC and EXT4 are run on a hot\ncache. The period of background /f_lushing is 10ms in FLAC\nwhile it is 100ms in EXT4.", "start_char_idx": 404723, "end_char_idx": 408269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "503456f6-e46d-47f5-bbf9-efbea49c4860": {"__data__": {"id_": "503456f6-e46d-47f5-bbf9-efbea49c4860", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5e732c1-b84d-466a-95ba-baf106746856", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "642be1485e02d8e2aff01ca2016617e1a919d7f5c3c14bbfca4d358062af416d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30375e0b-837a-4a1b-afcf-b29ec201e7d1", "node_type": "1", "metadata": {}, "hash": "db2c1dbfa74af11394f89b081057b675f183769b05ef9005313d06a764c2d738", "class_name": "RelatedNodeInfo"}}, "text": "Some of these works provide zero-copy PM I/O\nlibraries [ 18,27]. However, they require applications to allo-\ncate read/write bu \ufb00ers on PM to avoid data copy, and thus\nforce to ship the data processing from DRAM to PM, which\nis not friendly for some cases [ 29]. DAX-based runtime fo-\ncuses on programming directly on PM and can be seen as\ncomplementary to the /f_ile system.\n5 Preliminary Results\nWe compare the read/write performance in FLAC with\ncache-based ( FLAC and EXT4) and DAX-based (EXT4-DAX\nand NOVA) /f_ile systems. FLAC and EXT4 are run on a hot\ncache. The period of background /f_lushing is 10ms in FLAC\nwhile it is 100ms in EXT4. The experiments are run on aserver with two Intel Xeon Platinum 8380 CPU @ 2.30GHz,\n256GB RAM, and 1TB (128GB \u00d78) Intel 3DXPoint DCPMM.\nPorting FLAC to more complicated applications and com-\nparing with more recent /f_ile systems, such as ctFS [ 21], are\nongoing work.\nMicrobenchmark. The benchmark uses 2MB I/O to con-\ncurrently and randomly write/read 64GB data on 64 non-\nempty /f_iles (1GB per /f_ile), and no data contention in the\nexperiments. As Figure 3 (a) and (b) show, FLAC outper-\nforms other tested systems by more than 25.9 times and 13.7\ntimes in write and read, respectively. The zero-copy design\nofFLAC contributes to the main performance gain. In addi-\ntion, data copy during background /f_lushing does not block\nforeground writes, which signi /f_icantly reduces the impact\nof background /f_lushing on performance in write-intensive\nscenarios.\nReal-World Application. We port tar(v1.34) to FLAC ,a\ncommonly used archiving application. Its main process reads\nthe input /f_ile, archives it, and writes the data to an output\n/f_ile. The tarcontains little computation and represents an\nI/O-intensive case. Figure 3 (c) plots the execution time of\narchiving a /f_ile with increasing /f_ile size. On average, FLAC\nimproves NOVA, EXT4, and EXT4-DAX by 48.5%, 54%, and\n41.2%, respectively.\n6 Discussion and Future Work\nAlthough FLAC promises attractive performance improve-\nment in data I/Os, it still leaves some limitations and open\nchallenges for our feature work.\n1) Reduce COW page fault overhead. Data transfer be-\ntween FLAC space and application bu \ufb00er is implemented by\npage attaching. After attaching, the source and destination\nmemory are set to read-only for security, which makes the\n/f_irststore instruction to the source (write case) and desti-\nnation (read case) memory after attaching to trigger a COW\npage fault. Our analysis shows that TLB /f_lushing and data\ncopy are two of the main overheads in the COW page fault.\nWe have two ideas to reduce the COW page fault overhead:\nThe /f_irst idea is to /f_lush TLB in batch (the default is once\nper page). The second idea is to provide a new interface that\nallows the application to detach the original page mappings,\nthus completely avoiding COW page fault.\n42DIMES \u201923, October 23, 2023, Koblenz, Germany Yubo Liu, Yuxin Ren, Mingrui Liu, Hanjun Guo, Xie Miao, Xinwei Hu\n 130 200 270Execution Time (100 ms)(a) Random Write  0 3.5 7124816 Number of Threads 20 210 400Execution Time (100 ms)(b) Random Read 0 3.5 7124816 Number of Threads 0 20 40 60 80 1004K64K1M4M16M64MExecution Time (ms)File Size(c) tarEXT4-DAXNOVAEXT4FLAC\nFigure 3. Micr obenchmark and Application Performance (Lower is better).\n2) Improve the applicability. Zero-copy (page attach-\ning) can be performed only when the application bu \ufb00er and\nthe target memory in FLAC are aligned. Currently, FLAC\nasks applications to perform /f_ile accesses following this rule.", "start_char_idx": 407625, "end_char_idx": 411178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30375e0b-837a-4a1b-afcf-b29ec201e7d1": {"__data__": {"id_": "30375e0b-837a-4a1b-afcf-b29ec201e7d1", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "503456f6-e46d-47f5-bbf9-efbea49c4860", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d536a4993782daee92c6f636c26ba68f73e124b7018f51f0ee895c5fd7b19e6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e17d91ef-49a3-4489-a260-247b78a20864", "node_type": "1", "metadata": {}, "hash": "15e07cfbdb0cec26f90cb4e9ad1d772e4fc6e1e83c22a4e841fec6868b48c945", "class_name": "RelatedNodeInfo"}}, "text": "Micr obenchmark and Application Performance (Lower is better).\n2) Improve the applicability. Zero-copy (page attach-\ning) can be performed only when the application bu \ufb00er and\nthe target memory in FLAC are aligned. Currently, FLAC\nasks applications to perform /f_ile accesses following this rule.\nFor example, one way to adapt an application to FLAC is\nto ensure that the read/write bu \ufb00er and target /f_ile o\ufb00set\nare 4KB page aligned. To improve the applicability of FLAC ,\na possible solution is to provide a customized bu \ufb00er man-\nagement mechanism for applications. It allocates a bu \ufb00er\nlarger (and aligned) than the required the /f_ile I/O size and\nalways perform /f_ile access as page aligned. In this way, the\nbu\ufb00er may contain more data than the application needs, so\nthe mechanism maintains a sliding window to represent the\nvalid data in the bu \ufb00er, and the application calls an explicit\ninterface to move the window after each /f_ile I/O.\n3)FLAC -optimized cache policy. FLAC permits more\npowerful cache policies for heterogeneous memory. First,\nbecause FLAC is embedded in the VM subsystem, it can be\naware of more memory access behavior about the applica-\ntions ( e.g., allocation/free, reference count), which makes\nit possible for FLAC to make better caching decisions. Sec-\nond, the lower layer of heterogeneous memory is fast and\nbyte-addressable, thus FLAC can investigate more trade-o \ufb00s\nbetween data locality and miss ratio. In future work, we aim\nto collaborate these new insights brought by FLAC with tra-\nditional hotness-based methods to design an e \ufb03cient cache\npolicy.\n4) Ensure security. For data security, FLAC is imple-\nmented in the kernel, and userspace applications can use\nit only through syscalls. Pages are always mapped to the\napplication as read-only, which ensures that local operations\nof the application do not a \ufb00ect the data in the cache and\nother applications as they are handled by COW page fault.\nImplementing a storage system on top of FLAC brings se-\ncurity considerations for metadata, which can be solved by\nusing the userspace security mechanisms ( e.g., MPK [ 11,26])\nor putting metadata management in the kernel.\n5) Ensure crash consistency. FLAC ensures that data\nmodi /f_ication operations ( pflush_commit andpfree ) are\natomic. However, along with /f_lushing the data, the /f_ile sys-\ntem upon FLAC may need to update the related FS-levelmetadata ( e.g., page index) on PM. We plan to design a FS-\nFLAC collaboration logging mechanism, which ensures that\ndata /f_lushing and FS-level metadata updates are in a trans-\naction. The basic idea is to allow the /f_ile system to provide\nthe updated FS-level metadata to FLAC , and they are logged\nwith updated FLAC -level metadata in the same entry during\ndata modi /f_ication operations. The /f_ile system is also required\nto overload the FS-level recovery function that FLAC calls\nto commit the FS-level metadata log during recovery.\n6) Space overhead. Compared to traditional page cache,\nthe multi-version design of FLAC does not incur additional\nspace overhead. In FLAC , the new version of the page is\ncreated in the application runtime by COW page fault, so the\nnew version does not take up space in the page cache before it\nis overwritten to FLAC . After overwriting, the virtual address\nin the FLAC space is mapped to the new version, and the\nold version is reclaimed. Furthermore, the zero-copy caching\ndesign naturally brings the deduplication bene /f_it in some\ncases ( e.g., the application reuses the write bu \ufb00er and only\na small number of pages have been modi /f_ied). We plan to\nleverage this advantage to improve the space e \ufb03ciency of\nthe DRAM cache and PM.\n7 Conclusion\nHeterogeneous memory requires innovations of e \ufb00ective\nsoftware architecture to maximize its potential of various\nadvantages. We analyze the shortcomings of existing cache-\nbased and DAX-based /f_ile systems, and conclude that DRAM\ncache still has great potential in fast all-memory architec-\ntures. We propose FLAC ,a/f_lat cache framework for hetero-\ngeneous memory that embeds the cache into virtual memory\nmanagement.", "start_char_idx": 410882, "end_char_idx": 414992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e17d91ef-49a3-4489-a260-247b78a20864": {"__data__": {"id_": "e17d91ef-49a3-4489-a260-247b78a20864", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30375e0b-837a-4a1b-afcf-b29ec201e7d1", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "dc048d4beb7cb5c082426a749af3ad27ea430633d9a3886d5fad7e569941f3cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd654f44-dcfd-4468-a3b5-f30bb6a8e976", "node_type": "1", "metadata": {}, "hash": "b05d6a2fd9125b0ded1e569d5ee12dd94a92849359e4c527bb9288f48d79c43a", "class_name": "RelatedNodeInfo"}}, "text": "Furthermore, the zero-copy caching\ndesign naturally brings the deduplication bene /f_it in some\ncases ( e.g., the application reuses the write bu \ufb00er and only\na small number of pages have been modi /f_ied). We plan to\nleverage this advantage to improve the space e \ufb03ciency of\nthe DRAM cache and PM.\n7 Conclusion\nHeterogeneous memory requires innovations of e \ufb00ective\nsoftware architecture to maximize its potential of various\nadvantages. We analyze the shortcomings of existing cache-\nbased and DAX-based /f_ile systems, and conclude that DRAM\ncache still has great potential in fast all-memory architec-\ntures. We propose FLAC ,a/f_lat cache framework for hetero-\ngeneous memory that embeds the cache into virtual memory\nmanagement. FLAC unlocks the potential of cache through\ntwo new techniques: zero-copy caching and parallel-optimized\ncache management. We implement a /f_ile system prototype\nbased on FLAC and show that FLAC has signi /f_icantly better\nperformance than existing cache and DAX solutions.\nAcknowledgments\nWe thank the anonymous reviewers for their constructive\ncomments and feedback. We also thank our colleagues in\nthe Huawei OS Kernel Lab for their help.\n43Cache or Direct Access? DIMES \u201923, October 23, 2023, Koblenz, Germany\nReferences\n[1]Daniel Bittman, Peter Alvaro, Pankaj Mehra, Darrell D. E. Long, and\nEthan L. Miller. 2020. Twizzler: A Data-Centric OS for Non-Volatile\nMemory. In Proceedings of the USENIX Annual Technical Conference\n(ATC\u201920) .\n[2]Youmin Chen, Youyou Lu, Bohong Zhu, Andrea C. Arpaci-Dusseau,\nRemzi H. Arpaci-Dusseau, and Jiwu Shu. 2021. Scalable Persistent\nMemory File System with Kernel-Userspace Collaboration. In Proceed-\nings of USENIX Conference on File and Storage Technologies (FAST\u201921) .\n[3]Jungsik Choi, Jaewan Hong, Youngjin Kwon, and Hwansoo Han. 2020.\nLibnvmmio: Reconstructing Software IO Path with Failure-Atomic\nMemory-Mapped Interface. In Proceedings of the USENIX Annual Tech-\nnical Conference (ATC\u201920) .\n[4]CXL Consortium. 2023. Compute Express Link Speci /f_ication Revision\n3.0.h/t_tps://www.computeexpresslink.org/download-the-specification\n[5]Gen-Z Consortium. 2022. Gen-Z Final Speci /f_ications. h/t_tps:\n//genzconsortium.org/specifications/\n[6]Johnathan Corbet. 2020. EXT4-DAX. h/t_tps://lwn.net/Articles/717953\n[7]Peter Druschel and Larry L. Peterson. 1993. Fbufs: A High-Bandwidth\nCross-Domain Transfer Facility. In Proceedings of the ACM Symposium\non Operating Systems Principles (SOSP\u201993) .\n[8]Subramanya R. Dulloor, Sanjay Kumar, Anil Keshavamurthy, Philip\nLantz, Dheeraj Reddy, Rajesh Sankaran, and Je \ufb00Jackson. 2014. Sys-\ntem Software for Persistent Memory. In Proceedings of the Eleventh\nEuropean Conference on Computer Systems (EuroSys\u201914) .\n[9]Subramanya R. Dulloor, Amitabha Roy, Zheguang Zhao, Narayanan\nSundaram, Nadathur Satish, Rajesh Sankaran, Je \ufb00Jackson, and Karsten\nSchwan. 2016. Data Tiering in Heterogeneous Memory Systems. In Pro-\nceedings of the European Conference on Computer Systems (EuroSys\u201916) .\n[10] Sangjin Han, Scott Marshall, Byung-Gon Chun, and Sylvia Ratnasamy.\n2012. MegaPipe: A New Programming Interface for Scalable Network\nI/O. In Proceedings of the USENIX Conference on Operating Systems\nDesign and Implementation (OSDI\u201912) .\n[11] Mohammad Hedayati, Spyridoula Gravani, Ethan Johnson, John\nCriswell, Michael L. Scott, Kai Shen, and Mike Marty. 2019. Hodor:\nIntra-Process Isolation for High-Throughput Data Plane Libraries. In\nProceedings of the USENIX Annual Technical Conference (ATC\u201919) .", "start_char_idx": 414259, "end_char_idx": 417760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd654f44-dcfd-4468-a3b5-f30bb6a8e976": {"__data__": {"id_": "fd654f44-dcfd-4468-a3b5-f30bb6a8e976", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e17d91ef-49a3-4489-a260-247b78a20864", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f158dcf7a8fe880303e08a50795ded8a3cdd2059dd80902fe67b53a141f48c57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae1bfc67-d98d-47f1-88fc-cf43b4791fc7", "node_type": "1", "metadata": {}, "hash": "544bc6a805c363c3b695c3698d72be46c207fb99b498101ad6562ea67cbeab9d", "class_name": "RelatedNodeInfo"}}, "text": "2016. Data Tiering in Heterogeneous Memory Systems. In Pro-\nceedings of the European Conference on Computer Systems (EuroSys\u201916) .\n[10] Sangjin Han, Scott Marshall, Byung-Gon Chun, and Sylvia Ratnasamy.\n2012. MegaPipe: A New Programming Interface for Scalable Network\nI/O. In Proceedings of the USENIX Conference on Operating Systems\nDesign and Implementation (OSDI\u201912) .\n[11] Mohammad Hedayati, Spyridoula Gravani, Ethan Johnson, John\nCriswell, Michael L. Scott, Kai Shen, and Mike Marty. 2019. Hodor:\nIntra-Process Isolation for High-Throughput Data Plane Libraries. In\nProceedings of the USENIX Annual Technical Conference (ATC\u201919) .\n[12] Intel. 2022. 3D XPoint Breakthrough Non-Volatile Memory.\nh/t_tps://www.intel.com/content/www/us/en/architecture-and-\ntechnology/intel-micron-3d-xpoint-webcast.html\n[13] Intel. 2022. Persistent Memory Development Kit. h/t_tps://pmem.io/\npmdk\n[14] Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amir-\nsaman Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R.\nDulloor, Jishen Zhao, and Steven Swanson. 2019. Basic Performance\nMeasurements of the Intel Optane DC Persistent Memory Module.\nCoRR (2019). h/t_tps://doi.org/arXiv:1903.05714\n[15] Myoungsoo Jung. 2023. Hello bytes, bye blocks: PCIe storage meets\ncompute express link for memory expansion (CXL-SSD). In Proceedings\nof Conference on Hot Topics in Storage and File Systems (HotStorage\u201923) .\n[16] Rohan Kadekodi, Saurabh Kadekodi, Soujanya Ponnapalli, Harshad\nShirwadkar, Gregory R. Ganger, Aasheesh Kolli, and Vijay Chi-\ndambaram. 2021. WineFS: A Hugepage-Aware File System for Persis-\ntent Memory That Ages Gracefully. In Proceedings of the ACM Sympo-\nsium on Operating Systems Principles (SOSP\u201921) .\n[17] Rohan Kadekodi, Se Kwon Lee, Sanidhya Kashyap, Taesoo Kim,\nAasheesh Kolli, and Vijay Chidambaram. 2019. SplitFS: Reducing\nSoftware Overhead in File Systems for Persistent Memory. In Proceed-\nings of the ACM Symposium on Operating Systems Principles (SOSP\u201919) .\n[18] Juno Kim, Yun Joon Soh, Joseph Izraelevitz, Jishen Zhao, and Steven\nSwanson. 2020. SubZero: Zero-copy IO for Persistent Main Mem-\nory File Systems. In Proceedings of Asia-Paci /f_ic Workshop on Systems\n(APSys\u201920) .[19] Youngjin Kwon, Henrique Fingler, Tyler Hunt, Simon Peter, Emmett\nWitchel, and Thomas Anderson. 2017. Strata: A cross media /f_ile system.\nInProceedings of the ACM Symposium on Operating Systems Principles\n(SOSP\u201917) .\n[20] Benjamin C. Lee, Ping Zhou, Jun Yang, Youtao Zhang, Bo Zhao, Engin\nIpek, Onur Mutlu, and Doug Burger. 2010. Phase-Change Technology\nand the Future of Main Memory. IEEE Micro 30, 1 (2010), 143\u2013143.\n[21] Ruibin Li, Xiang Ren, Xu Zhao, Siwei He, Michael Stumm, and Ding\nYuan. 2022. ctFS: Replacing File Indexing with Hardware Memory\nTranslation through Contiguous File Allocation for Persistent Memory.\nInProceedings of USENIX Conference on File and Storage Technologies\n(FAST\u201922) .\n[22] Yubo Liu, Hongbo Li, Yutong Lu, Zhiguang Chen, Nong Xiao, and Ming\nZhao. 2020. HasFS: optimizing /f_ile system consistency mechanism on\nNVM-based hybrid storage architecture. Cluster Computing 23 (2020),\n2510\u20132515.\n[23] Jiaxin Ou, Jiwu Shu, and Youyou Lu. 2016. A High Performance File\nSystem for Non-Volatile Main Memory.", "start_char_idx": 417124, "end_char_idx": 420364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae1bfc67-d98d-47f1-88fc-cf43b4791fc7": {"__data__": {"id_": "ae1bfc67-d98d-47f1-88fc-cf43b4791fc7", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd654f44-dcfd-4468-a3b5-f30bb6a8e976", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "b2b5fa3658bc0faf8e3d852637c5742dcb338187558d8b5c82ebfda1dd6051da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07ddc96d-d9c3-4dab-b48f-48225e9d7a34", "node_type": "1", "metadata": {}, "hash": "b6ff4a67dc03152a11c3b8bf2ae29b0bffdc283f2c0a1cad4d88d92e0f267e40", "class_name": "RelatedNodeInfo"}}, "text": "IEEE Micro 30, 1 (2010), 143\u2013143.\n[21] Ruibin Li, Xiang Ren, Xu Zhao, Siwei He, Michael Stumm, and Ding\nYuan. 2022. ctFS: Replacing File Indexing with Hardware Memory\nTranslation through Contiguous File Allocation for Persistent Memory.\nInProceedings of USENIX Conference on File and Storage Technologies\n(FAST\u201922) .\n[22] Yubo Liu, Hongbo Li, Yutong Lu, Zhiguang Chen, Nong Xiao, and Ming\nZhao. 2020. HasFS: optimizing /f_ile system consistency mechanism on\nNVM-based hybrid storage architecture. Cluster Computing 23 (2020),\n2510\u20132515.\n[23] Jiaxin Ou, Jiwu Shu, and Youyou Lu. 2016. A High Performance File\nSystem for Non-Volatile Main Memory. In Proceedings of the Eleventh\nEuropean Conference on Computer Systems (EuroSys\u201916) .\n[24] Vivek S. Pai, Peter Druschel, and Willy Zwaenepoel. 1999. IO-Lite:\nA Uni /f_ied I/O Bu \ufb00ering and Caching System. In Proceedings of the\nUSENIX Conference on Operating Systems Design and Implementation\n(OSDI\u201999) .\n[25] Yuxin Ren, Gabriel Parmer, Teo Georgiev, and Gedare Bloom. 2016.\nCBufs: E \ufb03cient, System-Wide Memory Management and Sharing. In\nProceedings of the ACM SIGPLAN International Symposium on Memory\nManagement (ISMM\u201916) .\n[26] Vasily A. Sartakov, Llu\u00eds Vilanova, and Peter Pietzuch. 2021. CubicleOS:\nA Library OS with Software Componentisation for Practical Isolation.\nInProceedings of the International Conference on Architectural Support\nfor Programming Languages and Operating Systems (ASPLOS\u201921) .\n[27] Timothy Stamler, Deukyeon Hwang, Amanda Raybuck, Wei Zhang,\nand Simon Peter. 2022. zIO: Accelerating IO-Intensive Applications\nwith Transparent Zero-Copy IO. In Proceedings of the USENIX Confer-\nence on Operating Systems Design and Implementation (OSDI\u201922) .\n[28] Haris Volos, Andres Jaan Tack, and Michael M. Swift. 2011.\nMnemosyne: Lightweight Persistent Memory. In Proceedings of the\nInternational Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS\u201911) .\n[29] Yongfeng Wang, Yinjin Fu, Yubo Liu, Zhiguang Chen, and Nong Xiao.\n2022. Characterizing and Optimizing Hybrid DRAM-PM Main Mem-\nory System with Application Awareness. In Proceedings of Design,\nAutomation & Test in Europe Conference & Exhibition (DATE\u201922) .\n[30] Hobin Woo, Daegyu Han, Seungjoon Ha, Sam H. Noh, and Beomseok\nNam. 2023. On Stacking a Persistent Memory File System on Legacy\nFile Systems. In Proceedings of USENIX Conference on File and Storage\nTechnologies (FAST\u201923) .\n[31] Jian Xu and Steven Swanson. 2016. NOVA: A Log-Structured File Sys-\ntem for Hybrid Volatile/Non-Volatile Main Memories. In Proceedings\nof USENIX Conference on File and Storage Technologies (FAST\u201916) .\n[32] Jifei Yi, Mingkai Dong, Fangnuo Wu, and Haibo Chen. 2022. HTMFS:\nStrong Consistency Comes for Free with Hardware Transactional\nMemory in Persistent Memory File Systems. In Proceedings of USENIX\nConference on File and Storage Technologies (FAST\u201922) .\n44Memory Disaggregation: Advances and Open Challenges\nHasan Al Maruf, Mosharaf Chowdhury\nSymbioticLab, University of Michigan\nAbstract\nCompute and memory are tightly coupled within each server\nin traditional datacenters. Large-scale datacenter operators\nhave identi \uffffed this coupling as a root cause behind \uffffeet-\nwide resource underutilization and increasing Total Cost of\nOwnership (TCO). With the advent of ultra-fast networks\nand cache-coherent interfaces, memory disaggregation has\nemerged as a potential solution, whereby applications can\nleverage available memory even outside server boundaries.\nThis paper summarizes the growing research landscape\nof memory disaggregation from a software perspective and\nintroduces the challenges toward making it practical un-\nder current and future hardware trends.", "start_char_idx": 419720, "end_char_idx": 423430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07ddc96d-d9c3-4dab-b48f-48225e9d7a34": {"__data__": {"id_": "07ddc96d-d9c3-4dab-b48f-48225e9d7a34", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae1bfc67-d98d-47f1-88fc-cf43b4791fc7", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "aeb7e5478086634fa811cbae4bde2613b1493cf68f1c1b7b345989bfc419d8cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fa91dd3-e817-4c11-b195-03ea4dd8ca61", "node_type": "1", "metadata": {}, "hash": "6b089559dbc6c64bc5540d4235d0423f3dd7c459f063083ffc1bc0355239773e", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of USENIX\nConference on File and Storage Technologies (FAST\u201922) .\n44Memory Disaggregation: Advances and Open Challenges\nHasan Al Maruf, Mosharaf Chowdhury\nSymbioticLab, University of Michigan\nAbstract\nCompute and memory are tightly coupled within each server\nin traditional datacenters. Large-scale datacenter operators\nhave identi \uffffed this coupling as a root cause behind \uffffeet-\nwide resource underutilization and increasing Total Cost of\nOwnership (TCO). With the advent of ultra-fast networks\nand cache-coherent interfaces, memory disaggregation has\nemerged as a potential solution, whereby applications can\nleverage available memory even outside server boundaries.\nThis paper summarizes the growing research landscape\nof memory disaggregation from a software perspective and\nintroduces the challenges toward making it practical un-\nder current and future hardware trends. We also re \uffffect on\nour seven-year journey in the SymbioticLab to build a com-\nprehensive disaggregated memory system over ultra-fast\nnetworks. We conclude with some open challenges toward\nbuilding next-generation memory disaggregation systems\nleveraging emerging cache-coherent interconnects.\n1Introduction\nModern datacenter applications \u2013 low-latency online ser-\nvices, big data analytics, and AI/ML workloads alike \u2013 are of-\nten memory-intensive. As the number of users increases and\nwe collect more data in cloud datacenters, the overall mem-\nory demand of these applications continue to rise. Despite\ntheir performance bene \uffffts, memory-intensive applications\nexperience disproportionate performance loss whenever their\nworking sets do not completely \ufffft in the available memory.\nFor instance, our measurements across a range of memory-\nintensive applications show that if half their working sets do\nnot\ufffft in memory, performance can drop by 8\u21e5to25\u21e5[22].\nApplication developers often sidestep such disasters by\nover-allocating memory, but pervasive over-allocation in-\nevitably leads to datacenter-scale memory underutilization.\nIndeed, memory utilization at many hyperscalers hovers\naround 40%\u201360% [ 1,22,25,40]. Service providers running on\npublic clouds, such as Snow \uffffake, report 70%\u201380% underuti-\nlized memory on average [ 48]. Since DRAM is a signi \uffffcant\ndriver of infrastructure cost and power consumption [ 33],\nexcessive underutilization leads to high TCO.\nAt the same time, increasing the e \uffffective memory capac-\nity and bandwidth of each server to accommodate ever-larger\nworking sets is challenging as well. In fact, memory band-\nwidth is a bigger bottleneck than memory capacity today as\nthe former increases at a slower rate. For example, to increase\nmemory bandwidth by 3.6\u21e5in their datacenters, Meta hadto increase capacity by 16\u21e5[33]. To provide su \uffffcient mem-\nory capacity and/or bandwidth, computing and networking\nresources become stranded in traditional server platforms,\nwhich eventually causes \uffffeet-wide resource underutilization\nand increases TCO.\nMemory disaggregation addresses memory-related rightsiz-\ning problems at both software and hardware levels. Applica-\ntions are able to allocate memory as they need without being\nconstrained by server boundaries. Servers are not forced to\nadd more computing and networking resources when they\nonly need additional memory capacity or bandwidth. By ex-\nposing all unused memory across all the servers as a memory\npool to all memory-intensive applications, memory disag-\ngregation can improve both application-level performance\nand overall memory utilization. Multiple hardware vendors\nand hyperscalers have projected [ 9,10,28,33] up to 25%\nTCO savings without a \uffffecting application performance via\n(rack-scale) memory disaggregation.\nWhile the idea of leveraging remote machines\u2019 memory\nis decades old [ 15,18,20,29,31,35], only during the past\nfew years, the latency and bandwidth gaps between memory\nand communication technologies have come close enough\nto make it practical. The \uffffrst disaggregated memory1solu-\ntions (In \uffffniswap [ 22] and the rest) leveraged RDMA over\nIn\uffffniBand or Ethernet, but they are an order-of-magnitude\nslower than local memory. To bridge this performance gap\nand to address practical issues like performance isolation,\nresilience, scalability, etc., we have built a comprehensive set\nof software solutions.", "start_char_idx": 422541, "end_char_idx": 426836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fa91dd3-e817-4c11-b195-03ea4dd8ca61": {"__data__": {"id_": "0fa91dd3-e817-4c11-b195-03ea4dd8ca61", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07ddc96d-d9c3-4dab-b48f-48225e9d7a34", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "eab34f89fbbeb399314cd55763fccc15302b2fcaa91447191884f384a10e9e0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a307a07e-b57b-4d32-98e1-bcb7d8794f0b", "node_type": "1", "metadata": {}, "hash": "81777c06bca975c208c041c52c6e02d04af06f1fbff7129d722c339dfcf55ea3", "class_name": "RelatedNodeInfo"}}, "text": "Multiple hardware vendors\nand hyperscalers have projected [ 9,10,28,33] up to 25%\nTCO savings without a \uffffecting application performance via\n(rack-scale) memory disaggregation.\nWhile the idea of leveraging remote machines\u2019 memory\nis decades old [ 15,18,20,29,31,35], only during the past\nfew years, the latency and bandwidth gaps between memory\nand communication technologies have come close enough\nto make it practical. The \uffffrst disaggregated memory1solu-\ntions (In \uffffniswap [ 22] and the rest) leveraged RDMA over\nIn\uffffniBand or Ethernet, but they are an order-of-magnitude\nslower than local memory. To bridge this performance gap\nand to address practical issues like performance isolation,\nresilience, scalability, etc., we have built a comprehensive set\nof software solutions. More recently, with the rise of cache-\ncoherent Compute Express Link (CXL) [ 3] interconnects and\nhardware protocols, the gap is decreasing even more. We are\nat the cusp of taking a leap toward next-generation software-\nhardware co-designed disaggregated memory systems.\nThis short paper is equal parts a quick tutorial, a retro-\nspective on the In \uffffniswap project summarizing seven years\u2019\nworth of research, and a non-exhaustive list of future predic-\ntions based on what we have learned so far.\n2Memory Disaggregation\nSimply put, memory disaggregation exposes memory capac-\nity available in remote locations as a pool of memory and\nshares it across multiple servers over the network. It decou-\nples the available compute and memory resources, enabling\nindependent resource allocation in the cluster. A server\u2019s\n1Remote memory and far memory are often used interchangeably with the\nterm disaggregated memory.arXiv:2305.03943v1  [cs.DC]  6 May 2023Compute BladesMemory BladesCPUCacheCPUCacheCPUCacheNetworkDRAMPMEMDRAMDRAMDRAMDRAM(a) Physically DisaggregatedMonolithic ServerCPUMemoryNICDiskOSMonolithic ServerCPUMemoryNICDiskOSNetwork(b) Logically Disaggregated\nFigure 1: Physical vs. logical disaggregation architectures.\nlocal and remote memory together constitute its total physi-\ncal memory. An application\u2019s locality of memory reference\nallows the server to exploit its fast local memory to maintain\nhigh performance, while remote memory provides expanded\ncapacity with an increased access latency that is still orders-\nof-magnitude faster than accessing persistent storage (e.g.,\nHDD, SSD). The OS and/or application runtime provides the\nnecessary abstractions to expose all the available memory in\nthe cluster, hiding the complexity of setting up and access-\ning remote memory (e.g., connection setup, memory access\nsemantics, network packet scheduling, etc.) while providing\nresilience, isolation, security, etc. guarantees.\n2.1 Architectures\nMemory disaggregation systems have two primary cluster\nmemory architectures.\nPhysical Disaggregation. In a physically-disaggregated\narchitecture, compute and memory nodes are detached from\neach other where a cluster of compute blades are connected\nto one or more memory blades through network (e.g., PCIe\nbridge) [ 30] (Figure 1a). A memory node can be a traditional\nmonolithic server with low compute resource and large mem-\nory capacity, or it can be network-attached DRAM. For better\nperformance, the compute nodes are usually equipped with\na small amount of memory for caching purposes.\nLogical Disaggregation. In a logically-disaggregated ar-\nchitecture, traditional monolithic servers hosting both com-\npute and memory resources are connected to each other\nthrough the network (e.g., In \uffffniband, RoCEv2) (Figure 1b).\nThis is a popular approach for building a disaggregated mem-\nory system because one does not need to change existing\nhardware architecture; simply incorporating appropriate\nsoftware to provide a remote memory interface is su \uffffcient.\nIn such a setup, usually, each of the monolithic servers hasTable 1: Selected memory disaggregation proposals.", "start_char_idx": 426060, "end_char_idx": 429955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a307a07e-b57b-4d32-98e1-bcb7d8794f0b": {"__data__": {"id_": "a307a07e-b57b-4d32-98e1-bcb7d8794f0b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fa91dd3-e817-4c11-b195-03ea4dd8ca61", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d8f56901df85921cabee92daa15ad346409aba91d221276f54ff67a53757aeac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e61b449-85e3-43ab-a1ce-2e0264918692", "node_type": "1", "metadata": {}, "hash": "5f55ae1fa4235b60650229e134bd10218a9522dbb8757351131463f96a30711d", "class_name": "RelatedNodeInfo"}}, "text": "A memory node can be a traditional\nmonolithic server with low compute resource and large mem-\nory capacity, or it can be network-attached DRAM. For better\nperformance, the compute nodes are usually equipped with\na small amount of memory for caching purposes.\nLogical Disaggregation. In a logically-disaggregated ar-\nchitecture, traditional monolithic servers hosting both com-\npute and memory resources are connected to each other\nthrough the network (e.g., In \uffffniband, RoCEv2) (Figure 1b).\nThis is a popular approach for building a disaggregated mem-\nory system because one does not need to change existing\nhardware architecture; simply incorporating appropriate\nsoftware to provide a remote memory interface is su \uffffcient.\nIn such a setup, usually, each of the monolithic servers hasTable 1: Selected memory disaggregation proposals.\nAbstraction SystemHardware\nTransparentOS\nTransparentApplication\nTransparent\nVirtual\nMemory\nManagement\n(VMM)Global Memory [19] Yes No Yes\nMemory Blade [30] No No Yes\nIn\uffffniswap [22] Yes Yes Yes\nLeap [32] Yes No Yes\nLegoOS [44] Yes No Yes\nzSwap [25] Yes No Yes\nKona [14] Yes No Yes\nFastswap [12] Yes No Yes\nHydra [27] Yes Yes Yes\nVirtual File\nSystem (VFS)Memory Pager [31] Yes Yes No\nRemote Regions [11] Yes Yes No\nCustom\nAPIFaRM [17] Yes Yes No\nFaSST [24] Yes Yes No\nMemtrade [34] Yes Yes No\nProgramming\nRuntimeAIFM [42] Yes Yes No\nSemeru [49] Yes Yes No\ntheir own OS. In some cases, the OS itself can be disaggre-\ngated across multiple hosts [ 44]. Memory local to a host is\nusually prioritized for running local jobs. Unutilized memory\non remote machines can be pooled and exposed to the cluster\nas remote [14, 22, 27, 32, 34, 42, 44].\nHybrid Approach. Cache-coherent interconnects like\nCXL provides the opportunity to build a composable hetero-\ngeneous memory systems that combine logical and physi-\ncal disaggregation approaches. Multiple monolithic servers,\ncompute devices, memory nodes, or network specialized de-\nvices can be connected through fabric or switches where soft-\nware stacks can provide the cache-line granular or traditional\nvirtual memory-based disaggregated memory abstraction.\n2.2 Abstractions and Interfaces\nInterfaces to access disaggregated memory can either be\ntransparent to the application or need minor to complete\nre-write of applications (Table 1). The former has broader\napplicability, while the latter might have better performance.\nApplication-Transparent Interface. Access to remote\ndisaggregated memory without signi \uffffcant application\nrewrites typically relies on two primary mechanisms: disag-\ngregated Virtual File System (VFS) [ 11], that exposes remote\nmemory as \uffffles and disaggregated Virtual Memory Man-\nager (VMM) for remote memory paging [ 22,27,32,44]. In\nboth cases, data is communicated in small chunks or pages\n(typically, 4KB). In case of remote memory as \uffffles, pages\ngo through the \uffffle system before they are written to/read\nfrom the remote memory. For remote memory paging and\ndistributed OS, page faults cause the VMM to write pages to\nand read them from the remote memory. Remote memory\npaging is more suitable for traditional applications because\nit does not require software or hardware modi \uffffcations.\nNon-Transparent Interface. Another approach is to di-\nrectly expose remote memory through custom API (KV-\nstore, remote memory-aware library or system calls) and\n2CacheMain MemoryCXL-Memory(DDR, LPDDR, NVM, ...)Network-Attached MemorySSDHDDRegister0.2ns1-40ns80-140ns170-400ns2-4\u03bcs10-40\u03bcs3-10msFigure 2: Latency pro \uffffle of di \ufffferent memory technologies.\nmodify the applications incorporating these speci \uffffc APIs\n[17,24,34,41,42,49]. All the memory (de)allocation, trans-\nactions, synchronizations, etc. operations are handled by\nthe underlying implementations of these APIs. Performance\noptimizations like caching, local-vs-remote data placement,\nprefetching, etc. are often handled by the application.", "start_char_idx": 429121, "end_char_idx": 433015, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e61b449-85e3-43ab-a1ce-2e0264918692": {"__data__": {"id_": "5e61b449-85e3-43ab-a1ce-2e0264918692", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a307a07e-b57b-4d32-98e1-bcb7d8794f0b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "bf1d635da8bee82b564be14bf97a20eda0f05e648a1e61b587ef5e26cf9b8947", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e947418-3303-46b3-8413-10f44f2bd0ca", "node_type": "1", "metadata": {}, "hash": "174750f15d73a420124f46b02d2eed63b9ca472645a47778f98b2294acf43059", "class_name": "RelatedNodeInfo"}}, "text": "Non-Transparent Interface. Another approach is to di-\nrectly expose remote memory through custom API (KV-\nstore, remote memory-aware library or system calls) and\n2CacheMain MemoryCXL-Memory(DDR, LPDDR, NVM, ...)Network-Attached MemorySSDHDDRegister0.2ns1-40ns80-140ns170-400ns2-4\u03bcs10-40\u03bcs3-10msFigure 2: Latency pro \uffffle of di \ufffferent memory technologies.\nmodify the applications incorporating these speci \uffffc APIs\n[17,24,34,41,42,49]. All the memory (de)allocation, trans-\nactions, synchronizations, etc. operations are handled by\nthe underlying implementations of these APIs. Performance\noptimizations like caching, local-vs-remote data placement,\nprefetching, etc. are often handled by the application.\n2.3 Challenges in Practical Memory Disaggregation\nSimply relying on fast networks or interconnects is not su \uffff-\ncient to practical memory disaggregation. A comprehensive\nsolution must address challenges in multiple dimensions:\n\u2022High Performance. A disaggregated memory system in-\nvolves the network in its remote memory path, which is\nat least an order-of-magnitude slower than memory chan-\nnels attached to CPU and DRAM (80\u2013140 nanoseconds vs.\nmicroseconds; see Figure 2). Hardware-induced remote\nmemory latency is signi \uffffcant and impacts application\nperformance [ 22,32,33]. Depending on the abstraction,\nsoftware stacks can also introduce signi \uffffcant overheads.\nFor example, remote memory paging over existing VMM\ncan add tens of microseconds latency for a 4KB page [ 32].\n\u2022Performance Isolation. When multiple applications\nwith di \ufffferent performance requirements (e.g., latency-\nvs. bandwidth-sensitive workloads) compete for disaggre-\ngated memory, depending on where the applications are\nrunning and where the remote memory is located, they\nmay be contending for resources inside the server, on the\nNIC, and in the network on the hardware side and variety\nof resources in the application runtimes and OSes. This\nis further exacerbated by the presence of multiple tiers of\nmemory with di \ufffferent latency-bandwidth characteristics.\n\u2022Memory Heterogeneity. Memory hierarchy within a\nserver is already heterogeneous (Figure 2). Disaggregated\nmemory \u2013 both network-attached and emerging CXL\nmemory [ 21,28,33] \u2013 further increases heterogeneity\nin terms of latency-bandwidth characteristics. In such a\nsetup, simply allocating memory to applications is not\nenough. Instead, decisions like how much memory to al-\nlocate in which tier at what time is critical as well.\u2022Resilience to Expanded Failure Domains. Applica-\ntions relying on remote memory become susceptible to\nnew failure scenarios such as independent and correlated\nfailures of remote machines, evictions from and corrup-\ntions of remote memory, and network partitions. They also\nsu\uffffer from stragglers or late-arriving remote responses\ndue to network congestion and background tra \uffffc[16].\nThese uncertainties can lead to catastrophic failures and\nservice-level objective (SLO) violations.\n\u2022E\uffffciency and Scalability. Disaggregated memory sys-\ntems are inherently distributed. As the number of memory\nservers, the total amount of disaggregated memory, and\nthe number of applications increase, the complexity of\n\uffffnding unallocated remote memory in a large cluster, allo-\ncating them to applications without violating application-\nspeci \uffffc SLOs, and corresponding meta-data overhead of\nmemory management increase as well. Finding e \uffffcient\nmatching at scale is necessary for high overall utilization.\n\u2022Security. Although security of disaggregated memory is\noften sidestepped within the con \uffffnes of a private datacen-\nter, it is a major challenge for memory disaggregation in\npublic clouds. Since data residing in remote memory may\nbe read by entities without proper access, or corrupted\nfrom accidents or malicious behavior, the con \uffffdentiality\nand integrity of remote memory must be protected. Addi-\ntional concerns include side channel and remote rowham-\nmer attacks over the network [ 45,46], distributed coor-\ndinated attacks, lack of data con \uffffdentiality and integrity\nand client accountability during CPU bypass operations\n(e.g., when using RDMA for memory disaggregation).", "start_char_idx": 432313, "end_char_idx": 436448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e947418-3303-46b3-8413-10f44f2bd0ca": {"__data__": {"id_": "3e947418-3303-46b3-8413-10f44f2bd0ca", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e61b449-85e3-43ab-a1ce-2e0264918692", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "9f575310b82304e29d5ac3fef5dac7442218d85cefb66db21d94e7bf2cd0e8a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48844f4c-16e1-4406-9fcb-2895e6a62933", "node_type": "1", "metadata": {}, "hash": "7becddbe4c3214d9dd0dd908da354bb4f27673a549bc7b11e86872ce23f5b946", "class_name": "RelatedNodeInfo"}}, "text": "Finding e \uffffcient\nmatching at scale is necessary for high overall utilization.\n\u2022Security. Although security of disaggregated memory is\noften sidestepped within the con \uffffnes of a private datacen-\nter, it is a major challenge for memory disaggregation in\npublic clouds. Since data residing in remote memory may\nbe read by entities without proper access, or corrupted\nfrom accidents or malicious behavior, the con \uffffdentiality\nand integrity of remote memory must be protected. Addi-\ntional concerns include side channel and remote rowham-\nmer attacks over the network [ 45,46], distributed coor-\ndinated attacks, lack of data con \uffffdentiality and integrity\nand client accountability during CPU bypass operations\n(e.g., when using RDMA for memory disaggregation).\n3In\uffffniswap: A Retrospective\nTo the best of our knowledge, In \uffffniswap is the \uffffrst memory\ndisaggregation system with a comprehensive and cohesive\nset of solutions for all the aforementioned challenges. It ad-\ndresses host-level, network-level, and end-to-end aspects of\npractical memory disaggregation over RDMA. At a high level,\nIn\uffffniswap provides a paging-based remote memory abstrac-\ntion that can accommodate any application without changes,\nwhile providing a high-performance yet resilient, isolated,\nand secure data path to remote disaggregated memory.\nBootstrapping. Our journey started in 2016, when we\nsimply focused on building an application-transparent in-\nterface to remote memory that are distributed across many\nservers. In \uffffniswap [ 22] transparently exposed remote disag-\ngregated memory through paging without any modi \uffffcations\nto applications, hardware, or OSes of individual servers. It em-\nployed a block device with traditional I/O interface to VMM.\nThe block device divided its whole address space into smaller\nslabs and transparently mapped them across many servers\u2019\n3remote memory. In \uffffniswap captured 4KB page faults in run-\ntime and redirected them to remote memory using RDMA.\nFrom the very beginning, we wanted to design a system\nthat would scale without losing e \uffffciency down the line.\nTo this end, we designed decentralized algorithms to iden-\ntify free memory, to distribute memory slabs, and to evict\nslabs for memory reclamation. This removed the overhead of\ncentralized meta-data management without losing e \uffffciency.\nImproving Performance. In\uffffniswap\u2019s block layer-\nbased paging caused high latency overhead during remote\nmemory accesses. This happens because Linux VMM is not\noptimized for microsecond-scale operations. We gave up one\ndegree of freedom and designed Leap [ 32] in 2018 \u2013 we opti-\nmized the OS for remote memory data path by identifying\nand removing non-critical operations while paging.\nEven with the leanest data path, a reactive page fetch-\ning system must su \uffffer microsecond-scale network latency\non the critical path. Leap introduced a remote memory\nprefetcher to proactively bring in the correct pages into a\nlocal cache to provide sub-microsecond latency (comparable\nto that of a local page access) on cache hits.\nProviding Resilience. In\uffffniswap originally relied on lo-\ncal disks to tolerate remote failures, which resulted in slow\nfailure recovery. Maintaining multiple in-memory replicas\nwas not an option either as it e \uffffectively halved the total\ncapacity. We started exploring erasure coding as a memory-\ne\uffffcient alternative. Speci \uffffcally, we divided each page into\n:splits to generate Aencoded parity splits and spread the\n(:+A)splits to (:+A)failure domains \u2013 any :out of (:+A)\nsplits would then su \uffffce to decode the original data. How-\never, erasure coding was traditionally applied to large ob-\njects [ 38]. By 2019/20, we built Hydra [ 27] whose carefully\ndesigned data path could perform online erasure coding\nwithin a single-digit microsecond tail latency. Hydra also\nintroduced CodingSets, a new data placement scheme that\nbalanced availability and load balancing, while reducing the\nprobability of data loss by an order of magnitude even under\nlarge correlated failures.\nMulti-Tenancy Issues. We observed early on (circa\n2017) that accessing remote memory over a shared network\nsu\uffffers from contention in the NIC and inside the network\n[54].", "start_char_idx": 435692, "end_char_idx": 439858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48844f4c-16e1-4406-9fcb-2895e6a62933": {"__data__": {"id_": "48844f4c-16e1-4406-9fcb-2895e6a62933", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e947418-3303-46b3-8413-10f44f2bd0ca", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f9302462d498eff1263220e6f85a3f1c44dbdcbbdfde7fb39696a4754b6da4ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf07094a-e885-4242-ae58-6185d8c2e187", "node_type": "1", "metadata": {}, "hash": "18610a1effef6073e17f2511dcff451c864801106943861fe5e63d4e56f170a2", "class_name": "RelatedNodeInfo"}}, "text": "How-\never, erasure coding was traditionally applied to large ob-\njects [ 38]. By 2019/20, we built Hydra [ 27] whose carefully\ndesigned data path could perform online erasure coding\nwithin a single-digit microsecond tail latency. Hydra also\nintroduced CodingSets, a new data placement scheme that\nbalanced availability and load balancing, while reducing the\nprobability of data loss by an order of magnitude even under\nlarge correlated failures.\nMulti-Tenancy Issues. We observed early on (circa\n2017) that accessing remote memory over a shared network\nsu\uffffers from contention in the NIC and inside the network\n[54]. While our optimized data paths in Leap and Hydra\ncould address some of the challenges inside the host, they\ndid not extend to resource contentions in the RDMA NIC\n(RNIC). We designed Justitia [ 56] in 2020 to improve the\nnetwork bottleneck in RNICs by transparently monitoring\nthe latency pro \uffffles of each application and providing per-\nformance isolation. More recently, we have looked into im-\nproving Quality-of-Service (QoS) inside the network as well\n[55].CPU0CPU1Interconnect32 GB/s perlinkDRAMDRAM38.4 GB/s per channel~100 ns~180 ns(a) Without CXLCPU0CXL64 GB/s per x16 linkDRAMDRAM~100 ns~170-250 ns38.4 GB/s per channel(b) With CXL on PCIe 5.0\nFigure 3: A CXL system compared to a dual-socket server.\nExpanding to Public Clouds. While In \uffffniswap and re-\nlated projects were designed for cooperative private data-\ncenters, memory disaggregation in public clouds faces addi-\ntional concerns. In 2021, we \uffffnished designing Memtrade\n[34] to harvest all the idle memory within virtual machines\n(VMs) \u2013 be it unallocated, or allocated to an application but\ninfrequently utilized, and exposed them to a disaggregated\nmemory marketplace. Memtrade allows producer VMs to\nlease their idle application memory to remote consumer VMs\nfor a limited period of time while ensuring con \uffffdentiality\nand integrity. It employs a broker to match producers with\nconsumers while satisfying performance constraints.\nDetours Along the Way. Throughout this journey, we\ncollaborated on side quests like designing a decentralized\nresource management algorithm using RDMA primitives\n[51], meta-data management inside the network using pro-\ngrammable switches [ 53],\uffffne-grained compute disaggrega-\ntion [ 52] etc. Some of our forays into designing hardware\nsupport were nipped in the bud, often because we could\nnot\uffffnd the right partners. In hindsight, perhaps we were\nfortunate given how quickly the industry converged on CXL.\nSumming it Up. In\uffffniswap along with all its extensions\ncan provide near-memory performance for most memory-\nintensive applications even when 75% and sometimes more of\ntheir working sets reside in remote memory in an application-\nand hardware-transparent manner, in the presence of failures,\nload imbalance, and multiple tenants. After seven years, we\ndeclared victory on this chapter in 2022.\n4Hardware Trend: Cache-Coherent\nInterconnects\nAlthough networking technologies like In \uffffniBand and Eth-\nernet continue to improve, their latency remain considerably\n4Device\u2013Samsung\u2019s1stgeneration CXL Memory ExpanderCPU\u2013Intel Sapphire Rapids w/ CXL 1.1CPU\u2013AMD Genoa w/ CXL 1.1CPU\u2013NVIDIA Grace w/ CXL 2.0CPU\u2013AmpereOne-2 w/ CXL 2.0 on PCIe 5.0CPU\u2013Intel Diamond Rapids w/ CXL3.0 on PCIe 6.0Device\u20131stgen memory pooling controllersCPU\u2013AmpereOne-3 w/ CXL2.0 on PCIe 6.020221stHalf20222ndHalf202320242026Figure 4: CXL roadmap paves the way for memory pooling and disaggregation in next-generation datacenter design.\nhigh for providing a cache-coherent memory address space\nacross disaggregated memory devices. CXL (Compute Ex-\npress Link) [ 3] is a new processor-to-peripheral/accelerator\ncache-coherent interconnect protocol that builds on and\nextends the existing PCIe protocol by allowing coherent\ncommunication between the connected devices.2It provides\nbyte-addressable memory in the same physical address space\nand allows transparent memory allocation using standard\nmemory allocation APIs.", "start_char_idx": 439243, "end_char_idx": 443248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf07094a-e885-4242-ae58-6185d8c2e187": {"__data__": {"id_": "cf07094a-e885-4242-ae58-6185d8c2e187", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48844f4c-16e1-4406-9fcb-2895e6a62933", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6d01f9ad68aff0b5827483324a51d5cc199a00f77e02e1056a6af5878d2ea866", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f07c154e-8ac2-4e81-856e-5e2ac0359d72", "node_type": "1", "metadata": {}, "hash": "50861fe6bb3c587889849d26df67af7290b93b26161d21d7fe0d6e17cc45c4da", "class_name": "RelatedNodeInfo"}}, "text": "high for providing a cache-coherent memory address space\nacross disaggregated memory devices. CXL (Compute Ex-\npress Link) [ 3] is a new processor-to-peripheral/accelerator\ncache-coherent interconnect protocol that builds on and\nextends the existing PCIe protocol by allowing coherent\ncommunication between the connected devices.2It provides\nbyte-addressable memory in the same physical address space\nand allows transparent memory allocation using standard\nmemory allocation APIs. It also allows cache-line granularity\naccess to the connected devices and underlying hardware\nmaintains cache-coherency and consistency. With PCIe 5.0,\nCPU-to-CXL interconnect bandwidth is similar to the cross-\nsocket interconnects (Figure 3) on a dual-socket machine [ 57].\nCXL-Memory access latency is also similar to the NUMA ac-\ncess latency. CXL adds around 50-100 nanoseconds of extra\nlatency over normal DRAM access.\nCXL Roadmap. Today, CXL-enabled CPUs and memory\ndevices support CXL 1.0/1.1 (Figure 4) that enables a point-to-\npoint link between CPUs and accelerator memory or between\nCPUs and memory extenders. CXL 2.0 spec enables one-hop\nswitching that allows multiple accelerators without ( Type-\n1 device ) or with memory ( Type-2 device ) to be con \uffffgured\nto a single host and have their caches be coherent to the\nCPUs. It also allows memory pooling across multiple hosts\nusing memory expanding devices ( Type-3 device ). A CXL\nswitch has a fabric manager (it can be on-board or external)\nthat is in charge of the device address-space management.\nDevices can be hot-plugged to the switch. A virtual CXL\nswitch partitions the CXL-Memory and isolate the resources\nbetween multiple hosts. It provides telemetry for load on each\nconnected devices for load balancing and QoS management.\nCXL 3.0 adds multi-hop hierarchical switching \u2013 one can\nhave any complex types of network through cascading and\nfan-out. This expands the number of connected devices and\nthe complexity of the fabric to include non-tree topologies,\nlike Spine/Leaf, mesh- and ring-based architectures. CXL 3.0\nsupports PCIe 6.0 (64 GT/s i.e., up to 256 GB/s of throughput\nfor a x16 duplex link) and expand the horizon of very com-\nplex and composable rack-scale server design with varied\n2Prior industry standards in this space such as CCIX [ 2], OpenCAPI [ 8],\nGen-Z [ 5] etc. have all come together under the banner of CXL consortium.\nWhile there are some related research proposals (e.g., [ 26]), CXL is the de\nfacto industry standard at the time of writing this paper.GFAMGFAMGFAMNICNICCXL SwitchCXL SwitchCXL SwitchCPUMemoryCPUMemoryCPUAcceleratorCXL SwitchCXL SwitchCXL Switch\nFigure 5: CXL 3.0 enables a rack-scale server design with\ncomplex networking and composable memory hierarchy.\nmemory technologies (Figure 5). A new Port-Based Routing\n(PBR) feature provides a scalable addressing mechanism that\nsupports up to 4,096 nodes. Each node can be any of the\nexisting three types of devices or the new Global Fabric At-\ntached Memory (GFAM) device that supports di \ufffferent types\nof memory (i.e., Persistent Memory, Flash, DRAM, other fu-\nture memory types, etc.) together in a single device. Besides\nmemory pooling, CXL 3.0 enables memory sharing across\nmultiple hosts on multiple end devices. Connected devices\n(i.e., accelerators, memory expanders, NICs, etc.) can do peer-\nto-peer communicate bypassing the host CPUs.\nIn essence, CXL 3.0 enables large networks of memory\ndevices. This will proliferate software-hardware co-designed\nmemory disaggregation solutions that not only simplify and\nbetter implement previous-generation disaggregation solu-\ntions (e.g., In \uffffniswap) but also open up new possibilities.\n5Disaggregation Over Intra-Server CXL\nWith the emergence of new hardware technologies comes the\nopportunity to rethink and revisit past design decisions, and\nCXL is no di \ufffferent. Earlier software solutions for memory\ndisaggregation over RDMA are not optimized enough in CXL-\nbased because of its much lower latency bound, especially\nfor intra-server CXL (CXL 1.0/1.1) with 100s of nanoseconds\nlatency.", "start_char_idx": 442768, "end_char_idx": 446848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f07c154e-8ac2-4e81-856e-5e2ac0359d72": {"__data__": {"id_": "f07c154e-8ac2-4e81-856e-5e2ac0359d72", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf07094a-e885-4242-ae58-6185d8c2e187", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ab0ca26331c6f3cf5d1ba6a51eb6421f71d32979156093493e172a5d6539601e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f55a8490-a1f0-4c19-b905-43b4af61b4d3", "node_type": "1", "metadata": {}, "hash": "ba655a26d7af23e2b62f38faf1ed92d71f2be70161f018de3aa0a8406db9b086", "class_name": "RelatedNodeInfo"}}, "text": "can do peer-\nto-peer communicate bypassing the host CPUs.\nIn essence, CXL 3.0 enables large networks of memory\ndevices. This will proliferate software-hardware co-designed\nmemory disaggregation solutions that not only simplify and\nbetter implement previous-generation disaggregation solu-\ntions (e.g., In \uffffniswap) but also open up new possibilities.\n5Disaggregation Over Intra-Server CXL\nWith the emergence of new hardware technologies comes the\nopportunity to rethink and revisit past design decisions, and\nCXL is no di \ufffferent. Earlier software solutions for memory\ndisaggregation over RDMA are not optimized enough in CXL-\nbased because of its much lower latency bound, especially\nfor intra-server CXL (CXL 1.0/1.1) with 100s of nanoseconds\nlatency. Recent works in leveraging CXL 1.0/1.1 within a\nserver have focused on (tiered) memory pooling [ 28,33] be-\ncause a signi \uffffcant portion of datacenter application working\nsets can be o \uffffoaded to a slower-tier memory without ham-\npering performance [ 25,33,34]. We have recently worked\non two fundamental challenges in this context.\nMemory Usage Characterization. Datacenter applica-\ntions have diverse memory access latency and bandwidth re-\nquirements. Sensitivity toward di \ufffferent memory page types\n5can also vary across applications. Understanding and charac-\nterizing such behaviors is critical to designing heterogeneous\ntiered-memory systems. Chameleon [ 33] is a lightweight\nuser-space memory access behavior characterization tool\nthat can readily be deployed in production without disrupt-\ning running application(s) or modifying the OS. It utilizes the\nPrecise Event-Based Sampling (PEBS) mechanism of mod-\nern CPU\u2019s Performance Monitoring Unit (PMU) to collect\nhardware-level performance events related to memory ac-\ncesses. It then generates a heat-map of memory usage for\ndi\ufffferent page types and provides insights into an applica-\ntion\u2019s expected performance with multiple temperature tiers.\nMemory Management. Given applications\u2019 page char-\nacterizations, TPP [ 33] provides an OS-level transparent page\nplacement mechanism, to e \uffffciently place pages in a tiered-\nmemory system. TPP has three components: (a)a lightweight\nreclamation mechanism to demote colder pages to the slow\ntier; (b)decoupling the allocation and reclamation logic for\nmulti-NUMA systems to maintain a headroom of free pages\non the fast tier; and (c)a reactive page promotion mecha-\nnism that e \uffffciently identi \uffffes hot pages trapped in the slow\nmemory tier and promote them to the fast memory tier to\nimprove performance. It also introduces support for page\ntype-aware allocation across the memory tiers.\n6CXL-Disaggregated Memory at Rack-Scale\nand Beyond: Open Challenges\nAlthough higher than intra-server CXL latency, rack-scale\nCXL systems with a CXL switch (CXL 2.0) will experience\nmuch lower latency than RDMA-based memory disaggre-\ngation. With a handful of hops in CXL 3.0 setups, latency\nwill eventually reach a couple microseconds similar to that\nfound in today\u2019s RDMA-based disaggregated memory sys-\ntems. For next-generation memory disaggregation systems\nthat operate between these two extremes, i.e., rack-scale and\na little beyond, many open challenges exist. We may even\nhave to revisit some of our past design decisions (\u00a72). Here\nwe present a non-exhaustive list of challenges informed by\nour experience.\n6.1 Abstractions\nMemory Access Granularity. CXL enables cache-line\ngranular memory access over the connected devices, whereas\nexisting OS VMM modules are designed for page-granular\n(usually, 4KB or higher) memory access. Throughout their\nlifetimes, applications often write a small part of each page;\ntypically only 1-8 cache-lines out of 64 [ 14]. Page-granular\naccess causes large dirty data ampli \uffffcation and bandwidth\noveruse. In contrast, \uffffne-grained memory access over a large\nmemory pool causes high meta-data management overhead.\nBased on an application\u2019s memory access patterns, remotememory abstractions should support transparent and dy-\nnamic adjustments to memory access granularity.\nMemory-QoS Interface. Traditional solutions for mem-\nory page management focus on tracking (a subset of) pages\nand counting accesses to determine the heat of the page and\nthen moving pages around.", "start_char_idx": 446097, "end_char_idx": 450347, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f55a8490-a1f0-4c19-b905-43b4af61b4d3": {"__data__": {"id_": "f55a8490-a1f0-4c19-b905-43b4af61b4d3", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f07c154e-8ac2-4e81-856e-5e2ac0359d72", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "14e1fde9cb754064a096a1ddaed38d80ee7ee920cd0d6af7675fdc6e8bdc0fff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2ec76ca-350f-4e78-8b63-760941b6050f", "node_type": "1", "metadata": {}, "hash": "fd89f8b212a7ad6a27605777aa21d511842fdcb4b3edf446203cbca34ae60881", "class_name": "RelatedNodeInfo"}}, "text": "CXL enables cache-line\ngranular memory access over the connected devices, whereas\nexisting OS VMM modules are designed for page-granular\n(usually, 4KB or higher) memory access. Throughout their\nlifetimes, applications often write a small part of each page;\ntypically only 1-8 cache-lines out of 64 [ 14]. Page-granular\naccess causes large dirty data ampli \uffffcation and bandwidth\noveruse. In contrast, \uffffne-grained memory access over a large\nmemory pool causes high meta-data management overhead.\nBased on an application\u2019s memory access patterns, remotememory abstractions should support transparent and dy-\nnamic adjustments to memory access granularity.\nMemory-QoS Interface. Traditional solutions for mem-\nory page management focus on tracking (a subset of) pages\nand counting accesses to determine the heat of the page and\nthen moving pages around. While this is enough to provide\na two-level, hot-vs-cold QoS, it cannot capture the entire\nspectrum of page temperature. Potential solutions include\nassigning a QoS level to (1) an entire application; (2) individ-\nual data structures; (3) individual mmap() calls; or even (4)\nindividual memory accesses. Each of these approaches have\ntheir pros and cons. At one extreme, assigning a QoS level\nto an entire application maybe simple, but it cannot capture\ntime-varying page temperature of large, long-running ap-\nplications. At the other end, assigning QoS levels to individ-\nual memory accesses requires recompilation of all existing\napplications as well as cumbersome manual assignments,\nwhich can lead to erroneous QoS assignments. A combina-\ntion of aforementioned approaches may reduce developer\u2019s\noverhead while providing su \uffffcient \uffffexibility to perform\nspatiotemporal memory QoS management.\n6.2 Management and Runtime\nMemory Address Space Management. From CXL 2.0\nonward, devices can be hot-plugged to the CXL switches.\nDevice-attached memory is mapped to the system\u2019s coherent\naddress space and accessible to host using standard write-\nback semantics. Memory located on a CXL device can either\nbe mapped as Host-managed Device Memory (HDM) or Pri-\nvate Device Memory (PDM). To update the memory address\nspace for connected devices to di \ufffferent host devices, a sys-\ntem reset is needed; tra \uffffc towards the device needs to stop\nto alter device address mapping during this reset period.\nAn alternate solution to avoid this system reset is to map\nthe whole physical address space to each host when a CXL-\ndevice is added to the system. The VMM or fabric manager\nin the CXL switch will be responsible to maintain isolation\nduring address-space management. How to split the whole\naddress-space in to sizable memory blocks for the e \uffffcient\nphysical-to-virtual address translation of a large memory\nnetwork is an interesting challenge [26, 53].\nUni\uffffed Runtime for Compute Disaggregation. CXL\nType-2 devices (accelerator with memory) maintains cache\ncoherency with the CPU. CPU and Type-2 devices can inter-\nchangeably use each other\u2019s memory and both get bene \uffffted.\nFor example, applications that run on CPUs can bene \ufffft as\nthey can now access very high bandwidth GPU memory.\nSimilarly, for GPU users, it is bene \uffffcial for capacity expan-\nsion even though the memory bandwidth to and from CPU\nmemory will be lower. In such a setup, remote memory ab-\nstractions should track the availability of compute cores and\n6e\uffffciently perform near-memory computation to improve\nthe overall system throughput.\nFuture datacenters will likely be equipped with numerous\ndomain-speci \uffffc compute resources/accelerators. In such a\nsystem, one can borrow the idle cores of one compute re-\nsource and perform extra computation to increase the overall\nsystem throughput. A uni \uffffed runtime to support malleable\nprocesses that can be immediately decomposed into smaller\npieces and o \uffffoaded to any available compute nodes can\nimprove both application and cluster throughput [41, 52].\n6.3 Allocation Policies\nMemory Allocation in Heterogenous NUMA Clus-\nter.For better performance, hottest pages need to be on the\nfastest memory tier. However, due to memory capacity con-\nstraints across di \ufffferent tiers, it may not always be possible\nto utilize the fastest or performant memory tier.", "start_char_idx": 449498, "end_char_idx": 453704, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2ec76ca-350f-4e78-8b63-760941b6050f": {"__data__": {"id_": "d2ec76ca-350f-4e78-8b63-760941b6050f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f55a8490-a1f0-4c19-b905-43b4af61b4d3", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3c8c9752f4475882d8310b8b91a19c3111f1317d8215796c6a0f7f58df6f87ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d918ccac-5314-4ce4-b20d-7d9e71fef1c5", "node_type": "1", "metadata": {}, "hash": "62619faf9c94ff6acc84f4dd6cd32ea8a1115f84fbbc56e46f7be5c0ab96a22d", "class_name": "RelatedNodeInfo"}}, "text": "Future datacenters will likely be equipped with numerous\ndomain-speci \uffffc compute resources/accelerators. In such a\nsystem, one can borrow the idle cores of one compute re-\nsource and perform extra computation to increase the overall\nsystem throughput. A uni \uffffed runtime to support malleable\nprocesses that can be immediately decomposed into smaller\npieces and o \uffffoaded to any available compute nodes can\nimprove both application and cluster throughput [41, 52].\n6.3 Allocation Policies\nMemory Allocation in Heterogenous NUMA Clus-\nter.For better performance, hottest pages need to be on the\nfastest memory tier. However, due to memory capacity con-\nstraints across di \ufffferent tiers, it may not always be possible\nto utilize the fastest or performant memory tier. Determining\nwhat fraction of memory is needed at a particular memory\ntier to maintain the desired performance of an application at\ndi\ufffferent points of its life cycle is challenging. This is even\nmore di \uffffcult when multiple applications coexist. E \uffffcient\npromotion or demotion of pages of di \ufffferent temperatures\nacross memory tiers at rack scale is necessary. One can con-\nsider augmenting TPP by incorporating a lightweight but\ne\uffffective algorithm to select the migration target considering\nnode distances from the CPU, load on CPU-memory bus,\ncurrent load on di \ufffferent memory tiers, network state, and\nthe QoS requirements of the migration-candidate pages.\nAllocation Policy for Memory Bandwidth Expan-\nsion. For memory bandwidth-bound applications, CPU-to-\nDRAM bandwidth often becomes the bottleneck and in-\ncreases the average memory access latency. CXL\u2019s additional\nmemory bandwidth can help by spreading memory across\nthe top-tier and remote nodes. Instead of only placing cold\npages into CXL-Memory, which has low bandwidth con-\nsumption, an ideal solution should place the right amount of\nbandwidth-heavy, latency-insensitive pages to CXL-Memory.\nThe methodology to identify the ideal fraction of such work-\ning sets may even require hardware support.\nMemory Sharing and Consistency. CXL 3.0 allows\nmemory sharing across multiple devices. Through an en-\nhanced coherency semantics, multiple hosts can have a co-\nherent copy of a shared segment, with back invalidation\nfor synchronization. Memory sharing improves application-\nlevel performance by reducing unnecessary data movement\nand improves memory utilization. Sharing a large memory\naddress space, however, results in signi \uffffcant overhead and\ncomplexity in the system that plagued classic distributed\nshared memory (DSM) proposals [ 36]. Furthermore, sharing\nmemory across multiple devices increases the security threat\nin the presence of any malicious application run on the samehardware space. We believe that disaggregated memory sys-\ntems should cautiously approach memory sharing and avoid\nit unless it is absolutely necessary for speci \uffffc scenarios.\n6.4 Rack-Level Objectives\nRack-Scale Memory Temperature. To obtain insights\ninto an application\u2019s expected performance with multiple\ntemperature tiers, it is necessary to understand the heat\nmap of memory usage for that application. Existing hot page\nidenti \uffffcation mechanisms (including Chameleon) are limited\nto a single host OS or user-space mechanism. They either use\naccess bit-based mechanism [ 4,6,47], special CPU feature-\nbased (e.g., Intel PEBS) tools [ 39,43,50], or OS features [ 7,33]\nto determine the page temperature within a single server.\nSo far, there is no distributed mechanism to determine the\ncluster-wide relative page temperature. Combining the data\nof all the OS or user-space tools and coordinating between\nthem to \uffffnd rack-level hot pages is an important problem.\nCXL fabric manager is perhaps the place where one can\nget a cluster-wide view of hardware counters for each CXL\ndevice\u2019s load, hit, and access-related information. One can\nenvision extending Chameleon for rack-scale environments\nto provide observability into each application\u2019s per-device\nmemory temperature.\nHardware-Software Co-Design for a Better Ecosys-\ntem. Hardware features can further enhance performance\nof disaggregation systems in rack-scale setups. A memory-\nside cache and its associated prefetcher on the CXL ASIC\nor switch might help reduce the e \uffffective latency of CXL-\nMemory. Hardware support for data movement between\nmemory tiers can help reduce page migration overheads in\nan aggressively provisioned system with very small amount\nof local memory and high amount of CXL-Memory.", "start_char_idx": 452943, "end_char_idx": 457411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d918ccac-5314-4ce4-b20d-7d9e71fef1c5": {"__data__": {"id_": "d918ccac-5314-4ce4-b20d-7d9e71fef1c5", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2ec76ca-350f-4e78-8b63-760941b6050f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d6739ddc3bf91469e4e901ca0f1a778be7184f6fededc8e476cd552b321cc191", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bcdcd5f-8502-4f88-b03c-d4cf1903d2f9", "node_type": "1", "metadata": {}, "hash": "06437f61fee3a5512ed0c5cd02713fa092a29ddd1e52b51a1e7cf48691f744b2", "class_name": "RelatedNodeInfo"}}, "text": "CXL fabric manager is perhaps the place where one can\nget a cluster-wide view of hardware counters for each CXL\ndevice\u2019s load, hit, and access-related information. One can\nenvision extending Chameleon for rack-scale environments\nto provide observability into each application\u2019s per-device\nmemory temperature.\nHardware-Software Co-Design for a Better Ecosys-\ntem. Hardware features can further enhance performance\nof disaggregation systems in rack-scale setups. A memory-\nside cache and its associated prefetcher on the CXL ASIC\nor switch might help reduce the e \uffffective latency of CXL-\nMemory. Hardware support for data movement between\nmemory tiers can help reduce page migration overheads in\nan aggressively provisioned system with very small amount\nof local memory and high amount of CXL-Memory. Addition-\nally, the fabric manager of a CXL switch should implement\npolicies like fair queuing, congestion control, load balancing\netc. for better network management. Incorporating Leap\u2019s\nprefetcher and Hydra\u2019s erasure-coded resilience ideas into\nCXL switch designs can enhance system-wide performance.\nEnergy- and Carbon-Aware Memory Disaggrega-\ntion. Datacenters represent a large and growing source of\nenergy consumption and carbon emissions [ 13]. Some esti-\nmates place datacenters to be responsible for 1-2% of aggre-\ngate worldwide electricity consumption [ 23,37]. To reduce\nthe TCO and carbon footprint, and enhance hardware life\nexpectancy, datacenter rack maintain a physical energy bud-\nget or power cap. Rack-scale memory allocation, demotion,\nand promotion policies can be augmented by incorporating\nenergy-awareness in their decision-making process. In gen-\neral, we can introduce energy-awareness in the software\nstack that manage compute, memory, and network resources\nin a disaggregated cluster.\n77Conclusion\nWe started the In \uffffniswap project in 2016 with the conviction\nthat memory disaggregation is inevitable, armed only with a\nfew data points that hinted it might be within reach. As we\nconclude this paper in 2023, we have successfully built a com-\nprehensive software-based disaggregated memory solution\nover ultra-fast RDMA networks that can provide a seamless\nexperience for most memory-intensive applications. With\ndiverse cache-coherent interconnects \uffffnally converging un-\nder the CXL banner, the entire industry (and ourselves) are\nat the cusp of taking a leap toward next-generation software-\nhardware co-designed disaggregated systems. Join us. Mem-\nory disaggregation is here to stay.\nAcknowledgements\nJuncheng Gu, Youngmoon Lee, and Yiwen Zhang co-led dif-\nferent aspects of the In \uffffniswap project alongside the authors.\nWe thank Yiwen Zhang for his feedback on this paper. Spe-\ncial thanks to our many collaborators, contributors, users,\nand cloud resource providers (namely, CloudLab, Chameleon\nCloud, and UM ConFlux) for making In \uffffniswap successful.\nOur expeditions into next-generation memory disaggrega-\ntion solutions have greatly bene \uffffted from our collaborations\nwith Meta. Our research was supported in part by National\nScience Foundation grants (CCF-1629397, CNS-1845853, and\nCNS-2104243) and generous gifts from VMware and Meta.\nReferences\n[1]Alibaba Cluster Trace 2018. https://github .com/alibaba/clusterdata/\nblob/master/cluster-trace-v2018/trace_2018 .md.\n[2]CCIX. https://www .ccixconsortium .com/.\n[3]Compute Express Link (CXL). https://www .computeexpresslink .org/.\n[4]DAMON: Data Access MONitoring Framework for\nFun and Memory Management Optimizations. https:\n//www .linuxplumbersconf .org/event/7/contributions/659/\nattachments/503/1195/damon_ksummit_2020 .pdf.\n[5]Gen-Z. https://genzconsortium .org/.\n[6]Idle page tracking-based working set estimation. https://lwn .net/\nArticles/460762/.\n[7]NUMA Balancing (AutoNUMA). https://mirrors .edge .kernel .org/\npub/linux/kernel/people/andrea/autonuma/autonuma _bench-\n20120530 .pdf.\n[8]OpenCAPI. https://opencapi .org/.\n[9]Rack-scale computing at Yahoo!", "start_char_idx": 456613, "end_char_idx": 460565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bcdcd5f-8502-4f88-b03c-d4cf1903d2f9": {"__data__": {"id_": "2bcdcd5f-8502-4f88-b03c-d4cf1903d2f9", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d918ccac-5314-4ce4-b20d-7d9e71fef1c5", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3b3a5214e7e9b1aacefeab565216ecab3a957f7d2898cd6729453dca665696fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce6c6699-936b-4bf0-97c7-d487c74e5449", "node_type": "1", "metadata": {}, "hash": "496ee122b1dd378bf0b5e229821e48ecc78c97552595a8066926b1ccf051cba2", "class_name": "RelatedNodeInfo"}}, "text": "[3]Compute Express Link (CXL). https://www .computeexpresslink .org/.\n[4]DAMON: Data Access MONitoring Framework for\nFun and Memory Management Optimizations. https:\n//www .linuxplumbersconf .org/event/7/contributions/659/\nattachments/503/1195/damon_ksummit_2020 .pdf.\n[5]Gen-Z. https://genzconsortium .org/.\n[6]Idle page tracking-based working set estimation. https://lwn .net/\nArticles/460762/.\n[7]NUMA Balancing (AutoNUMA). https://mirrors .edge .kernel .org/\npub/linux/kernel/people/andrea/autonuma/autonuma _bench-\n20120530 .pdf.\n[8]OpenCAPI. https://opencapi .org/.\n[9]Rack-scale computing at Yahoo! http://www .intel.com/content/dam/\nwww/public/us/en/documents/presentation/idf15-yahoo-rack-scale-\ncomputing-presentation .pdf.\n[10] Tencent explores datacenter resource-pooling using Intel rack scale\narchitecture (Intel RSA). http://www .intel .com/content/dam/www/\npublic/us/en/documents/white-papers/rsa-tencent-paper .pdf.\n[11] M. K. Aguilera, N. Amit, I. Calciu, X. Deguillard, J. Gandhi, S. No-\nvakovi \u0107, A. Ramanathan, P. Subrahmanyam, L. Suresh, K. Tati,\nR. Venkatasubramanian, and M. Wei. Remote regions: a simple ab-\nstraction for remote memory. In USENIX ATC , 2018.\n[12] E. Amaro, C. Branner-Augmon, Z. Luo, A. Ousterhout, M. K. Aguilera,\nA. Panda, S. Ratnasamy, and S. Shenker. Can far memory improve job\nthroughput? In EuroSys , 2020.[13] T. Anderson, A. Belay, M. Chowdhury, A. Cidon, and I. Zhang. Tree-\nhouse: A case for carbon-aware datacenter software. In HotCarbon ,\n2022.\n[14] I. Calciu, M. T. Imran, I. Puddu, S. Kashyap, H. A. Maruf, O. Mutlu, and\nA. Kolli. Rethinking software runtimes for disaggregated memory. In\nASPLOS , 2021.\n[15] H. Chen, Y. Luo, X. Wang, B. Zhang, Y. Sun, and Z. Wang. A transparent\nremote paging model for virtual machines. In International Workshop\non Virtualization Technology , 2008.\n[16] J. Dean and L. A. Barroso. The tail at scale. Communications of the\nACM , 56(2):74\u201380, 2013.\n[17] A. Dragojevi \u0107, D. Narayanan, O. Hodson, and M. Castro. FaRM: Fast\nremote memory. In NSDI , 2014.\n[18] S. Dwarkadas, N. Hardavellas, L. Kontothanassis, R. Nikhil, and R. Stets.\nCashmere-VLM: Remote memory paging for software distributed\nshared memory. In IPPS/SPDP , 1999.\n[19] M. J. Feeley, W. E. Morgan, E. Pighin, A. R. Karlin, H. M. Levy, and C. A.\nThekkath. Implementing global memory management in a workstation\ncluster. In ACM SIGOPS Operating Systems Review , volume 29, pages\n201\u2013212. ACM, 1995.\n[20] E. W. Felten and J. Zahorjan. Issues in the implementation of a remote\nmemory paging system. Technical Report 91-03-09, University of\nWashington, Mar 1991.\n[21] D. Gouk, S. Lee, M. Kwon, and M. Jung. Direct access, High-\nPerformance memory disaggregation with DirectCXL. In USENIX\nATC, 2022.\n[22] J. Gu, Y. Lee, Y. Zhang, M. Chowdhury, and K. G. Shin. E \uffffcient\nmemory disaggregation with In \uffffniswap. In NSDI , 2017.", "start_char_idx": 459961, "end_char_idx": 462829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce6c6699-936b-4bf0-97c7-d487c74e5449": {"__data__": {"id_": "ce6c6699-936b-4bf0-97c7-d487c74e5449", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bcdcd5f-8502-4f88-b03c-d4cf1903d2f9", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e9125e3efbfe6cf45b1d73d8aed42a683e852aee5aee4389aedcf9fc5130c410", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0316081b-bcb9-4172-b58e-6ac439bf5871", "node_type": "1", "metadata": {}, "hash": "189aa75aef0f5fd52e92065f506fee15f299478f10e570a6b833e2485896bd3e", "class_name": "RelatedNodeInfo"}}, "text": "Implementing global memory management in a workstation\ncluster. In ACM SIGOPS Operating Systems Review , volume 29, pages\n201\u2013212. ACM, 1995.\n[20] E. W. Felten and J. Zahorjan. Issues in the implementation of a remote\nmemory paging system. Technical Report 91-03-09, University of\nWashington, Mar 1991.\n[21] D. Gouk, S. Lee, M. Kwon, and M. Jung. Direct access, High-\nPerformance memory disaggregation with DirectCXL. In USENIX\nATC, 2022.\n[22] J. Gu, Y. Lee, Y. Zhang, M. Chowdhury, and K. G. Shin. E \uffffcient\nmemory disaggregation with In \uffffniswap. In NSDI , 2017.\n[23] N. Jones. How to stop data centres from gobbling up the world\u2019s\nelectricity. Nature , 561:163\u2013166, 2018.\n[24] A. Kalia, M. Kaminsky, and D. G. Andersen. FaSST: Fast, scalable\nand simple distributed transactions with two-sided (RDMA) datagram\nRPCs. In OSDI , 2016.\n[25] A. Lagar-Cavilla, J. Ahn, S. Souhlal, N. Agarwal, R. Burny, S. Butt,\nJ. Chang, A. Chaugule, N. Deng, J. Shahid, G. Thelen, K. A. Yurt-\nsever, Y. Zhao, and P. Ranganathan. Software-de \uffffned far memory in\nwarehouse-scale computers. In ASPLOS , 2019.\n[26] S.-s. Lee, Y. Yu, Y. Tang, A. Khandelwal, L. Zhong, and A. Bhattachar-\njee. MIND: In-network memory management for disaggregated data\ncenters. In SOSP , 2021.\n[27] Y. Lee, H. A. Maruf, M. Chowdhury, A. Cidon, and K. G. Shin. Hydra :\nResilient and highly available remote memory. In FAST , 2022.\n[28] H. Li, D. S. Berger, S. Novakovic, L. Hsu, D. Ernst, P. Zardoshti, M. Shah,\nS. Rajadnya, S. Lee, I. Agarwal, M. D. Hill, M. Fontoura, and R. Bianchini.\nPond: CXL-based memory pooling systems for cloud platforms. In\nASPLOS , 2023.\n[29] S. Liang, R. Noronha, and D. K. Panda. Swapping to remote memory\nover In \uffffniBand: An approach using a high performance network block\ndevice. In IEEE International Conference on Cluster Computing , 2005.\n[30] K. Lim, J. Chang, T. Mudge, P. Ranganathan, S. K. Reinhardt, and T. F.\nWenisch. Disaggregated memory for expansion and sharing in blade\nservers. SIGARCH , 2009.\n[31] E. P. Markatos and G. Dramitinos. Implementation of a reliable remote\nmemory pager. In USENIX ATC , 1996.\n[32] H. A. Maruf and M. Chowdhury. E \uffffectively prefetching remote mem-\nory with Leap. In USENIX ATC , 2020.\n[33] H. A. Maruf, H. Wang, A. Dhanotia, J. Weiner, N. Agarwal, P. Bhat-\ntacharya, C. Petersen, M. Chowdhury, S. Kanaujia, and P. Chauhan.\nTPP: Transparent page placement for CXL-enabled tiered-memory. In\nASPLOS , 2023.\n[34] H. A. Maruf, Y. Zhong, H. Wong, M. Chowdhury, A. Cidon, and C. Wald-\nspurger. Memtrade: A disaggregated-memory marketplace for public\n8clouds. In SIGMETRICS , 2023.\n[35] T. Newhall, S. Finney, K. Ganchev, and M. Spiegel. Nswap: A network\nswapping module for Linux clusters. In Euro-Par , 2003.", "start_char_idx": 462267, "end_char_idx": 464996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0316081b-bcb9-4172-b58e-6ac439bf5871": {"__data__": {"id_": "0316081b-bcb9-4172-b58e-6ac439bf5871", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce6c6699-936b-4bf0-97c7-d487c74e5449", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "b0d7a7ed197f0da55b4c85c4571307afeba3220455d88157d66dcefba53fcb65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cc89d25-1d1a-440f-a270-5c8c662d68b2", "node_type": "1", "metadata": {}, "hash": "db87b3556d2aae9efa5a9644ebbe764540178d50e51e66b49d1fc8dc87e6fb81", "class_name": "RelatedNodeInfo"}}, "text": "TPP: Transparent page placement for CXL-enabled tiered-memory. In\nASPLOS , 2023.\n[34] H. A. Maruf, Y. Zhong, H. Wong, M. Chowdhury, A. Cidon, and C. Wald-\nspurger. Memtrade: A disaggregated-memory marketplace for public\n8clouds. In SIGMETRICS , 2023.\n[35] T. Newhall, S. Finney, K. Ganchev, and M. Spiegel. Nswap: A network\nswapping module for Linux clusters. In Euro-Par , 2003.\n[36] B. Nitzberg and V. Lo. Distributed shared memory: A survey of issues\nand algorithms. Computer , 24(8):52\u201360, 1991.\n[37] F. Pearce. Energy hogs: Can world\u2019s huge data centers be made more\ne\uffffcient? Yale Environment , 2018.\n[38] K. Rashmi, M. Chowdhury, J. Kosaian, I. Stoica, and K. Ramchandran.\nEC-Cache: Load-balanced, low-latency cluster caching with online\nerasure coding. In OSDI , 2016.\n[39] A. Raybuck, T. Stamler, W. Zhang, M. Erez, and S. Peter. HeMem:\nScalable tiered memory management for big data applications and real\nNVM. In SOSP , 2021.\n[40] C. Reiss, A. Tumanov, G. R. Ganger, R. H. Katz, and M. A. Kozuch.\nHeterogeneity and dynamicity of clouds at scale: Google trace analysis.\nInSoCC , 2012.\n[41] Z. Ruan, S. J. Park, M. K. Aguilera, A. Belay, and M. Schwarzkopf.\nNu: Achieving microsecond-scale resource fungibility with logical\nprocesses. In NSDI , 2023.\n[42] Z. Ruan, M. Schwarzkopf, M. K. Aguilera, and A. Belay. AIFM: High-\nperformance, application-integrated far memory. In OSDI , 2020.\n[43] H. Servat, A. J. Pe\u00f1a, G. Llort, E. Mercadal, H.-C. Hoppe, and J. Labarta.\nAutomating the application data placement in hybrid memory systems.\nInIEEE International Conference on Cluster Computing , 2017.\n[44] Y. Shan, Y. Huang, Y. Chen, and Y. Zhang. LegoOS: A disseminated,\ndistributed OS for hardware resource disaggregation. In OSDI , 2018.\n[45] A. Tatar, R. K. Konoth, E. Athanasopoulos, C. Giu \uffffrida, H. Bos, and\nK. Razavi. Throwhammer: Rowhammer attacks over the network and\ndefenses. In ATC, 2018.\n[46] S.-Y. Tsai, M. Payer, and Y. Zhang. Pythia: Remote oracles for the\nmasses. In USENIX Security , 2019.[47] Vladimir Davydov. Idle Memory Tracking. https://lwn .net/Articles/\n639341/.\n[48] M. Vuppalapati, J. Miron, R. Agarwal, D. Truong, A. Motivala, and\nT. Cruanes. Building an elastic query engine on disaggregated storage.\nInNSDI , 2020.\n[49] C. Wang, H. Ma, S. Liu, Y. Li, Z. Ruan, K. Nguyen, M. D. Bond, R. Ne-\ntravali, M. Kim, and G. H. Xu. Semeru: A Memory-Disaggregated\nmanaged runtime. In OSDI , 2020.\n[50] K. Wu, Y. Huang, and D. Li. Unimem: Runtime data managementon\nnon-volatile memory-based heterogeneous main memory. In SC, 2017.\n[51] D. Y. Yoon, M. Chowdhury, and B. Mozafari. Distributed lock manage-\nment with RDMA: Decentralization without starvation. In SIGMOD ,\n2018.\n[52] J. You, J. Wu, X. Jin, and M. Chowdhury. Ship compute or ship data?\nwhy not both?", "start_char_idx": 464617, "end_char_idx": 467398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cc89d25-1d1a-440f-a270-5c8c662d68b2": {"__data__": {"id_": "4cc89d25-1d1a-440f-a270-5c8c662d68b2", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0316081b-bcb9-4172-b58e-6ac439bf5871", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5e4245dd4c3024fbc016768f36cfed38bdeb30211582fa5ff939b9919c35ac3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4af30c0-29b2-4a8c-b56a-b594d9a18fd7", "node_type": "1", "metadata": {}, "hash": "37a1fc5f18e4b362945757af51737c86399952ee985a9c0c2454a9504b0dd9c5", "class_name": "RelatedNodeInfo"}}, "text": "Semeru: A Memory-Disaggregated\nmanaged runtime. In OSDI , 2020.\n[50] K. Wu, Y. Huang, and D. Li. Unimem: Runtime data managementon\nnon-volatile memory-based heterogeneous main memory. In SC, 2017.\n[51] D. Y. Yoon, M. Chowdhury, and B. Mozafari. Distributed lock manage-\nment with RDMA: Decentralization without starvation. In SIGMOD ,\n2018.\n[52] J. You, J. Wu, X. Jin, and M. Chowdhury. Ship compute or ship data?\nwhy not both? In NSDI , 2021.\n[53] Z. Yu, Y. Zhang, V. Braverman, M. Chowdhury, and X. Jin. NetLock:\nFast, centralized lock management using programmable switches. In\nSIGCOMM , 2020.\n[54] Y. Zhang, J. Gu, Y. Lee, M. Chowdhury, and K. G. Shin. Performance\nisolation anomalies in RDMA. In ACM SIGCOMM KBNets , 2017.\n[55] Y. Zhang, G. Kumar, N. Dukkipati, X. Wu, P. Jha, M. Chowdhury, and\nA. Vahdat. Aequitas: Admission control for performance-critical RPCs\nin datacenters. In ACM SIGCOMM , 2022.\n[56] Y. Zhang, Y. Tan, B. Stephens, and M. Chowdhury. Justitia: Software\nMulti-Tenancy in hardware Kernel-Bypass networks. In NSDI , 2022.\n[57] W. Zhao and J. Ning. Project Tioga Pass Rev 0.30 : Facebook Server Intel\nMotherboard V4.0 Spec. https://www .opencompute .org/documents/\nfacebook-server-intel-motherboard-v40-spec.\n9Pond: CXL-Based Memory Pooling Systems for Cloud Platforms\nHuaicheng Li\u2020, Daniel S. Berger\u21e4\u2021, Stanko Novakovic\u21e4, Lisa Hsu\u21e4, Dan Ernst\u21e4,\nPantea Zardoshti\u21e4, Monish Shah\u21e4, Samir Rajadnya\u21e4, Scott Lee\u21e4, Ishwar Agarwal\u21e4,\nMark D. Hill\u21e4\u0000, Marcus Fontoura\u21e4, Ricardo Bianchini\u21e4\n\u2020Virginia Tech and CMU\u21e4Microsoft Azure\u2021University of Washington\u0000University of Wisconsin-Madison\nAbstract\nPublic cloud providers seek to meet stringent performance\nrequirements and low hardware cost. A key driver of per-\nformance and cost is main memory. Memory pooling\npromises to improve DRAM utilization and thereby re-\nduce costs. However, pooling is challenging under cloud\nperformance requirements. This paper proposes Pond,\nthe \ufb01rst memory pooling system that both meets cloud\nperformance goals and signi\ufb01cantly reduces DRAM cost.\nPond builds on the Compute Express Link (CXL) standard\nfor load/store access to pool memory and two key insights.\nFirst, our analysis of cloud production traces shows that\npooling across 8-16 sockets is enough to achieve most\nof the bene\ufb01ts. This enables a small-pool design with\nlow access latency. Second, it is possible to create ma-\nchine learning models that can accurately predict how\nmuch local and pool memory to allocate to a virtual ma-\nchine (VM) to resemble same-NUMA-node memory per-\nformance. Our evaluation with 158 workloads shows\nthat Pond reduces DRAM costs by 7% with performance\nwithin 1-5% of same-NUMA-node VM allocations.\n1.Introduction\nMotivation. Many public cloud customers deploy their\nworkloads in the form of virtual machines (VMs), for\nwhich they get virtualized compute with performance\napproaching that of a dedicated cloud, but without having\nto manage their own on-premises datacenter. This creates\na major challenge for public cloud providers: achieving\nexcellent performance for opaque VMs ( i.e., providers do\nnot know and should not inspect what is running inside\nthe VMs) at a competitive hardware cost.\nA key driver of both performance and cost is main\nmemory. The gold standard for memory performance is\nfor accesses to be served by the same NUMA node as\nthe cores that issue them, leading to latencies in tens of\nnanoseconds. A common approach is to preallocate all\nVM memory on the same NUMA node as the VM\u2019s cores.", "start_char_idx": 466971, "end_char_idx": 470468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4af30c0-29b2-4a8c-b56a-b594d9a18fd7": {"__data__": {"id_": "f4af30c0-29b2-4a8c-b56a-b594d9a18fd7", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cc89d25-1d1a-440f-a270-5c8c662d68b2", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8675d6a82b99350612bf2f069590e4c9eafea8f1b3d4a0f661a45799443dcf64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "467bd94b-41ad-4b0a-a709-c9bd3bf6f5b5", "node_type": "1", "metadata": {}, "hash": "4aa2c5d445d84f51779e3913cd347c9de4504252a4e7d446213e15ee4f24fd56", "class_name": "RelatedNodeInfo"}}, "text": "1.Introduction\nMotivation. Many public cloud customers deploy their\nworkloads in the form of virtual machines (VMs), for\nwhich they get virtualized compute with performance\napproaching that of a dedicated cloud, but without having\nto manage their own on-premises datacenter. This creates\na major challenge for public cloud providers: achieving\nexcellent performance for opaque VMs ( i.e., providers do\nnot know and should not inspect what is running inside\nthe VMs) at a competitive hardware cost.\nA key driver of both performance and cost is main\nmemory. The gold standard for memory performance is\nfor accesses to be served by the same NUMA node as\nthe cores that issue them, leading to latencies in tens of\nnanoseconds. A common approach is to preallocate all\nVM memory on the same NUMA node as the VM\u2019s cores.\nPreallocating and statically pinning memory also facilitate\nthe use of virtualization accelerators [ 1\u20136], which are\nenabled by default, for example, on AWS and Azure [ 7,8].\nAt the same time, DRAM has become a major portion of\nhardware cost due to its poor scaling properties with only\nnascent alternatives [ 9\u201315]. For example, DRAM can be\n50% of server cost [ 16].Through analysis of production traces from Azure, we\nidentify memory stranding as a dominant source of mem-\nory waste and a potential source of massive cost savings.\nStranding happens when all cores of a server are rented\n(i.e., allocated to customer VMs) but unallocated memory\ncapacity remains and cannot be rented. We \ufb01nd that up to\n25% of DRAM becomes stranded as more cores become\nallocated to VMs.\nLimitations of the state of the art. Despite this signi\ufb01-\ncant amount of stranding, reducing DRAM usage in the\npublic cloud is challenging due to its stringent perfor-\nmance requirements. For example, existing techniques\nfor process-level memory compression [ 17,18] require\npage fault handling, which adds microseconds of latency,\nand moving away from statically preallocated memory.\nPooling memory via memory disaggregation is a\npromising approach because stranded memory can be re-\nturned to the disaggregated pool and used by other servers.\nUnfortunately, existing pooling systems also have mi-\ncrosecond access latencies and require page faults [ 1,19\u2013\n24] or changes to the VM guest [ 17,21\u201323,25\u201338].\nOur work. This work describes Pond, the \ufb01rst system to\nachieve both same-NUMA-node memory performance\nand competitive cost for public cloud platforms. To\nachieve this, Pond combines hardware and systems tech-\nniques. It relies on the Compute Express Link (CXL)\ninterconnect standard [ 39], which enables cacheable\nload/store ( ld/st ) accesses to pooled memory on In-\ntel, AMD, and ARM processors [ 40\u201342] at nanosecond-\nscale latencies. CXL access via loads/stores is a game\nchanger as it allows memory to remain statically preal-\nlocated while physically being located in a shared pool.\nHowever, even with loads/stores, CXL accesses still face\nhigher latencies than same-NUMA-node accesses. Pond\nintroduces systems support for CXL-based pooling that\ndramatically reduces the impact of this higher latency.\nPond is feasible because of four key insights. First, by\nanalyzing traces from 100 production clusters at Azure,\nwe \ufb01nd that pool sizes between 8-16 sockets lead to suf\ufb01-\ncient DRAM savings. The pool size de\ufb01nes the number\nof CPU sockets able to use pool memory. Further, analy-\nsis of CXL topologies lead us to estimate that CXL will\nadd 70-90ns to access latencies over same-NUMA-node\nDRAM with a pool size of 8-16 sockets, and add more\nthan 180ns for rack-scale pooling. We conclude that\n1arXiv:2203.00241v4  [cs.OS]  21 Oct 2022Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\ngrouping 8 dual-socket (or 16 single-socket) servers is\nenough to achieve most of the bene\ufb01ts of pooling.\nSecond, by emulating either 64ns or 140ns of memory\naccess overheads, we \ufb01nd that 43% and 37% of 158 work-\nloads are within 5% of the performance on same-NUMA-\nnode DRAM when entirely allocated in pool memory.", "start_char_idx": 469655, "end_char_idx": 473744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "467bd94b-41ad-4b0a-a709-c9bd3bf6f5b5": {"__data__": {"id_": "467bd94b-41ad-4b0a-a709-c9bd3bf6f5b5", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4af30c0-29b2-4a8c-b56a-b594d9a18fd7", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "1e4306bcd34c5a804b2801c3368029587afbd7efe9d1c228b8d037dd00e89f23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78451caa-56b9-446f-83db-c4248359c47f", "node_type": "1", "metadata": {}, "hash": "f14889b07db54b303aa8b06f123505157aa0306d705fe5f46f2119186a9b5e4b", "class_name": "RelatedNodeInfo"}}, "text": "We conclude that\n1arXiv:2203.00241v4  [cs.OS]  21 Oct 2022Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\ngrouping 8 dual-socket (or 16 single-socket) servers is\nenough to achieve most of the bene\ufb01ts of pooling.\nSecond, by emulating either 64ns or 140ns of memory\naccess overheads, we \ufb01nd that 43% and 37% of 158 work-\nloads are within 5% of the performance on same-NUMA-\nnode DRAM when entirely allocated in pool memory.\nHowever, more than 21% of workloads suffer a perfor-\nmance loss above 25%. This emphasizes the need for\nsmall pools and shows the challenge with achieving same-\nNUMA-node performance. This characterization also\nallows us to train a machine learning (ML) model that can\nidentify a subset of insensitive workloads ahead of time\nto be allocated on the Pond memory pool.\nThird, we observe through measurements at Azure that\n\u21e050% of all VMs touch less than 50% of their rented\nmemory. Conceptually, allocating untouched memory\nfrom the pool should not have any performance impact\neven for latency-sensitive VMs. We \ufb01nd that \u2014 while\nthis concept does not hold for the uniform address spaces\nassumed in prior work [ 1,19\u201324] \u2014 it does hold if we\nexpose pool memory to a VM\u2019s guest OS as a zero-core\nvirtual NUMA ( zNUMA ) node, i.e., a node with memory\nbut no cores, like Linux\u2019s CPU-less NUMA [ 43]. Our\nexperiments show zNUMA effectively biases memory\nallocations away from the zNUMA node. Thus, a VM\nwith a zNUMA sized to match its untouched memory will\nindeed not see any performance impact.\nFourth, Pond can allocate CXL memory with same-\nNUMA-node performance using correct predictions of a)\nwhether a VM will be latency-sensitive and b)a VM\u2019s\namount of untouched memory. For incorrect predictions,\nPond introduces a novel monitoring system that detects\npoor memory performance and triggers a mitigation that\nmigrates the VM to use only same-NUMA-node memory.\nFurther, we \ufb01nd that all inputs to train and run Pond\u2019s ML\nmodels can be obtained from existing hardware telemetry\nwith no measurable overhead.\nArtifacts. CXL is still a year from broad deployment.\nMeanwhile, deploying Pond requires extensive testing\nwithin Azure\u2019s system software and distributed software\nstack. We implement Pond on top of an emulation layer\nthat is deployed on production servers. This allows us to\nprove the key concepts behind Pond by exercising the VM\nallocation work\ufb02ow, zNUMA, and by measuring guest\nperformance. Additionally, we support the four insights\nfrom above by reporting from extensive experiments and\nmeasurements in Azure\u2019s datacenters. We evaluate the\neffectiveness of pooling using simulations based on VM\ntraces from 100 production clusters.\nContributions. Our main contributions are:\n\u2022The \ufb01rst public characterization of memory stranding\nand untouched memory at a large public cloud provider.\n\u2022The \ufb01rst analysis of the effectiveness and latency of\nMemoryDeviceRequest (Req)Data Response (DRS) Cache Miss\nCXLPort\nMemoryDeviceCache Write Back\nCXLPortRequest with Data (RwD)No Data Response (NDR)\nCXL &PCIe PHY\nArb/Mux\nTransaction &Link LayersCPUFabric\nPCIeWires\n4ns2ns19nsRound-trip latency measuredon Intel Sapphire Rapids \nIntel/AMD/ARMCore/LLC/Fabric\nIntel/AMD/ARMCore/LLC/FabricFigure 1: CXL Request Flow (\u00a7 2).CPU cache misses and\nwrite-backs to addresses mapped to CXL devices are translated\nto requests on a CXL port by the HDM decoder. Intel measures\nthe round-trip port latency to be 25ns.\ndifferent CXL memory pool sizes.\n\u2022Pond, the \ufb01rst CXL-based full-stack memory pool that\nis practical and performant for cloud deployment.\n\u2022An accurate prediction model for latency and resource\nmanagement at datacenter scale. These models enable\na con\ufb01gurable performance slowdown of 1-5%.\n\u2022An extensive evaluation that validates Pond\u2019s design in-\ncluding the performance of zNUMA and our prediction\nmodels in a production setting.", "start_char_idx": 473230, "end_char_idx": 477170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78451caa-56b9-446f-83db-c4248359c47f": {"__data__": {"id_": "78451caa-56b9-446f-83db-c4248359c47f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "467bd94b-41ad-4b0a-a709-c9bd3bf6f5b5", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8219f2a66149244780cf990ca4467df88e18245d94352d3d26d3acdac7356991", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e5a5b45-efbe-4187-9e4b-0ba344ea2b4a", "node_type": "1", "metadata": {}, "hash": "e112bea165a05fff6fe3f001918994c48509ab57bd9f401b842b46c404cdc0d3", "class_name": "RelatedNodeInfo"}}, "text": "Intel measures\nthe round-trip port latency to be 25ns.\ndifferent CXL memory pool sizes.\n\u2022Pond, the \ufb01rst CXL-based full-stack memory pool that\nis practical and performant for cloud deployment.\n\u2022An accurate prediction model for latency and resource\nmanagement at datacenter scale. These models enable\na con\ufb01gurable performance slowdown of 1-5%.\n\u2022An extensive evaluation that validates Pond\u2019s design in-\ncluding the performance of zNUMA and our prediction\nmodels in a production setting. Our analysis shows that\nwe can reduce DRAM needs by 7% with a Pond pool\nspanning 16 sockets, which corresponds to hundreds of\nmillions of dollars for a large cloud provider.\n2.Background\nHypervisor memory management. Public cloud work-\nloads are virtualized [ 44]. To maximize performance and\nminimize overheads, hypervisors perform minimal mem-\nory management and rely on virtualization accelerators\nto improve I/O performance [ 1,45\u201347]. Examples of\ncommon accelerators are direct I/O device assignment\n(DDA) [ 1,45] and Single Root I/O Virtualization (SR-\nIOV) [ 46,47]. Accelerated networking is enabled by\ndefault on AWS and Azure [ 7,8]. As pointed out in prior\nwork, virtualization acceleration requires statically preal-\nlocating (or \u201cpinning\u201d) a VM\u2019s entire address space [ 1\u20136].\nMemory stranding. Cloud VMs demand a vector of re-\nsources ( e.g., CPUs, memory, etc.) [ 48\u201351]. Scheduling\nVMs thus leads to a multi-dimensional bin-packing prob-\nlem [ 49,52\u201354] which is complicated by constraints such\nas spreading VMs across multiple failure domains. Con-\nsequently, it is dif\ufb01cult to provision servers that closely\nmatch the resource demands of the incoming VM mix.\nWhen the DRAM-to-core ratio of VM arrivals and the\nserver resources do not match, tight packing becomes\nmore dif\ufb01cult. We de\ufb01ne a resource as stranded when\nit is technically available to be rented to a customer, but\nis practically unavailable as some other resource has ex-\nhausted. The typical scenario for memory stranding is\nthat all cores have been rented, but there is still memory\navailable in the server.\nReducing stranding. Multiple techniques can re-\n2Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n010203040\n60 7080 90Scheduled CPU Cores [%]Stranded Memory [%]5th Percentile95th Percentile\nOutliers[a]   Stranding vs.      CPU utilizations\n01234567Racks01 5 3 045 60 75Time [Days][b] Stranding over time \nFigure 2: Memory stranding. (a) Stranding increases signi\ufb01-\ncantly as more CPU cores are scheduled; (b) Stranding changes\ndynamically over time.\nduce memory stranding. For example, oversubscribing\ncores [ 55,56] enables more memory to be rented. How-\never, oversubscription only applies to a subset of VMs\nfor performance reasons. Our measurements at Azure\n(\u00a73.1) include clusters that enable oversubscription and\nstill show signi\ufb01cant memory stranding.\nThe approach we target is to disaggregate a portion\nof memory into a pool that is accessible by multiple\nhosts [ 31,57,58]. This breaks the \ufb01xed hardware con\ufb01g-\nuration of servers. By dynamically reassigning memory\nto different hosts at different times, we can shift memory\nresources to where they are needed, instead of relying\non each individual server to be con\ufb01gured for all cases\npessimistically. Thus, we can provision servers close to\nthe average DRAM-to-core ratios and tackle deviations\nvia the memory pool.\nPooling via CXL. CXL contains multiple protocols in-\ncluding ld/st memory semantics (CXL.mem) and I/O\nsemantics (CXL.io). CXL.mem maps device memory to\nthe system address space. Last-level cache (LLC) misses\nto CXL memory addresses translate into requests on a\nCXL port whose reponses bring the missing cachelines\n(Figure 1). Similarly, LLC write-backs translate into CXL\ndata writes. Neither action involves page faults or DMAs.\nCXL memory is virtualized using hypervisor page tables\nand the memory-management unit and is thus compati-\nble with virtualization acceleration.", "start_char_idx": 476686, "end_char_idx": 480701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e5a5b45-efbe-4187-9e4b-0ba344ea2b4a": {"__data__": {"id_": "7e5a5b45-efbe-4187-9e4b-0ba344ea2b4a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78451caa-56b9-446f-83db-c4248359c47f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e39f02341071cd78d0124208334e69910a3fe48dff2fe2bf54e55cf904721b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb2fab86-fbcd-43bf-a248-1045590d6439", "node_type": "1", "metadata": {}, "hash": "9522bc513d952b93e39c618b6e6a549c3b9185278d716692c934d81955346dce", "class_name": "RelatedNodeInfo"}}, "text": "Thus, we can provision servers close to\nthe average DRAM-to-core ratios and tackle deviations\nvia the memory pool.\nPooling via CXL. CXL contains multiple protocols in-\ncluding ld/st memory semantics (CXL.mem) and I/O\nsemantics (CXL.io). CXL.mem maps device memory to\nthe system address space. Last-level cache (LLC) misses\nto CXL memory addresses translate into requests on a\nCXL port whose reponses bring the missing cachelines\n(Figure 1). Similarly, LLC write-backs translate into CXL\ndata writes. Neither action involves page faults or DMAs.\nCXL memory is virtualized using hypervisor page tables\nand the memory-management unit and is thus compati-\nble with virtualization acceleration. The CXL.io protocol\nfacilitates device discovery and con\ufb01guration. CXL 1.1\ntargets directly-attached devices, 2.0 [ 59,60] adds switch-\nbased pooling, and 3.0 [ 61,62] standardizes switch-less\npooling (\u00a7 4) and higher bandwidth.\nCXL.mem uses PCIe\u2019s eletrical interface with custom\nlink and transaction layers for low latency. With PCIe 5.0,\n859095100\n28 1 6 3 264Pool Size [CPU Sockets]Required Overall DRAM [%]10%30%50%Percentage of pool memory    assigned to each VMFigure 3: Impact of pool size (\u00a7 3.1).Small pools of 32\nsockets are suf\ufb01cient to signi\ufb01cantly reduce memory needs.\nthe bandwidth of a birectional \u21e58-CXL port at a typical\n2:1 read:write-ratio matches a DDR5-4800 channel. CXL\nrequest latencies are largely determined by the CXL port.\nIntel measures round-trip CXL port traversals at 25ns [ 63]\nwhich, when combined with expected controller-side la-\ntencies, leads to an end-to-end overhead of 70ns for CXL\nreads, compared to NUMA-local DRAM reads. While\nFPGA-based prototypes report higher latency [ 64,65], In-\ntel\u2019s measurements match industry-expectations for ASIC-\nbased memory controllers [ 62\u201364].\n3.Memory Stranding & Workload Sensitiv-\nity to Memory Latency\n3.1.Stranding at Azure\nThis section quanti\ufb01es the severity of memory stranding\nand untouched memory at Azure using production data.\nDataset. We measure stranding in 100 cloud clusters\nover a 75-day period. These clusters host mainstream\n\ufb01rst-party and third-party VM workloads. They are rep-\nresentative of the majority of the server \ufb02eet. We select\nclusters with similar deployment years, but spanning all\nmajor regions on the planet. A trace from each cluster\ncontains millions of per-VM arrival/departure events, with\nthe time, duration, resource demands, and server-id.\nMemory stranding. Figure 2a shows the daily average\namount of stranded DRAM across clusters, bucketed by\nthe percentage of scheduled CPU cores. In clusters where\n75% of CPU cores are scheduled for VMs, 6% of memory\nis stranded. This grows to over 10% when \u21e085% of\nCPU cores are allocated to VMs. This makes sense since\nstranding is an artifact of highly utilized nodes, which\ncorrelates with highly utilized clusters. Outliers are shown\nby the error bars, representing 5thand 95thpercentiles.\nAt 95th, stranding reaches 25% during high utilization\nperiods. Individual outliers even reach 30% stranding.\nFigure 2b shows stranding over time across 8 racks.\nA workload change (around day 36) suddenly increased\nstranding signi\ufb01cantly. Furthermore, stranding can affect\nmany racks concurrently ( e.g., racks 2, 4\u20137) and it is\n3Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n020406080100\nP1\u2192P13 YCSB A\u2192FML/Web, etc.", "start_char_idx": 480012, "end_char_idx": 483463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb2fab86-fbcd-43bf-a248-1045590d6439": {"__data__": {"id_": "eb2fab86-fbcd-43bf-a248-1045590d6439", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e5a5b45-efbe-4187-9e4b-0ba344ea2b4a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "42df2277c429be95e21629e4ad571af994dd8ec22c3e358ff54c140f02675e6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "098a79dc-c1f1-412c-adcd-035eb5d1c2b9", "node_type": "1", "metadata": {}, "hash": "86af529e001e273e7c09d521598624131fb4ef240763cf8cd582f52209fbc0a7", "class_name": "RelatedNodeInfo"}}, "text": "This makes sense since\nstranding is an artifact of highly utilized nodes, which\ncorrelates with highly utilized clusters. Outliers are shown\nby the error bars, representing 5thand 95thpercentiles.\nAt 95th, stranding reaches 25% during high utilization\nperiods. Individual outliers even reach 30% stranding.\nFigure 2b shows stranding over time across 8 racks.\nA workload change (around day 36) suddenly increased\nstranding signi\ufb01cantly. Furthermore, stranding can affect\nmany racks concurrently ( e.g., racks 2, 4\u20137) and it is\n3Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n020406080100\nP1\u2192P13 YCSB A\u2192FML/Web, etc. bc, bfs, cc,pr, sssp, tcQueries 1\u219222 501.perlbench_r\u2192657.xz_sfacesim, vips, fft, etc.ProprietaryRedisVoltDBSpark GAPBSTPC-H SPEC CPU 2017PARSECSPLASH2xSlowdown (%)Local:   78ns, remote: 142ns (182%)Local: 115ns, remote: 255ns (222%)Slowdown: performance under all remotememory relative to all local memoryNot run on redcon\ufb01guration,insuf\ufb01cient DRAMper NUMA node\nFigure 4: Performance slowdowns when memory latency increases by 182-222% (\u00a7 3.3).Workloads have different sensitivity\nto additional memory latency (as in CXL). X-axis shows 158 representative workloads; Y is the normalized performance slowdown,\ni.e., performance under higher (remote) latency relative to all local memory. \u201cProprietary\u201d denotes production workloads at Azure.\n0.2.4.6.81\n05255075100Threeoutliersunder255nswith>100%slowdowns(max124%)Slowdown(%)Remote:142ns(182%)Remote:255ns(222%)CDFofslowdownsunderCXL\nFigure 5: CDF of slowdowns (\u00a7 3.3).Higher remote latency\n(red) only slightly affects the head of the distribution (work-\nloads with less than 5% slowdown). The body and tail of the\ndistribution see signi\ufb01cantly higher slowdowns.\ngenerally hard to predict which clusters/racks will have\nstranded memory.\nNUMA spanning. Many VMs are small and can \ufb01t on a\nsingle socket. On two-socket systems, the hypervisor at\nAzure seeks to schedule such that VMs \ufb01t entirely (cores\nand memory) on a single NUMA node. In rare cases, we\nseeNUMA spanning where a VM has all of its cores on\none socket and a small amount of memory from another\nsocket. We \ufb01nd that spanning occurs for about 2-3% of\nVMs and fewer than 1% of memory pages, on average.\nSavings from pooling. Azure currently does not pool\nmemory. However, by analyzing its VM-to-server traces,\nwe can estimate the amount of DRAM that could be saved\nvia pooling. Figure 3presents average reductions from\npooling DRAM when VMs are scheduled with a \ufb01xed\npercentage of either 10%, 30%, or 50% of pool DRAM.\nThe pool size refers to the number of sockets that can\naccess the same DRAM pool. As the pool size increases,\nthe \ufb01gure shows that required overall DRAM decreases.\nHowever, this effect diminishes for larger pools. For ex-\nample, with a \ufb01xed 50% pool DRAM, a pool with 32\nsockets saves 12% of DRAM while a pool with 64 sock-\nets saves 13% of DRAM. Note that allocating a \ufb01xed 50%\nof memory to pool DRAM leads to signi\ufb01cant perfor-\nmance loss compared to socket-local DRAM (\u00a7 6). Pond\novercomes this challenge with multiple techniques (\u00a7 4).Summary and implications. From this analysis, we\ndraw a few important observations and implications for\nPond:\n\u2022We observe 3-27% of stranded memory in production\nat the 95thpercentile, with some outliers at 36%.\n\u2022Almost all VMs \ufb01t into one NUMA node.\n\u2022Pooling memory across 16-32 sockets can reduce clus-\nter memory demand by 10%.", "start_char_idx": 482755, "end_char_idx": 486258, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "098a79dc-c1f1-412c-adcd-035eb5d1c2b9": {"__data__": {"id_": "098a79dc-c1f1-412c-adcd-035eb5d1c2b9", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb2fab86-fbcd-43bf-a248-1045590d6439", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e6c75c0ee16032579b5ab7e10b311dd72efa3eebdaad9b7c19835bd589dcc844", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e0a5b1d-80a0-41de-9c79-563bfb4ceaf3", "node_type": "1", "metadata": {}, "hash": "801b459dc5e6a45d5f8ae461b858d0cfdc92d0040e7632dec2e14f37e72925be", "class_name": "RelatedNodeInfo"}}, "text": "For ex-\nample, with a \ufb01xed 50% pool DRAM, a pool with 32\nsockets saves 12% of DRAM while a pool with 64 sock-\nets saves 13% of DRAM. Note that allocating a \ufb01xed 50%\nof memory to pool DRAM leads to signi\ufb01cant perfor-\nmance loss compared to socket-local DRAM (\u00a7 6). Pond\novercomes this challenge with multiple techniques (\u00a7 4).Summary and implications. From this analysis, we\ndraw a few important observations and implications for\nPond:\n\u2022We observe 3-27% of stranded memory in production\nat the 95thpercentile, with some outliers at 36%.\n\u2022Almost all VMs \ufb01t into one NUMA node.\n\u2022Pooling memory across 16-32 sockets can reduce clus-\nter memory demand by 10%. This suggests that mem-\nory pooling can produce signi\ufb01cant cost reductions but\nassumes that a high percentage of DRAM can be allo-\ncated on memory pools. When implementing DRAM\npools with cross-NUMA latencies, providers must care-\nfully mitigate potential performance impacts.\n3.2.VM Memory Usage at Azure\nWe use Pond\u2019s telemetry on opaque VMs (\u00a7 4.2) to char-\nacterize the percentage of untouched memory across our\ncloud clusters. Generally, we \ufb01nd that while VM memory\nusage varies across clusters, all clusters have a signi\ufb01cant\nfraction of VMs with untouched memory. Overall, the\n50thpercentile is 50% untouched memory.\nSummary and implications. From this analysis, we\ndraw key observations and implications for Pond:\n\u2022VM memory usage varies widely.\n\u2022In the cluster with the least amount of untouched mem-\nory, still over 50% of VMs have more than 20% un-\ntouched memory. Thus, there is plenty of untouched\nmemory that can be disaggregated at no performance\npenalty.\n\u2022The challenges are (1) predicting how much untouched\nmemory a VM is likely to have and (2) con\ufb01ning the\nVM\u2019s accesses to local memory. Pond addresses both.\n3.3.Workload Sensitivity to Memory Latency\nTo characterize the performance impact of CXL latency\nfor typical workloads in Azure\u2019s datacenters, we evalu-\nate 158 workloads under two scenarios of emulated CXL\naccess latencies: 182% and 222% increase in memory\n4Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n...EMC128 PCIe 5.0 lanes(16 hosts w/ x8 CXL links)12 channelsDDR516-socket PondIODCCDCCDCCDCCDCCDCCDCCDCCDCCDCCDCCDCCDComparison: AMD Genoa   128PCIe 5.0 lanes12     channelsDDR5  397mm2CPUCPUCPU...EMC64 PCIe 5.0 lanes(8 hosts w/ x8 CXL links)6 channelsDDR58-socket PondCPUCPUCPU...\nEMC(8 hosts w/ x8 CXL links)12 channelsDDR532/64-socket PondCPUCPUCPU...SwitchCPUCPUCPU...Switch...96 PCIe 5.0 lanes(4 EMCs w/x8 CXL links)(8 switches w/x8 CXL links)...64 PCIe 5.0 lanes\u22481/2 Genoa IOD\u2248 Genoa IOD\nFigure 6: External memory controller (EMC) (\u00a7 4.1).The\nEMC is multi-headed which allows connecting multiple CXL\nhosts and DDR5 DIMMs. A 16-socket Pond requires 128 PCIe\n5.0 lanes and 12 DDR5 channels, which is comparable to the IO-\ndie (IOD) on AMD Genoa [ 42,66]. Larger Pond con\ufb01gurations\ncombine a switch with the multi-headed EMC.\nlatency, respectively. We then compare the workload\nperformance to NUMA-local memory placement. Experi-\nmental details are in \u00a7 6.1. Figures 4and5show workload\nslowdowns relative to NUMA-local performance for both\nscenarios.\nUnder a 182% increase in memory latency, we \ufb01nd that\n26% of the 158 workloads experience less than 1% slow-\ndown under CXL. An additional 17% of workloads see\nless than 5% slowdowns. At the same time, some work-\nloads are severely affected with 21% of the workloads\nfacing >25% slowdowns.", "start_char_idx": 485604, "end_char_idx": 489130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e0a5b1d-80a0-41de-9c79-563bfb4ceaf3": {"__data__": {"id_": "6e0a5b1d-80a0-41de-9c79-563bfb4ceaf3", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "098a79dc-c1f1-412c-adcd-035eb5d1c2b9", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a36fa093f698d3f7e0d5de981e1041b0dc84dfcce129f666ba7ff4e298d4997f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fe6f05b-2220-444b-b458-4f60e288f309", "node_type": "1", "metadata": {}, "hash": "8ac8a4a12b2755aec747343da1f27795e74f367738c60cdcdea3d45d9fcda372", "class_name": "RelatedNodeInfo"}}, "text": "Larger Pond con\ufb01gurations\ncombine a switch with the multi-headed EMC.\nlatency, respectively. We then compare the workload\nperformance to NUMA-local memory placement. Experi-\nmental details are in \u00a7 6.1. Figures 4and5show workload\nslowdowns relative to NUMA-local performance for both\nscenarios.\nUnder a 182% increase in memory latency, we \ufb01nd that\n26% of the 158 workloads experience less than 1% slow-\ndown under CXL. An additional 17% of workloads see\nless than 5% slowdowns. At the same time, some work-\nloads are severely affected with 21% of the workloads\nfacing >25% slowdowns.\nDifferent workload classes are affected differently,\ne.g., GAPBS (graph processing) workloads generally see\nhigher slowdowns. However, the variability within each\nworkload class is typically much higher than across work-\nload classes. For example, within GAPBS even the same\ngraph kernel reacts very differently to CXL latency, based\non different graph datasets. Overall, every workload class\nhas at least one workload with less than 5% slowdown\nand one workload with more than 25% slowdown (except\nSPLASH2x).\nAzure\u2019s proprietary workloads are less impacted than\nthe overall workload set. Of the 13 production workloads,\n6 do not see noticeable impact ( <1%); 2 see \u21e05% slow-\ndown; and the remaining half are impacted by 10\u201328%.\nThis is in part because these production workloads are\nNUMA-aware and often include data placement optimiza-\ntions.\nUnder a 222% increase in memory latency, we \ufb01nd that\n23% of the 158 workloads experience less than 1% slow-\ndown under CXL. An additional 14% of workloads see\nless than 5% slowdowns. More than 37% of workloads\nface >25% slowdowns. Generally, we \ufb01nd that higher\nlatency magni\ufb01es the effects seen under lower latency:\nworkloads performing well under 182% latency also tend\nto perform well under 222% latency; workloads severely\naffected by 182% are even more affected by 222%.\nSummary and implications. While the performance of\nCXL PortFlight timeRetimerAddress mapping, permission (ACL)Network-on-chip (NOC)Switch arbitration (ARB)Core/LLC/FabricMemory Controller (MC) MC & DRAM25ns5ns20ns5ns10ns10ns40ns45ns\nCPUCPU\nEMC\nCXLPort25ns\nCXLPort\nACLNOC25ns15ns\nCore/LLC/Fabric\nMC &DRAM45ns40ns5ns\nEMC\nCXLPort25ns\nCXLPort25ns15ns5+20+5ns\nEMC\nSwitch\nCXLPort\nCXLPort25ns\nCXLPort25ns15nsRe-timer\nCore/LLC/Fabric40ns\nCore/LLC/Fabric40ns\nCore/LLC/Fabric40ns\nMC &DRAM45ns\nMC &DRAM45ns\nMC &DRAM45nsLocal DRAM (85ns)8-socket Pond (155ns, 182%)16-socket Pond (180ns, 212%)32/64-socket Pond (>270ns, 318%)EMCPMCPUCPUEMCCPUCPUCPUCPUCPUCPU8-socketPondCPUCPUCPUCPUCPUCPUEMCPMEMC16-socket PondCPUCPUCPUCPUCPUCPUCPUCPURe-timerRe-timerRe-timerRe-timerRe-timerRe-timerRe-timerRe-timer\nCXLPort25ns25ns20ns\nARBNOC\n<500mm>500mm\n5+20+5nsRe-timer5+20+5nsRe-timerLatency assumptions\nACLNOCACLNOCFigure 7: Pool size and latency tradeoffs (\u00a7 4.1).Small Pond\npools of 8-16 sockets add only 75-90ns relative to NUMA-local\nDRAM. Latency increases for larger pools that require retimers\nand a switch.\nsome workloads is insensitive to disaggregated memory\nlatency, some are heavily impacted. This motivates our\ndesign decision to include socket-local DRAM alongside\npool DRAM to mitigate CXL latency impact for those\nlatency-sensitive workloads. Memory pooling solutions\ncan be effective if they\u2019re are effective at identi\ufb01ying\nsensitive workloads.", "start_char_idx": 488547, "end_char_idx": 491886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fe6f05b-2220-444b-b458-4f60e288f309": {"__data__": {"id_": "0fe6f05b-2220-444b-b458-4f60e288f309", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e0a5b1d-80a0-41de-9c79-563bfb4ceaf3", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "98884d8074b1a667a6b201170026e750e533f34f4df4d8ddfea666978b37b450", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c5546bf-644d-4c3d-bc04-821d9fe301d8", "node_type": "1", "metadata": {}, "hash": "f284dff3cf4dcd72d20a15e12f70f9ecbb2fc76d332546cfadc10b8629401e4d", "class_name": "RelatedNodeInfo"}}, "text": "Latency increases for larger pools that require retimers\nand a switch.\nsome workloads is insensitive to disaggregated memory\nlatency, some are heavily impacted. This motivates our\ndesign decision to include socket-local DRAM alongside\npool DRAM to mitigate CXL latency impact for those\nlatency-sensitive workloads. Memory pooling solutions\ncan be effective if they\u2019re are effective at identi\ufb01ying\nsensitive workloads.\n4.Pond Design\nOur measurements and observations at Azure (\u00a7 2\u20133) lead\nus to de\ufb01ne the following design goals.\nG1Performance comparable to NUMA-local DRAM\nG2Compatibility with virtualization accelerators\nG3Compatibility with opaque VMs and unchanged\nguest OSes/applications\nG4Low host resource overhead\nTo quantify (G1), we de\ufb01ne a performance degrada-\ntion margin (PDM) for a given workload as the allowable\nslowdown relative to running the workload entirely on\nNUMA-local DRAM. Pond seeks to achieve a con\ufb01g-\nurable PDM,e.g., 1%, for a con\ufb01gurable tail-percentage\n(TP) of VMs, e.g., 98% (\u00a7 3.1). To achieve this high per-\nformance, Pond uses a small but fast CXL pool (\u00a7 4.1).\nAs Pond\u2019s memory savings come from pooling instead of\noversubscription, Pond must minimize pool fragmenta-\ntion and wastage in its system software layer (\u00a7 4.2). To\nachieve (G2), Pond preallocates local and pool memory\nat VM start. Pond decides this allocation in its allocation,\nperformance monitoring, and mitigation pipeline (\u00a7 4.3).\nThis pipeline uses novel prediction models to achieve the\nPDM(\u00a74.4). Finally, Pond overcomes VM-opaqueness\n(G3) and host-overheads (G4) using lightweight hardware\ncounter telemetry.\n5Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n0100200300181 6 3 264Pool Size [Sockets]Access Latency [ns]Switches onlyPond with multi-headed design1 SwitchNUMA-local latency baseline2 SwitchesNo switches1 Switch-36%\nFigure 8: Pool access latency comparison (\u00a7 4.1).Pond\nreduces latencies by 1/3 compared to switch-only designs.\n4.1.Hardware Layer\nHosts within a Pond pool have separate cache coherency\ndomains and run separate hypervisors. Pond uses an\nownership model where pool memory is explicitly moved\namong hosts. A new external memory controller (EMC)\nASIC implements the pool using multiple DDR5 channels\naccessed through a collection of CXL ports running at\nPCIe 5 speeds.\nEMC memory management. The EMC offers mul-\ntiple CXL ports and appears to each host as a single\nlogical memory device [ 59,60]. In CXL 3.0 [ 61,62],\nthis con\ufb01guration is standardized as multi-headed device\n(MHD) [ 62, \u00a72.5]. The EMC exposes its entire capacity\non each port ( e.g., to hosts) via a Host-managed Device\nMemory (HDM) decoder. Hosts program each EMC\u2019s\naddress range but treat them initially as of\ufb02ine. Pond\ndynamically assigns memory at the granularity of 1GB\nmemory slices. Each slice is assigned to at most one\nhost at a given time and hosts are explicitly noti\ufb01ed about\nchanges (\u00a7 4.2). Tracking 1024 slices (1TB) and 64 hosts\n(6 bits) requires 768B of EMC state. The EMC imple-\nments dynamic slice assignment by checking permission\nof each memory access, i.e., whether requestor and owner\nof the cacheline\u2019s slice match. Disallowed accesses result\nin fatal memory errors.\nEMC ASIC design. The EMC offers multiple \u21e58-CXL\nports, which communicate with DDR5 memory con-\ntrollers (MC) via an on-chip network (NOC). The MCs\nmust offer the same reliability, availability, and service-\nability capabilities [ 67,68] as server-grade memory con-\ntrollers including memory error correction, management,\nand isolation. A key design parameter of Pond\u2019s EMC is\nthe pool size, which de\ufb01nes the number of CPU sockets\nable to use pool memory.", "start_char_idx": 491469, "end_char_idx": 495205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c5546bf-644d-4c3d-bc04-821d9fe301d8": {"__data__": {"id_": "8c5546bf-644d-4c3d-bc04-821d9fe301d8", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fe6f05b-2220-444b-b458-4f60e288f309", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e4038436923816cfa1814b29deb355766237e6539f3e494c53f737959c20868b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ad1ca74-d927-4467-9603-8209d5b0c177", "node_type": "1", "metadata": {}, "hash": "01d308fa47be98ad42a609c3cd43b41ff09c97ff4ff2f6c4b6d6a419d31b8c9a", "class_name": "RelatedNodeInfo"}}, "text": "Tracking 1024 slices (1TB) and 64 hosts\n(6 bits) requires 768B of EMC state. The EMC imple-\nments dynamic slice assignment by checking permission\nof each memory access, i.e., whether requestor and owner\nof the cacheline\u2019s slice match. Disallowed accesses result\nin fatal memory errors.\nEMC ASIC design. The EMC offers multiple \u21e58-CXL\nports, which communicate with DDR5 memory con-\ntrollers (MC) via an on-chip network (NOC). The MCs\nmust offer the same reliability, availability, and service-\nability capabilities [ 67,68] as server-grade memory con-\ntrollers including memory error correction, management,\nand isolation. A key design parameter of Pond\u2019s EMC is\nthe pool size, which de\ufb01nes the number of CPU sockets\nable to use pool memory. We \ufb01rst observe that the EMC\u2019s\nIO, (De)Serializer, and MC requirements resemble AMD\nGenoa\u2019s 397mm2IO-die (IOD) [ 42,66]. Figure 6shows\nthat EMC requirements for a 16-socket Pond parallel the\nIOD\u2019s requirements, with a small 8-socket Pond paral-\nleling half an IOD. Thus, up to 16-sockets can directly\nconnect to an EMC. Pool sizes of 32-64 would combine\nCXL switches with Pond\u2019s multi-headed EMC. The opti-\nHost 1(H1)Host 2(H2)EMC 1Hosts map local andEMC memory at boot \nPoolManager(PM)Usedby VM1Usedby VM2VM2leaves\nVMschedulerTimet=3t=2t=1t=0H1H1H1H1H1H1H2Releasecapacity(H1,y)xyxyxyNew VM(1GB onpool)Addcapacity(H2,y)H1t=4\nUsedby VM3xyLocalDRAMEMCmemorystarts inof\ufb02inestatexyAdd capacityevents leadthe host OSto online theassociated1GB sliceFigure 9: Pool management example (\u00a7 4.2).Pond assigns\npool memory to at most one host at a time. This example shows\nPond\u2019s asynchronous memory release strategy which engages\nwhen a VM departs ( t=1 and t=2). During VM scheduling,\nmemory is added to the corresponding host before the VM starts\n(t=3 and t=4).\nmal design point balances the potential pool savings for\nlarger pool sizes (\u00a7 6) with the added cost of larger EMCs\nand switches.\nEMC Latency. While latency is affected by propaga-\ntion delays, it is dominated by CXL port latency, and\nany use of CXL retimers and CXL switches. Port laten-\ncies are discussed in \u00a7 2and [ 63]. Retimers are devices\nused to maintain CXL/PCIe signal integrity over longer\ndistances and add about 10ns of latency in each direc-\ntion [ 69,70]. In datacenter conditions, signal integrity\nsimulations [ 71] indicate that CXL could require retimers\nabove 500mm. Switches add at least 70ns of latency due\nto ports/arbitration/NOC with estimates above 100ns [ 72].\nFigure 7breaks down Pond\u2019s latency for different pool\nsizes. Figure 8compares Pond\u2019s latency to a design that\nrelies only on switches instead of a multi-headed EMC.\nWe \ufb01nd that Pond reduces latencies by 1/3 with 8-and\n16-socket pools adding only 70-90ns relative to NUMA-\nlocal DRAM. In practice, we expect Pond to be deployed\nprimarily with small 8/16-socket pools, given the latency\nand cost overheads, and diminishing returns of larger\npools (\u00a7 3). Modern CPUs can connect to multiple EMCs\nwhich allows scaling to meet bandwidth and capacity\ngoals for different clusters.\n4.2.System Software Layer\nPond\u2019s system software involves multiple components.\nPool memory ownership. Pool management involves\nassigning Pond\u2019s memory slices to hosts and reclaiming\nthem for the pool (Figure 9). It involves 1) implementing\nthe control paths for pool-level memory assignment and\n2) preventing pool memory fragmentation.\n6Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\nFigure 10: zNUMA (\u00a7 4.2).zNUMA seen from a Linux VM.", "start_char_idx": 494465, "end_char_idx": 498050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ad1ca74-d927-4467-9603-8209d5b0c177": {"__data__": {"id_": "3ad1ca74-d927-4467-9603-8209d5b0c177", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c5546bf-644d-4c3d-bc04-821d9fe301d8", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6d8b3bf408ea24db1c277345297c7327549641d64ea121522fb60a904376c5fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb57fbb8-c588-4ef3-bfa7-5368b8056ec8", "node_type": "1", "metadata": {}, "hash": "aab1c90c1fe4d2614982b2fd0859c3716e852259a62ab1e19462fc6a21131e83", "class_name": "RelatedNodeInfo"}}, "text": "In practice, we expect Pond to be deployed\nprimarily with small 8/16-socket pools, given the latency\nand cost overheads, and diminishing returns of larger\npools (\u00a7 3). Modern CPUs can connect to multiple EMCs\nwhich allows scaling to meet bandwidth and capacity\ngoals for different clusters.\n4.2.System Software Layer\nPond\u2019s system software involves multiple components.\nPool memory ownership. Pool management involves\nassigning Pond\u2019s memory slices to hosts and reclaiming\nthem for the pool (Figure 9). It involves 1) implementing\nthe control paths for pool-level memory assignment and\n2) preventing pool memory fragmentation.\n6Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\nFigure 10: zNUMA (\u00a7 4.2).zNUMA seen from a Linux VM.\nHosts discover local and pool capacity through CXL de-\nvice discovery and map them to their address space. Once\nmapped, the pool address range is marked hot-pluggable\nand \u201cnot enabled.\u201d Slice assignment is controlled at run-\ntime via a Pool Manager (PM) that is colocated on the\nsame blade as the EMCs (Figure 7). In Pond\u2019s current\ndesign, the PM is connected to EMCs and CPU sockets\nvia a low-power management bus ( e.g.,[73]). To allocate\npool memory, the Pool Manager triggers two types of in-\nterrupts at the EMC and host driver. Add_capacity(host,\nslice) interrupts the host driver which reads the address\nrange to be hot-plugged. The driver then communicates\nwith the OS memory manager to bring the memory on-\nline. The EMC adds the host id to its permission table at\nthe slice offset. Release_capacity(host, slice) works\nsimilarly by of\ufb02ining the slice on the host and resetting\nthe slice\u2019s permission table entry on the EMC. An al-\nternative to this design would be inband-communication\nusing the Dynamic Capacity Device (DCD) feature in\nCXL 3.0 [ 62, \u00a79.13]. This change would maintain the\nsame functionality for Pond.\nPond must avoid fragmenting its online pool memory\nas the contiguous 1GB address range must be free before\nit can be of\ufb02ined for reassignment to another host. Pool\nmemory is allocated to VMs in 1GB-aligned increments\n(\u00a74.3). While this prevents fragmentation due to VM\nstarts and completions, our experience has shown that host\nagents and drivers can allocate pool memory and cause\nfragmentation. Pond thus uses a special-purpose memory\npartition that is only available to the hypervisor. Host\nagents and drivers allocate memory in host-local memory\npartition, which effectively contains fragmentation.\nWith these optimizations, of\ufb02ining 1GB slices empir-\nically takes 10-100 milliseconds/GB. Onlining memory\nis near instantaneous with microseconds/GB. These ob-\nservations are re\ufb02ected in Pond\u2019s asynchronous release\nstrategy (\u00a7 4.3).\nFailure management. Hosts only interleave across local\nmemory. This minimizes the EMCs\u2019 blast radius and\nfacilitate memory hot-plugging. EMC failures affect only\nVMs with memory on that EMC, while VMs with memory\non other EMCs continue normally. CPU/host failures are\nisolated and associated pool memory is reallocated to\nother hosts. Pool Manager failures prevent reallocating\npool memory but do not affect the datapath.\nExposing pool memory to VMs. VMs that use both\nVMschedulerMLSystemPoolManagerA1A2A5VM1Server BladesVM2HypervisorMemory PoolsEMCQoSMonitorMitigationManagerVM requestA3A4B1B2B3WorkloadpredictionFigure 11: Pond control plane work\ufb02ow (\u00a7 4.3). A) The\nVM scheduler uses ML-based predictions that identify latency-\nsensitive VMs and their likely amount of untouched memory to\ndecide on VM placement (see Figure 13).B)The monitoring\npipeline recon\ufb01gures VMs if quality-of-service (QoS) not met.\nNUMA-local and pool memory see pool memory as a\nzNUMA node. The hypervisor creates a zNUMA node by\nadding a memory block ( node_memblk ) without an entry\nin the node_cpuid in the SLIT/SRAT tables [ 74]. We\nlater show the guest-OS preferentially allocates memory\nfrom the local NUMA node before going to zNUMA (\u00a7 6).", "start_char_idx": 497229, "end_char_idx": 501239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb57fbb8-c588-4ef3-bfa7-5368b8056ec8": {"__data__": {"id_": "fb57fbb8-c588-4ef3-bfa7-5368b8056ec8", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ad1ca74-d927-4467-9603-8209d5b0c177", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8842d4474afd15da0ab394c8935feaea284a8729924646b1c388029c060db175", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7063425c-d2a7-437e-b7c7-08499595b055", "node_type": "1", "metadata": {}, "hash": "5b9c48cf824dfe6a3d2c79d4b485b550111f21753e523d9258bded68dd8eb492", "class_name": "RelatedNodeInfo"}}, "text": "A) The\nVM scheduler uses ML-based predictions that identify latency-\nsensitive VMs and their likely amount of untouched memory to\ndecide on VM placement (see Figure 13).B)The monitoring\npipeline recon\ufb01gures VMs if quality-of-service (QoS) not met.\nNUMA-local and pool memory see pool memory as a\nzNUMA node. The hypervisor creates a zNUMA node by\nadding a memory block ( node_memblk ) without an entry\nin the node_cpuid in the SLIT/SRAT tables [ 74]. We\nlater show the guest-OS preferentially allocates memory\nfrom the local NUMA node before going to zNUMA (\u00a7 6).\nThus, if zNUMA is sized to the amount of untouched\nmemory, it is never going to be used. Figure 10shows the\nview of a Linux VM which includes the correct latency in\nthe NUMA distance matrix ( numa_slit ). This facilitates\nguest-OS NUMA-aware memory management [ 75,76]\nfor the rare case that the zNUMA is used (\u00a7 4.4).\nRecon\ufb01guration of memory allocation. To remain com-\npatible with ( G2), local and pool memory mapping gen-\nerally remain static during a VM\u2019s lifetime. There are\ntwo exceptions that are implemented today. When live-\nmigrating a VM or when remapping a page with a memory\nfault, the hypervisor temporarily disables virtualization ac-\nceleration and the VM falls back to a slower I/O path [ 77].\nBoth events are quick and transient and typically only\nhappen once during a VM\u2019s lifetime. We implement a\nthird variant which allows Pond a one-time correction to\na suboptimal memory allocation. Speci\ufb01cally, if the host\nhas local memory available, Pond disables the accelera-\ntor, copies all of the VM\u2019s memory to local memory, and\nenables the accelerator again. This takes about 50ms for\nevery GB of pool memory that Pond allocated to the VM.\nTelemetry for opaque VMs. Pond requires two types of\ntelemetry for VMs. First, we use the core-performance-\nmeasurement-unit (PMU) to gather hardware counters\nrelated to memory performance. Speci\ufb01cally, we use the\ntop-down-method for analysis (TMA) [ 78,79]. TMA\ncharacterizes how the core pipeline slots are used. For\nexample, we use the \u201cmemory-bound\u201d metric, which is\nde\ufb01ned as pipeline stalls due to memory loads and stores.\nFigure 12lists these metrics. While TMA was developed\nfor Intel, its relevant parts are available on AMD and\nARM as well [ 80]. We modify Azure\u2019s production hy-\npervisor to associate these metrics with individual VMs\n(\u00a75) and record VM counter samples in a distributed\n7Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\nCPUTMA pipeline slot for: backend-bound,memory-bound,store-bound,DRAM-latency-boundOther counters: LLC MPI,memory bandwidth,memory parallelismOf\ufb02inetestrunsA/B testson internalworkloadsRelative slowdown if on pool memoryModel trainingMetricsFeaturesLabelsLatency insensitive?CorePMU\nFigure 12: Metrics and training of the latency insensi-\ntive model (\u00a7 4.2).This model uses metrics from the core\u2019s\nperformance-measurement-unit (PMU). It is trained with labels\ngathered from of\ufb02ine runs and internal workloads.\ndatabase. All our core-PMU-metrics use simple coun-\nters and induce negligible overhead (unlike event-based\nsampling [ 81,82]).\nSecond, we use hypervisor telemetry to track a VM\u2019s\nuntouched pages. We use an existing hypervisor counter\nthat tracks guest-committed memory, which overestimates\nused memory. This counter is available for 98% of Azure\nVMs. We also scan access bits in the hypervisor page ta-\nble (\u00a7 5). Since we only seek untouched pages, frequently\naccess bits reset is not required. This minimizes overhead.\n4.3.Distributed Control Plane Layer\nFigure 11shows the two tasks performed by Pond\u2019s con-\ntrol plane: (A) predictions to allocate memory during VM\nscheduling and (B) QoS monitoring and resolution.\nPredictions and VM scheduling (A). Pond uses ML-\nbased prediction models (\u00a7 4.4) to decide how much pool\nmemory to allocate for a VM during scheduling.", "start_char_idx": 500676, "end_char_idx": 504616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7063425c-d2a7-437e-b7c7-08499595b055": {"__data__": {"id_": "7063425c-d2a7-437e-b7c7-08499595b055", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb57fbb8-c588-4ef3-bfa7-5368b8056ec8", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ba3eed62e55fd073b007834d1d5bc091f755fb4c7e349f81f9590cdc1cbac864", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32132b13-671e-400b-ac63-5e4e8baf9ebe", "node_type": "1", "metadata": {}, "hash": "dabaef54855641db26fb26504de08340022912033d5a37406bcf366c7e391ca0", "class_name": "RelatedNodeInfo"}}, "text": "Second, we use hypervisor telemetry to track a VM\u2019s\nuntouched pages. We use an existing hypervisor counter\nthat tracks guest-committed memory, which overestimates\nused memory. This counter is available for 98% of Azure\nVMs. We also scan access bits in the hypervisor page ta-\nble (\u00a7 5). Since we only seek untouched pages, frequently\naccess bits reset is not required. This minimizes overhead.\n4.3.Distributed Control Plane Layer\nFigure 11shows the two tasks performed by Pond\u2019s con-\ntrol plane: (A) predictions to allocate memory during VM\nscheduling and (B) QoS monitoring and resolution.\nPredictions and VM scheduling (A). Pond uses ML-\nbased prediction models (\u00a7 4.4) to decide how much pool\nmemory to allocate for a VM during scheduling. After\na VM request arrives (A1), the scheduler queries the dis-\ntributed ML serving system (A2) for a prediction on how\nmuch local memory to allocate for the VM. The scheduler\nthen informs the Pool Manager about the target host and\nassociated pool memory needs (A3). The Pool Manager\ntriggers a memory onlining work\ufb02ow using the con\ufb01gu-\nration bus to the EMCs and host (A4). Memory onlining\nis fast enough to not block a VM\u2019s start time (\u00a7 4.2). The\nscheduler informs the hypervisor to start the VM on a\nzNUMA node matching the onlined memory amount.\nMemory of\ufb02ining is slow and cannot happen on the\ncritical path of VM starts (\u00a7 4.2). Pond resolves this by\nalways keeping a buffer of unallocated pool memory. This\nbuffer is replenished when VMs terminate and hosts asyn-\nchronously release associated slices.\nQoS monitoring (B). Pond continuously inspects the per-\nformance of all running VMs via its QoS monitor. The\nmonitor queries hypervisor and hardware performance\ncounters (B1) and uses an ML model of latency sensi-\ntivity (\u00a7 4.4) to decide whether the VM\u2019s performance\nimpact exceeds the PDM. In this case, the monitor asks\nits mitigation manager (B2) to trigger a memory recon-\nHypervisoraccess bitsCorePMUCPUWorkload history?DecisionLatency insensitive?Untouched memory?Prediction model\nEntirely pool DRAMPool DRAM=untouchedEntirely local DRAMActionYesNoYesNoYesNo\nLatency insensitive?NoYesRecon\ufb01guration mitigation(A) VM scheduling(B) QoS monitoringOverpredicted untouched?Continue monitoringNoYesVM type, OS, Region,Percentiles of memoryusage in previous VMby same Customer,Workload name.VM Metadata\nCorePMUCPUFigure 13: Pond prediction models (\u00a7 4.4). Pond\u2019s two\nprediction models (dark grey) rely on telemetry (blue boxes) that\nis available for all VM types, including third-party opaque VMs.\n\ufb01guration (\u00a7 4.2) through the hypervisor (B3). After this\nrecon\ufb01guration, the VM uses only local memory.\n4.4.Prediction Models\nPond\u2019s VM scheduling (A) and QoS monitoring (B) algo-\nrithms rely on two prediction models (in Figure 13).\nPredictions for VM scheduling (A). For scheduling, we\n\ufb01rst check if we can correlate a workload history with the\nVM requested. This works by checking if there have been\nprevious VMs with the same metadata as the request VM,\ne.g., the customer-id, VM type, and location. This is based\non the observation that VMs from the same customer tend\nto exhibit similar behavior [ 48].\nIf we have prior workload history, we make a predic-\ntion on whether this VM is likely to be memory latency\ninsensitive, i.e., its performance would be within the PDM\nwhile using only pool memory. (Model details appear\nbelow.) Latency-insensitive VMs are allocated entirely\non pool DRAM.\nIf the VM has no workload history or is predicted to\nbe latency-sensitive, we predict untouched memory ( UM)\nover its lifetime. Interestingly, UMpredictions with only\ngeneric VM metadata such as customer history, VM type,\nguest OS, and location are accurate (\u00a7 6). VMs without\nuntouched memory ( UM=0) are allocated entirely with\nlocal DRAM. VMs with a UM>0are allocated with a\nrounded-down GB-aligned percentage of pool memory\nand a corresponding zNUMA node; the remaining mem-\nory is allocated on local DRAM.", "start_char_idx": 503874, "end_char_idx": 507832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32132b13-671e-400b-ac63-5e4e8baf9ebe": {"__data__": {"id_": "32132b13-671e-400b-ac63-5e4e8baf9ebe", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7063425c-d2a7-437e-b7c7-08499595b055", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "fffaca73eacd3e0738718738060bf77e23b7c56839bd8eac9d60ce88141a845c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71ea6d5c-e554-49bf-b7cb-ed90207bd8ba", "node_type": "1", "metadata": {}, "hash": "74954b2491ea0f38b3b712547a3d04d9e98ddcb4f9996b66ac92106029ce27ec", "class_name": "RelatedNodeInfo"}}, "text": "If we have prior workload history, we make a predic-\ntion on whether this VM is likely to be memory latency\ninsensitive, i.e., its performance would be within the PDM\nwhile using only pool memory. (Model details appear\nbelow.) Latency-insensitive VMs are allocated entirely\non pool DRAM.\nIf the VM has no workload history or is predicted to\nbe latency-sensitive, we predict untouched memory ( UM)\nover its lifetime. Interestingly, UMpredictions with only\ngeneric VM metadata such as customer history, VM type,\nguest OS, and location are accurate (\u00a7 6). VMs without\nuntouched memory ( UM=0) are allocated entirely with\nlocal DRAM. VMs with a UM>0are allocated with a\nrounded-down GB-aligned percentage of pool memory\nand a corresponding zNUMA node; the remaining mem-\nory is allocated on local DRAM.\nIf we underpredict UM, the VM will not touch the slower\npool memory as the guest OS prioritizes allocating local\nDRAM. If we overpredict UM, we rely on the QoS monitor\nfor mitigation. Importantly, Pond always keeps a VM\u2019s\nmemory mapped in hypervisor page tables at all times.\nThis means that even if our predictions happen to be\nincorrect, performance does not fall off a cliff.\n8Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\nVM: memory, cores, OS;Location: region,  availability zone;Customer history fromprevious VMs:  0/25/50/75/100   percentile untouched;Workload nameGuest-committedmemory counterAccess bits neverset since VM startUntouched memory percentageModel trainingMetadataFeaturesLabelsUntouched memory?Hypervisorpage tablesAddress   A M\nFigure 14: Training of the untouched memory model (\u00a7 4.4).\nThis model uses VM metadata as features and labels of un-\ntouched memory gathered from hypervisor telemetry.\nQoS monitoring (B). For zNUMA VMs, Pond monitors\nif it overpredicted the amount of untouched memory dur-\ning scheduling. For pool-backed VMs and zNUMA VMs\nwith less untouched memory than predicted, we use the\nsensitivity model to determine whether the VM workload\nis suffering excessive performance loss. If not, the QoS\nmonitor initiates a live VM migration to a con\ufb01guration\nallocated entirely on local DRAM.\nModel details. Pond\u2019s two ML prediction models con-\nsume telemetry that is available for opaque VMs from\nPond\u2019s system software layer (\u00a7 4.2). Figure 12shows\nfeatures, labels, and the training procedure for the latency\ninsensitivity model. The model uses supervised learning\n(\u00a75) with core-PMU metrics as features and the slowdown\nof pool memory relative to NUMA-local memory as la-\nbels. Pond gets samples of slowdowns from of\ufb02ine test\nruns and A/B tests of internal workloads which make their\nperformance numbers available. These feature-label-pairs\nare used to retrain the model daily. As the core-PMU is\nlightweight (\u00a7 5), Pond continuously measures core-PMU\nmetrics at VM runtime. This enable the QoS monitor to\nreact quickly and enables retaining a history of VMs that\nhave been latency sensitive.\nFigure 14shows the inputs and training procedure for\nthe untouched-memory model. The model uses super-\nvised learning (details in \u00a7 5) with VM metadata as fea-\ntures and the minimum untouched memory over each\nVM\u2019s lifetime as labels. Its most important feature is\na range of percentiles ( e.g., 80th\u201399th) of the recorded\nuntouched memory by a customer\u2019s VMs in the last week.\nParameterization of prediction models. Pond\u2019s latency\ninsensitivity model is parameterized to stay below a target\nrate of false positives (FP),i.e., workloads it incorrectly\nspeci\ufb01es as latency insensitive but which are actually\nsensitive to memory latency. This parameter enforces a\ntradeoff as the percentage of workloads that are labeled as\nlatency insensitive ( LI) is a function of FP. For example,\na rate of 0.1% FPmay force the model to 5% of LI.\nSimilarly, Pond\u2019s untouched memory model is parame-\nterized to stay below a target rate of overpredictions (OP),\ni.e., workloads that touch more memory than predictedand thus would use memory pages on the zNUMA node.\nThis parameter enforces a tradeoff as the percentage of\nuntouched memory ( UM) is a function of OP.", "start_char_idx": 507034, "end_char_idx": 511209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71ea6d5c-e554-49bf-b7cb-ed90207bd8ba": {"__data__": {"id_": "71ea6d5c-e554-49bf-b7cb-ed90207bd8ba", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32132b13-671e-400b-ac63-5e4e8baf9ebe", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5d774b2bdbbb0bc38b8d82dbcb897fb3ef391653254e1dbbded4de4f4170ecb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8e3e49d-e209-46da-8ca0-2ec2e1651030", "node_type": "1", "metadata": {}, "hash": "69fda51b10d07ff9b6c2f05ba8c7627ed70605284e42921a2d3b3e9a57a64486", "class_name": "RelatedNodeInfo"}}, "text": "Parameterization of prediction models. Pond\u2019s latency\ninsensitivity model is parameterized to stay below a target\nrate of false positives (FP),i.e., workloads it incorrectly\nspeci\ufb01es as latency insensitive but which are actually\nsensitive to memory latency. This parameter enforces a\ntradeoff as the percentage of workloads that are labeled as\nlatency insensitive ( LI) is a function of FP. For example,\na rate of 0.1% FPmay force the model to 5% of LI.\nSimilarly, Pond\u2019s untouched memory model is parame-\nterized to stay below a target rate of overpredictions (OP),\ni.e., workloads that touch more memory than predictedand thus would use memory pages on the zNUMA node.\nThis parameter enforces a tradeoff as the percentage of\nuntouched memory ( UM) is a function of OP. For example,\na rate of 0.1% OPmay force the model to 3% of UM.\nWith two models and their respective parameters, Pond\nneeds to decide how to balance FPandOPbetween the\ntwo models. This balance is done by solving an optimiza-\ntion problem based on the given performance degradation\nmargin ( PDM) and the target percentage of VMs that meet\nthis margin ( TP). Speci\ufb01cally, Pond seeks to maximize\nthe average amount of memory that is allocated on the\nCXL pool, which is de\ufb01ned by LIandUM, while keep-\ning the percentage of false positives ( FP) and untouched\noverpredictions ( OP) below the TP.\nmaximize (LIPDM)+(UM)\nsubject to (FPPDM)+(OP)\uf8ff(100\u0000TP) (1)\nNote that TPessentially de\ufb01nes how often the QoS moni-\ntor has to engage and initiate memory recon\ufb01gurations.\nBesides PDMandTP, Pond has no other parameters as\nit automatically solves the optimization problem from\nEq.(1). The models rely on their respective framework\u2019s\ndefault hyperparameters (\u00a7 5).\n5.Implementation\nWe implement and evaluate Pond on production servers\nthat emulate pool latency. Pond artifacts are open-sourced\nathttps://github.com/vtess/Pond .\nSystem software. This implementation comprises three\nparts. First, we emulate a single-socket system with a\nCXL pool on a two-socket server by disabling all cores\nin one socket, while keeping its memory accessible from\nthe other socket. This memory mimics the pool.\nSecond, we change Azure\u2019s hypervisor to instantiate\narbitrary zNUMA topologies. We extend the API between\nthe control plane and the host to pass the desired zNUMA\ntopology to the hypervisor.\nThird, we implement support in Azure\u2019s hypervisor for\nthe telemetry required for training Pond\u2019s models. We\nextend each virtual core\u2019s metadata with a copy of its core-\nPMU state and transfer this state when it gets scheduled\non different physical cores. Pond samples core-PMU\ncounters once per second, which takes 1ms. We enable\naccess bit scanning in hypervisor page tables. We scan\nand reset access bits every 30 minutes, which takes 10s.\nDistributed control plane. We train our prediction mod-\nels by aggregating daily telemetry into a central database.\nThe latency insensitivity model uses a simple random\nforest (RandomForest) from Scikit-learn [ 83] to classify\nwhether a workload exceeds the PDM. The model uses a\nset of 200 hardware counters as supported by current Intel\nprocessors. The untouched memory model uses a gradient\n9Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\nboosted regression model (GBM) from LightGBM [ 84]\nand makes a quantile regression prediction with a con\ufb01g-\nurable target percentile. After exporting to ONNX [ 85],\nthe prototype adds the prediction (the size of zNUMA)\non the VM request path using a custom inference serv-\ning system similar to [ 86\u201388]. Azure\u2019s VM scheduler\nincorporates zNUMA requests and pool memory as an\nadditional dimension into its bin packing, similar to other\ncluster schedulers [ 49,89\u201393].\n6.Evaluation\nOur evaluation addresses the performance of zNUMA\nVMs (\u00a7 6.2,\u00a76.3), the accuracy of Pond\u2019s prediction mod-\nels (\u00a7 6.4), and Pond\u2019s end-to-end DRAM savings (\u00a7 6.5).\n6.1.Experimental Setup\nWe evaluate the performance of our prototype using\n158 cloud workloads.", "start_char_idx": 510439, "end_char_idx": 514497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8e3e49d-e209-46da-8ca0-2ec2e1651030": {"__data__": {"id_": "f8e3e49d-e209-46da-8ca0-2ec2e1651030", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71ea6d5c-e554-49bf-b7cb-ed90207bd8ba", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "2d82d9a84656a369bea77bc9cdfedf6753c7ef14ab0724269894ca1ef208f81c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ade1b98-f700-472b-92bd-2e1ff740d4be", "node_type": "1", "metadata": {}, "hash": "45e2d4b5684d6545c584717e0b1095fe87d903d96f84267982f1561287752610", "class_name": "RelatedNodeInfo"}}, "text": "After exporting to ONNX [ 85],\nthe prototype adds the prediction (the size of zNUMA)\non the VM request path using a custom inference serv-\ning system similar to [ 86\u201388]. Azure\u2019s VM scheduler\nincorporates zNUMA requests and pool memory as an\nadditional dimension into its bin packing, similar to other\ncluster schedulers [ 49,89\u201393].\n6.Evaluation\nOur evaluation addresses the performance of zNUMA\nVMs (\u00a7 6.2,\u00a76.3), the accuracy of Pond\u2019s prediction mod-\nels (\u00a7 6.4), and Pond\u2019s end-to-end DRAM savings (\u00a7 6.5).\n6.1.Experimental Setup\nWe evaluate the performance of our prototype using\n158 cloud workloads. Speci\ufb01cally, our workloads\nspan in-memory databases and KV-stores (Redis [ 94],\nVoltDB [ 95], and TPC-H on MySQL [ 96]), data and\ngraph processing (Spark [ 97] and GAPBS [ 98]), HPC\n(SPLASH2x [ 99]), CPU and shared-memory benchmarks\n(SPEC CPU [ 100] and PARSEC [ 101]), and a range\nof Azure\u2019s internal workloads (Proprietary). Figure 4\noverviews these workloads. We quantify DRAM savings\nwith simulations.\nPrototype setup. We run experiments on production\nservers at Azure and similarly-con\ufb01gured lab servers. The\nproduction servers use either two Intel Skylake 8157M\nsockets with each 384GB of DDR4, or two AMD EPYC\n7452 sockets with each 512GB of DDR4. On Intel, we\nmeasure 78ns NUMA-local latency and 80GB/s band-\nwidth and 142ns remote latency and 30GB/s bandwidth\n(3/4 of a CXL \u21e58 link). On AMD, we measure 115ns\nNUMA-local latency and 255ns remote latency. Our\nBIOS disables hyper-threading, turbo-boost, and C-states.\nWe use performance results of VMs entirely backed\nby NUMA-local DRAM as our baseline . We present\nzNUMA performance as normalized slowdowns, i.e., the\nratio to the baseline. Performance metrics are workload\nspeci\ufb01c, e.g., job runtime, throughput and tail latency, etc.\nEach experiment involves running the application with\none of 7 zNUMA sizes (as percentages of the workload\u2019s\nmemory footprint in Figure 16). With at least three rep-\netitions of each run and 158 workloads, our evaluation\nspans more than 3,500 experiments and 10,000 machine\nhours. Most experiments used lab servers; we spot check\noutliers on production servers.\nSimulations. Our simulations are based on traces of pro-\nduction VM requests and their placement on servers. The\ntraces are from randomly selected 100 clusters across 34\ndatacenters globally over 75 days.\nWorkloads Traf\ufb01c to zNUMA\nVideo 0.25%\nDatabase 0.06%\nKV store 0.11%\nAnalytics 0.38%\nFigure 15: Effectiveness of zNUMA (\u00a7 6.2).Latency sensi-\ntive workloads get a local vNUMA node large enough to cover\nthe workload\u2019s footprint. zNUMA nodes holds the VM\u2019s remain-\ning memory on Pond CXL pool. Access bit scans, e.g., for Video\n(right), show that this con\ufb01guration indeed minimizes traf\ufb01c to\nthe zNUMA node.\nThe simulator implements different memory allocation\npolicies and tracks each server and each pool\u2019s memory\ncapacity at second accuracy. Generally, the simulator\nschedules VMs on the same nodes as in the trace and\nchanges their memory allocation to match the policy. For\nrare cases where a VM does not \ufb01t on a server, e.g., due to\ninsuf\ufb01cient pool memory, the simulator moves the VMs\nto another server.\nModel evaluation. We evaluate our model with produc-\ntion resource logs. About 80% of VMs have suf\ufb01cient\nhistory to make a sensitivity prediction. Our deployment\ndoes not report each workload\u2019s perceived performance\n(opaque VMs). We thus evaluate latency sensitivity model\nbased on our 158 workloads.\n6.2.zNUMA VMs on Production Nodes\nWe perform a small-scale experiment on Azure produc-\ntion nodes to validate zNUMA VMs.", "start_char_idx": 513892, "end_char_idx": 517488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ade1b98-f700-472b-92bd-2e1ff740d4be": {"__data__": {"id_": "0ade1b98-f700-472b-92bd-2e1ff740d4be", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8e3e49d-e209-46da-8ca0-2ec2e1651030", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "4b3391cbb95d337d48a9f28a1531c1ef9132709a123fed1932c072d3931b975c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85904e96-d6fa-4d3e-a12d-30718b7725d0", "node_type": "1", "metadata": {}, "hash": "479312959a1d4ff91405a99e78e77c5052653d2607e1a11becbe53182e639497", "class_name": "RelatedNodeInfo"}}, "text": "Generally, the simulator\nschedules VMs on the same nodes as in the trace and\nchanges their memory allocation to match the policy. For\nrare cases where a VM does not \ufb01t on a server, e.g., due to\ninsuf\ufb01cient pool memory, the simulator moves the VMs\nto another server.\nModel evaluation. We evaluate our model with produc-\ntion resource logs. About 80% of VMs have suf\ufb01cient\nhistory to make a sensitivity prediction. Our deployment\ndoes not report each workload\u2019s perceived performance\n(opaque VMs). We thus evaluate latency sensitivity model\nbased on our 158 workloads.\n6.2.zNUMA VMs on Production Nodes\nWe perform a small-scale experiment on Azure produc-\ntion nodes to validate zNUMA VMs. The experiment\nevaluates four internal workloads: an audio/video confer-\nencing application, a database service, a key-value store,\nand a business analytics service. To see the effectiveness\nof zNUMA, we assume a correct prediction of untouched\nmemory, i.e., the local footprint \ufb01ts into the VM\u2019s local\nvNUMA node. Figure 15shows access bit scans over 48\nhours from the video workload and a table that shows the\ntraf\ufb01c to the zNUMA node for the four workloads.\nFinding 1. We \ufb01nd that zNUMA nodes are effective at\ncontaining the memory access to the local vNUMA node.\nA small fraction of accesses goes to the zNUMA node.\nWe suspect that this is in part due to the guest OS memory\nmanager\u2019s metadata that is explicitly allocated on each\n10Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n{\n0204060\n00 % 1 0 % 2 0 % 4 0 %60% 75% 100%Pool Memory [%]Slowdown [%]CorrectUntouchedPredictionAll local memoryOverprediction of untouched memory \nFigure 16: Slowdown under different pool allocations\n(\u00a76.3).Performance for no pool memory and a correctly-sized\nzNUMA is comparable. Small slowdowns arise from run-to-run\nvariation. Slowdown become noticable as soon as the workload\nspills into the zNUMA and steadily increases until the whole\nworkload is allocated on pool memory (100% spilled).\nvNUMA node. We \ufb01nd that the video workload sends\nfewer than 0.25% of memory accesses to the zNUMA\nnode. Similarly, the other three workloads send 0.06-\n0.38% of memory access to the zNUMA node. Accesses\nwithin the local vNUMA node are spread out\nImplications. With a negligible fraction of memory ac-\ncesses on zNUMA, we expect negligible performance\nimpact given a correct prediction of untouched memory.\n6.3.zNUMA VMs in the Lab\nWe scale up our evaluation to 158 workloads in a lab set-\nting. Since we fully control these workloads, we can now\nalso explicitly measure their performance. We rerun each\nworkload on all-local memory, a correctly sized zNUMA\n(0% spilled), differently-sized zNUMA nodes sized be-\ntween 10-100% of the workload\u2019s footprint. Figure 16\nshows a violin plot of associated slowdowns. This setup\ncovers both normal behavior (all-local and 0% spill) and\nmisprediction behavior for latency sensitive workloads.\nThus, this is effectively a sensitivity study.\nFinding 2. With a correct prediction of untouched mem-\nory, workload slowdowns have a similar distribution to\nall-local memory.\nImplications. This performance result is expected since\nthe zNUMA node is rarely accessed (\u00a7 6.2). Our evalua-\ntion can thus assume no performance impact under correct\npredictions of untouched memory (\u00a7 6.5).\nFinding 3. For overpredictions of untouched memory\n(and correspondingly undersized local vNUMA nodes),\nthe workload spills into zNUMA. Many workloads see\nan immediate impact on slowdown. Slowdowns further\nincrease if more workload memory spills into zNUMA.\nSome workloads are slowed down by as much as 30-35%\nwith 20-75% of workload memory spilled and up to 50%\nif entirely allocated on pool memory.", "start_char_idx": 516801, "end_char_idx": 520579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85904e96-d6fa-4d3e-a12d-30718b7725d0": {"__data__": {"id_": "85904e96-d6fa-4d3e-a12d-30718b7725d0", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ade1b98-f700-472b-92bd-2e1ff740d4be", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5e28f36de92895733e9981ba03bc1885fb414c44bd4cf16ef45aef4a21e10dad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f9b0d31-3ba2-4f83-9a0c-2bcec1c61d97", "node_type": "1", "metadata": {}, "hash": "920da28fb5093114eac02f82001f8e10e48041b3158c66fba15c0ca55f7ce131", "class_name": "RelatedNodeInfo"}}, "text": "Thus, this is effectively a sensitivity study.\nFinding 2. With a correct prediction of untouched mem-\nory, workload slowdowns have a similar distribution to\nall-local memory.\nImplications. This performance result is expected since\nthe zNUMA node is rarely accessed (\u00a7 6.2). Our evalua-\ntion can thus assume no performance impact under correct\npredictions of untouched memory (\u00a7 6.5).\nFinding 3. For overpredictions of untouched memory\n(and correspondingly undersized local vNUMA nodes),\nthe workload spills into zNUMA. Many workloads see\nan immediate impact on slowdown. Slowdowns further\nincrease if more workload memory spills into zNUMA.\nSome workloads are slowed down by as much as 30-35%\nwith 20-75% of workload memory spilled and up to 50%\nif entirely allocated on pool memory. We use access bit\nscans to verify that these workloads indeed actively access\n0102001 0 2 030 40 5060Workloads Insensitive to CXL Latency [%]False Positives:Slowdown > PDM [%]Memory-BoundDRAM-BoundRandomForestFigure 17: Latency insensitivity model (\u00a7 6.4).As we in-\ncrease how many workloads are marked as insensitive ( LI), the\nrate of false positives ( FP) increases. Pond\u2019s RandomForest\nslightly outperforms a heuristic based only on the DRAM-bound\nTMA performance counter.\ntheir entire working set.\nImplications. Allocating a \ufb01xed percentage of pool\nDRAM to VMs would lead to signi\ufb01cant performance\nslowdowns. There are only two strategies to reduce this\nimpact: 1) identify which workloads will see slowdowns\nand 2) allocate untouched memory on the pool. Pond\nemploys both strategies.\n6.4.Performance of Prediction Models\nWe evaluate Pond\u2019s prediction models (\u00a7 4.4) and its com-\nbined prediction model based on Eq.( 1).\n6.4.1. Predicting Latency Sensitivity Pond seeks to pre-\ndict whether a VM is latency insensitive, i.e., whether\nrunning the workload on pool memory would stay within\nthe performance degradation margin ( PDM). We tested the\nmodel for PDMbetween 1-10% and on both 182% and\n222% latency increases, but report details only for 5%\nand 182%. Other PDMvalues lead to qualitatively simi-\nlar results. The 222% model is 16% less effective given\nthe same false positive rate target. We compare thresh-\nolds on memory and DRAM boundedness [ 78,79] to our\nRandomForest (\u00a7 5).\nFigure 17shows the model\u2019s false positive rate as a\nfunction of the percentage of workloads labeled as latency\ninsensitive, similar to a precision-recall curve [ 102]. Error\nbars show 99% con\ufb01dence from a 100-fold validation\nbased on randomly splitting into equal-sized training and\ntesting datasets.\nFinding 4. While DRAM boundedness is correlated with\nslowdown, we \ufb01nd examples where high slowdown oc-\ncurs even for a small percentage of DRAM boundedness.\nFor example, multiple workloads exceed 20% slowdown\nwith just two percent of DRAM boundedness.\nImplication. This shows the general hardness of predict-\ning whether workloads exceed the PDM. Heuristic as well\nas predictors will make statistical errors.\nFinding 5. We \ufb01nd that \u201cDRAM bound\u201d signi\ufb01cantly\noutperforms \u201cMemory bound\u201d (Figure 17). Our Random-\n11Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n0510152025\n010 20 3040 50Average Untouched Memory [% GB-Hours]Overpredictions [% VMs]Fixed Amount / VMGBM Model 1GB-aligned\nFigure 18: Untouched memory model (\u00a7 6.4).As we in-\ncrease untouched memory ( UM), our GBM has a signi\ufb01cantly\nlower rate of overpredictions ( OP) that a strawman model.\nForest performs slightly better than \u201cDRAM bound\u201d.\nImplication. Our RandomForest can place 30% of work-\nloads on the pool with only 2% of false positives.\n6.4.2. Predicting Untouched Memory Pond predicts\nthe amount of untouched memory over a VM\u2019s future life-\ntime (\u00a7 4.4). We evaluate this model using metadata and\nresource usage logs from 100 clusters over 75 days.", "start_char_idx": 519796, "end_char_idx": 523685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f9b0d31-3ba2-4f83-9a0c-2bcec1c61d97": {"__data__": {"id_": "6f9b0d31-3ba2-4f83-9a0c-2bcec1c61d97", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85904e96-d6fa-4d3e-a12d-30718b7725d0", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "67ff560077ed30d89ee106b49742870ab3ae8e57afb3c46aaecac75517e185e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84f2cd68-89d1-43e4-9be9-7aeb8b5a54c5", "node_type": "1", "metadata": {}, "hash": "db6ec1415e9c2ba7ab55630489a697702d8721ad359ca8c4b2170e126a363627", "class_name": "RelatedNodeInfo"}}, "text": "Forest performs slightly better than \u201cDRAM bound\u201d.\nImplication. Our RandomForest can place 30% of work-\nloads on the pool with only 2% of false positives.\n6.4.2. Predicting Untouched Memory Pond predicts\nthe amount of untouched memory over a VM\u2019s future life-\ntime (\u00a7 4.4). We evaluate this model using metadata and\nresource usage logs from 100 clusters over 75 days. The\nmodel is trained nightly and evaluated on the subsequent\nday. Figure 18compares our GBM model to the heuristic\nthat assumes a \ufb01xed fraction of memory as untouched\nacross all VMs. The \ufb01gure shows the overprediction rate\nas a function of the average amount of untouched memory.\nFigure 19shows a production version of the untouched\nmemory model during the \ufb01rst 110 days of 2022.\nFinding 6. We \ufb01nd that the GBM model is 5\u21e5more\naccurate than the static policy, e.g., when labeling 20% of\nmemory as untouched, GBM overpredicts only 2.5% of\nVMs while the static policy overpredicts 12%.\nImplication. Our prediction model identi\ufb01es 25% of un-\ntouched memory while only overpredicting 4% of VMs.\nFinding 7. The production version of our model performs\nsimilarly to the simulated model. Distributional shifts lead\nto some variability over time.\nImplication. We \ufb01nd that accurately predicting un-\ntouched memory is practical and a realistic assumption.\n6.4.3. Combined Prediction Models We characterize\nPond\u2019s combined models (Eq. (1)) using \u201cscheduling mis-\npredictions\u201d, i.e., the fraction of VMs that will exceed the\nPDM. This incorporates the overpredictions of untouched\nmemory, how much the model overpredicted, and the\nprobability of this overprediction leading to a workload\nexceeding the PDM. Further, Pond uses its QoS monitor\nto mitigate up to 1% of mispredictions. Figure 20shows\nscheduling mispredictions as a function of the average\namount of cluster DRAM that is allocated on its pools for\n182% and 222% memory latency increases, respectively.\nFinding 8. Pond\u2019s combined model outperforms its in-\ndividual models by \ufb01nding their optimal combination.\n01020304050\n02 550 75100Day in 2022Average UntouchedMemory [%]0510Overpredictions[% of VMs]Figure 19: Untouched memory ML model performance in\nproduction (\u00a7 6.4).Our production model targets 4% over-\npredictions ( OP). It average untouched memory percentage is\nsimilar to the simulated model (Figure 18).\nImplication. With a 2% scheduling misprediction target,\nPond can schedule 44% and 35% of DRAM on pools with\n182% and 222% memory latency increases, respectively.\n6.5.End-to-end Reduction in Stranding\nWe characterize Pond\u2019s end-to-end performance while\nconstraining its rate of scheduling mispredictions. Fig-\nure21shows the reduction in aggregate cluster memory\nas a function of pool size for Pond under 182% and 222%\nmemory latency increase, respectively, and a strawman\nstatic allocation policy. We evaluate multiple scenarios;\nthe \ufb01gure shows PDM=5% and TP=98%. In this scenario,\nthe strawman statically allocates each VM with 15% of\npool DRAM. About 10% of VMs would touch the pool\nDRAM (Figure 18). Of those touching pool DRAM, we\u2019d\nexpect that about1\n4would see a slowdown exceeding a\nPDM=5% (Figure 16). So, the strawman would have about\n2.5% of scheduling mispredictions.\nFinding 9. At a pool size of 16 sockets, Pond reduces\noverall DRAM requirements by 9% and 7% under 182%\nand 222% latency increases, respectively. Static reduces\nDRAM by 3%. When varying PDMbetween 1 and 10%\nandTPbetween 90 and 99.9% we \ufb01nd the relative savings\nof the three systems to be qualitatively similar.\nImplication. Pond can safely reduce cost. A QoS mon-\nitor that mitigates more than 1% of mispredictions, can\nachieve more aggressive performance targets ( PDM).\nFinding 10.", "start_char_idx": 523318, "end_char_idx": 527012, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84f2cd68-89d1-43e4-9be9-7aeb8b5a54c5": {"__data__": {"id_": "84f2cd68-89d1-43e4-9be9-7aeb8b5a54c5", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f9b0d31-3ba2-4f83-9a0c-2bcec1c61d97", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "0613c74fe2ffbbc3272f4608e298830ee91992aa5c8a74c438082f4345135444", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5494beb7-f295-408e-a525-90452352328e", "node_type": "1", "metadata": {}, "hash": "3a3d8fbf15c6b9d2186b5f68de6a0df49cdf14f995687ac6b07edca374280675", "class_name": "RelatedNodeInfo"}}, "text": "Of those touching pool DRAM, we\u2019d\nexpect that about1\n4would see a slowdown exceeding a\nPDM=5% (Figure 16). So, the strawman would have about\n2.5% of scheduling mispredictions.\nFinding 9. At a pool size of 16 sockets, Pond reduces\noverall DRAM requirements by 9% and 7% under 182%\nand 222% latency increases, respectively. Static reduces\nDRAM by 3%. When varying PDMbetween 1 and 10%\nandTPbetween 90 and 99.9% we \ufb01nd the relative savings\nof the three systems to be qualitatively similar.\nImplication. Pond can safely reduce cost. A QoS mon-\nitor that mitigates more than 1% of mispredictions, can\nachieve more aggressive performance targets ( PDM).\nFinding 10. Throughout the simulations, Pond\u2019s pool\nmemory of\ufb02ining speeds remain below 1GB/s and 10GB/s\nfor 99.99% and 99.999% of VM starts, respectively.\nImplication. Pond is practical and achieve design goals.\n7.Discussion\nRobustness of ML. Similar to other oversubscribed re-\nsources (CPU [ 55] and disks [ 103]), customers may\n12Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n012345\n02 040 60Average Pool DRAM [%]Slowdown > PDM [%]At 182% (142ns)At 222% (255ns)Pond with CXL latency emulated\nFigure 20: Combined model (\u00a7 6.4).Pond\u2019s overall tradeoff\nbetween average allocation of pool memory and mispredictions\nafter solving Eq. (1).\noveruse resources to get local memory. When multiplex-\ning resource for millions of customers, any individual\ncustomer\u2019s behavior will have a small impact. Providers\ncan also provide small discounts when resources are not\nfully utilized.\nAlternatives to static memory preallocation. Pond is\ndesigned for compatibility static memory as potential\nworkarounds are not yet practical. The PCIe Address\nTranslation Service (ATS/PRI) [ 104] enables compatibil-\nity with page faults. Unfortunately, ATS/PRI-devices are\nnot yet broadly available [ 1]. Virtual IOMMUs [ 2,5,6]\nallow \ufb01ne-grained pinning but require guest OS changes\nand introduce overhead.\n8.Related Work\nHardware-level disaggregation: Hardware-based disag-\ngregation designs [ 19,58,105\u2013109] are not easily de-\nployable as they do not rely on commodity hardware.\nFor instance, ThymesisFlow [ 58] and Clio [ 105] propose\nFPGA-based rack-scale memory disaggregation designs\non top of OpenCAPI [ 110] and RDMA. Their hardware\nlayer shares goals with Pond. Their software goals dif-\nfer fundamentally, e.g., ThymesisFlow advocates appli-\ncation changes for performance, while Pond focuses on\nplatform-level ML-driven pool memory management that\nis transparent to users.\nHypervisor/OS level disaggregation: Hypervisor/OS\nlevel approaches [ 17,21,22,31\u201338] rely on page faults\nand access monitoring to maintain the working set in lo-\ncal DRAM. Such OS-based approaches bring signi\ufb01cant\noverhead, jitter, and are incompatible with virtualization\nacceleration ( e.g., DDA).\nRuntime/application level disaggregation: Runtime-\nbased disaggregation designs [ 23,25,26] propose cus-\ntomized APIs for remote memory access. While effective,\nthis approach requires developers to explicitly use these\nmechanisms at the application level.\nMemory tiering: Prior works have considered the\nbroader impact of extended memory hierarchies and how\n859095100\n28 1 6 3 264Pool Scope [CPU Sockets]Required Overall DRAM [%]Fixed 15% percentage of VM memory with CXL latency at 182%Pond with CXL latency at 222%Pond with CXL latency at 182%Figure 21: Memory savings under performance constraints\n(\u00a76.5).Simulated end-to-end evaluation of memory savings\nachieved by Pond under PDM=5% and scheduling mispredic-\ntions TP=98%.\nto handle them [ 17,111\u2013114]. For example, Google\nachieves 6 \u00b5s latency via proactive hot/cold page detec-\ntion and compression [ 17,115].", "start_char_idx": 526353, "end_char_idx": 530123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5494beb7-f295-408e-a525-90452352328e": {"__data__": {"id_": "5494beb7-f295-408e-a525-90452352328e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84f2cd68-89d1-43e4-9be9-7aeb8b5a54c5", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3fbecb6fdb1af2d62a7ae5b426c7a6cc71e348626c50be746722bb5ec0820f6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3825240b-86c6-4ed6-bc66-9c6f428ac893", "node_type": "1", "metadata": {}, "hash": "9c5e496c5499a8ec891aa17f5898282ee9cdd653f467235f8c1a1d80d2537ad9", "class_name": "RelatedNodeInfo"}}, "text": "While effective,\nthis approach requires developers to explicitly use these\nmechanisms at the application level.\nMemory tiering: Prior works have considered the\nbroader impact of extended memory hierarchies and how\n859095100\n28 1 6 3 264Pool Scope [CPU Sockets]Required Overall DRAM [%]Fixed 15% percentage of VM memory with CXL latency at 182%Pond with CXL latency at 222%Pond with CXL latency at 182%Figure 21: Memory savings under performance constraints\n(\u00a76.5).Simulated end-to-end evaluation of memory savings\nachieved by Pond under PDM=5% and scheduling mispredic-\ntions TP=98%.\nto handle them [ 17,111\u2013114]. For example, Google\nachieves 6 \u00b5s latency via proactive hot/cold page detec-\ntion and compression [ 17,115]. Nimble [ 75] optimizes\nLinux\u2019s page tracking mechanism to tier pages for in-\ncreased migration bandwidth. Pond takes a different ML-\nbased approach looking at memory pooling design at the\nplatform-level and is orthogonal to these works.\nML for systems: ML is increasingly applied to tackle\nsystems problems, such as cloud ef\ufb01ciency [ 48,55], mem-\nory/storage optimizations [ 116,117], microservices [ 118],\ncaching/prefetching policies [ 119,120]. We uniquely ap-\nply ML methods for untouched memory prediction to\nsupport pooled memory provisioning to VMs without\njeopardizing QoS.\nCoherent memory and NUMA optimizations: Tradi-\ntional cache coherent NUMA architectures [ 121] use\nspecialized interconnects to implement a shared address\nspace. There are also system-level optimizations for\nNUMA, such as NUMA-aware data placement [ 122] and\nproactive page migration [ 76]. NUMA scheduling poli-\ncies [ 123\u2013125] balance compute and memory across\nNUMA nodes. Pond\u2019s ownership overcomes the need for\ncoherence across the memory pool. zNUMA\u2019s zero-core\nnature requires rethinking of existing optimizations which\nare largely optimized for symmetric NUMA systems.\n9.Conclusion\nDRAM costs are an increasing cost factor for cloud\nproviders. This paper is motivated by the observation\nof stranded and untouched memory across 100 produc-\ntion cloud clusters. We proposed Pond, the \ufb01rst full-\nstack memory pool that satis\ufb01es the requirements of cloud\nproviders. Pond comprises contributions at the hardware,\nsystem software, and distributed system layers. Our re-\nsults showed that Pond can reduce the amount of needed\nDRAM by 7% with a pool size of 16 sockets and assum-\ning CXL increases latency by 222%. This translates into\nan overall reduction of 3.5% in cloud server cost.\n13Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\nReferences\n[1]Ilya Lesokhin, Haggai Eran, Shachar Raindel, Guy Shapiro, Sagi\nGrimberg, Liran Liss, Muli Ben-Yehuda, Nadav Amit, and Dan\nTsafrir. Page Fault Support for Network Controllers. In Proceed-\nings of the 22nd ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems\n(ASPLOS) , 2017.\n[2]Kun Tian, Yu Zhang, Luwei Kang, Yan Zhao, and Yaozu Dong.\ncoIOMMU: A Virtual IOMMU with Cooperative DMA Buffer\nTracking for Ef\ufb01cient Memory Management in Direct I/O. In\nProceedings of the 2020 USENIX Annual Technical Conference\n(ATC) , 2020.\n[3]Ben-Ami Yassour, Muli Ben-Yehuda, and Orit Wasserman. On\nthe DMA Mapping Problem in Direct Device Assignment. In\nProceedings of the 3rd ACM International Conference on Systems\nand Storage (SYSTOR) , 2010.\n[4]Paul Willmann, Scott Rixner, and Alan L. Cox. Protection Strate-\ngies for Direct Access to Virtualized I/O Devices. In Proceedings\nof the USENIX Annual Technical Conference (ATC) , 2008.\n[5]Nadav Amit, Muli Ben-Yehuda, IBM Research, Dan Tsafrir, and\nAssaf Schuster. vIOMMU: Ef\ufb01cient IOMMU Emulation. In\nProceedings of the 2011 USENIX Annual Technical Conference\n(ATC) , 2011.", "start_char_idx": 529401, "end_char_idx": 533203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3825240b-86c6-4ed6-bc66-9c6f428ac893": {"__data__": {"id_": "3825240b-86c6-4ed6-bc66-9c6f428ac893", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5494beb7-f295-408e-a525-90452352328e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "7a7959d4c52aef1600dd931ce51644e3ab7073e52a3ac23322999c3a95aac639", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90580101-1853-46bb-8bdb-f1a6905e8017", "node_type": "1", "metadata": {}, "hash": "e0abe2077e50ffd0373bec0ae7e71399873b24620ae58e7fe977c4f14cd36b2d", "class_name": "RelatedNodeInfo"}}, "text": "[3]Ben-Ami Yassour, Muli Ben-Yehuda, and Orit Wasserman. On\nthe DMA Mapping Problem in Direct Device Assignment. In\nProceedings of the 3rd ACM International Conference on Systems\nand Storage (SYSTOR) , 2010.\n[4]Paul Willmann, Scott Rixner, and Alan L. Cox. Protection Strate-\ngies for Direct Access to Virtualized I/O Devices. In Proceedings\nof the USENIX Annual Technical Conference (ATC) , 2008.\n[5]Nadav Amit, Muli Ben-Yehuda, IBM Research, Dan Tsafrir, and\nAssaf Schuster. vIOMMU: Ef\ufb01cient IOMMU Emulation. In\nProceedings of the 2011 USENIX Annual Technical Conference\n(ATC) , 2011.\n[6]Muli Ben-Yehuda, Michael D. Day, Zvi Dubitzky, Michael Fac-\ntor, Nadav Har\u2019El, Abel Gordon, Anthony Liguori, Orit Wasser-\nman, and Ben-Ami Yassour. The Turtles Project: Design and\nImplementation of Nested Virtualization. In Proceedings of the\n9th USENIX Symposium on Operating Systems Design and Im-\nplementation (OSDI) , 2010.\n[7]AWS: Enhanced networking support. https://docs.aws.a\nmazon.com/AWSEC2/latest/UserGuide/enhanced-net\nworking.html#supported_instances .\n[8]Azure Accelerated Networking: Supported VM instances. http\ns://docs.microsoft.com/en-us/azure/virtual-net\nwork/accelerated-networking-overview#supported\n-vm-instances .\n[9]Onur Mutlu. Memory Scaling: A Systems Architecture Perspec-\ntive. In IEEE International Memory Workshop (IMW) , 2013.\n[10] Prashant Nair, Dae-Hyun Kim, and Moinuddin K. Qureshi.\nArchShield: Architectural Framework for Assisting DRAM\nScaling by Tolerating High Error Rates. In Proceedings of the\n40th Annual International Symposium on Computer Architecture\n(ISCA) , 2013.\n[11] Onur Mutlu. Main Memory Scaling: Challenges and Solution\ndirections. In More than Moore Technologies for Next Generation\nComputer Design . Springer, 2015.\n[12] Sung-Kye Park. Technology Scaling Challenge and Future\nProspects of DRAM and NAND Flash Memory. In IEEE In-\nternational Memory Workshop (IMW) , 2015.\n[13] Shigeru Shiratake. Scaling and Performance Challenges of Fu-\nture DRAM. In IEEE International Memory Workshop (IMW) ,\n2020.\n[14] The Next New Memories. https://semiengineering.com/\nthe-next-new-memories/ , 2019.\n[15] Micron Ends 3D XPoint Memory. https://www.forbes.c\nom/sites/tomcoughlin/2021/03/16/micron-ends-3d\n-xpoint-memory/ , 2021.\n[16] CXL And Gen-Z Iron Out A Coherent Interconnect Strategy.\nhttps://www.nextplatform.com/2020/04/03/cxl-and-\ngen-z-iron-out-a-coherent-interconnect-strateg\ny/, 2020.\n[17] Andres Lagar-Cavilla, Junwhan Ahn, Suleiman Souhlal, Neha\nAgarwal, Radoslaw Burny, Shakeel Butt, Jichuan Chang, Ashwin\nChaugule, Nan Deng, Junaid Shahid, Greg Thelen, Kamil Adam\nYurtsever, Yu Zhao, and Parthasarathy Ranganathan. Software-De\ufb01ned Far Memory in Warehouse-Scale Computers. In Proceed-\nings of the 24th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems\n(ASPLOS) , 2019.\n[18] Johannes Weiner, Niket Agarwal, Dan Schatzberg, Leon Yang,\nHao Wang, Blaise Sanouillet, Bikash Sharma, Tejun Heo,\nMayank Jain, Chunqiang Tang, and Dimitrios Skarlatos. TMO:\nTransparent Memory Of\ufb02oading in Datacenters. In Proceed-\nings of the 27th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems\n(ASPLOS) , 2022.\n[19] Kevin T. Lim, Jichuan Chang, Trevor N. Mudge, Parthasarathy\nRanganathan, Steven K. Reinhardt, and Thomas F. Wenisch.", "start_char_idx": 532617, "end_char_idx": 535977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90580101-1853-46bb-8bdb-f1a6905e8017": {"__data__": {"id_": "90580101-1853-46bb-8bdb-f1a6905e8017", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3825240b-86c6-4ed6-bc66-9c6f428ac893", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "c7d7b78ba103ca5fb7a5cee0b304971abdd9b915f1c4780f0a63b51b00500728", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cd155e9-ea68-46a1-a4a5-eec25df2a6cd", "node_type": "1", "metadata": {}, "hash": "58bb9dc1f0152f6cbb70814b182760e3a3a58f39bb84a95e0ebf47df27037076", "class_name": "RelatedNodeInfo"}}, "text": "Software-De\ufb01ned Far Memory in Warehouse-Scale Computers. In Proceed-\nings of the 24th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems\n(ASPLOS) , 2019.\n[18] Johannes Weiner, Niket Agarwal, Dan Schatzberg, Leon Yang,\nHao Wang, Blaise Sanouillet, Bikash Sharma, Tejun Heo,\nMayank Jain, Chunqiang Tang, and Dimitrios Skarlatos. TMO:\nTransparent Memory Of\ufb02oading in Datacenters. In Proceed-\nings of the 27th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems\n(ASPLOS) , 2022.\n[19] Kevin T. Lim, Jichuan Chang, Trevor N. Mudge, Parthasarathy\nRanganathan, Steven K. Reinhardt, and Thomas F. Wenisch. Dis-\naggregated Memory for Expansion and Sharing in Blade Servers.\nInProceedings of the 36th Annual International Symposium on\nComputer Architecture (ISCA) , 2009.\n[20] Kevin Lim, Yoshio Turner, Jichuan Chang, J Renato Santos, and\nParthasarathy Ranganathan. Disaggregated Memory Bene\ufb01ts for\nServer Consolidation. HP Laboratories , 2011.\n[21] Wenqi Cao and Ling Liu. Hierarchical Orchestration of Disag-\ngregated Memory. IEEE Transactions on Computers (TC) , 69(6),\nJune 2020.\n[22] Kevin Lim, Yoshio Turner, Jose Renato Santos, Alvin AuY-\noung, Jichuan Chang, Parthasarathy Ranganathan, and Thomas F.\nWenisch. System-level Implications of Disaggregated Memory.\nInProceedings of the 18th International Symposium on High\nPerformance Computer Architecture (HPCA-18) , 2012.\n[23] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K. Aguilera, and\nAdam Belay. AIFM: High-Performance, Application-Integrated\nFar Memory. In Proceedings of the 14th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI) , 2020.\n[24] Sebastian Angel, Mihir Nanavati, and Siddhartha Sen. Disaggre-\ngation and the Application. In The 12th USENIX Workshop on\nHot Topics in Cloud Computing (HotCloud) , 2020.\n[25] Chenxi Wang, Haoran Ma, Shi Liu, Yuanqi Li, Zhenyuan Ruan,\nKhanh Nguyen, Michael D. Bond, Ravi Netravali, Miryung Kim,\nand Guoqing Harry Xu. Semeru: A Memory-Disaggregated Man-\naged Runtime. In Proceedings of the 14th USENIX Symposium\non Operating Systems Design and Implementation (OSDI) , 2020.\n[26] Irina Calciu, M. Talha Imran, Ivan Puddu, Sanidhya Kashyap,\nHasan Al Maruf, Onur Mutlu, and Aasheesh Kolli. Rethinking\nSoftware Runtimes for Disaggregated Memory. In Proceed-\nings of the 26th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems\n(ASPLOS) , 2021.\n[27] Aleksandar Dragojevic, Dushyanth Narayanan, Miguel Castro,\nand Orion Hodson. FaRM: Fast Remote Memory. In Proceedings\nof the 11th USENIX Symposium on Networked Systems Design\nand Implementation (NSDI) , 2014.\n[28] Marcos K. Aguilera, Nadav Amit, Irina Calciu, Xavier Deguil-\nlard, Jayneel Gandhi, Stanko Novakovic, Arun Ramanathan,\nPratap Subrahmanyam, Lalith Suresh, Kiran Tati, Rajesh\nVenkatasubramanian, and Michael Wei. Remote Regions: a\nSimple Abstraction for Remote Memory. In Proceedings of the\n2018 USENIX Annual Technical Conference (ATC) , 2018.\n[29] Shin-Yeh Tsai, Yizhou Shan, and Yiying Zhang. Disaggregating\nPersistent Memory and Controlling Them Remotely: An Explo-\nration of Passive Disaggregated Key-Value Stores. In Proceed-\nings of the 2020 USENIX Annual Technical Conference (ATC) ,\n2020.", "start_char_idx": 535280, "end_char_idx": 538595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cd155e9-ea68-46a1-a4a5-eec25df2a6cd": {"__data__": {"id_": "7cd155e9-ea68-46a1-a4a5-eec25df2a6cd", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90580101-1853-46bb-8bdb-f1a6905e8017", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6aaf47a2947647bfe9d858b5177c23a9e8e39029e25e3d1eebe2d55154ca68ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a6b4007-a624-498d-8967-9012ac02aa30", "node_type": "1", "metadata": {}, "hash": "cd04c69c96749f3e414c993d27c38a2994427a4dba49004e67e1b1ce102ef8af", "class_name": "RelatedNodeInfo"}}, "text": "[28] Marcos K. Aguilera, Nadav Amit, Irina Calciu, Xavier Deguil-\nlard, Jayneel Gandhi, Stanko Novakovic, Arun Ramanathan,\nPratap Subrahmanyam, Lalith Suresh, Kiran Tati, Rajesh\nVenkatasubramanian, and Michael Wei. Remote Regions: a\nSimple Abstraction for Remote Memory. In Proceedings of the\n2018 USENIX Annual Technical Conference (ATC) , 2018.\n[29] Shin-Yeh Tsai, Yizhou Shan, and Yiying Zhang. Disaggregating\nPersistent Memory and Controlling Them Remotely: An Explo-\nration of Passive Disaggregated Key-Value Stores. In Proceed-\nings of the 2020 USENIX Annual Technical Conference (ATC) ,\n2020.\n[30] Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Kon-\nstantin Taranov, Dejan Milojicic, and Gustavo Alonso. Farview:\nDisaggregated Memory with Operator Of\ufb02oading for Database\nEngines. arXiv:2106.07102 , 2021.\n[31] Peter X. Gao, Akshay Narayan, Sagar Karandikar, Joao Car-\nreira, Sangjin Han, Rachit Agarwal, Sylvia Ratnasamy, and Scott\n1Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\nShenker. Network Requirements for Resource Disaggregation.\nInProceedings of the 12th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI) , 2016.\n[32] Juncheng Gu, Youngmoon Lee, Yiwen Zhang, Mosharaf Chowd-\nhury, and Kang G. Shin. Ef\ufb01cient Memory Disaggregation with\nIn\ufb01niswap. In Proceedings of the 14th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI) , 2017.\n[33] Yizhou Shan, Yutong Huang, Yilun Chen, and Yiying Zhang.\nLegoOS: A Disseminated, Distributed OS for Hardware Resource\nDisaggregation. In Proceedings of the 13th USENIX Symposium\non Operating Systems Design and Implementation (OSDI) , 2018.\n[34] Kwangwon Koh, Kangho Kim, Seunghyub Jeon, and Jaehyuk\nHuh. Disaggregated Cloud Memory with Elastic Block Manage-\nment. IEEE Transactions on Computers (TC) , 68(1), 2019.\n[35] Emmanuel Amaro, Christopher Branner-Augmon, Zhihong Luo,\nAmy Ousterhout, Marcos K. Aguilera, Aurojit Panda, Sylvia\nRatnasamy, and Scott Shenker. Can Far Memory Improve Job\nThroughput? In Proceedings of the 2020 EuroSys Conference\n(EuroSys) , 2020.\n[36] Hasan Al Maruf and Mosharaf Chowdhury. Effectively Prefetch-\ning Remote Memory with Leap. In Proceedings of the 2020\nUSENIX Annual Technical Conference (ATC) , 2020.\n[37] Blake Caldwell, Sepideh Goodarzy, Sangtae Ha, Richard Han,\nEric Keller, Eric Rozner, and Youngbin Im. FluidMem: Full,\nFlexible, and Fast Memory Disaggregation for the Cloud. In\nInternational Conference on Distributed Computing Systems\n(ICDCS) , 2020.\n[38] Youngmoon Lee, Hassan Al Maruf, Mosharaf Chowdhury, and\nKang G. Shin. Mitigating the Performance-Ef\ufb01ciency Tradeoff\nin Resilient Memory Disaggregation. arXiv:1910.09727 , 2019.\n[39] Compute Express Link Speci\ufb01cation. Available at https://ww\nw.computeexpresslink.org , 2020.\n[40] Sapphire Rapids Uncovered: 56 Cores, 64GB HBM2E, Multi-\nChip Design. https://www.tomshardware.com/news/in\ntel-sapphire-rapids-xeon-scalable-specificatio\nns-and-features , 2021.\n[41] CXL Consortium Member Spotlight: Arm. https://www.co\nmputeexpresslink.org/post/cxl-consortium-member-\nspotlight-arm , 2021.\n[42] AMD Unveils Workload-Tailored Innovations and Products at\nThe Accelerated Data Center Premiere.", "start_char_idx": 537996, "end_char_idx": 541278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a6b4007-a624-498d-8967-9012ac02aa30": {"__data__": {"id_": "6a6b4007-a624-498d-8967-9012ac02aa30", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cd155e9-ea68-46a1-a4a5-eec25df2a6cd", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3abbbfbadc76f1d644dba3dcf5dc2b61848aaaa9161072cc0cc6ea6272e67811", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac430d4f-1d10-4fb8-ad2a-7b57b2aec456", "node_type": "1", "metadata": {}, "hash": "75808563d1caf7dc279dcc4f6f930f9e49cb92072433d697430fdb15e6a66102", "class_name": "RelatedNodeInfo"}}, "text": "Mitigating the Performance-Ef\ufb01ciency Tradeoff\nin Resilient Memory Disaggregation. arXiv:1910.09727 , 2019.\n[39] Compute Express Link Speci\ufb01cation. Available at https://ww\nw.computeexpresslink.org , 2020.\n[40] Sapphire Rapids Uncovered: 56 Cores, 64GB HBM2E, Multi-\nChip Design. https://www.tomshardware.com/news/in\ntel-sapphire-rapids-xeon-scalable-specificatio\nns-and-features , 2021.\n[41] CXL Consortium Member Spotlight: Arm. https://www.co\nmputeexpresslink.org/post/cxl-consortium-member-\nspotlight-arm , 2021.\n[42] AMD Unveils Workload-Tailored Innovations and Products at\nThe Accelerated Data Center Premiere. https://www.amd.co\nm/en/press-releases/2021-11-08-amd-unveils-wor\nkload-tailored-innovations-and-products-the-ac\ncelerated , 2021.\n[43] Christopher Lameter. Flavors of Memory supported by Linux,\nTheir Use and Bene\ufb01t. https://events19.linuxfounda\ntion.org/wp-content/uploads/2017/11/The-Flavor\ns-of-Memory-Supported-by-Linux-their-Use-and-B\nenefit-Christoph-Lameter-Jump-Trading-LLC.pdf ,\n2019.\n[44] Alexandru Agache, Marc Brooker, Andreea Florescu, Alexandra\nIordache, Anthony Liguori, Rolf Neugebauer, Phil Piwonka, and\nDiana-Maria Popa. Firecracker: Lightweight Virtualization for\nServerless Applications. In Proceedings of the 17th USENIX\nSymposium on Networked Systems Design and Implementation\n(NSDI) , 2020.\n[45] Intel Virtualization Technology for Directed I/O. https://so\nftware.intel.com/content/dam/develop/external/\nus/en/documents/vt-directed-io-spec.pdf , 2020.\n[46] Huaicheng Li, Mingzhe Hao, Stanko Novakovic, Vaibhav Gogte,\nSriram Govindan, Dan R. K. Ports, Irene Zhang, Ricardo Bian-\nchini, Haryadi S. Gunawi, and Anirudh Badam. LeapIO: Ef\ufb01cient\nand Portable Virtual NVMe Storage on ARM SoCs. In Proceed-\nings of the 25th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems\n(ASPLOS) , 2020.[47] Single-Root Input/Output Virtualization. httpp://www.pcis\nig.com/specifications/iov/single_root , 2019.\n[48] Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich,\nMarcus Fontoura, and Ricardo Bianchini. Resource Central:\nUnderstanding and Predicting Workloads for Improved Resource\nManagement in Large Cloud Platforms. In Proceedings of the\n26th ACM Symposium on Operating Systems Principles (SOSP) ,\n2017.\n[49] Ori Hadary, Luke Marshall, Ishai Menache, Abhisek Pan, Esa-\nias E Greeff, David Dion, Star Dorminey, Shailesh Joshi, Yang\nChen, Mark Russinovich, and Thomas Moscibroda. Protean: VM\nAllocation Service at Scale. In Proceedings of the 14th USENIX\nSymposium on Operating Systems Design and Implementation\n(OSDI) , 2020.\n[50] Abhishek Verma, Madhukar Korupolu, and John Wilkes. Evaluat-\ning Job Packing in Warehouse-scale Computing. In International\nConference on Cluster Computing (Cluster) , 2014.\n[51] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Op-\npenheimer, Eric Tune, and John Wilkes. Large-scale cluster\nmanagement at Google with Borg. In Proceedings of the 2015\nEuroSys Conference (EuroSys) , 2015.\n[52] Rina Panigrahy, Kunal Talwar, Lincoln Uyeda, and Udi Wieder.\nHeuristics for Vector Bin Packing. https://www.microsof\nt.com/en-us/research/publication/heuristics-for-\nvector-bin-packing/ , 2011.\n[53] Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula,\nSriram Rao, and Aditya Akella.", "start_char_idx": 540663, "end_char_idx": 543965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac430d4f-1d10-4fb8-ad2a-7b57b2aec456": {"__data__": {"id_": "ac430d4f-1d10-4fb8-ad2a-7b57b2aec456", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a6b4007-a624-498d-8967-9012ac02aa30", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "7ce14ce82c9c6639e9232c41a3e8eaf1ca49b72ea22d4e1c1ca134386ab7e51b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d33fe1bc-f856-494d-893f-4e9b941e93e0", "node_type": "1", "metadata": {}, "hash": "b96a58c223de156ebdcb055595529e7b79330b180df5d0de409df4387c18320c", "class_name": "RelatedNodeInfo"}}, "text": "Evaluat-\ning Job Packing in Warehouse-scale Computing. In International\nConference on Cluster Computing (Cluster) , 2014.\n[51] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Op-\npenheimer, Eric Tune, and John Wilkes. Large-scale cluster\nmanagement at Google with Borg. In Proceedings of the 2015\nEuroSys Conference (EuroSys) , 2015.\n[52] Rina Panigrahy, Kunal Talwar, Lincoln Uyeda, and Udi Wieder.\nHeuristics for Vector Bin Packing. https://www.microsof\nt.com/en-us/research/publication/heuristics-for-\nvector-bin-packing/ , 2011.\n[53] Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula,\nSriram Rao, and Aditya Akella. Multi-Resource Packing for\nCluster Schedulers. In Proceedings of the ACM Special Interest\nGroup on Data Communication (SIGCOMM) , 2014.\n[54] Yossi Azar, Ilan Reuven Cohen, Seny Kamara, and Bruce Shep-\nherd. Tight Bounds for Online Vector Bin Packing. In Pro-\nceedings of the 45th ACM symposium on Theory of Computing\n(STOC) , 2013.\n[55] Yawen Wang, Kapil Arya, Marios Kogias, Manohar Vanga,\nAditya Bhandari, Neeraja J. Yadwadkar, Siddhartha Sen, Sameh\nElnikety, Christos Kozyrakis, and Ricardo Bianchini. SmartHar-\nvest: Harvesting Idle CPUs Safely and Ef\ufb01ciently in the Cloud.\nInProceedings of the 2021 EuroSys Conference (EuroSys) , 2021.\n[56] Pradeep Ambati, \u00cd\u00f1igo Goiri, Felipe Vieira Frujeri, Alper Gun,\nKe Wang, Brian Dolan, Brian Corell, Sekhar Pasupuleti, Thomas\nMoscibroda, Sameh Elnikety, Marcus Fontoura, and Ricardo\nBianchini. Providing SLOs for Resource-Harvesting VMs in\nCloud Platforms. In Proceedings of the 14th USENIX Symposium\non Operating Systems Design and Implementation (OSDI) , 2020.\n[57] Marcos K. Aguilera, Nadav Amit, Irina Calciu, Xavier Deguil-\nlard, Jayneel Gandhi, Pratap Subrahmanyam, Lalith Suresh, Ki-\nran Tati, Rajesh Venkatasubramanian, and Michael Wei. Remote\nMemory in the Age of Fast Networks. In Proceedings of the 8th\nACM Symposium on Cloud Computing (SoCC) , 2017.\n[58] Christian Pinto, Dimitris Syrivelis, Michele Gazzetti, Panos\nKoutsovasilis, Andrea Reale, Kostas Katrinis, and Peter Hofs-\ntee. ThymesisFlow: A Software-De\ufb01ned, HW/SW Co-Designed\nInterconnect Stack for Rack-Scale Memory Disaggregation. In\n53rd Annual IEEE/ACM International Symposium on Microar-\nchitecture (MICRO-53) , 2020.\n[59] CXL 2.0 Speci\ufb01cation. https://www.computeexpresslink\n.org/download-the-specification , 2020.\n[60] Compute Express Link 2.0 White Paper. https://b373ea\nf2-67af-4a29-b28c-3aae9e644f30.filesusr.com/ug\nd/0c1418_14c5283e7f3e40f9b2955c7d0f60bebe.pdf ,\n2021.\n[61] Debendra Das Sharma. CXL 3.0: New Features for Increased\nScale and Optimized Resource Utilization. In Flash Memory\nSummit , 2022.\n[62] CXL 3.0 Speci\ufb01cation. https://www.computeexpresslink\n.org/download-the-specification , 2022.\n2Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n[63] Debendra Das Sharma. Compute Express Link: An Open\nIndustry-standard Interconnect Enabling Heterogenous Data-\ncentric Computing. In Proceedings of the 29th IEEE Hot In-\nterconnects symposium (HotI29) , 2022.", "start_char_idx": 543332, "end_char_idx": 546448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d33fe1bc-f856-494d-893f-4e9b941e93e0": {"__data__": {"id_": "d33fe1bc-f856-494d-893f-4e9b941e93e0", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac430d4f-1d10-4fb8-ad2a-7b57b2aec456", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5749635adeecaeef40c3a7e008623a11e5cd77c60fcd7debe52ac0f5a6703af0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a8e8823-6aff-40ca-9ef3-0243719fc978", "node_type": "1", "metadata": {}, "hash": "ff9fbb95606cef80015df800093ff3ce9acf84b21c7a46ca63e015d82d44485e", "class_name": "RelatedNodeInfo"}}, "text": "[61] Debendra Das Sharma. CXL 3.0: New Features for Increased\nScale and Optimized Resource Utilization. In Flash Memory\nSummit , 2022.\n[62] CXL 3.0 Speci\ufb01cation. https://www.computeexpresslink\n.org/download-the-specification , 2022.\n2Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\n[63] Debendra Das Sharma. Compute Express Link: An Open\nIndustry-standard Interconnect Enabling Heterogenous Data-\ncentric Computing. In Proceedings of the 29th IEEE Hot In-\nterconnects symposium (HotI29) , 2022.\n[64] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes\nWeiner, Niket Agarwal, Pallab Bhattacharya, Chris Petersen,\nMosharaf Chowdhury, Shobhit Kanaujia, and Prakash Chauhan.\nTPP: Transparent Page Placement for CXL-Enabled Tiered Mem-\nory.arXiv:2206.02878 , 2022.\n[65] Donghyun Gouk, Sangwon Lee, Miryeong Kwon, , and Myoung-\nsoo Jung. Direct Access, High-Performance Memory Disaggre-\ngation with DirectCXL. In Proceedings of the 2022 USENIX\nAnnual Technical Conference (ATC) , 2022.\n[66] AMD EPYC Genoa and SP5 Platform Leaked. https://docs\n.aws.amazon.com/AWSEC2/latest/UserGuide/disk-p\nerformance.html , 2021.\n[67] Reliability, Availability, and Serviceability (RAS) Integration\nand Validation Guide for the Intel Xeon Processor E7 Family.\nhttps://www.intel.com/content/dam/develop/exte\nrnal/us/en/documents/emca2-integration-validat\nion-guide-556978.pdf , 2015.\n[68] AMD EPYC brings new RAS capability. https://www.amd.\ncom/system/files/2017-06/AMD-EPYC-Brings-New-R\nAS-Capability.pdf , 2017.\n[69] CXL Use-cases Driving the Need For Low Latency Performance\nRetimers. https://www.microchip.com/en-us/about/bl\nog/learning-center/cxl--use-cases-driving-the-\nneed-for-low-latency-performance-reti , 2021.\n[70] Enabling PCIe 5.0 System Level Testing and Low Latency Mode\nfor CXL. https://www.asteralabs.com/videos/aries-\nsmart-retimer-for-pcie-gen-5-and-cxl/ , 2021.\n[71] Elene Chobanyan, Casey Morrison, and Pegah Alavi. End-to-\nEnd System-Level Simulations with Retimers for PCIe Gen5 &\nCXL. DesignCon, slides available at https://www.asterala\nbs.com/wp-content/themes/astera-labs/images/re\ntimer-cxl.pdf , 2020.\n[72] Timothy Prickett Morgan. Pci-express 5.0: The unintended but\nformidable datacenter interconnect. DesignCon, slides available\nathttps://www.nextplatform.com/2021/02/03/pci-ex\npress-5-0-the-unintended-but-formidable-datace\nnter-interconnect/ , 2021.\n[73] MIPI I3C Bus Sensor Speci\ufb01cation. https://www.mipi.org\n/specifications/i3c-sensor-specification , 2021.\n[74] UEFI. Advanced Con\ufb01guration and Power Interface Speci\ufb01ca-\ntion, 2021.\n[75] Zi Yan, Daniel Lustig, David Nellans, and Abhishek Bhattachar-\njee. Nimble Page Management for Tiered Memory Systems. In\nProceedings of the 24th ACM International Conference on Ar-\nchitectural Support for Programming Languages and Operating\nSystems (ASPLOS) , 2019.\n[76] Huang Ying. AutoNUMA: Optimize Memory Placement for\nMemory Tiering System. https://lwn.net/Articles/835\n402/ , 2019.\n[77] Adam Ruprecht, Danny Jones, Dmitry Shiraev, Greg Harmon,\nMaya Spivak, Michael Krebs, Miche Baker-Harvey, and Tyler\nSanderson. Vm live migration at scale. ACM SIGPLAN Notices ,\n53(3), March 2018.\n[78] Ahmad Yasin.", "start_char_idx": 545861, "end_char_idx": 549113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a8e8823-6aff-40ca-9ef3-0243719fc978": {"__data__": {"id_": "9a8e8823-6aff-40ca-9ef3-0243719fc978", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d33fe1bc-f856-494d-893f-4e9b941e93e0", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "0061f63daa198b64843b5c6f48a6c2d655d024f762492437151a727d21761b94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4289d59c-11ae-426b-9242-63779a78e1e6", "node_type": "1", "metadata": {}, "hash": "a50de19d051f6ff16337c700fcc6e4865a9ca79413f660d7e598d0e6725e8e6f", "class_name": "RelatedNodeInfo"}}, "text": "[75] Zi Yan, Daniel Lustig, David Nellans, and Abhishek Bhattachar-\njee. Nimble Page Management for Tiered Memory Systems. In\nProceedings of the 24th ACM International Conference on Ar-\nchitectural Support for Programming Languages and Operating\nSystems (ASPLOS) , 2019.\n[76] Huang Ying. AutoNUMA: Optimize Memory Placement for\nMemory Tiering System. https://lwn.net/Articles/835\n402/ , 2019.\n[77] Adam Ruprecht, Danny Jones, Dmitry Shiraev, Greg Harmon,\nMaya Spivak, Michael Krebs, Miche Baker-Harvey, and Tyler\nSanderson. Vm live migration at scale. ACM SIGPLAN Notices ,\n53(3), March 2018.\n[78] Ahmad Yasin. A Top-Down Method for Performance Analysis\nand Counters Architecture. In IEEE International Symposium on\nPerformance Analysis of Systems and Software (ISPASS) , 2014.\n[79] Top-down Microarchitecture Analysis Method. https://so\nftware.intel.com/content/www/us/en/develop/doc\numentation/vtune-cookbook/top/methodologies/to\np-down-microarchitecture-analysis-method.html ,\n2021.\n[80] Mateusz Jarus and Ariel Oleksiak. Top-Down CharacterizationApproximation Based on Performance Counters Architecture for\nAMD Processors. Simulation Modelling Practice and Theory ,\n2016.\n[81] Soramichi Akiyama and Takahiro Hirofuchi. Quantitative Evalu-\nation of Intel PEBS Overhead for Online System-noise Analysis.\nInProceedings of the 7th International Workshop on Runtime\nand Operating Systems for Supercomputers (ROSS) , 2017.\n[82] Amanda Raybuck, Tim Stamler, Wei Zhang, Mattan Erez, and\nSimon Peter. HeMem: Scalable Tiered Memory Management\nfor Big Data Applications and Real NVM. In Proceedings of the\n28th ACM Symposium on Operating Systems Principles (SOSP) ,\n2021.\n[83] Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent\nMichel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter\nPrettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:\nMachine Learning in Python. Journal of Machine Learning\nResearch (JMLR) , 12, 2011.\n[84] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen,\nWeidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A Highly\nEf\ufb01cient Gradient Boosting Decision Tree. Advances in Neural\nInformation Processing Systems (NIPS) , 2017.\n[85] ONNX. Open Neural Network Exchange: the Open Standard for\nMachine Learning Interoperability. https://onnx.ai/ , 2021.\n[86] Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harm-\nsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and\nJordan Soyke. Tensor\ufb02ow-serving: Flexible, High-performance\nML Serving. In Proceedings of the 31st Conference on Neural\nInformation Processing Systems (NIPS) , 2017.\n[87] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin,\nJoseph E. Gonzalez, and Ion Stoica. Clipper: A Low-Latency\nOnline Prediction Serving System. In Proceedings of the 14th\nUSENIX Symposium on Networked Systems Design and Imple-\nmentation (NSDI) , 2017.\n[88] Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich,\nMarcus Fontoura, and Ricardo Bianchini. Resource Central:\nUnderstanding and Predicting Workloads for Improved Resource\nManagement in Large Cloud Platforms. In Proceedings of the\n26th ACM Symposium on Operating Systems Principles (SOSP) ,\n2017.\n[89] Brendan Burns, Brian Grant, David Oppenheimer, Eric Brewer,\nand John Wilkes. Borg, Omega, and Kubernetes. Communica-\ntions of the ACM , 59(5), 2016.\n[90] Christina Delimitrou and Christos Kozyrakis. Paragon: Qos-\naware scheduling for heterogeneous datacenters. ACM SIGPLAN\nNotices , 48(4), 2013.\n[91] Christina Delimitrou, Daniel Sanchez, and Christos Kozyrakis.", "start_char_idx": 548503, "end_char_idx": 552040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4289d59c-11ae-426b-9242-63779a78e1e6": {"__data__": {"id_": "4289d59c-11ae-426b-9242-63779a78e1e6", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a8e8823-6aff-40ca-9ef3-0243719fc978", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a9732c8df01c324fb0eb9afaa16db4fc9616e901da94a6b9dfb1ebd2ff60723d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c385b07-63a7-47a8-8cb6-28f8b9451eee", "node_type": "1", "metadata": {}, "hash": "76bd05dc40072f0985b3a2b62d401d56a1e005a6b2ff9418ff70e18c2711a4b0", "class_name": "RelatedNodeInfo"}}, "text": "[88] Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich,\nMarcus Fontoura, and Ricardo Bianchini. Resource Central:\nUnderstanding and Predicting Workloads for Improved Resource\nManagement in Large Cloud Platforms. In Proceedings of the\n26th ACM Symposium on Operating Systems Principles (SOSP) ,\n2017.\n[89] Brendan Burns, Brian Grant, David Oppenheimer, Eric Brewer,\nand John Wilkes. Borg, Omega, and Kubernetes. Communica-\ntions of the ACM , 59(5), 2016.\n[90] Christina Delimitrou and Christos Kozyrakis. Paragon: Qos-\naware scheduling for heterogeneous datacenters. ACM SIGPLAN\nNotices , 48(4), 2013.\n[91] Christina Delimitrou, Daniel Sanchez, and Christos Kozyrakis.\nTarcil: Reconciling Scheduling Speed and Quality in Large\nShared Clusters. In Proceedings of the 6th ACM Symposium\non Cloud Computing (SoCC) , 2015.\n[92] Malte Schwarzkopf, Andy Konwinski, Michael Abdel-Malek,\nand John Wilkes. Omega: \ufb02exible, scalable schedulers for large\ncompute clusters. In Proceedings of the 2013 EuroSys Confer-\nence (EuroSys) , 2013.\n[93] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Op-\npenheimer, Eric Tune, and John Wilkes. Large-scale Cluster\nManagement at Google with Borg. In Proceedings of the 2015\nEuroSys Conference (EuroSys) , 2015.\n[94] Redis. https://redis.io , 2021.\n[95] VoltDB. https://www.voltdb.com , 2021.\n[96] TPC-H Benchmark. http://www.tpc.org/tpch , 2021.\n[97] HiBench: The Bigdata Micro Benchmark Suite. https://gi\nthub.com/Intel-bigdata/HiBench , 2021.\n[98] Scott Beamer, Krste Asanovi \u00b4c, and David Patterson. The GAP\nBenchmark Suite. arXiv:1508.03619 , 2015.\n[99] Xusheng Zhan, Yungang Bao, Christian Bienia, and Kai Li. PAR-\nSEC3.0: A Multicore Benchmark Suite with Network Stacks\n3Appears in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u201923) (Preprint)\nand SPLASH-2X. ACM SIGARCH Computer Architecture News\n(CAN) , 44(5), 2017.\n[100] SPEC CPU 2017. https://www.spec.org/cpu2017 , 2021.\n[101] Christian Bienia, Sanjeev Kumar, Jaswinder Pal Singh, and Kai\nLi. The PARSEC Benchmark Suite: Characterization and Archi-\ntectural Implications. In IEEE International Conference on Par-\nallel Architectures and Compilation Techniques (PACT) , 2008.\n[102] Michael Buckland and Fredric Gey. The Relationship Between\nRecall and Precision. Journal of the American Society for Infor-\nmation Science , 45(1), 1994.\n[103] AWS: optimize disk performance for instance store volumes.\nhttps://docs.aws.amazon.com/AWSEC2/latest/User\nGuide/disk-performance.html .\n[104] PCI Express Address Translation Services. https://compos\nter.com.ua/documents/ats_r1.1_26Jan09.pdf , 2009.\n[105] Zhiyuan Guo, Yizhou Shan, Xuhao Luo, Yutong Huang, and\nYiying Zhang. Clio: A Hardware-Software Co-Designed Dis-\naggregated Memory System. In Proceedings of the 27th ACM\nInternational Conference on Architectural Support for Program-\nming Languages and Operating Systems (ASPLOS) , 2022.\n[106] Vlad Nitu, Boris Teabe, Alain Tchana, Canturk Isci, and Daniel\nHagimont. Welcome to Zombieland: Practical and Energy-\nef\ufb01cient Memory Disaggregation in a Datacenter. In Proceedings\nof the 2018 EuroSys Conference (EuroSys) , 2018.\n[107] Irina Calciu, Ivan Puddu, Aasheesh Kolli, Andreas Nowatzyk,\nJayneel Gandhi, Onur Mutlu, and Pratap Subrahmanyam.", "start_char_idx": 551364, "end_char_idx": 554672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c385b07-63a7-47a8-8cb6-28f8b9451eee": {"__data__": {"id_": "6c385b07-63a7-47a8-8cb6-28f8b9451eee", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4289d59c-11ae-426b-9242-63779a78e1e6", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e562e2e83f6c735bc24a528e4bddcd89a2556534809c5bdbc3f4c1ac38432919", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cbd77c1-40db-4bc4-9d24-fd207b4e9aad", "node_type": "1", "metadata": {}, "hash": "f25d243b1db3448af165a209eac74699242c611d3d7a24353defd37f5d3101b3", "class_name": "RelatedNodeInfo"}}, "text": "Clio: A Hardware-Software Co-Designed Dis-\naggregated Memory System. In Proceedings of the 27th ACM\nInternational Conference on Architectural Support for Program-\nming Languages and Operating Systems (ASPLOS) , 2022.\n[106] Vlad Nitu, Boris Teabe, Alain Tchana, Canturk Isci, and Daniel\nHagimont. Welcome to Zombieland: Practical and Energy-\nef\ufb01cient Memory Disaggregation in a Datacenter. In Proceedings\nof the 2018 EuroSys Conference (EuroSys) , 2018.\n[107] Irina Calciu, Ivan Puddu, Aasheesh Kolli, Andreas Nowatzyk,\nJayneel Gandhi, Onur Mutlu, and Pratap Subrahmanyam. Project\nPBerry: FPGA Acceleration for Remote Memory. In Proceed-\nings of the 17th Workshop on Hot Topics in Operating Systems\n(HotOS XVII) , 2019.\n[108] K. Katrinis, D. Syrivelis, D. Pnevmatikatos, G. Zervas,\nD. Theodoropoulos, I. Koutsopoulos, K. Hasharoni, D. Raho,\nC. Pinto, F. Espina, S. Lopez-Buedo, Q. Chen, M. Nemirovsky,\nD. Roca, H. Klos, and T. Berends. Rack-scale Disaggregated\nCloud Data Centers: The dReDBox Project Vision. In Design\nAutomation and Test in Europe (DATE) , 2016.\n[109] Jorge Gonzalez, Alexander Gazman, Maarten Hattink, Mauricio\nG. Palma, Meisam Bahadori, Ruth Rubio-Noriega, Lois Orosa,\nMadeleine Glick, Onur Mutlu, Keren Bergman, and Rodolfo\nAzevedo. Optically Connected Memory for Disaggregated Data\nCenters. In IEEE 32nd International Symposium on Computer\nArchitecture and High Performance Computing (SBAC-PAD) ,\n2020.\n[110] OpenCAPI Consortium. https://opencapi.org/ , 2021.\n[111] Sudarsun Kannan, Ada Gavrilovska, Vishal Gupta, and Karsten\nSchwan. HeteroOS: OS Design for Heterogeneous Memory\nManagement in Datacenters. In Proceedings of the 44th An-\nnual International Symposium on Computer Architecture (ISCA) ,\n2017.\n[112] Neha Agarwal and Thomas F. Wenisch. Thermostat: Application-\ntransparent Page Management for Two-tiered Main Memory. In\nProceedings of the 22nd ACM International Conference on Ar-\nchitectural Support for Programming Languages and Operating\nSystems (ASPLOS) , 2017.\n[113] Ahmed Abulila, Vikram Sharma Mailthody, Zaid Qureshi, Jian\nHuang, Nam Sung Kim, Jinjun Xiong, and Wen mei Hwu. Flat-\nFlash: Exploiting the Byte-Accessibility of SSDs within a Uni-\n\ufb01ed Memory-Storage Hierarchy. In Proceedings of the 24th ACM\nInternational Conference on Architectural Support for Program-\nming Languages and Operating Systems (ASPLOS) , 2019.\n[114] Jonghyeon Kim, Wonkyo Choe, and Jeongseob Ahn. Exploring\nthe Design Space of Page Management for Multi-Tiered Memory\nSystems. In Proceedings of the 2021 USENIX Annual Technical\nConference (ATC) , 2021.\n[115] Linux Memory Management Documentation - zswap. https:\n//www.kernel.org/doc/html/latest/vm/zswap.html ,\n2020.[116] Giulio Zhou and Martin Maas. Learning on Distributed Traces\nfor Data Center Storage Systems. In Proceedings of the 4th\nConference on Machine Learning and Systems (MLSys) , 2021.\n[117] Martin Maas, David G. Andersen, Michael Isard, Moham-\nmad Mahdi Javanmard, Kathryn S. McKinley, and Colin Raffel.\nLearning-based Memory Allocation for C++ Server Workloads.\nInProceedings of the 25th ACM International Conference on Ar-\nchitectural Support for Programming Languages and Operating\nSystems (ASPLOS) , 2020.\n[118] Yanqi Zhang, Weizhe Hua, Zhuangzhuang Zhou, G. Edward Suh,\nand Christina Delimitrou. Sinan: ML-Based and QoS-Aware Re-\nsource Management for Cloud Microservices.", "start_char_idx": 554101, "end_char_idx": 557462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cbd77c1-40db-4bc4-9d24-fd207b4e9aad": {"__data__": {"id_": "3cbd77c1-40db-4bc4-9d24-fd207b4e9aad", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c385b07-63a7-47a8-8cb6-28f8b9451eee", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "707510997cdec1363361cd2e77bbd070505a0b956f930dc12849a17e30ea5d50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35e8b006-71a1-4d0a-85d2-77d804f95787", "node_type": "1", "metadata": {}, "hash": "364992f27b3261e40c352d62a7b22d9c243757bc95ab3686b94a495745263882", "class_name": "RelatedNodeInfo"}}, "text": "[116] Giulio Zhou and Martin Maas. Learning on Distributed Traces\nfor Data Center Storage Systems. In Proceedings of the 4th\nConference on Machine Learning and Systems (MLSys) , 2021.\n[117] Martin Maas, David G. Andersen, Michael Isard, Moham-\nmad Mahdi Javanmard, Kathryn S. McKinley, and Colin Raffel.\nLearning-based Memory Allocation for C++ Server Workloads.\nInProceedings of the 25th ACM International Conference on Ar-\nchitectural Support for Programming Languages and Operating\nSystems (ASPLOS) , 2020.\n[118] Yanqi Zhang, Weizhe Hua, Zhuangzhuang Zhou, G. Edward Suh,\nand Christina Delimitrou. Sinan: ML-Based and QoS-Aware Re-\nsource Management for Cloud Microservices. In Proceedings of\nthe 26th ACM International Conference on Architectural Support\nfor Programming Languages and Operating Systems (ASPLOS) ,\n2021.\n[119] Zhan Shi, Akanksha Jain, Kevin Swersky, Milad Hashemi,\nParthasarathy Ranganathan, and Calvin Lin. A Hierarchical\nNeural Model of Data Prefetching. In Proceedings of the 26th\nACM International Conference on Architectural Support for Pro-\ngramming Languages and Operating Systems (ASPLOS) , 2021.\n[120] Zhan Shi, Xiangru Huang, Akanksha Jain, and Calvin Lin. Ap-\nplying Deep Learning to the Cache Replacement Problem. In\n52nd Annual IEEE/ACM International Symposium on Microar-\nchitecture (MICRO-52) , 2019.\n[121] James Laudon and Daniel Lenoski. The SGI Origin: A ccNUMA\nHighly Scalable Server. In Proceedings of the 24th Annual Inter-\nnational Symposium on Computer Architecture (ISCA) , 1997.\n[122] Baptiste Lepers, Vivien Qu\u00e9ma, and Alexandra Fedorova. Thread\nand Memory Placement on NUMA Systems: Asymmetry Mat-\nters. In Proceedings of the 2015 USENIX Annual Technical\nConference (ATC) , 2015.\n[123] Jonathan Corbet. Autonuma: the other approach to numa schedul-\ning.https://lwn.net/Articles/488709/ , 2012.\n[124] Dulloor Subramanya Rao and Karsten Schwan. vNUMA-mgr:\nManaging VM memory on NUMA platforms. In IEEE Inter-\nnational Conference on High Performance Computing (HiPC) ,\n2010.\n[125] Ming Liu and Tao Li. Optimizing Virtual Machine Consoli-\ndation Performance on NUMA Server Architecture for Cloud\nWorkloads. In Proceedings of the 41th Annual International\nSymposium on Computer Architecture (ISCA) , 2014.\n4Memory disaggregation:\nwhy now and what are the challenges\nMarcos K. Aguilera1, Emmanuel Amaro1, Nadav Amit1, Erika Hunhoff2, Anil Yelam3, Gerd Zellweger1\n1VMware Research2University of Colorado, Boulder3UC San Diego\nAbstract\nHardware disaggregation has emerged as one of the most\nfundamental shifts in how we build computer systems\nover the past decades. While disaggregation has been\nsuccessful for several types of resources (storage, power,\nand others), memory disaggregation has yet to happen.\nWe make the case that the time for memory disaggre-\ngation has arrived. We look at past successful disaggre-\ngation stories and learn that their success depended on\ntwo requirements: addressing a burning issue and being\ntechnically feasible. We examine memory disaggrega-\ntion through this lens and \ufb01nd that both requirements\nare \ufb01nally met. Once available, memory disaggregation\nwill require software support to be used effectively. We\ndiscuss some of the challenges of designing an operating\nsystem that can utilize disaggregated memory for itself\nand its applications.\n1 Introduction\nHardware disaggregation , or simply disaggregation ,\nmeans separating hardware resources (e.g., disks, GPU,\nmemory) that have been traditionally combined in a sin-\ngle server enclosure. Disaggregation has physical and log-\nical implications. Physically, the disaggregated resource\nis placed in a separate box or chassis, increasing its dis-\ntance from other resources. Logically, the disaggregated\nresource becomes less coupled with the rest of the sys-\ntem.\nFigure 1illustrates memory disaggregation: the mem-\nory moves from the con\ufb01nes of a host into a memory pool,\nwhich can then be accessed by multiple servers.", "start_char_idx": 556785, "end_char_idx": 560735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35e8b006-71a1-4d0a-85d2-77d804f95787": {"__data__": {"id_": "35e8b006-71a1-4d0a-85d2-77d804f95787", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cbd77c1-40db-4bc4-9d24-fd207b4e9aad", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "58e3bd543480842e56f8c4cc07f8b17d061a663a6d72bce3df52b9e721bc6ef4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdd6d74c-21de-44ed-a1eb-bf097c06ddc3", "node_type": "1", "metadata": {}, "hash": "a9c3300023a01c5d6d99295d7fe88caab53f5f3f56656f7910a879ba0f93ada7", "class_name": "RelatedNodeInfo"}}, "text": "Once available, memory disaggregation\nwill require software support to be used effectively. We\ndiscuss some of the challenges of designing an operating\nsystem that can utilize disaggregated memory for itself\nand its applications.\n1 Introduction\nHardware disaggregation , or simply disaggregation ,\nmeans separating hardware resources (e.g., disks, GPU,\nmemory) that have been traditionally combined in a sin-\ngle server enclosure. Disaggregation has physical and log-\nical implications. Physically, the disaggregated resource\nis placed in a separate box or chassis, increasing its dis-\ntance from other resources. Logically, the disaggregated\nresource becomes less coupled with the rest of the sys-\ntem.\nFigure 1illustrates memory disaggregation: the mem-\nory moves from the con\ufb01nes of a host into a memory pool,\nwhich can then be accessed by multiple servers. This pro-\nvides two bene\ufb01ts: servers can access large amounts of\nmemory in the pool (more than a server can have locally)\nand servers can use the disaggregated memory to ef\ufb01-\nciently share data. Note one need not disaggregate allserversCPUmemoryCPUmemoryCPUmemoryCPUmemoryCPUmemoryCPUmemCPUmemCPUmemCPUmemCPUmemdisaggmemoryserversmemorypoolFigure 1: With traditional servers (left), we have CPU,\nmemory, disks, and network adapters on a single enclo-\nsure. With memory disaggregation (right), we move some\nof that memory to a pool. Servers still have some local\nmemory but they can also access the memory in the pool\nto obtain additional memory or share data.\nmemory: some of it remains local memory in servers.\nMemory disaggregation originated from ideas in the\n1990s [ 11], and Intel proposed a commercial architecture\nin 2013 [ 16]. Nevertheless, memory disaggregation has\nyet to happen in production systems.\nTo understand why, we look at history to study other\nforms of successful disaggregation: mainframe comput-\ners, storage, power and cooling, and GPUs (Section 2).\nFrom these efforts, we learn that disaggregation succeeds\nbased on two requirements: the need to solve a burning\nissue and the availability of a feasible technology.\nWe argue that these two requirements are now met for\nmemory disaggregation as big data increasingly moves\nto memory, and memory becomes an expensive resource;\nmeanwhile, the emergence of Compute eXpress Link\n(CXL) provides technical feasibility (Section 3).38\nOnce available, disaggregated memory can signi\ufb01-\ncantly bene\ufb01t data-intensive systems, such as machine\nlearning data pipelines, distributed computing frame-\nworks, and database systems. These systems can lever-\nage the two main capabilities of disaggregated memory\n(larger memories and data sharing across servers), but\nthey will require appropriate operating system support.\nWe discuss the challenges of building an operating sys-\ntem that can leverage disaggregated memory to bene\ufb01t\nboth applications and the operating system itself (Sec-\ntion4).\n2 Disaggregation: the past\nDisaggregation brings different bene\ufb01ts and trade-offs\nthat depend on the disaggregated resource. In the rest of\nthis section, we discuss past successful disaggregation\nefforts and the lessons we can learn from them.\n2.1 From mainframes to clusters\nThe transition from large mainframe computers to com-\nputer clusters in the 1970s and 1980s can be seen as an\nextreme form of disaggregation where many resources\nwere simultaneously disaggregated: a monolithic box\nwas separated into multiple minicomputers that worked\ntogether. Initially, these minicomputers were tightly cou-\npled to provide the illusion of a single system (e.g., in a\nV AXcluster, one could uniformly handle processes, users,\nand disks from any computer in the cluster). Over time,\nminicomputers became less and less coupled with each\nother, losing their single-system transparency in favor\nof larger systems deployed across broader geographic\nlocations (e.g., Unix clusters). This trend eventually led\nto systems that connect minicomputers in different or-\nganizations (often universities or national laboratories)\nover long-distance lines using modems at \ufb01rst, or ded-\nicated lines later, to exchange email and access usenet\nnewsgroups.\nHere, disaggregation was motivated initially by cost\nand scale (mainframes were expensive) and later by ac-\ncessibility (users around the country wanted access to\ncomputers, but mainframes were only available in a few\nlocations, while the smaller computers could be spread\naround). Ultimately, mainframe disaggregation led to the\ncreation of the Internet.\n2.2 Storage disaggregation\nStorage disaggregation followed a gradual path starting\nin the early 1990s.", "start_char_idx": 559875, "end_char_idx": 564478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdd6d74c-21de-44ed-a1eb-bf097c06ddc3": {"__data__": {"id_": "bdd6d74c-21de-44ed-a1eb-bf097c06ddc3", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35e8b006-71a1-4d0a-85d2-77d804f95787", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3057718aa7a63bd1cccbae83ea8b35ba1c503ae62410da5db8b8bfae91975ec1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37593358-e774-4741-9978-f10dced5d88e", "node_type": "1", "metadata": {}, "hash": "01f85e425e9283a2dea7bf1fbfdc2a75d9ee8b801fc2ff8c0e174b73e8935554", "class_name": "RelatedNodeInfo"}}, "text": "Over time,\nminicomputers became less and less coupled with each\nother, losing their single-system transparency in favor\nof larger systems deployed across broader geographic\nlocations (e.g., Unix clusters). This trend eventually led\nto systems that connect minicomputers in different or-\nganizations (often universities or national laboratories)\nover long-distance lines using modems at \ufb01rst, or ded-\nicated lines later, to exchange email and access usenet\nnewsgroups.\nHere, disaggregation was motivated initially by cost\nand scale (mainframes were expensive) and later by ac-\ncessibility (users around the country wanted access to\ncomputers, but mainframes were only available in a few\nlocations, while the smaller computers could be spread\naround). Ultimately, mainframe disaggregation led to the\ncreation of the Internet.\n2.2 Storage disaggregation\nStorage disaggregation followed a gradual path starting\nin the early 1990s. It started with the realization thatif a computer crashes, all data in its locally attached\ndisks become inaccessible. To address this problem and\nimprove data availability, the industry introduced dual-\nported SCSI disks, which could be connected to two com-\nputers simultaneously. This idea evolved into storage area\nnetworks (SANs), where disk arrays are attached to a net-\nwork fabric connected to many computers which can\naccess any disk. The network fabric was initially dedi-\ncated for storage (Fibre Channel) but later proposals used\ngeneral-purpose networks (iSCSI, NVMe over Fabrics).\nAlthough storage disaggregation was initially moti-\nvated by improved data availability, it later brought other\nbene\ufb01ts: manageability (it is easier to manage a central\npool of storage than storage spread over all computers),\nreliability (the central storage pool became sophisticated\nfault-tolerant disk arrays), and disaster tolerance (disk\narrays could copy data to other disk arrays across wide\narea networks).\n2.3 Power and cooling disaggregation\nPower supplies and fans are clunky components that tend\nto fail often. Thus, for reliability, servers tend to have\ntwo sets of power supplies and fans, but this is a signi\ufb01-\ncant waste of space and money: a rack with 40 servers\nwould require 80 power supplies and fans. It makes much\nmore sense to disaggregate the power supplies and fans\nso that many servers can share a few. Blade servers ac-\ncomplished this in the 2000s, where a single chassis with\nits own power supplies and fans housed multiple servers.\nThis idea has motivated the rack designs in the Open\nCompute Project [ 25] and Open19 [ 26], which are used\nin today\u2019s hyperscalers: each rack has power supplies\nand fans, and rack servers take DC power as input.\nPower and cooling disaggregation was initially moti-\nvated by density and cost in blade servers, and it eventu-\nally led to new data center designs.\n2.4 GPU disaggregation\nGraphics Processing Units, or GPUs, started as graphics\ncards that produced a computer\u2019s video output. Later,\nGPUs gained general-purpose acceleration support for\nSingle Instruction Multiple Data computations (e.g., vec-\ntor and matrix operations), often used in machine learning\nalgorithms. Modern GPUs are extremely powerful and\nhave a much higher degree of parallelism than proces-\nsors by several orders of magnitude. This power comes\nat a high cost in price and power consumption. If many\nservers need access to GPUs, it is expensive to provi-\nsion each server with its own GPU. If those GPUs are\nused at different times, their utilization will be low. With39GPU disaggregation, the GPUs in a pool are accessible\nto many servers, so one can provision a smaller number\nof GPUs and increase their utilization. This idea was \ufb01rst\nimplemented in software in the mid-2010s by redirecting\nCUDA calls to a remote server housing the GPU (e.g.,\nBitfusion [ 6]). Later, composable infrastructure systems\ngained GPU disaggregation support (e.g., HPE Synergy,\nDell PowerEdge MX, Supermicro SuperCloud Composer,\nLenovo ThinkAgile CP).\nGPU disaggregation was motivated by utilization and\ncost, and we believe the future will bring other bene\ufb01ts\nas the idea gains further adoption.\n2.5 Lessons\nLooking at the past success of hardware disaggregation,\nwe identify some common patterns. We observe that\nthere are two requirements for disaggregation of a re-\nsource to succeed.", "start_char_idx": 563552, "end_char_idx": 567884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37593358-e774-4741-9978-f10dced5d88e": {"__data__": {"id_": "37593358-e774-4741-9978-f10dced5d88e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdd6d74c-21de-44ed-a1eb-bf097c06ddc3", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "992a46fc159502702da2128982ed6cf30430e5ee25b5223eb38eb325cf6faff1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "356aa619-1ff6-42d0-9af2-a450d6e2082f", "node_type": "1", "metadata": {}, "hash": "8981606a640c271b2c5048f3aedf57a1c76eaa321eb9021694fbe12d1359a196", "class_name": "RelatedNodeInfo"}}, "text": "With39GPU disaggregation, the GPUs in a pool are accessible\nto many servers, so one can provision a smaller number\nof GPUs and increase their utilization. This idea was \ufb01rst\nimplemented in software in the mid-2010s by redirecting\nCUDA calls to a remote server housing the GPU (e.g.,\nBitfusion [ 6]). Later, composable infrastructure systems\ngained GPU disaggregation support (e.g., HPE Synergy,\nDell PowerEdge MX, Supermicro SuperCloud Composer,\nLenovo ThinkAgile CP).\nGPU disaggregation was motivated by utilization and\ncost, and we believe the future will bring other bene\ufb01ts\nas the idea gains further adoption.\n2.5 Lessons\nLooking at the past success of hardware disaggregation,\nwe identify some common patterns. We observe that\nthere are two requirements for disaggregation of a re-\nsource to succeed. First, it needs to address a burning\nissue; that is, it needs a powerful, compelling motivation:\ncost and scalability for mainframe disaggregation, data\navailability for storage disaggregation, density and cost\nfor power and cooling disaggregation, and utilization and\ncost for GPU disaggregation. A burning issue is required\nbecause disaggregation needs signi\ufb01cant and simultane-\nous investment from many parties: chip makers, device\nmakers, systems integrators, and sometimes operating\nsystem and application vendors. These parties must over-\ncome signi\ufb01cant inertia to build solutions that work well\ntogether, and this is only possible with a strong use case.\nThe second requirement to disaggregate a resource is a\nfeasible technology , which often requires addressing non-\ntrivial technical problems at many levels of the system\nstack: cluster systems, remote storage systems, blades,\ncomposable hardware, etc.\nAnother lesson we learn from the past is that, once\ndisaggregation happens, it brings a much broader impact\nthan originally anticipated. Mainframe disaggregation\nled to a revolution in distributed systems; storage disag-\ngregation created an entire industry of storage appliances;\npower and cooling disaggregation led to new data center\ndesigns; and GPU disaggregation is still ongoing so the\njury is still out, but it will possibly create new computing\nparadigms for processing of data pipelines.\n3 Memory disaggregation: why now?\nMemory disaggregation originated with paging to re-\nmote memory in the early 1990s [ 11]. The network had\nbecome fast enough that it was more ef\ufb01cient to page\nto remote memory using the network than to a localdisk. These initial efforts provided a rudimentary form of\nmemory disaggregation: a processor accessing disaggre-\ngated memory incurs a page fault, bringing the data to\nlocal memory before the processor can resume execution.\nFull-\ufb02edged memory disaggregation was \ufb01rst proposed\nin [21], where a processor can issue loads and stores di-\nrectly to disaggregated memory, without incurring page\nfaults, thus improving performance.\nWhile memory disaggregation is an old idea, it has\nnot yet been commercially successful. As we mentioned\nbefore, Intel proposed a Rack Scale Architecture in\n2013 [ 16] that included memory disaggregation. How-\never, the proposal did not succeed because it was based\non the technology of silicon photonics [ 32], which has\nnot yet prevailed.\nThat begs the question: why is the right time for mem-\nory disaggregation now? We next explain why we believe\nthat is the case, by arguing that memory now satis\ufb01es\nthe two requirements for successful disaggregation, as\ndescribed in Section 2.5: addressing a burning issue and\nthe existence of feasible technology.\nBurning issue. Memory is increasingly becoming a\nproblematic resource in computing systems, due to four\nreasons. First, the need for memory is surging as big\ndata moves to memory for faster processing in a broad\nrange of systems and applications, such as in-memory\ndatabases, data analytics, streaming analytics, and ma-\nchine learning. Second, the memory capacity of indi-\nvidual servers is constrained due to physical limitations\nin the density of memory DIMMs and the number of\nDIMMs that can be connected to a processor. Third,\nover time, applications need more memory but upgrading\nmemory is notoriously challenging due to the stringent\nmemory population rules for servers [ 1]\u2014for example,\nall memory controllers should have the same number and\nsizes of DIMMs, and all memory channels should have\nthe same capacity. Failure to follow these rules results in\npoor memory bandwidth. Fourth, the cost of memory is\ngrowing due to the proliferation of new use cases: smart\nvehicles, 5G phones, gaming consoles, and data centers.", "start_char_idx": 567079, "end_char_idx": 571650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "356aa619-1ff6-42d0-9af2-a450d6e2082f": {"__data__": {"id_": "356aa619-1ff6-42d0-9af2-a450d6e2082f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37593358-e774-4741-9978-f10dced5d88e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "61246e9b228e0043dec901960120499fcd29b0cd08986a00b3ef92eeb82a2b0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "163c397a-21c7-4cc0-a296-82647ae298dc", "node_type": "1", "metadata": {}, "hash": "25da171927b659a06cf57110d1eee89116d73e3c1afc227d6c31ed784790dbdf", "class_name": "RelatedNodeInfo"}}, "text": "Second, the memory capacity of indi-\nvidual servers is constrained due to physical limitations\nin the density of memory DIMMs and the number of\nDIMMs that can be connected to a processor. Third,\nover time, applications need more memory but upgrading\nmemory is notoriously challenging due to the stringent\nmemory population rules for servers [ 1]\u2014for example,\nall memory controllers should have the same number and\nsizes of DIMMs, and all memory channels should have\nthe same capacity. Failure to follow these rules results in\npoor memory bandwidth. Fourth, the cost of memory is\ngrowing due to the proliferation of new use cases: smart\nvehicles, 5G phones, gaming consoles, and data centers.\nMeanwhile, supply remains restricted by an oligopoly\nof only three memory companies (Samsung, SK Hynix,\nMicron). As a result, memory can dominate the cost of\nthe system. In fact, cloud operators report that memory\ncan constitute 50% of server cost [ 20] and 37% of total\ncost of ownership [ 23].\nFeasible technology. The technology that enables dis-\naggregated memory has signi\ufb01cantly advanced. Commer-\ncially, Remote Direct Memory Access (RDMA) has be-40come commodity with technologies such as RDMA over\nConverged Ethernet (RoCE), which provides RDMA on\nEthernet networks. Furthermore, standards for disaggre-\ngated memory are emerging, initially through the GenZ\nConsortium [ 12] and more recently through Compute\neXpress Link (CXL [ 9]). CXL allows devices to pro-\nvide cache-coherent memory on the PCIe bus; version\n1 of CXL enables local memory expansion cards while\nsubsequent versions will enable memory disaggregation\nvia cards connected to memory pools. Research proto-\ntypes that leverage these technologies, including The\nMachine [ 18,34] based on Gen-Z, and DirectCXL [ 13]\nand Pond [ 20] based on CXL, demonstrate the feasibil-\nity of disaggregated memory. Moreover, CXL version 1\nis already supported by the latest server processors by\nIntel and AMD, while device vendors gradually intro-\nduce local memory expansion cards. We anticipate CXL\nmemory disaggregation to come soon after (say, in 3\u20135\nyears) following a similar trajectory as storage disaggre-\ngation: beginning with multi-ported memory connected\nto a few hosts (say, 2\u20138), followed by CXL switches on\ndedicated memory fabrics connecting memory to a rack\nof servers, and ultimately reaching convergence of the\nnetwork and memory fabric (e.g., CXL over Ethernet or\nIP). A noteworthy feature of CXL is that it will allow the\nmemory pool to use different memory technology from\nones locally attached to servers. Consequently, as servers\nevolve to use the newer DDR5 memory, the pool can be\nprovisioned with older DDR3 and DDR4 memory that is\nincompatible or too slow be used in the server.\n4An operating system for disaggregated\nmemory\nWe are developing a new operating system (OS) that of-\nfers \ufb01rst-class support for disaggregated memory. Our\ngoal is to allow the OS and applications to take full advan-\ntage of the capabilities of disaggregated memory: greater\nmemory capacity and the ability to share data ef\ufb01ciently\nacross servers. We refer to the union of the servers and\nthe disaggregated memory as the cluster . This new OS\ndiffers from traditional distributed OSes in two key ways.\nFirst, we want the OS to conveniently expose disaggre-\ngated memory to applications so they can bene\ufb01t from it.\nSecond, the OS itself can use disaggregated memory to\nimprove its design\u2014for example, by sharing kernel data\nstructures across servers.\nDisaggregated memory allows the OS to provide the il-\nlusion of a single system across the cluster to applications.\nIn particular, the OS will allow processes to have threads\nrunning on different servers, and these threads can ac-cess a uni\ufb01ed \ufb01le system. In addition, as processes can\nspan multiple servers, we need a communication stack\nthat can provide a uni\ufb01ed view across the cluster, so an\nexternal client can reach application processes regardless\nof which servers are running them.\nIn terms of disaggregated memory management, the\nOS will offer both transparent and targeted memory al-\nlocations. A transparent allocation can be served from\neither local or disaggregated memory (i.e., the process\ndoes not care), while a targeted allocation chooses be-\ntween local memory or the memory pool.", "start_char_idx": 570959, "end_char_idx": 575262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "163c397a-21c7-4cc0-a296-82647ae298dc": {"__data__": {"id_": "163c397a-21c7-4cc0-a296-82647ae298dc", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "356aa619-1ff6-42d0-9af2-a450d6e2082f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3f560f147b75e7e55be77f4505588c1d378be360cdeddc47e21d2f0161af64b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09c535d1-6be3-4eb9-acdc-dd6a90cec0fc", "node_type": "1", "metadata": {}, "hash": "705a0389fb83cea294a3cdc18d970750d73cf540e8e2c853c6a076714259b198", "class_name": "RelatedNodeInfo"}}, "text": "Second, the OS itself can use disaggregated memory to\nimprove its design\u2014for example, by sharing kernel data\nstructures across servers.\nDisaggregated memory allows the OS to provide the il-\nlusion of a single system across the cluster to applications.\nIn particular, the OS will allow processes to have threads\nrunning on different servers, and these threads can ac-cess a uni\ufb01ed \ufb01le system. In addition, as processes can\nspan multiple servers, we need a communication stack\nthat can provide a uni\ufb01ed view across the cluster, so an\nexternal client can reach application processes regardless\nof which servers are running them.\nIn terms of disaggregated memory management, the\nOS will offer both transparent and targeted memory al-\nlocations. A transparent allocation can be served from\neither local or disaggregated memory (i.e., the process\ndoes not care), while a targeted allocation chooses be-\ntween local memory or the memory pool. Transparent\nallocations allow disaggregated memory to be used as an\nextension of local memory. In contrast, a targeted allo-\ncation provides faster memory (if allocating locally) or\nshareable memory (if allocating from the pool).\nProviding \ufb01rst-class support for shareable disaggre-\ngated memory will bene\ufb01t data-intensive distributed ap-\nplications, which often incur signi\ufb01cant overheads when\nsharing data because they must rely on traditional net-\nwork stacks (e.g., RPC, TCP) and pay the costs of serial-\nization, deserialization, and copying [ 17]. Concretely, we\ntarget data processing systems (e.g., machine learning\npipelines [ 15]), distributed computing frameworks (e.g.,\nSpark [ 37], Ray [ 35]), cluster schedulers (e.g., Kuber-\nnetes), and some traditional applications such as database\nsystems (SQL and NoSQL), web servers, and \ufb01le servers.\nFor example, a distributed computing framework can\noptimize object movement between tasks by using disag-\ngregated memory; a database system doing distributed\nquery processing may create shared intermediate data and\nmaterialized views in disaggregated memory; a cluster\nscheduler also needs to share executables, job inputs, and\njob outputs; a web server or \ufb01le server can share caches;\nand all of these systems can bene\ufb01t from a larger memory\ncapacity that disaggregated memory can provide.\n4.1 Challenges\nBuilding an OS for disaggregated memory raises a num-\nber of challenges.\nMemory allocation. When a process allocates mem-\nory, the OS must decide from where. If the application\ndoes not care if the memory is local or disaggregated,\nthe OS must pick a type based on memory availability\nand locality. This presents a memory tiering problem,\nwhere two types of memory offer different trade-offs:\nlocal memory is faster and private, while disaggregated\nmemory is larger and shared. Even if the application\nmakes a targeted request for local or disaggregated mem-\nory, the OS needs appropriate mechanisms to organize the41memory. Traditional schemes such as buddy allocators\nand slab allocators need to be revisited since allocation\nrequests will now be distributed across servers and must\nscale to large memory sizes [ 22] and a large number of\nprocesses, while keeping fragmentation under control. A\nsolution is to handle all allocations centrally at one of\nthe servers; another solution is to allocate memory in\na distributed fashion, which requires ef\ufb01cient coordina-\ntion across servers. These solutions trade off simplicity,\nglobal optimality, allocation latency, and fragmentation.\nMemory migration presents a related problem: the OS\ncan move physical memory (while keeping virtual ad-\ndresses) to remove fragmentation and improve locality\n(e.g., a process has freed some local memory so that data\nin disaggregated memory can be migrated locally for\nfaster access). Memory migration requires policies of\nwhen and what to migrate, analogous to non-uniform\nmemory access (NUMA) migration policies.\nScheduling. Where should processes and their threads\nexecute? This decision depends on several factors. First,\nthe servers eligible to schedule a thread depend on their\nresource availability (CPU, local memory, etc.). Second,\nprocesses that share buffers in disaggregated memory\ncan bene\ufb01t from cache locality if they are scheduled\non the same server or a small number of servers; this\naf\ufb01nity needs to be considered. Third, the OS should\nprovide a balanced consumption of per-server CPU and\nmemory; to do so, the scheduler and memory allocator\nmust collaborate.", "start_char_idx": 574327, "end_char_idx": 578784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09c535d1-6be3-4eb9-acdc-dd6a90cec0fc": {"__data__": {"id_": "09c535d1-6be3-4eb9-acdc-dd6a90cec0fc", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "163c397a-21c7-4cc0-a296-82647ae298dc", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "0acadee43e7280c1c498680331890a5caada7e54387fd42ca3c4cd345a5f0298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c31b763-6004-43ef-ac2d-5d78e0d62b79", "node_type": "1", "metadata": {}, "hash": "c8cc9a52241c7596599d64a5d4b3242e82c075b2eb184bbc2fdc1c5bef2956a5", "class_name": "RelatedNodeInfo"}}, "text": "Memory migration requires policies of\nwhen and what to migrate, analogous to non-uniform\nmemory access (NUMA) migration policies.\nScheduling. Where should processes and their threads\nexecute? This decision depends on several factors. First,\nthe servers eligible to schedule a thread depend on their\nresource availability (CPU, local memory, etc.). Second,\nprocesses that share buffers in disaggregated memory\ncan bene\ufb01t from cache locality if they are scheduled\non the same server or a small number of servers; this\naf\ufb01nity needs to be considered. Third, the OS should\nprovide a balanced consumption of per-server CPU and\nmemory; to do so, the scheduler and memory allocator\nmust collaborate. Fourth, the memory pool may have\nmemory regions with different distances to a server, and\nit may have regions that are only accessible to some of\nthe servers due to the topology of the memory fabric; this\nis another case where the scheduler and allocator must\ncoordinate. A scheduler design that weighs these factors\ncan be centralized (as we can centralize the allocator), but\na distributed design can scale better and provide lower\nlatency.\nAddressing. Disaggregated memory will have a pool\nmemory controller that connects the pool memory to the\nservers. Servers will map virtual addresses to physical\naddresses, and the pool controller will map physical ad-\ndresses to pool addresses. Translating addresses in the\npool controller allows the system to easily move mem-\nory within the pool (without it, all servers using a shared\nbuffer would have to \ufb01x their mappings when memory\nis moved), which in turn is useful for maintenance (e.g.,\nadding or removing DIMMs to the pool) and optimization\n(e.g., for pools that have regions with different memory\ntypes). This creates many questions: what should thegranularity of these mappings be, what is the underlying\nmechanism used to maintain these mappings, and who\nwill maintain the mappings. Answering these questions\nwill require a co-design of the pool controller (which per-\nforms the actual translations) and the OS (which provides\nthe functional requirements). Another related challenge\nis how to correctly share pointers to shared buffers across\nservers. A simple solution is to map the buffers to the\nsame virtual address at all servers, but this solution may\nnot scale well as it requires memory allocations to return\nunique virtual addresses across processes in the cluster\n(if virtual addresses are not unique, buffers cannot be\nshared between processes).\nOS state. Operating systems keep many types of in-\nternal state (e.g., process tables, device state, network\nconnections), which raises three challenges for a disag-\ngregated memory OS. First, we must identify the state\nneeded locally at each server and the state that must be\nshared across servers. For example, some memory allo-\ncation metadata should be shared because we would like\ndifferent processes to communicate using disaggregated\nmemory; similarly, some OS scheduler state should be\nshared to support ef\ufb01cient process synchronization mech-\nanisms (e.g., wait queues for locks being held). On the\nother hand, device state is local because we do not wish\nto expose devices across servers. An exception for this\nis the storage and network devices, which we address\nbelow.\nThe second challenge is ef\ufb01ciently coordinating ac-\ncess by different servers. Although this problem exists in\ntraditional operating systems, it is more severe when us-\ning disaggregated memory since the coordination cost is\nhigher (disaggregated memory is slower than local mem-\nory), and more parties can coordinate (thousands of cores\nacross all CPUs in all servers). In theory, RCU, wait-free\ndata structures, or \ufb01ne-grained locking mechanisms can\nbe used to tackle this challenge. However, with much\nhigher memory latencies and number of cores, even the\nmost sophisticated approaches eventually succumb under\ncontention. Therefore, we need radically new approaches\nto manage OS state that scales well for reads and writes\nacross machines. Partitioning (for writes) and replica-\ntion (for reads) are often employed strategies to solve\nthese challenges. Replication trades off extra memory\nconsumption for more performance so ideally it can be\napplied in a dynamic way, using only memory the OS\ncan currently spare.\nThe third challenge is that the larger capacity of disag-\ngregated memory will result in much larger kernel data\nstructures, to the point that they will require their own42memory management mechanisms (e.g., memory migra-\ntion).", "start_char_idx": 578092, "end_char_idx": 582626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c31b763-6004-43ef-ac2d-5d78e0d62b79": {"__data__": {"id_": "3c31b763-6004-43ef-ac2d-5d78e0d62b79", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09c535d1-6be3-4eb9-acdc-dd6a90cec0fc", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "7a5195e1a96e2dd92ee833d0d22f0717021767dd981afc53d87a0dfac9b852ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91245482-9d09-44f6-8934-e9550d494389", "node_type": "1", "metadata": {}, "hash": "017fbb5bbd9d24f73a024cf0b071f999bb83f836adc8d5b3235e935950c79bbc", "class_name": "RelatedNodeInfo"}}, "text": "In theory, RCU, wait-free\ndata structures, or \ufb01ne-grained locking mechanisms can\nbe used to tackle this challenge. However, with much\nhigher memory latencies and number of cores, even the\nmost sophisticated approaches eventually succumb under\ncontention. Therefore, we need radically new approaches\nto manage OS state that scales well for reads and writes\nacross machines. Partitioning (for writes) and replica-\ntion (for reads) are often employed strategies to solve\nthese challenges. Replication trades off extra memory\nconsumption for more performance so ideally it can be\napplied in a dynamic way, using only memory the OS\ncan currently spare.\nThe third challenge is that the larger capacity of disag-\ngregated memory will result in much larger kernel data\nstructures, to the point that they will require their own42memory management mechanisms (e.g., memory migra-\ntion). These mechanisms are normally used on user mem-\nory and they can be hard to apply to kernel memory (e.g.,\nmigration of kernel memory can cause deadlocks as ker-\nnel pages get write-protected and subsequently cause a\npage fault).\nFile system. To provide a single \ufb01le system view across\nall servers, a simple solution is to mount a network \ufb01le\nsystem (e.g., NFS, CIFS) on all servers. However, this\napproach misses an opportunity to leverage disaggre-\ngated memory. File systems can use the larger capacity\nof disaggregated memory for caching (buffer cache, page\ncache, inode cache, etc.), and they can use the sharing\nability of disaggregated memory to share these caches\nacross servers to improve cache utilization and reduce\ndisk IO\u2014if a server reads a \ufb01le, another server can \ufb01nd its\ncontents in the disaggregated memory cache. Designing\na disaggregated memory \ufb01le system raises the question\nof how to keep the various in-memory \ufb01le system data\nstructures, how to synchronize access to these structures,\nand how to schedule IO on disk. The result will be a type\nof distributed \ufb01le system that is uniquely designed for\ndisaggregated memory.\nExternal communication. In addition to local network\nendpoints for each server, we believe the OS should pro-\nvide network endpoints for the entire cluster. That is,\nthe cluster should have an external IP address (or a few\nIP addresses) where communication with that IP on a\ngiven port is transparently mapped to a server in the clus-\nter. According to application needs, the target server can\nbe \ufb01xed or it can be chosen from a number of servers\nthat balance load among themselves. This functionality is\nuseful for implementing scalable and highly available ser-\nvices, such as web servers, \ufb01le servers, etc. To implement\nthis feature, the OS leverages disaggregated memory to\nmaintain shared network state (e.g., which processes are\nbound to which ports) and an ef\ufb01cient way to coordinate\nshared access.\nFailures. Disaggregated memory can experience par-\ntial or even total failures as the pool crashes or loses\nconnectivity to the servers. The OS needs to handle such\nfailures gracefully by containing the problem to the parts\nof the system that used the failed memory. It may be nec-\nessary to kill processes that lost data, but not necessarily\ntake down the entire OS. Moreover, the OS will support\nthe notion of optional memory , which is memory that is\nnot vital for the execution of the process, such as caches\nor reconstructible data. If a failure affects only optionalmemory of a process, the process receives an exception\nand continues running. Finally, the OS needs to cleanly\ndecouple the data structures that it keeps in disaggregated\nmemory to minimize the blast radius. In some cases, it\ncould make sense to keep critical kernel data structures\nentirely in local memory.\nSecurity. We need to \ufb01nd a suitable trust model for the\ncluster. In a typical OS, the kernel is trusted and processes\nare isolated from each other. Our OS can offer defense\nin depth by providing isolation of access to the (shared)\ndisaggregated memory; in particular, there should be iso-\nlation between processes in different servers and perhaps\neven between kernels in different servers. A related issue\nis to provide address space isolation in disaggregated\nmemory for locations that are not being shared across\nservers. Here, again, is an opportunity for a co-design\nof the disaggregated memory controller and OS, where\nthe controller provides isolation mechanisms (e.g., ac-\ncess control for servers) while the OS sets the policy.", "start_char_idx": 581750, "end_char_idx": 586191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91245482-9d09-44f6-8934-e9550d494389": {"__data__": {"id_": "91245482-9d09-44f6-8934-e9550d494389", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c31b763-6004-43ef-ac2d-5d78e0d62b79", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "28f538d6994fccf4f883a52583adc43904acb59329b76cff3b259f2fbbcfbbbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84b35d94-701e-4ee9-90ba-d565c8694fd4", "node_type": "1", "metadata": {}, "hash": "3567e94342cc7818c652b637ac12af14a29aa367925bcd9a2609de78c7084cff", "class_name": "RelatedNodeInfo"}}, "text": "In some cases, it\ncould make sense to keep critical kernel data structures\nentirely in local memory.\nSecurity. We need to \ufb01nd a suitable trust model for the\ncluster. In a typical OS, the kernel is trusted and processes\nare isolated from each other. Our OS can offer defense\nin depth by providing isolation of access to the (shared)\ndisaggregated memory; in particular, there should be iso-\nlation between processes in different servers and perhaps\neven between kernels in different servers. A related issue\nis to provide address space isolation in disaggregated\nmemory for locations that are not being shared across\nservers. Here, again, is an opportunity for a co-design\nof the disaggregated memory controller and OS, where\nthe controller provides isolation mechanisms (e.g., ac-\ncess control for servers) while the OS sets the policy.\nEnclaves play an important role as they can provide a\nthin trusted computing base across servers, which can\nbe used to control the hardware mechanisms. Disaggre-\ngated memory encryption is another interesting capability\nand even more relevant than encryption of local mem-\nory, as we expect hot-swap of disaggregated memory\nsimilar to hot-swap of disks, where it is desirable to pro-\ntect data of memory being taken out (DRAM chips keep\ndata residues for some time after they are powered down,\nwhich can lead to known attacks).\n5 Other related work\nLegoOS [ 31] is an OS designed for hardware disaggre-\ngation of all resources (not just memory) based on the\nnotion of loosely coupled monitors. Our goal is different,\nas we are focused on disaggregated memory, speci\ufb01cally\non how to build an OS that allows applications to best\nleverage shareable disaggregated memory. The notion\nof a single system image across a cluster is provided by\ncluster OSes (e.g., VMS [ 27] for V AXclusters); we be-\nlieve it is worth revisiting these design with disaggregated\nmemory and modern hardware in mind.\nFirebox [ 3] and dReDBox [ 5] provide hardware plat-\nforms that disaggregate multiple resources, which fur-\nther motivates OSs designed for disaggregation. Work on\nHPE\u2019s The Machine included not just the hardware for\npersistent disaggregated memory, but also software sup-\nport [ 18] and OS challenges [ 10], which overlap some of\nthe challenges we describe but with a different perspec-\ntive (e.g., due to the fact that disaggregated memory is43persistent or due to the speci\ufb01cs of their hardware design).\nUnfortunately, a detailed write-up of their OS design is\nnot yet available.\nMemory vendors are providing toolkits (e.g., [ 33]) to\nutilize CXL memory in existing OSes. This effort is ade-\nquate for CXL memory expansion cards\u2014which provide\nadditional local memory\u2014but we believe shareable dis-\naggregated memory will require a new OS or signi\ufb01cant\nOS modi\ufb01cations.\nRemote memory access using RDMA has become\nwidely available with RoCE [ 29] and have enabled sys-\ntems that provide disaggregated memory through pag-\ning [1,14], user libraries [ 30,38], or by leveraging cache\ncoherence mechanisms [ 8]. These systems provide one\nbene\ufb01t of disaggregated memory (larger memory capac-\nity) but does not allow processes in different servers to\nshare memory. Some of these systems use software-based\nsolutions that are likely to underperform the hardware-\nbased disaggregation that is enabled by CXL.\nRecent memory tiering systems have focused on mech-\nanisms akin to NUMA migration with the goal of main-\ntaining hot subsets of memory in fast DRAM while mi-\ngrating cold subsets to slower locally-attached memory\n(e.g., NVM) [ 23,28,36]. Therefore, these systems neither\npresent disaggregated memory to applications, nor do\nthey consider sharing of such memory across processes\nin multiple servers.\nDisaggregated memory can be seen as a form of dis-\ntributed shared memory [ 2,4,7,19,24], which has been\nextensively studied decades ago and recently. There is an\nimportant lesson to learn from this body of work: cache\ncoherence is extremely hard to scale. We thus believe\ncache-coherent disaggregated memory systems will be\nlimited to a rack or a few racks of servers.\n6 Conclusion\nMemory disaggregation will \ufb01nally become a reality,\nenabled by the emergence of CXL and its adoption by\nsuppliers of memory, chipsets, controllers, servers, op-\nerating systems, and software.", "start_char_idx": 585355, "end_char_idx": 589655, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84b35d94-701e-4ee9-90ba-d565c8694fd4": {"__data__": {"id_": "84b35d94-701e-4ee9-90ba-d565c8694fd4", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91245482-9d09-44f6-8934-e9550d494389", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e79153ddae18d24dc9f8747bdfefccd02d43234adb14a86c5d7bd7eb1202f9bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5720807-474f-45d7-903e-0bbd458ae2b3", "node_type": "1", "metadata": {}, "hash": "5a58bad282f17a7fbdb870453d75237d9924d33b1040c7d0a9f54d9d655fc583", "class_name": "RelatedNodeInfo"}}, "text": "Therefore, these systems neither\npresent disaggregated memory to applications, nor do\nthey consider sharing of such memory across processes\nin multiple servers.\nDisaggregated memory can be seen as a form of dis-\ntributed shared memory [ 2,4,7,19,24], which has been\nextensively studied decades ago and recently. There is an\nimportant lesson to learn from this body of work: cache\ncoherence is extremely hard to scale. We thus believe\ncache-coherent disaggregated memory systems will be\nlimited to a rack or a few racks of servers.\n6 Conclusion\nMemory disaggregation will \ufb01nally become a reality,\nenabled by the emergence of CXL and its adoption by\nsuppliers of memory, chipsets, controllers, servers, op-\nerating systems, and software. This industry alignment\nis driven by the memory problems faced by distributed\ndata-intensive applications. Once available, memory dis-\naggregation will introduce many OS challenges that must\nbe addressed to best use this technology for memory\nexpansion and memory sharing across servers.\nReferences\n[1]Emmanuel Amaro, Christopher Branner-Augmon,\nZhihong Luo, Amy Ousterhout, Marcos K Aguil-era, Aurojit Panda, Sylvia Ratnasamy, and Scott\nShenker. Can far memory improve job through-\nput? In European Conference on Computer Sys-\ntems, pages 1\u201316, April 2020.\n[2]Cristiana Amza, Alan L. Cox, Shandya Dwarkadas,\nPete Keleher, Honghui Lu, Ramakrishnan Raja-\nmony, Weimin Yu, and Willy Zwaenepoel. Tread-\nMarks: Shared memory computing on networks of\nworkstations. IEEE Computer , 29(2):18\u201328, Febru-\nary 1996.\n[3]Krste Asanovi \u00b4c. FireBox: A hardware building\nblock for 2020 Warehouse-Scale computers. In\nUSENIX Conference on File and Storage Technolo-\ngies, February 2014. Keynote talk.\n[4]J. K. Bennett, J. B. Carter, and W. Zwaenepoel.\nMunin: Distributed shared memory based on type-\nspeci\ufb01c memory coherence. In ACM Symposium on\nPrinciples and Practice of Parallel Programming ,\npages 168\u2013176, March 1990.\n[5]Maciej Bielski, Ilias Syrigos, Kostas Katrinis, Dim-\nitris Syrivelis, Andrea Reale, Dimitris Theodor-\nopoulos, Nikolaos Alachiotis, Dionisios N. Pnev-\nmatikatos, Evert H. Pap, Georgios Zervas, Vaib-\nhawa Mishra, Arsalan Saljoghei, Alvise Rigo,\nJose Fernando Zazo, Sergio Lopez-Buedo, Mart\u00ed\nTorrents, Ferad Zyulkyarov, Michael Enrico, and\nOscar Gonzalez de Dios. dReDBox: Materializ-\ning a full-stack rack-scale system prototype of a\nnext-generation disaggregated datacenter. In De-\nsign, Automation & Test in Europe Conference &\nExhibition , pages 1093\u20131098, March 2018.\n[6]VMware Bitfusion. https://core.vmware.com/\nbitfusion .\n[7]Qingchao Cai, Wentian Guo, Hao Zhang, Divyakant\nAgrawal, Gang Chen, Beng Chin Ooi, Kian-Lee\nTan, Yong Meng Teo, and Sheng Wang. Ef\ufb01cient\ndistributed memory management with RDMA and\ncaching. Proceedings of the VLDB Endowment ,\n11(11):1604\u20131617, July 2018.\n[8]Irina Calciu, M. Talha Imran, Ivan Puddu, Sanid-\nhya Kashyap, Hasan Al Maruf, Onur Mutlu, and\nAasheesh Kolli. Rethinking software runtimes for\ndisaggregated memory. In ACM International Con-\nference on Architectural Support for Programming\nLanguages and Operating Systems , April 2021.\n[9]Compute eXpress Link. https://www.\ncomputeexpresslink.org .44[10] Paolo Faraboschi, Kimberly Keeton, Tim Marsland,\nand Dejan Milojicic. Beyond processor-centric op-\nerating systems. In Workshop on Hot Topics in\nOperating Systems , May 2015.\n[11] E. Felten and J. Zahorjan. Issues in the implementa-\ntion of a remote memory paging system.", "start_char_idx": 588920, "end_char_idx": 592381, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5720807-474f-45d7-903e-0bbd458ae2b3": {"__data__": {"id_": "b5720807-474f-45d7-903e-0bbd458ae2b3", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84b35d94-701e-4ee9-90ba-d565c8694fd4", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "c5b70e272f5f4523c2917305c4b9215709b4386988f2b36e3de6d2762b2d8ff9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96e2d7a6-f347-4f7f-9d4a-19d8f735486e", "node_type": "1", "metadata": {}, "hash": "928faf080e223a678a9f0b11743a15e0ceb88704f3fcf9543aa670bf54231b45", "class_name": "RelatedNodeInfo"}}, "text": "[8]Irina Calciu, M. Talha Imran, Ivan Puddu, Sanid-\nhya Kashyap, Hasan Al Maruf, Onur Mutlu, and\nAasheesh Kolli. Rethinking software runtimes for\ndisaggregated memory. In ACM International Con-\nference on Architectural Support for Programming\nLanguages and Operating Systems , April 2021.\n[9]Compute eXpress Link. https://www.\ncomputeexpresslink.org .44[10] Paolo Faraboschi, Kimberly Keeton, Tim Marsland,\nand Dejan Milojicic. Beyond processor-centric op-\nerating systems. In Workshop on Hot Topics in\nOperating Systems , May 2015.\n[11] E. Felten and J. Zahorjan. Issues in the implementa-\ntion of a remote memory paging system. Technical\nReport CSE TR 91-03-09, University of Washing-\nton, March 1991.\n[12] Gen-Z consortium. https://en.wikipedia.\norg/wiki/Gen-Z_(consortium) .\n[13] Donghyun Gouk, Sangwon Lee, Miryeong Kwon,\nand Myoungsoo Jung. Direct access, high-\nperformance memory disaggregation with Di-\nrectCXL. In USENIX Annual Technical Conference ,\npages 287\u2013294, June 2022.\n[14] Juncheng Gu, Youngmoon Lee, Yiwen Zhang,\nMosharaf Chowdhury, and Kang G Shin. Ef\ufb01cient\nmemory disaggregation with In\ufb01niswap. In Sympo-\nsium on Networked Systems Design and Implemen-\ntation , pages 649\u2013667, March 2017.\n[15] Hannes Hapke and Catherine Nelson. Building\nMachine Learning Pipelines . O\u2019Reilly Media, Inc,\nJuly 2020.\n[16] Intel rack scale architecture. https:\n//www-conf.slac.stanford.edu/\nxldb2016/talks/published/Tues_6_\nMohan-Kumar-Rack-Scale-XLDB-Updated.\npdf.\n[17] Svilen Kanev, Juan Pablo Darago, Kim Hazel-\nwood, Parthasarathy Ranganathan, Tipp Moseley,\nGu-Yeon Wei, and David Brooks. Pro\ufb01ling a\nwarehouse-scale computer. In International Sym-\nposium on Computer Architecture , pages 158\u2013169,\nJune 2015.\n[18] Kimberly Keeton. Memory driven computing. In\nUSENIX Conference on File and Storage Technolo-\ngies, February 2017. Keynote presentation.\n[19] Seung-seob Lee, Yanpeng Yu, Yupeng Tang,\nAnurag Khandelwal, Lin Zhong, and Abhishek\nBhattacharjee. MIND: In-network memory man-\nagement for disaggregated data centers. In ACM\nSymposium on Operating Systems Principles , pages\n488\u2013504, October 2021.\n[20] Huaicheng Li, Daniel S. Berger, Stanko Novakovic,\nLisa Hsu, Dan Ernst, Pantea Zardoshti, MonishShah, Samir Rajadnya, Scott Lee, Ishwar Agarwal,\nMark D. Hill, Marcus Fontoura, and Ricardo Bian-\nchini. Pond: CXL-based memory pooling systems\nfor cloud platforms. In ACM International Con-\nference on Architectural Support for Programming\nLanguages and Operating Systems , March 2023.\n[21] Kevin Lim, Jichuan Chang, Trevor Mudge,\nParthasarathy Ranganathan, Steven K Reinhardt,\nand Thomas F Wenisch. Disaggregated memory\nfor expansion and sharing in blade servers.\nACM SIGARCH Computer Architecture News ,\n37(3):267\u2013278, June 2009.\n[22] Mark Mansi and Michael M. Swift. /0sim: Prepar-\ning system software for a world with terabyte-\nscale memories. In ACM International Confer-\nence on Architectural Support for Programming\nLanguages and Operating Systems , page 267\u2013282,\nMarch 2020.\n[23] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Jo-\nhannes Weiner, Niket Agarwal, Pallab Bhattacharya,\nChris Petersen, Mosharaf Chowdhury, Shobhit\nKanaujia, and Prakash Chauhan. TPP: Transparent\npage placement for CXL-enabled tiered-memory.\nInACM International Conference on Architectural\nSupport for Programming Languages and Operat-\ning Systems , page 742\u2013755, March 2023.", "start_char_idx": 591752, "end_char_idx": 595118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96e2d7a6-f347-4f7f-9d4a-19d8f735486e": {"__data__": {"id_": "96e2d7a6-f347-4f7f-9d4a-19d8f735486e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5720807-474f-45d7-903e-0bbd458ae2b3", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8e0deab758c7653056778e2082d0a70e9533841c4e51645f70e68bc85299eeb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "696b14d9-e527-415a-8d2f-6b3ba25095ad", "node_type": "1", "metadata": {}, "hash": "d7fc440e26a15b13a5303b1e9c8a98ed639b96504d8cee19b1e311f0e4e875db", "class_name": "RelatedNodeInfo"}}, "text": "ACM SIGARCH Computer Architecture News ,\n37(3):267\u2013278, June 2009.\n[22] Mark Mansi and Michael M. Swift. /0sim: Prepar-\ning system software for a world with terabyte-\nscale memories. In ACM International Confer-\nence on Architectural Support for Programming\nLanguages and Operating Systems , page 267\u2013282,\nMarch 2020.\n[23] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Jo-\nhannes Weiner, Niket Agarwal, Pallab Bhattacharya,\nChris Petersen, Mosharaf Chowdhury, Shobhit\nKanaujia, and Prakash Chauhan. TPP: Transparent\npage placement for CXL-enabled tiered-memory.\nInACM International Conference on Architectural\nSupport for Programming Languages and Operat-\ning Systems , page 742\u2013755, March 2023.\n[24] Jacob Nelson, Brandon Holt, Brandon Myers, Pre-\nston Briggs, Luis Ceze, Simon Kahan, and Mark\nOskin. Latency-tolerant software distributed shared\nmemory. In USENIX Annual Technical Conference ,\npages 291\u2013305, July 2015.\n[25] Open compute project. https://www.\nopencompute.org .\n[26] Open19. https://www.open19.org .\n[27] OpenVMS. https://en.wikipedia.org/wiki/\nOpenVMS .\n[28] Amanda Raybuck, Tim Stamler, Wei Zhang, Mat-\ntan Erez, and Simon Peter. HeMem: Scalable tiered\nmemory management for big data applications and\nreal NVM. In ACM Symposium on Operating Sys-\ntems Principles , pages 392\u2013407, October 2021.\n[29] RDMA over Converged Ethernet. https:\n//en.wikipedia.org/wiki/RDMA_over_\nConverged_Ethernet .45[30] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K\nAguilera, and Adam Belay. AIFM: High-\nperformance, application-integrated far memory. In\nSymposium on Operating Systems Design and Im-\nplementation , pages 315\u2013332, November 2020.\n[31] Yizhou Shan, Yutong Huang, Yilun Chen, and Yiy-\ning Zhang. LegoOS: A disseminated, distributed\nos for hardware resource disaggregation. In Sympo-\nsium on Operating Systems Design and Implemen-\ntation , pages 69\u201387, October 2018.\n[32] Silicon photonics. https://en.wikipedia.org/\nwiki/Silicon_photonics .\n[33] Scalable memory development kit. https://\ngithub.com/OpenMPDK/SMDK .\n[34] Paul Teich. HPE powers up The Ma-\nchine architecture, January 2017. https:\n//www.nextplatform.com/2017/01/09/\nhpe-powers-machine-architecture .\n[35] Stephanie Wang, Eric Liang, Edward Oakes, Ben-\njamin Hindman, Frank Sifei Luan, Audrey Cheng,\nand Ion Stoica. Ownership: A distributed futures\nsystem for \ufb01ne-grained tasks. In Symposium on Net-\nworked Systems Design and Implementation , pages\n671\u2013686, April 2021.\n[36] Zi Yan, Daniel Lustig, David Nellans, and Abhishek\nBhattacharjee. Nimble page management for tiered\nmemory systems. In ACM International Confer-\nence on Architectural Support for Programming\nLanguages and Operating Systems , pages 331\u2013345,\nApril 2019.\n[37] Matei Zaharia, Mosharaf Chowdhury, Michael J.\nFranklin, Scott Shenker, and Ion Stoica. Spark:\nCluster computing with working sets. In Workshop\non Hot Topics in Cloud Computing , June 2010.\n[38] Yang Zhou, Hassan MG Wassel, Sihang Liu, Jiaqi\nGao, James Mickens, Minlan Yu, Chris Kennelly,\nPaul Turner, David E Culler, Henry M Levy, et al.\nCarbink: Fault-tolerant far memory. In Symposium\non Operating Systems Design and Implementation ,\npages 55\u201371, July 2022.", "start_char_idx": 594424, "end_char_idx": 597584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "696b14d9-e527-415a-8d2f-6b3ba25095ad": {"__data__": {"id_": "696b14d9-e527-415a-8d2f-6b3ba25095ad", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96e2d7a6-f347-4f7f-9d4a-19d8f735486e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f90c75c7ea18f3337b5e2f03c5add7d531d4238df6415c871ebee35a98e794e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad9d8f3c-92f8-431f-af0a-3b7caa9f335d", "node_type": "1", "metadata": {}, "hash": "e2a64d000c81e48e88f11fecaecb7ce590933a274e7231b33cd2bbf21e898bee", "class_name": "RelatedNodeInfo"}}, "text": "Nimble page management for tiered\nmemory systems. In ACM International Confer-\nence on Architectural Support for Programming\nLanguages and Operating Systems , pages 331\u2013345,\nApril 2019.\n[37] Matei Zaharia, Mosharaf Chowdhury, Michael J.\nFranklin, Scott Shenker, and Ion Stoica. Spark:\nCluster computing with working sets. In Workshop\non Hot Topics in Cloud Computing , June 2010.\n[38] Yang Zhou, Hassan MG Wassel, Sihang Liu, Jiaqi\nGao, James Mickens, Minlan Yu, Chris Kennelly,\nPaul Turner, David E Culler, Henry M Levy, et al.\nCarbink: Fault-tolerant far memory. In Symposium\non Operating Systems Design and Implementation ,\npages 55\u201371, July 2022.\n46Persistent Memory Research in the Post-Optane Era\nPeter Desnoyers\nNortheastern University\np.desnoyers@northeastern.eduIan Adams\nIntel Corporation\nian.f.adams@intel.comTyler Estro\nStony Brook University\ntestro@cs.stonybrook.edu\nAnshul Gandhi\nStony Brook University\nanshul@cs.stonybrook.eduGeo\uffffKuenning\nHarvey Mudd College\ngeo\uffff@cs.hmc.eduMike Mesnier\nIntel Corporation\nmichael.mesnier@intel.com\nCarl Waldspurger\nCarl Waldspurger Consulting\ncarl@waldspurger.orgAvani Wildani\nEmory University and Cloud \uffffare\nagadani@gmail.comErez Zadok\nStony Brook University\nezk@fsl.cs.sunysb.edu\nABSTRACT\nAfter over a decade of researcher anticipation for the arrival\nof persistent memory (PMem), the \uffffrst shipments of 3D\nXPoint-based Intel Optane Memory in 2019 were quickly\nfollowed by its cancellation in 2022. Was this another case of\nan idea quickly fading from future to past tense, relegating\nwork in this area to the graveyard of failed technologies?\nThe recently introduced Compute Express Link (CXL) may\no\uffffer a path forward, with its persistent memory pro \uffffle o\uffffer-\ning a universal PMem attachment point. Yet new technolo-\ngies for memory-speed persistence seem years o \uffff, and may\nnever become competitive with evolving DRAM and \uffffash\nspeeds. Without persistent memory itself, is future PMem\nresearch doomed? We o \uffffer two arguments for why reports\nof the death of PMem research are greatly exaggerated.\nFirst, the bulk of persistent-memory research has not in\nfact addressed memory persistence, but rather in-memory\ncrash consistency, which was never an issue in prior sys-\ntems where CPUs could not observe post-crash memory\nstates. CXL memory pooling allows multiple hosts to share\na single memory, all in di \ufffferent failure domains, raising\ncrash-consistency issues even with volatile memory.\nSecond, we believe CXL necessitates a \u201cdisaggregation\u201d of\nPMem research. Most work to date assumed a single tech-\nnology and set of features, i.e., speed, byte addressability,\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for pro \ufffft or commercial advantage and that\ncopies bear this notice and the full citation on the \uffffrst page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speci \uffffc\npermission and/or a fee. Request permissions from permissions@acm.org.\nDIMES \u201923, October 23, 2023, Koblenz, Germany\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 979-8-4007-0300-3/23/10. . . $15.00\nhttps://doi.org/10.1145/3609308.3625268and CPU load/store access. With an open interface allowing\nnew topologies and diverse PMem technologies, we argue\nfor the need to examine these features individually and in\ncombination.\nWhile one form of PMem may have been canceled, we\nargue that the research problems it raised not only remain\nrelevant but have expanded in a CXL-based future.\nCCS CONCEPTS\n\u2022Information systems !Storage class memory .", "start_char_idx": 596934, "end_char_idx": 600746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad9d8f3c-92f8-431f-af0a-3b7caa9f335d": {"__data__": {"id_": "ad9d8f3c-92f8-431f-af0a-3b7caa9f335d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "696b14d9-e527-415a-8d2f-6b3ba25095ad", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "1caf6addf332631c0df56f2b9b9d3b22a801d3ee7276b85c78a0bfecf1d7a447", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60d4c6d4-91e6-40d5-9b5d-ea8a95590c6f", "node_type": "1", "metadata": {}, "hash": "e83502b4b19633c8ad4dde8c9daa32c5547338cacf452554a091b7f1ba1a6270", "class_name": "RelatedNodeInfo"}}, "text": "Request permissions from permissions@acm.org.\nDIMES \u201923, October 23, 2023, Koblenz, Germany\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 979-8-4007-0300-3/23/10. . . $15.00\nhttps://doi.org/10.1145/3609308.3625268and CPU load/store access. With an open interface allowing\nnew topologies and diverse PMem technologies, we argue\nfor the need to examine these features individually and in\ncombination.\nWhile one form of PMem may have been canceled, we\nargue that the research problems it raised not only remain\nrelevant but have expanded in a CXL-based future.\nCCS CONCEPTS\n\u2022Information systems !Storage class memory .\nKEYWORDS\nPersistent memory, PMem, 3D XPoint, Optane, CXL.\nACM Reference Format:\nPeter Desnoyers, Ian Adams, Tyler Estro, Anshul Gandhi, Geo \uffff\nKuenning, Mike Mesnier, Carl Waldspurger, Avani Wildani, and Erez\nZadok. 2023. Persistent Memory Research in the Post-Optane Era.\nIn1st Workshop on Disruptive Memory Systems (DIMES \u201923), October\n23, 2023, Koblenz, Germany. ACM, New York, NY, USA, 8 pages.\nhttps://doi.org/10.1145/3609308.3625268\n1 INTRODUCTION\nAs CPU processing speeds and core counts continue to grow,\nso too do the I/O speeds needed to feed data to ever-faster\nCPUs, with some workloads ( e.g., indexes, Bloom \ufffflters)\nbeing particularly sensitive to I/O latency. Yet as storage\nlatencies drop into the 10s of microseconds, improvements\nin device speed begin to be overshadowed by software de-\nlays and overheads in the OS storage stack. While various\nstrategies have been used to reduce these overheads [ 54],\npersistent memory allows them to be bypassed entirely for\nmost accesses.\nIn recent years, the availability of persistent memory\n(PMem) has spurred a \uffffurry of research [ 5,12,18,22,\n25,26,28,29,37,47,53]. PMem\u2019s unique properties en-\ncouraged research in the storage community and beyond:\n23\nDIMES \u201923, October 23, 2023, Koblenz, Germany P. Desnoyers et al.\nalgorithms [ 6,10,12], compilers [ 23,32,33], data struc-\ntures [ 18,28,29,37],\uffffle systems [ 25,31,48], key-value\nstores [ 5,26,55], operating systems [ 3,27,40,50], and even\nnon-systems areas have been a \uffffected. Industry e \ufffforts pro-\nduced the Storage Networking Industry Association (SNIA)\nprogramming model [46] and the PMDK [21] library.\nWhen Intel canceled its 3D XPoint-based Optane product\nline [ 20], researchers were suddenly left wondering whether\npersistent-memory technologies had any future. Yet behind\nthe headlines, both Micron [ 36] and Intel [ 19] embraced\nthe industry Compute Express Link (CXL) [ 8] standard as\ntheir future direction for persistent and hierarchical memory.\nOthers have also begun to discuss the lessons learned and\noutline future prospects for PMem [2, 15, 24, 45].\nPersistent memory has in e \uffffect taken one step backwards,\nlosing a storage technology, and one tentative step forwards,\ngaining an alternate, arguably superior interface. This new\ninterface is not only vendor-independent but multipurpose,\nwith use cases ( e.g., cache-coherent GPU-to-host access, CXL\nmemory pooling) that are likely to ensure its viability inde-\npendent of market demands for persistent memory. Before\nCXL, only CPU vendors could consider integrating persistent\nmemory into a system; with CXL, even academic researchers\ncan design and deploy FPGA-based PMem prototypes. But\nshould they?\nAnswering this question requires examining the de \uffffning\ncharacteristics of PMem in more detail: (a) persistence, (b)\nbyte addressability, and (c) direct access via CPU load/store\ninstructions.\nByte addressability reduces the cost of small accesses;\nload/store access dramatically accelerates some I/O tasks,\nproviding direct user-space access without kernel interven-\ntion.", "start_char_idx": 600087, "end_char_idx": 603805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60d4c6d4-91e6-40d5-9b5d-ea8a95590c6f": {"__data__": {"id_": "60d4c6d4-91e6-40d5-9b5d-ea8a95590c6f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad9d8f3c-92f8-431f-af0a-3b7caa9f335d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "701865267d63d2a218cdc8a15c1a26a68a48d7a0d98776a396aad7861331cc8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9aaf7c42-afb3-4904-9588-745ece879401", "node_type": "1", "metadata": {}, "hash": "95cac859c8b3b00a3ee10801b2242ffab48ad113228db7d9ed44aa132c6596e3", "class_name": "RelatedNodeInfo"}}, "text": "This new\ninterface is not only vendor-independent but multipurpose,\nwith use cases ( e.g., cache-coherent GPU-to-host access, CXL\nmemory pooling) that are likely to ensure its viability inde-\npendent of market demands for persistent memory. Before\nCXL, only CPU vendors could consider integrating persistent\nmemory into a system; with CXL, even academic researchers\ncan design and deploy FPGA-based PMem prototypes. But\nshould they?\nAnswering this question requires examining the de \uffffning\ncharacteristics of PMem in more detail: (a) persistence, (b)\nbyte addressability, and (c) direct access via CPU load/store\ninstructions.\nByte addressability reduces the cost of small accesses;\nload/store access dramatically accelerates some I/O tasks,\nproviding direct user-space access without kernel interven-\ntion. In addition to those features, Optane provided near-\nDRAM speed and better-than-DRAM cost per bit.\nGiven multiple potential persistent technologies and meth-\nods of access, we believe it is important to consider PMem\u2019s\nfeatures\u2014including persistence itself\u2014individually as well\nas in various combinations. Is load/store access important,\nor would user-space byte-granular access via an RDMA-like\nmechanism provide similar performance? Which Optane\nperformance improvements require near-DRAM speed, vs.\nthose that are enabled by merely better-than-NVMe per-\nformance? What about the non-persistent case with CXL\nmemory pooling, and does multi-host access across multi-\nple failure domains pose the same challenges as single-host\nPMem, or new ones? Finally, how important is price, and in\nparticular would PMem be viable if it were no cheaper than\nDRAM?1\n1We note that \u201ccheaper than DRAM\u201d is a vague target, as high-density\nDIMMs carry a cost premium of up to 10 \u21e5over lower densities.2 WHAT IS PERSISTENT MEMORY?\nBypersistent Memory orPMem we refer to media with\nbyte-addressable access ( e.g., via hardware access at cache-\nline granularity)via CPU load/store instructions, with coher-\nent caching, but with the persistence properties of storage.\nPMem supports direct memory access (DMA) by other de-\nvices, and is fast enough to warrant waiting for a load in-\nstruction rather than context-switching to another thread as\nis done with slower storage ( e.g., NAND Flash) [ 42]. Software\nsupport ( e.g., via libraries conforming to the SNIA NVM Pro-\ngramming Model [ 46]) allows PMem implementations using\nnatively persistent media ( e.g., 3D XPoint) or natively volatile\n(e.g., DRAM) devices with hardware support for persistence\nin the event of power loss.\nAdditional higher-level functions supported by the SNIA\nmodel include: (a) PMem-aware \uffffle systems\u2014 e.g.,ext4 with\nthe DAX option\u2014which provide naming, access control, and\nthe ability to map persistent data into the virtual address\nspace. (b) Library APIs that allow applications to discover\nwhether store instructions are considered persistent as soon\nas they are visible to other threads, or if \uffffush operations are\nrequired to guarantee that stores have been committed. (c)\nSoftware mechanisms to detect failures unique to PMem, e.g.,\nan incomplete \uffffush on fail execution after a power failure.\nA Brief timeline of PMem products. Battery-backed RAM\nhas a long history of use for RAID stripe bu \uffffers [16], and\nbefore that magnetic core memory was persistent across\npower loss2[39]. However persistent memory as we know\nit can be traced to shortly after 2000\u2014both conceptual work\nonstorage-class memory [11] and products in the form of\nNVDIMM-N [49], DRAM DIMMs with energy storage and\n\uffffash backup that allow memory contents to last across power\nloss. NVDIMMs used standard memory sockets, but required\nplatform support for power-loss noti \uffffcation. They were\nshipped by several companies for nearly a decade [ 49], but\nbecause they were much more expensive than conventional\nDRAM, they were rarely if ever deployed as entire storage\nsystems.\nLater in that decade, emerging technologies such as Phase\nChange Memory [ 11] resulted in sustained research interest\nin persistent memory, accelerated by Intel and Micron\u2019s an-\nnouncement of 3D XPoint memory and Intel\u2019s Optane plans.\nIn 2019, Intel began shipping Optane memory devices, us-\ning the DDR-T variant of standard memory sockets.", "start_char_idx": 602999, "end_char_idx": 607245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9aaf7c42-afb3-4904-9588-745ece879401": {"__data__": {"id_": "9aaf7c42-afb3-4904-9588-745ece879401", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60d4c6d4-91e6-40d5-9b5d-ea8a95590c6f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a2c726de57d8d495744e41f5082466233f7e1a5918c09b68349c41576ce8bb4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca1fa7bd-214d-4245-b566-04c9c43df42e", "node_type": "1", "metadata": {}, "hash": "870261cc804e3082fc127261e2fecb38527832183c025549c104f62eccce3010", "class_name": "RelatedNodeInfo"}}, "text": "NVDIMMs used standard memory sockets, but required\nplatform support for power-loss noti \uffffcation. They were\nshipped by several companies for nearly a decade [ 49], but\nbecause they were much more expensive than conventional\nDRAM, they were rarely if ever deployed as entire storage\nsystems.\nLater in that decade, emerging technologies such as Phase\nChange Memory [ 11] resulted in sustained research interest\nin persistent memory, accelerated by Intel and Micron\u2019s an-\nnouncement of 3D XPoint memory and Intel\u2019s Optane plans.\nIn 2019, Intel began shipping Optane memory devices, us-\ning the DDR-T variant of standard memory sockets. Optane\nhad much higher capacity and lower cost per gigabyte than\nNVDIMM-Ns, since it leveraged the native persistence of\n3D XPoint. However, it had lower performance\u2014around 3 \u21e5\nthe latency of DRAM, with bandwidth somewhat lower for\nread and much lower for write [ 22]. Since Optane greatly\n2Due to cost, this persistence was rarely used for anything except boot\nloaders.\n24Persistent Memory Research in the Post-Optane Era DIMES \u201923, October 23, 2023, Koblenz, Germany\nPMem\nUserKernelStandardFile API\nPMem driverApplicationFile System\n    Application\nApplication\nStandardRaw DeviceAccess\nLoad /StoreStandardFile API\nPMem-AwareFile SystemMMUMappings\nDAXPMem driver\nBlock I/O\nFigure 1: Block and PMem data paths. Direct access\n(DAX, upper right) incurs no software overhead.\noutperformed NAND Flash, its primary use case was as a\npersistent write cache for very large data structures such as\nin-memory databases. Due to its high capacity and (arguably)\nlower cost per gigabyte, Optane was also considered as a\npotential second tier of volatile memory, cached by DRAM.\nMicron stopped production of 3D XPoint in 2021, and\nin 2022 Intel discontinued their Optane product line. As\nof this writing no other high-capacity PMem products are\navailable commercially, and no future 3D XPoint products\nare expected. The number of companies shipping NVDIMM-\nNs has declined recently, although they are still available in\ncapacities of around 16\u201332GB.\nPMem bene \uffffts.Figure 1 illustrates the di \ufffference between\nthe common Block I/O data path and the PMem data path. The\nrightmost application in the \uffffgure uses standard \uffffle APIs to\nopen and memory-map a PMem \uffffle; all PMem I/O is then able\nto use the standard load/store model. This is made possible\nby the DAX (Direct Access) feature in speci \uffffc\uffffle systems,\nallowing mmap to directly map underlying address-space-\nresident memory, along with hardware persistence support,\ne.g., enabled by appropriate PMDK library operations.\nThese accesses are far more e \uffffcient than access via the\nblock I/O data path. In the PMem case individual instruc-\ntions retrieve data from cache, while the memory controller\nissues a single read to the memory device for each cache\nline accessed. In contrast, block access typically requires\nuser/kernel transitions for each access, multiple PCIe trans-\nactions for data and descriptor transfers and doorbell register\nwrites, and a signi \uffffcant in-kernel software path3.\nThe performance di \ufffference is even larger for small ac-\ncesses, as block I/Os are typically rounded up to a 4 KB block\nsize, while PMem is accessed at cache-line granularity. Data\nstructures can be mapped into application memory as shown\n3User-space access through SPDK [ 54] can reduce the software overhead of\nthis process, but the PCIe overhead remains.by the rightmost arrow in the \uffffgure, and then accessed di-\nrectly, without needing to copy data into DRAM. This ability\nto access persistent data in place is one of the major bene \uffffts\nof PMem [44].\nPMem challenges. Systems supporting PMem have two\nlevels of store persistence, as per the SNIA Programming\nmodel. The most common level, avoiding the need for more\nexpensive platform logic, requires applications to \uffffush stores\nexplicitly to ensure persistence. While storage has always\nworked this way, programmers are not used to having to\n\uffffush memory stores; this introduces new software complex-\nities.", "start_char_idx": 606614, "end_char_idx": 610635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca1fa7bd-214d-4245-b566-04c9c43df42e": {"__data__": {"id_": "ca1fa7bd-214d-4245-b566-04c9c43df42e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9aaf7c42-afb3-4904-9588-745ece879401", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f6955b6558f4d4459e334b48e63f57d3afd132ed73839e2400e9e15b4966c99b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49dff4df-dac0-4281-9efd-87bab0dc3ac8", "node_type": "1", "metadata": {}, "hash": "eb1f67d2e8ba463090834ba411b32b3267a00a4c4bd3a733c19f99e883169cc1", "class_name": "RelatedNodeInfo"}}, "text": "Data\nstructures can be mapped into application memory as shown\n3User-space access through SPDK [ 54] can reduce the software overhead of\nthis process, but the PCIe overhead remains.by the rightmost arrow in the \uffffgure, and then accessed di-\nrectly, without needing to copy data into DRAM. This ability\nto access persistent data in place is one of the major bene \uffffts\nof PMem [44].\nPMem challenges. Systems supporting PMem have two\nlevels of store persistence, as per the SNIA Programming\nmodel. The most common level, avoiding the need for more\nexpensive platform logic, requires applications to \uffffush stores\nexplicitly to ensure persistence. While storage has always\nworked this way, programmers are not used to having to\n\uffffush memory stores; this introduces new software complex-\nities. The problem is exacerbated by existing code that as-\nsumes block writes are atomic, which allows atomic updates\nof large data structures. Libraries like PMDK [ 44] normally\nhandle some of the complex logic around \uffffushing and trans-\nactions, but signi \uffffcant work may be needed to adapt existing\nsoftware [34].\nThe second level of persistence is provided by platforms\nthat automatically \uffffush all CPU caches to PMem on power\nloss or system crash. This relieves the software from that\nresponsibility. But since this feature is not guaranteed to exist\non every platform with PMem, the software must typically\nhandle both cases, so no complexity is avoided.\nThe lack of native language support for PMem is also\nproblematic, requiring libraries to use non-idiomatic con-\nstructs like preprocessor macros to support PMem, adding\nto debugging complexity. Although it is possible that id-\niomatic, usable PMem extensions to high-level languages\nwill emerge in the future, such improvements typically ar-\nrive only slowly. The fact that software must be modi \uffffed to\nuse PMem at all is itself a problem, since software changes\nare expensive. To mitigate this, a number of ways to leverage\nPMem transparently have emerged. Ideas such as Whole\nSystem Persistence [ 38] and Whole Process Persistence [ 17]\ncan leverage the bene \uffffts of PMem\u2019s in-place access without\napplication modi \uffffcation. In many cases language support for\ntransparent use of PMem may be di \uffffcult\u2014existing code of-\nten assumes that data structures are assembled in ephemeral\nbu\uffffers, never visible outside a limited range of code; the lack\nof bu \uffffering in PMem accesses may necessitate signi \uffffcant\nchanges in strategy.\nFinally, a consistent pain point for PMem has been that\nit isdirectly attached to a single host; if the host goes down,\naccess to that persistent data is lost. This di \uffffers from other\nstorage systems that can be attached on the network and\nmade accessible from multiple hosts ( e.g., NAS, SAN). Several\nsolutions to replicate PMem in software have been imple-\nmented [13, 14, 52], but they increase complexity further.\n25DIMES \u201923, October 23, 2023, Koblenz, Germany P. Desnoyers et al.\n3 CXL: A NEW PMEM INTERFACE\nCPU changes were needed to e \uffffciently support new PMem\nproducts. Modi \uffffcations to the DDR protocol supported\nvariable timing [ 1] and power-loss noti \uffffcation. For perfor-\nmance [ 7,9], new instructions and memory controller de-\nsigns [43] were needed to quickly and reliably persist data.\nIn 2019, the \uffffrst version of the Compute Express Link (CXL)\nspeci \uffffcation was released by a consortium of over 250 com-\npanies. As of November 2020, version 2.0 of the CXL speci-\n\uffffcation contains \uffffrst-class support for PMem, rather than\nadding it as an afterthought as was done for DDR protocols.\nCXL 1.1 and 2.0 run over PCIe 5, while CXL 3.0 uses PCIe\n6, introducing three new protocols:\n\u2022CXL.io : PCIe functionality, including device enumer-\nation and PCIe-style data transfers.\n\u2022CXL.cache : allows device caches as part of the CPU\ncache-coherency domain.\n\u2022CXL.mem : allows hosts to access device-attached\nmemory with cache-coherent loads and stores.", "start_char_idx": 609851, "end_char_idx": 613766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49dff4df-dac0-4281-9efd-87bab0dc3ac8": {"__data__": {"id_": "49dff4df-dac0-4281-9efd-87bab0dc3ac8", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca1fa7bd-214d-4245-b566-04c9c43df42e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3ae46b159723bc1368d75d9299643be03c036b2412feeae2ebde6b74709c931b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e055fe99-5e2e-4852-9b2a-25289d61df72", "node_type": "1", "metadata": {}, "hash": "e6a01ba7a8bde272f1abedb859b9d7f1f4eea7791a10318bc9cb589118c9ac59", "class_name": "RelatedNodeInfo"}}, "text": "In 2019, the \uffffrst version of the Compute Express Link (CXL)\nspeci \uffffcation was released by a consortium of over 250 com-\npanies. As of November 2020, version 2.0 of the CXL speci-\n\uffffcation contains \uffffrst-class support for PMem, rather than\nadding it as an afterthought as was done for DDR protocols.\nCXL 1.1 and 2.0 run over PCIe 5, while CXL 3.0 uses PCIe\n6, introducing three new protocols:\n\u2022CXL.io : PCIe functionality, including device enumer-\nation and PCIe-style data transfers.\n\u2022CXL.cache : allows device caches as part of the CPU\ncache-coherency domain.\n\u2022CXL.mem : allows hosts to access device-attached\nmemory with cache-coherent loads and stores.\nA CXL Type 3 Memory Device , built using the CXL.io and\nCXL.mem protocols, allows OSes to have a single, generic\ndriver supporting both volatile memory and PMem, even\non the same device [ 8,24]. Moreover, while previous PMem\ndevices required explicit CPU support, CXL allows indepen-\ndent vendors and even researchers to build a wide variety of\nPMem devices. CXL incorporates the lessons learned from\nprior PMem products, and in many cases allows binary com-\npatibility for applications developed for NVDIMM [ 49] and\nOptane devices.\nWith CXL, memory pooling , supported by the Multi-\nHeaded Device (MHD) model in CXL 3.0, allows multiple\nhosts to access memory presented by a single device. This\nprovides the ability to disaggregate both volatile and persis-\ntent memory, and to dynamically assign it to di \ufffferent hosts\nover time [ 30], as shown in Figure 2, providing a separate\n\u201cmemory appliance\u201d with its own power, failure domain, and\nreliability characteristics. As an example, Pond [ 30] uses a\ncustom controller to provide single-host cache coherence,\ncoupled with dynamic access control assigning each memory\nregion to a single host at a time.\nSuch memory pooling o \uffffers an opportunity for appli-\ncation-transparent data replication across failure-domain\nboundaries. This in turn addresses a key limitation of prior\nPMem con \uffffgurations, where such replication required ex-\nplicit application support, typically requiring slower soft-\nware intervention rather than being implemented in hard-\nware.\nPond and similar approaches allow non-concurrent shar-\ning of memory where, for example, a new host may take\nMemory Media\nHost 0DisaggregatedMemory PoolHost N\n\u2026CXLCXLMemory Media\nHost 0DisaggregatedMemory PoolHost N\n\u2026CXLCXLFigure 2: Basic CXL memory pooling.\nownership of a memory region after a crash; additional fea-\ntures allow concurrent sharing of memory regions by multi-\nple hosts, with either non-coherent access (requiring explicit\n\uffffush operations) or optionally with full cache coherency\nacross hosts.\nMemory pooling and sharing also introduce new security\nconcerns. The CXL speci \uffffcation supports low-level encryp-\ntion for memory and interconnect links [ 8, Section 11.0];\nfurther research is needed in this area.\n4 RESEARCH GOING FORWARD\nAlthough other persistent memory technologies predated\nit, 3D XPoint was perhaps the \uffffrst solid-state technology\nto o\uffffer both cost and performance midway between con-\ntemporary main memory and block storage technologies\u2014\nthe cheaper-than-DRAM, faster-than-NAND \uffffash window.\nSince this \u201cstorage-class memory\u201d window is a moving target,\nthe emergence of a new and competitive persistent-memory\ntechnology is heavily dependent on progress at both ends\nof this window\u2014progress driven by enormous investments\nbased on the size of these markets.\nAs a result, it is entirely possible that we will not see a\nsolid-state memory technology arise that directly replaces 3D\nXPoint. Yet we argue that in the CXL era, persistent-memory\nresearch remains just as relevant, for two reasons:\n\u2022Hybrid persistent memory [ 41]. Even in the absence of\nnew technologies, hybrid strategies combining DRAM,\n\uffffash, and energy storage will enable future CXL-\nattached persistent-memory systems at varying price\nand performance points.\n\u2022Multi-host consistency. PMem raised the new (at\nthe time) problem of crash consistency for memory;\nin previous systems memory contents were lost on\npower failure, and the CPU could never observe crash-\ninconsistent memory states.", "start_char_idx": 613113, "end_char_idx": 617261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e055fe99-5e2e-4852-9b2a-25289d61df72": {"__data__": {"id_": "e055fe99-5e2e-4852-9b2a-25289d61df72", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49dff4df-dac0-4281-9efd-87bab0dc3ac8", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f0271dc1d7b1e8f4daba2053d46a0af5e67f060b41856bfc3e58ed70fa1052f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5460990b-5c6b-45df-9565-72a1d63de2ee", "node_type": "1", "metadata": {}, "hash": "04bfeaa746a4098ef4838b41eca3ae55b54b5478a1916d68d158b42b81c21091", "class_name": "RelatedNodeInfo"}}, "text": "As a result, it is entirely possible that we will not see a\nsolid-state memory technology arise that directly replaces 3D\nXPoint. Yet we argue that in the CXL era, persistent-memory\nresearch remains just as relevant, for two reasons:\n\u2022Hybrid persistent memory [ 41]. Even in the absence of\nnew technologies, hybrid strategies combining DRAM,\n\uffffash, and energy storage will enable future CXL-\nattached persistent-memory systems at varying price\nand performance points.\n\u2022Multi-host consistency. PMem raised the new (at\nthe time) problem of crash consistency for memory;\nin previous systems memory contents were lost on\npower failure, and the CPU could never observe crash-\ninconsistent memory states. CXL memory pooling al-\nlows memory to be observed from multiple indepen-\ndent failure domains, leading to similar challenges\n26Persistent Memory Research in the Post-Optane Era DIMES \u201923, October 23, 2023, Koblenz, Germany\n#Example Storage / Memory Technologies Coherent Byte-addressable Persistent\n1Volatile RAM disk 8 8 8\n2Conventional HDD, SSD 8 8 4\n3Incoherent load/store to PCIe address space 8 4 8\n4Byte-addressable I/O device ( e.g., object storage) 8 4 4\n5Memory pooling 4 4 8\n6Conventional PMem use cases, including NVDIMM 4 4 4\nTable 1: A taxonomy of storage technologies that may support coherency, byte-addressability, and persistence.\neven in the absence of persistence, while doing so un-\nder a range of topologies and speeds.\nChanges brought about by CXL. Historically ( i.e., before\nPMem) memory researchers have not had to worry about\nissues like data persistence, durability, and availability; these\nwere issues speci \uffffc to storage systems. PMem changed this\nand opened up a decade\u2019s worth of research. A key resulting\nartifact is PMDK [ 21]\u2014a suite of libraries providing a single\nconsistency model across a range of hardware persistence\nfeatures. Storage researchers working at the device or block\nlevel watched with interest as memory researchers tackled\nkey storage issues like transactions and atomic writes.\nLooking up from the block layer, PMem changed very\nlittle. Researchers quickly dealt with the low-hanging fruit\n(e.g., block-mode abstractions to PMem [ 4]). Otherwise, there\nwere few opportunities at the storage ( i.e., block) layer.\nBut CXL will change this in two ways: (1) by bringing\nmemory abstractions to a standardized I/O interconnect, and\n(2) by making persistence optional (as discussed earlier, CXL\nworks with both volatile and non-volatile memory). This\nmeans that memory and storage researchers will need to\ncoordinate, especially if the goal is an optimized solution\nthat spans all hardware and software layers.\nFor example, although a CXL device could be exposed\nas a hybrid device with a completely separate memory API\n(CXL.mem and/or CXL.cache) and storage API (CXL.io), de-\nsigning such a solution is a missed opportunity. Rather, the\nmemory \u201chalf\u201d of a device should leverage the storage half\nfor bulk data, and the storage half should leverage the mem-\nory half for coherence and byte addressability. One example\nis a computational SSD that modi \uffffes data in host memory,\nwithout resorting to bulk DMA operations. Alternatively,\nconsider a GPU or an FPGA using CXL.cache to gain coher-\nent access to host memory. If that same data is destined for\nblock storage, we do not want to send it to the PCI layer a\nsecond time; the data may already be partially present in the\ndevice, just in a memory form. Hence, CXL introduces the\nneed for the memory and storage halves to coordinate, and\ntherein lies the potential for new research.New research opportunities. We introduce new opportuni-\nties brought about by CXL across three dimensions: persis-\ntence, byte addressability, and coherence. We consider six\nof the eight possible combinations: three map to existing\nmemory or storage technologies and three are entirely new,\nrepresenting research opportunities going forward.\nFor the taxonomy in Table 1, we de \uffffnepersistent as being\nable to survive a cold reboot or loss of power, coherent4to\nmean that read operations (across CPUs or hosts as appro-\npriate) will transparently see the result of write operations\nfrom other CPUs or hosts, and byte-addressable as allow-\ning accesses smaller than a single sector (512 bytes).", "start_char_idx": 616564, "end_char_idx": 620833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5460990b-5c6b-45df-9565-72a1d63de2ee": {"__data__": {"id_": "5460990b-5c6b-45df-9565-72a1d63de2ee", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e055fe99-5e2e-4852-9b2a-25289d61df72", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e528f4ac77227708e061a73465d6d085b485435212c8592c42e70d3d99d0e45c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6ce0406-fac2-41cc-b343-86f73a1db1f1", "node_type": "1", "metadata": {}, "hash": "5dcf88110227a4ea0517d5b54fb341e1721f6155a87430039a976877e0c0080a", "class_name": "RelatedNodeInfo"}}, "text": "Hence, CXL introduces the\nneed for the memory and storage halves to coordinate, and\ntherein lies the potential for new research.New research opportunities. We introduce new opportuni-\nties brought about by CXL across three dimensions: persis-\ntence, byte addressability, and coherence. We consider six\nof the eight possible combinations: three map to existing\nmemory or storage technologies and three are entirely new,\nrepresenting research opportunities going forward.\nFor the taxonomy in Table 1, we de \uffffnepersistent as being\nable to survive a cold reboot or loss of power, coherent4to\nmean that read operations (across CPUs or hosts as appro-\npriate) will transparently see the result of write operations\nfrom other CPUs or hosts, and byte-addressable as allow-\ning accesses smaller than a single sector (512 bytes). It is\nworth noting that byte addressability does not require a co-\nherent memory interface. Indeed, object storage protocols\nalready allow for byte-granular access [ 35] on the PCIe bus\nusing versions of standard I/O commands; we therefore treat\ncoherency and byte-addressability independently.\nA number of rows represent conventional storage tech-\nnologies. Rows 1 and 2 represent RAM disks and conven-\ntional block devices such as NVMe drives. Access is at a\nblock granularity, and cached data ( i.e., kernel bu \uffffer caches)\nis managed \u201cmanually\u201d. Row 6, in turn, corresponds to ex-\nisting PMem architectures, combining persistence, cache\ncoherency, and byte addressability.\nOther combinations are less common. In row 3, read and\nwrite operations can be performed at byte granularity, but\nwithout coherence or persistence. PCIe address space pro-\nvides these semantics, with operations performed via load\nand store instructions. Although the NVMe speci \uffffcation\nde\uffffnes an optional PCIe address space allowing such di-\nrect access, it is not supported by any commonly available\ndevices. Alternatively, In \uffffniBand RDMA verbs provide an\nI/O-operation-based mechanism that is byte-addressable but\no\uffffers noncoherent access to (remote) volatile memory.\nIn row 4, byte addressability and persistence are combined\nwith non-coherent access, e.g., via I/O commands rather than\n4We note that coherence in the non-byte-addressable model is not novel, as\nit is the traditional access model for block devices.\n27DIMES \u201923, October 23, 2023, Koblenz, Germany P. Desnoyers et al.\nCPU load/store operations. This model is used by object stor-\nage devices that provide byte-aligned read and write opera-\ntions, although it could also be applied to \uffffat address spaces.\nAt present there are no commercially available modern object\nstorage devices; the \uffffat-address-space model corresponds to\nRDMA access to remote persistent memory.\nCombinations with cache-coherent access at block granu-\nlarity seem either impossible or impractical, and are omitted\nfrom Table 1.\nFinally, row 5\u2014cache-coherent byte-addressable access to\nvolatile storage\u2014corresponds to CXL memory pooling with\nvolatile RAM.\nResearch questions. In a post-Optane landscape with\nCXL attached volatile and non-volatile devices, we see a\nrange of problems which remain to be addressed.\nLatency and memory access: Optane memory is no\nslower than cross-NUMA-node access to DRAM, while poten-\ntial future technologies may have signi \uffffcantly higher worst-\ncase latency. At what point are architectural changes in the\nCPU or memory controller needed to address non-uniform\naccess times? Is there a point where software-controlled\naccess commands become more e \uffffcient than handling op-\nerations with wildly di \ufffferent latencies within the hardware\npipeline?\nPerformance factors: Optane memory provides both\nbyte addressability and low latency\u2014 10\u21e5less than the fastest\n(Optane) NVMe devices, and 100\u21e5less than typical ones.\nOptane-based applications and systems have been shown\nto provide signi \uffffcantly higher performance than NVMe-\nbased ones, but how much of this improvement is due to\nbyte addressability, and how much due to performance? Fu-\nture PMem technologies may be slower than Optane, and\nthe answer to this question is important for assessing their\npotential.\nMemory pooling and crash consistency: Will the ap-\nproaches used to provide crash consistency with a single\nhost attached to a single persistent memory be appropriate\nfor multiple attached hosts across multiple failure domains?", "start_char_idx": 620014, "end_char_idx": 624368, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6ce0406-fac2-41cc-b343-86f73a1db1f1": {"__data__": {"id_": "d6ce0406-fac2-41cc-b343-86f73a1db1f1", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5460990b-5c6b-45df-9565-72a1d63de2ee", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "2e3cec3236381f18465d7673e710eaf9b8d907a9efd126d333b9aed6b6d63fc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a0beae8-dabe-442d-b522-7cfac485e7ac", "node_type": "1", "metadata": {}, "hash": "0ef7154abcfda5c6220bba719f0b947a4633689de6de490124a61d8c1736c494", "class_name": "RelatedNodeInfo"}}, "text": "Performance factors: Optane memory provides both\nbyte addressability and low latency\u2014 10\u21e5less than the fastest\n(Optane) NVMe devices, and 100\u21e5less than typical ones.\nOptane-based applications and systems have been shown\nto provide signi \uffffcantly higher performance than NVMe-\nbased ones, but how much of this improvement is due to\nbyte addressability, and how much due to performance? Fu-\nture PMem technologies may be slower than Optane, and\nthe answer to this question is important for assessing their\npotential.\nMemory pooling and crash consistency: Will the ap-\nproaches used to provide crash consistency with a single\nhost attached to a single persistent memory be appropriate\nfor multiple attached hosts across multiple failure domains?\nApplication intent: Operating systems go to great\nlengths to infer application intent, allowing, e.g., I/O prefetch-\ning and migration of data to lower-performance memory\ntiers. This is more di \uffffcult with PMem, where accesses are\nperformed by hardware rather than software, and may be\nespecially important for hybrid PMem systems.\nByte-granular I/O devices: High-performance PMem-\nbased systems often achieve some of their gains by perform-\ning small atomic updates to stored data structures, e.g., by\natomically swapping pointers [ 51]. Extensions to the NVMe\nprotocol might allow such accesses to be performed on ex-\nternal storage, without coherent load/store access from the\nCPU. Is direct load/store access even necessary to achievethe bene \uffffts of byte-granular access, or can I/O protocols\nevolve to incorporate this model?\nCollectively, these CXL-enabled opportunities motivate\nmore distributed storage systems research, including job de-\ncomposition, scheduling, safely sharing data, and program-\nming and managing storage devices that speak both byte and\nblock protocols. It remains to be seen whether this takes the\nform of computational memory, computational storage, or\nsome hybrid. Indeed, CXL will blur the lines between mem-\nory and storage, allowing us to rethink and expand the role\nof a \u201cdevice.\u201d Devices will become computing peers, bringing\na wide and exciting array of possibilities.\n5 CONCLUSION\nWe posit that the current lack of commercial PMem avail-\nability does not detract from its importance and promise\nas a core storage technology, both in academia and indus-\ntry. The Compute Express Link (CXL) interconnect carries\nforward the lessons from previous PMem implementations\nand lowers the barrier for developing new PMem products.\nThe wide adoption of the CXL standard allays vendor lock-in\nconcerns, and is a core reason that we believe PMem is worth\ncontinued research e \uffffort. In particular, CXL enables one to\nconsider each PMem attribute separately or in combination:\nbyte addressability, persistence, and direct access via CPU\nload/store instructions. Finally, new CXL features such as\nmemory pooling and sharing are seeing considerable interest\nas rich areas for future PMem research and development.\nACKNOWLEDGMENTS\nWe thank Andrew Rudo \ufffffor his extensive contributions\nto this work. We thank the anonymous reviewers for their\nconstructive feedback. This work was made possible in part\nthanks to Dell-EMC, NetApp, Facebook, and IBM support; a\nSUNY/IBM Alliance award; and NSF awards CNS-1910327,\nCCF-1918225, CNS-1900706, CNS-1951880, CNS-2106263,\nCNS-2106434, and CNS-2214980.\nREFERENCES\n[1]JEDEC Solid State Technology Association. 2021. JEDEC Publishes\nDDR4 NVDIMM-P Bus Protocol Standard.\n[2]Lawrence Benson, Marcel Weisgut, and Tilmann Rabl. 2023. What\nWe Can Learn from Persistent Memory for CXL. In 20th Conference\non Database Systems for Business, Technology and Web (BTW) , Bir-\ngitta K\u00f6nig-Ries, Stefanie Scherzinger, Wolfgang Lehner, and Gottfried\nVossen (Eds.). Gesellschaft f\u00fcr Informatik e.V., Dresden, Germany, 535\u2013\n554. https://doi.org/10.18420/BTW2023-48\n[3]Miao Cai and Hao Huang. 2021. A survey of operating system support\nfor persistent memory. Frontiers of Computer Science 15 (2021), 154207.\n[4]Feng Chen, Michael Mesnier, and Scott Hahn. 2014. A Protected Block\nDevice for Persistent Memory.", "start_char_idx": 623627, "end_char_idx": 627718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a0beae8-dabe-442d-b522-7cfac485e7ac": {"__data__": {"id_": "0a0beae8-dabe-442d-b522-7cfac485e7ac", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6ce0406-fac2-41cc-b343-86f73a1db1f1", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ff946de745ff6c3fb7e9f25401a3cb0e8068b340ebd08234eb8c3857b021eabf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17fabd49-d262-49a2-9f36-8fe70014a2bc", "node_type": "1", "metadata": {}, "hash": "519d4e0c5e49e39b18a2d8551759f766e841a03312fa629dae1e9721c5aa94d7", "class_name": "RelatedNodeInfo"}}, "text": "2023. What\nWe Can Learn from Persistent Memory for CXL. In 20th Conference\non Database Systems for Business, Technology and Web (BTW) , Bir-\ngitta K\u00f6nig-Ries, Stefanie Scherzinger, Wolfgang Lehner, and Gottfried\nVossen (Eds.). Gesellschaft f\u00fcr Informatik e.V., Dresden, Germany, 535\u2013\n554. https://doi.org/10.18420/BTW2023-48\n[3]Miao Cai and Hao Huang. 2021. A survey of operating system support\nfor persistent memory. Frontiers of Computer Science 15 (2021), 154207.\n[4]Feng Chen, Michael Mesnier, and Scott Hahn. 2014. A Protected Block\nDevice for Persistent Memory. In Proceedings of the 30th Symposium\non Mass Storage Systems and Technologies (MSST) . IEEE, Santa Clara,\nCA, 1\u201312.\n28Persistent Memory Research in the Post-Optane Era DIMES \u201923, October 23, 2023, Koblenz, Germany\n[5]Youmin Chen, Youyou Lu, Fan Yang, Qing Wang, Yang Wang, and\nJiwu Shu. 2020. FlatStore: An E \uffffcient Log-Structured Key-Value\nStorage Engine for Persistent Memory. In Proceedings of the Twenty-\nFifth International Conference on Architectural Support for Programming\nLanguages and Operating Systems . ACM, Lausanne, Switzerland, 1077\u2013\n1091.\n[6]Zhaole Chu, Yongping Luo, and Peiquan Jin. 2021. An E \uffffcient Sorting\nAlgorithm for Non-Volatile Memory. Int. J. Softw. Eng. Knowl. Eng. 31\n(2021), 1603\u20131621.\n[7]Joel Coburn, Adrian M. Caul \uffffeld, Ameen Akel, Laura M. Grupp, Ra-\njesh K. Gupta, Ranjit Jhala, and Steven Swanson. 2011. NV-Heaps:\nMaking Persistent Objects Fast and Safe with Next-Generation, Non-\nVolatile Memories. In Proceedings of the Sixteenth International Confer-\nence on Architectural Support for Programming Languages and Operat-\ning Systems (ASPLOS) . ACM, Newport Beach, CA, 105\u2013118.\n[8]Compute Express Link. 2022. Compute Express Link (CXL) Speci \uffffca-\ntion. Available from http://www.computeexpresslink.org .\n[9]Jeremy Condit, Edmund B. Nightingale, Christopher Frost, Engin Ipek,\nBenjamin Lee, Doug Burger, and Derrick Coetzee. 2009. Better I/O\nthrough Byte-Addressable, Persistent Memory. In Proceedings of the\nACM SIGOPS 22nd Symposium on Operating Systems Principles . ACM,\nBig Sky, Montana, USA, 133\u2013146. https://doi.org/10.1145/1629575.\n1629589\n[10] Laxman Dhulipala, Charles McGu \uffffey, Hong Kyu Kang, Yan Gu, Guy E.\nBlelloch, Phillip B. Gibbons, and Julian Shun. 2019. Sage: Parallel\nSemi-Asymmetric Graph Algorithms for NVRAMs. Proc. VLDB Endow.\n13 (2019), 1598\u20131613.\n[11] R. F. Freitas and W. W. Wilcke. 2008. Storage-Class Memory: The Next\nStorage System Technology. IBM Journal of Research and Development\n52, 4/5 (July 2008), 439\u2013447. https://doi.org/10.1147/rd.524.0439\n[12] G. Gill, Roshan Dathathri, Loc Hoang, Ramesh V. Peri, and Keshav\nPingali. 2019. Single Machine Graph Analytics on Massive Datasets\nusing Intel Optane DC Persistent Memory. Proceedings of the VLDB\nEndowment 13 (2019), 1304 \u2013 1318.\n[13] Tomasz Gromadzki and Jan Marian Michalski. 2019. Persistent\nMemory Replication Over Traditional RDMA Part 4: Persistent\nMemory Development Kit (PMDK)-Based PMEM Replication.\nhttps://www.intel.com/content/www/us/en/developer/articles/\ntechnical/persistent-memory-replication-over-traditional-\nrdma-part-4-persistent-memory-development.html .", "start_char_idx": 627151, "end_char_idx": 630312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17fabd49-d262-49a2-9f36-8fe70014a2bc": {"__data__": {"id_": "17fabd49-d262-49a2-9f36-8fe70014a2bc", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a0beae8-dabe-442d-b522-7cfac485e7ac", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "25b353e102088036bc3f0b5473de2a342ce3fca238f92ef9d63afbb8f1c76990", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87d72e2f-7ac5-4e21-baf5-d8ef597aed14", "node_type": "1", "metadata": {}, "hash": "72f10f76ed6f3de772d332c6f9a91f8a1c41ead8e92ca3083f5145210b7e5444", "class_name": "RelatedNodeInfo"}}, "text": "https://doi.org/10.1147/rd.524.0439\n[12] G. Gill, Roshan Dathathri, Loc Hoang, Ramesh V. Peri, and Keshav\nPingali. 2019. Single Machine Graph Analytics on Massive Datasets\nusing Intel Optane DC Persistent Memory. Proceedings of the VLDB\nEndowment 13 (2019), 1304 \u2013 1318.\n[13] Tomasz Gromadzki and Jan Marian Michalski. 2019. Persistent\nMemory Replication Over Traditional RDMA Part 4: Persistent\nMemory Development Kit (PMDK)-Based PMEM Replication.\nhttps://www.intel.com/content/www/us/en/developer/articles/\ntechnical/persistent-memory-replication-over-traditional-\nrdma-part-4-persistent-memory-development.html .\n[14] Shashank Gugnani, Scott Guthridge, Frank Schmuck, Owen Anderson,\nDeepavali Bhagwat, and Xiaoyi Lu. 2022. Arcadia: A Fast and Reliable\nPersistent Memory Replicated Log. arXiv:cs.DC/2206.12495\n[15] Jim Handy and Tom Coughlin. 2023. Optane\u2019s Dead: Now What?\nComputer 56, 3 (2023), 125\u2013130. https://doi.org/10.1109/MC.2023.\n3235096\n[16] Dave Hitz, James Lau, and Michael Malcolm. 1994. File System Design\nfor an NFS File Server Appliance. In Proceedings of the USENIX Winter\n1994 Technical Conference (ATC) . USENIX Association, San Francisco,\nCalifornia, 19\u201319.\n[17] George Hodgkins, Yi Xu, Steven Swanson, and Joseph Izraele-\nvitz. 2023. Zhuque: Failure Isn\u2019t an Option, It\u2019s an Excep-\ntion. http://nvmw.ucsd.edu/nvmw2023-program/nvmw2023-\npaper16-presentation_slides.pdf 14th Non-Volatile Memories Work-\nshop.\n[18] Deukyeon Hwang, Wook-Hee Kim, Youjip Won, and Beomseok Nam.\n2018. Endurable Transient Inconsistency in Byte-Addressable Persis-\ntent B+-Tree. In USENIX Conference on File and Storage Technologies .\nUSENIX Association, Oakland, CA, 187\u2013200.\n[19] Intel Corporation. 2022. Intel Optane persistent memory\nand Intel\u00aeXeon\u00aescalable processors o \uffffer a practical migra-\ntion path to memory expansion, tiering, and pooling withCompute Express Link (CXLTM)-attached memory devices.\nhttps://www.intel.com/content/dam/www/central-libraries/\nus/en/documents/2022-11/optane-pmem-to-cxl-tech-brief.pdf\n[20] Intel Corporation. 2022. Intel Reports Second-Quarter 2022 Financial\nResults. https://www.intc.com/news-events/press-releases/detail/\n1563/intel-reports-second-quarter-2022- \uffffnancial-results .\n[21] Intel Corporation. 2023. Persistent Memory Development Kit (PMDK).\npmem.io .\n[22] Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amir-\nsaman Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R.\nDulloor, Jishen Zhao, and Steven Swanson. 2019. Basic Performance\nMeasurements of the Intel Optane DC Persistent Memory Module.\nhttps://doi.org/10.48550/ARXIV.1903.05714\n[23] Jungi Jeong and Changhee Jung. 2021. PMEM-Spec: Persistent Memory\nSpeculation (Strict Persistency Can Trump Relaxed Persistency). In\nProceedings of the 26th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems (ASPLOS) .\nACM, virtual, 517\u2013529.\n[24] Myoungsoo Jung. 2022. Hello Bytes, Bye Blocks: PCIe Storage Meets\nCompute Express Link for Memory Expansion (CXL-SSD). In Proceed-\nings of the 14th ACM Workshop on Hot Topics in Storage and File Systems\n(HotStorage) . ACM, Virtual Event, 45\u201351.", "start_char_idx": 629696, "end_char_idx": 632853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87d72e2f-7ac5-4e21-baf5-d8ef597aed14": {"__data__": {"id_": "87d72e2f-7ac5-4e21-baf5-d8ef597aed14", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17fabd49-d262-49a2-9f36-8fe70014a2bc", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a4e4f017a4f05316a6518ab90804f5e6c41a196bd6fbfb8fe30d85985ff2413b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89d8e7fb-e9cb-42da-9274-f9ed68a60e1a", "node_type": "1", "metadata": {}, "hash": "aa1828583c3afa32c2e0da5f3689a6a5fe9b0eb60d7746ebbfad5da84e08e008", "class_name": "RelatedNodeInfo"}}, "text": "2019. Basic Performance\nMeasurements of the Intel Optane DC Persistent Memory Module.\nhttps://doi.org/10.48550/ARXIV.1903.05714\n[23] Jungi Jeong and Changhee Jung. 2021. PMEM-Spec: Persistent Memory\nSpeculation (Strict Persistency Can Trump Relaxed Persistency). In\nProceedings of the 26th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems (ASPLOS) .\nACM, virtual, 517\u2013529.\n[24] Myoungsoo Jung. 2022. Hello Bytes, Bye Blocks: PCIe Storage Meets\nCompute Express Link for Memory Expansion (CXL-SSD). In Proceed-\nings of the 14th ACM Workshop on Hot Topics in Storage and File Systems\n(HotStorage) . ACM, Virtual Event, 45\u201351. https://doi.org/10.1145/\n3538643.3539745\n[25] Rohan Kadekodi, Se Kwon Lee, Sanidhya Kashyap, Taesoo Kim,\nAasheesh Kolli, and Vijay Chidambaram. 2019. SplitFS: Reducing\nSoftware Overhead in File Systems for Persistent Memory. In Pro-\nceedings of the 27th ACM Symposium on Operating Systems Principles\n(SOSP) . ACM, Huntsville, Ontario, Canada, 494\u2013508.\n[26] Olzhas Kaiyrakhmet, Song Yeon Lee, Beomseok Nam, Sam H. Noh, and\nYoung ri Choi. 2019. SLM-DB: Single-Level Key-Value Store with Per-\nsistent Memory. In USENIX Conference on File and Storage Technologies .\nUSENIX Association, Boston, MA, 191\u2013205.\n[27] Rajat Kateja, Andrew Pavlo, and Greg Ganger. 2020. Vilamb: Low Over-\nhead Asynchronous Redundancy for Direct Access NVM. , 17 pages.\nhttps://arxiv.org/abs/2004.09619\n[28] Se Kwon Lee, Jayashree Mohan, Sanidhya Kashyap, Taesoo Kim, and\nVijay Chidambaram. 2019. RECIPE: Converting Concurrent DRAM\nIndexes to Persistent-Memory Indexes. In Proceedings of the 27th ACM\nSymposium on Operating Systems Principles (SOSP) . ACM, Huntsville,\nOntario, Canada, 462\u2013477.\n[29] Lucas Lersch, Xiangpeng Hao, Ismail Oukid, Tianzheng Wang, and\nThomas Willhalm. 2019. Evaluating Persistent Memory Range Indexes.\nProc. VLDB Endow. 13 (2019), 574\u2013587.\n[30] Huaicheng Li, Daniel S. Berger, Stanko Novakovic, Lisa R. Hsu, Dan\nErnst, Pantea Zardoshti, Monish Shah, Samir Rajadnya, Scott Lee,\nIshwar Agarwal, Mark D. Hill, Marcus Fontoura, and Ricardo Bianchini.\n2022. Pond: CXL-Based Memory Pooling Systems for Cloud Platforms.\nInProceedings of the 28th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems (ASPLOS) ,\nVol. 2. ACM, Lausanne, Switzerland, 574\u2013587.\n[31] Jen-Kuang Liu and Sheng-De Wang. 2022. CFFS: A Persistent Mem-\nory File System for Contiguous File Allocation With Fine-Grained\nMetadata. IEEE Access 10 (2022), 91678\u201391698.\n[32] Qingrui Liu, Joseph Izraelevitz, Se Kwon Lee, Michael L. Scott, Sam H.\nNoh, and Changhee Jung. 2018. iDO: Compiler-Directed Failure Atom-\nicity for Nonvolatile Memory. In 51st Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO) . IEEE, Fukuoka, Japan, 258\u2013\n270.\n[33] Sara Mahdizadeh-Shahri, Seyed Armin Vakil-Ghahani, and Aasheesh\nKolli. 2020. (Almost) Fence-less Persist Ordering. In 53rd Annual\nIEEE/ACM International Symposium on Microarchitecture (MICRO) .\nACM, Virtual, 539\u2013554.\n29DIMES \u201923, October 23, 2023, Koblenz, Germany P. Desnoyers et al.", "start_char_idx": 632178, "end_char_idx": 635302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89d8e7fb-e9cb-42da-9274-f9ed68a60e1a": {"__data__": {"id_": "89d8e7fb-e9cb-42da-9274-f9ed68a60e1a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87d72e2f-7ac5-4e21-baf5-d8ef597aed14", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f4caea33f6113dac5fb40d46421f9cfb034865ef8ef0892a162b34746a5db040", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e3a7c3b-4d72-4ea5-b0d2-92bf530d6f9a", "node_type": "1", "metadata": {}, "hash": "468c7a3bcaf3bf46a9de8667d89e682da07773b0767bfc78f2096d67fc11929b", "class_name": "RelatedNodeInfo"}}, "text": "[32] Qingrui Liu, Joseph Izraelevitz, Se Kwon Lee, Michael L. Scott, Sam H.\nNoh, and Changhee Jung. 2018. iDO: Compiler-Directed Failure Atom-\nicity for Nonvolatile Memory. In 51st Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO) . IEEE, Fukuoka, Japan, 258\u2013\n270.\n[33] Sara Mahdizadeh-Shahri, Seyed Armin Vakil-Ghahani, and Aasheesh\nKolli. 2020. (Almost) Fence-less Persist Ordering. In 53rd Annual\nIEEE/ACM International Symposium on Microarchitecture (MICRO) .\nACM, Virtual, 539\u2013554.\n29DIMES \u201923, October 23, 2023, Koblenz, Germany P. Desnoyers et al.\n[34] Virendra J. Marathe, Margo Seltzer, Steve Byan, and Tim Harris. 2017.\nPersistent Memcached: Bringing Legacy Code to Byte-Addressable\nPersistent Memory. In 9th USENIX Workshop on Hot Topics in Storage\nand File Systems (HotStorage 17) . USENIX Association, Santa Clara,\nCA. https://www.usenix.org/conference/hotstorage17/program/\npresentation/marathe\n[35] Michael P. Mesnier, Gregory R. Ganger, and Erik Riedel. 2003. Object-\nbased Storage. IEEE Communications 44, 8 (August 2003), 84\u201390.\n[36] Micron. 2021. Micron Updates Data Center Portfolio Strategy to\nAddress Growing Opportunity for Memory and Storage Hierarchy\nInnovation. https://investors.micron.com/news-releases/news-\nrelease-details/micron-updates-data-center-portfolio-strategy-\naddress-growing\n[37] Moohyeon Nam, Hokeun Cha, Young ri Choi, Sam H. Noh, and\nBeomseok Nam. 2019. Write-Optimized Dynamic Hashing for Persis-\ntent Memory. In USENIX Conference on File and Storage Technologies .\nUSENIX Association, Boston, MA, 31\u201344.\n[38] Dushyanth Narayanan and Orion Hodson. 2012. Whole-System Per-\nsistence. SIGARCH Comput. Archit. News 40, 1 (mar 2012), 401\u2013410.\nhttps://doi.org/10.1145/2189750.2151018\n[39] Emerson W. Pugh, Lyle R. Johnson, and John H. Palmer. 2003. IBM\u2019s\n360 and early 370 systems . MIT Press, Cambridge, Massachusetts.\n[40] Han Jie Qiu, Sihang Liu, Xinyang Song, Samira Khan, and Gennady\nPekhimenko. 2022. Pavise: Integrating Fault Tolerance Support for\nPersistent Memory Applications. In Proceedings of the International\nConference on Parallel Architectures and Compilation Techniques . ACM,\nChicago, IL, 109\u2013123.\n[41] The Register. 2022. Last week Intel killed Optane. Today, Kioxia and\nEverspin announced comparable tech. https://www.theregister.\ncom/2022/08/02/kioxia_everspin_persistent_memory/\n[42] Andy Rudo \uffff. 2017. Persistent Memory Programming. USENIX ;login:\n42, 2 (July 2017), 34\u201340.\n[43] Andy M. Rudo \uffff. 2016. Deprecating the PCOMMIT Instruc-\ntion. https://www.intel.com/content/www/us/en/developer/\narticles/technical/deprecate-pcommit-instruction.html .\n[44] Steve Scargall. 2020. Programming Persistent Memory: A Comprehensive\nGuide for Developers . Apress, New York, New York. 5\u20137 pages. https:\n//doi.org/10.1007/978-1-4842-4932-1\n[45] Xinyang (Kevin) Song, Sihang Liu, and Gennady Pekhimenko. 2022.\nPersistent Memory \u2014- A New Hope. https://www.sigarch.org/\npersistent-memory-a-new-hope/\n[46] Storage Networking Industry Association. 2017. NVM Programming\nModel (NPM).", "start_char_idx": 634729, "end_char_idx": 637773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e3a7c3b-4d72-4ea5-b0d2-92bf530d6f9a": {"__data__": {"id_": "2e3a7c3b-4d72-4ea5-b0d2-92bf530d6f9a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89d8e7fb-e9cb-42da-9274-f9ed68a60e1a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "faf561da7f47dd326f80d446d1615ed5748f2de7b70bb5bf86a4c09c5aee5d33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6975f378-2f02-4f65-9436-4a502e37551d", "node_type": "1", "metadata": {}, "hash": "194dc8a5a2c427ddec53f0667cdca9e6403e6a24d2de248866467bb60e6d206e", "class_name": "RelatedNodeInfo"}}, "text": "[43] Andy M. Rudo \uffff. 2016. Deprecating the PCOMMIT Instruc-\ntion. https://www.intel.com/content/www/us/en/developer/\narticles/technical/deprecate-pcommit-instruction.html .\n[44] Steve Scargall. 2020. Programming Persistent Memory: A Comprehensive\nGuide for Developers . Apress, New York, New York. 5\u20137 pages. https:\n//doi.org/10.1007/978-1-4842-4932-1\n[45] Xinyang (Kevin) Song, Sihang Liu, and Gennady Pekhimenko. 2022.\nPersistent Memory \u2014- A New Hope. https://www.sigarch.org/\npersistent-memory-a-new-hope/\n[46] Storage Networking Industry Association. 2017. NVM Programming\nModel (NPM). https://www.snia.org/sites/default/ \uffffles/technical-\nwork/npm/release/SNIA-NVM-Programming-Model-v1.2.pdf\n[47] Alexander van Renen, Lukas Vogel, Viktor Leis, Thomas Neumann,\nand Alfons Kemper. 2019. Persistent Memory I/O Primitives. In Pro-\nceedings of the 15th International Workshop on Data Management on\nNew Hardware (DaMoN) . ACM, Amsterdam, Netherlands, 1\u20137.\n[48] Jingyu Wang, Shengan Zheng, Ziyi Lin, Yuting Chen, and Linpeng\nHuang. 2022. Zebra: An E \uffffcient, RDMA-Enabled Distributed Per-\nsistent Memory File System. In International Conference on Database\nSystems for Advanced Applications . ACM, Virtual, 341\u2013349.\n[49] Wikipedia. 2023. NVDIMM \u2014 Wikipedia, The Free Encyclo-\npedia. http://en.wikipedia.org/w/index.php?title=NVDIMM&\noldid=1141063008 . [Online; accessed 27-March-2023].\n[50] Jian Xu, Juno Kim, Amir Saman Memaripour, and Steven Swanson.\n2019. Finding and Fixing Performance Pathologies in Persistent Mem-\nory Software Stacks. In Proceedings of the Twenty-Fourth International\nConference on Architectural Support for Programming Languages and\nOperating Systems (ASPLOS) . ACM, Providence, RI, 427\u2013439.\n[51] Jian Xu and Steven Swanson. 2016. NOVA: A Log-structured File\nSystem for Hybrid Volatile/Non-volatile Main Memories. In Proceedingsof the 14th Usenix Conference on File and Storage Technologies . USENIX\nAssociation, Santa Clara, CA, 323\u2013338.\n[52] Jian Xu, Lu Zhang, Amirsaman Memaripour, Akshatha Gangadharaiah,\nAmit Borase, Tamires Brito Da Silva, Steven Swanson, and Andy Rudo \uffff.\n2017. NOVA-Fortis: A Fault-Tolerant Non-Volatile Main Memory File\nSystem. In Proceedings of the 26th Symposium on Operating Systems\nPrinciples . ACM, Shanghai China, 478\u2013496. https://doi.org/10.1145/\n3132747.3132761\n[53] Jian Yang, Juno Kim, Morteza Hoseinzadeh, Joseph Izraelevitz, and\nSteven Swanson. 2020. An Empirical Guide to the Behavior and Use of\nScalable Persistent Memory. In USENIX Conference on File and Storage\nTechnologies (FAST) . USENIX Association, Santa Clara, CA, 169\u2013182.\n[54] Ziye Yang, James R. Harris, Benjamin Walker, Daniel Verkamp, Chang-\npeng Liu, Cunyin Chang, Gang Cao, Jonathan Stern, Vishal Verma,\nand Luse E. Paul. 2017. SPDK: A Development Kit to Build High Per-\nformance Storage Applications. In 2017 IEEE International Conference\non Cloud Computing Technology and Science (CloudCom) . IEEE, Hong\nKong, 154\u2013161. https://doi.org/10.1109/CloudCom.2017.14\n[55] Wenhui Zhang, Xingsheng Zhao, Song Jiang, and Hong Jiang. 2021.\nChameleonDB: A Key-value Store for Optane Persistent Memory. In\nProceedings of the Sixteenth European Conference on Computer Systems\n(Eurosys) . ACM, Edinburgh, Scotland, 194\u2013209.", "start_char_idx": 637184, "end_char_idx": 640418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6975f378-2f02-4f65-9436-4a502e37551d": {"__data__": {"id_": "6975f378-2f02-4f65-9436-4a502e37551d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e3a7c3b-4d72-4ea5-b0d2-92bf530d6f9a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "320a159a7249a13aa1b731c169efcb0c34cb0e886c7c656ea0939d36b3554804", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59e3dc07-2910-4b08-841b-f9992c27f228", "node_type": "1", "metadata": {}, "hash": "d6c20e2d56c49b4e4bd63a740468eb6443ba7be0ea00db054742e7f5f2c16ea9", "class_name": "RelatedNodeInfo"}}, "text": "[54] Ziye Yang, James R. Harris, Benjamin Walker, Daniel Verkamp, Chang-\npeng Liu, Cunyin Chang, Gang Cao, Jonathan Stern, Vishal Verma,\nand Luse E. Paul. 2017. SPDK: A Development Kit to Build High Per-\nformance Storage Applications. In 2017 IEEE International Conference\non Cloud Computing Technology and Science (CloudCom) . IEEE, Hong\nKong, 154\u2013161. https://doi.org/10.1109/CloudCom.2017.14\n[55] Wenhui Zhang, Xingsheng Zhao, Song Jiang, and Hong Jiang. 2021.\nChameleonDB: A Key-value Store for Optane Persistent Memory. In\nProceedings of the Sixteenth European Conference on Computer Systems\n(Eurosys) . ACM, Edinburgh, Scotland, 194\u2013209.\n30This paper is included in the Proceedings of the  \n2022 USENIX Annual Technical Conference.\nJuly 11\u201313, 2022 \u2022 Carlsbad, CA, USA\n978-1-939133-29-8\nOpen access to the Proceedings of the \n2022 USENIX Annual Technical Conference \nis sponsored byDirect Access, High-Performance Memory \nDisaggregation with DirectCXL\nDonghyun Gouk, Sangwon Lee, Miryeong Kwon, and Myoungsoo Jung, Computer \nArchitecture and Memory Systems Laboratory, Korea Advanced Institute of Science \nand Technology (KAIST)\nhttps://www.usenix.org/conference/atc22/presentation/goukDirect Access, High-Performance Memory Disaggregation with D IRECT CXL\nDonghyun Gouk, Sangwon Lee, Miryeong Kwon, Myoungsoo Jung\nComputer Architecture and Memory Systems Laboratory,\nKorea Advanced Institute of Science and Technology (KAIST)\nhttp://camelab.org\nAbstract\nNew cache coherent interconnects such as CXL have recently\nattracted great attention thanks to their excellent hardware\nheterogeneity management and resource disaggregation capa-\nbilities. Even though there is yet no real product or platform\nintegrating CXL into memory disaggregation, it is expected\nto make memory resources practically and ef\ufb01ciently disag-\ngregated much better than ever before.\nIn this paper, we propose directly accessible memory dis-\naggregation, DIRECT CXL that straight connects a host pro-\ncessor complex and remote memory resources over CXL\u2019s\nmemory protocol ( CXL.mem ). To this end, we explore a practi-\ncal design for CXL-based memory disaggregation and make\nit real. As there is no operating system that supports CXL,\nwe also offer CXL software runtime that allows users to uti-\nlize the underlying disaggregated memory resources via sheer\nload/store instructions. Since DIRECT CXL does not require\nany data copies between the host memory and remote memory,\nit can expose the true performance of remote-side disaggre-\ngated memory resources to the users.\n1 Introduction\nMemory disaggregation has attracted great attention thanks\nto its high memory utilization, transparent elasticity, and re-\nsource management ef\ufb01ciency [ 1\u20133]. Many studies have ex-\nplored various software and hardware approaches to realize\nmemory disaggregation and put signi\ufb01cant efforts into making\nit practical in large-scale systems [ 4\u201316].\nWe can broadly classify the existing memory disaggrega-\ntion runtimes into two different approaches based on how they\nmanage data between a host and memory server(s): i) page-\nbased and ii) object-based. The page-based approach [ 4\u201310]\nutilizes virtual memory techniques to use disaggregated mem-\nory without a code change. It swaps page cache data resid-\ning on the host\u2019s local DRAMs from/to the remote mem-\nory systems over a network in cases of a page fault. On the\nother hand, the object-based approach handles disaggregated\nmemory from a remote using their own database such as a\nkey-value store instead of leveraging the virtual memory sys-\ntems [ 11\u201316]. This approach can address the challenges im-\nposed by address translation (e.g., page faults, context switch-\ning, and write ampli\ufb01cation), but it requires signi\ufb01cant source-\nlevel modi\ufb01cations and interface changes.While there are many variants, all the existing approaches\nneed to move data from the remote memory to the host mem-\nory over remote direct memory access ( RDMA )[4,5,11\u201313,\n15,16] (or similar \ufb01ne-grain network interfaces [ 7,9,10,17]).\nIn addition, they even require managing locally cached data\nin either the host or memory nodes.", "start_char_idx": 639775, "end_char_idx": 643894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59e3dc07-2910-4b08-841b-f9992c27f228": {"__data__": {"id_": "59e3dc07-2910-4b08-841b-f9992c27f228", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6975f378-2f02-4f65-9436-4a502e37551d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5869656811343b92938327ec9336c7b473574c81a364a4107f7cb578a5622aa7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3e6c2dd-2f73-40a6-bfad-708f8713dbeb", "node_type": "1", "metadata": {}, "hash": "e7a52c1566647b43d1734fcc4e35c3afddc36c471410100e92893e30ba8c26a1", "class_name": "RelatedNodeInfo"}}, "text": "On the\nother hand, the object-based approach handles disaggregated\nmemory from a remote using their own database such as a\nkey-value store instead of leveraging the virtual memory sys-\ntems [ 11\u201316]. This approach can address the challenges im-\nposed by address translation (e.g., page faults, context switch-\ning, and write ampli\ufb01cation), but it requires signi\ufb01cant source-\nlevel modi\ufb01cations and interface changes.While there are many variants, all the existing approaches\nneed to move data from the remote memory to the host mem-\nory over remote direct memory access ( RDMA )[4,5,11\u201313,\n15,16] (or similar \ufb01ne-grain network interfaces [ 7,9,10,17]).\nIn addition, they even require managing locally cached data\nin either the host or memory nodes. Unfortunately, the data\nmovement and its accompanying operations (e.g., page cache\nmanagement) introduce redundant memory copies and soft-\nware fabric intervention, which makes the latency of disaggre-\ngated memory longer than that of local DRAM accesses by\nmultiple orders of magnitude. In this work, we advocate com-\npute express link (CXL [ 18]), which is a new concept of open\nindustry standard interconnects offering high-performance\nconnectivity among multiple host processors, hardware accel-\nerators, and I/O devices [ 19]. CXL is originally designed to\nachieve the excellency of heterogeneity management across\ndifferent processor complexes, but both industry and academia\nanticipate its cache coherence ability can help improve mem-\nory utilization and alleviate memory over-provisioning with\nlow latency [ 20\u201322]. Even though CXL exhibits a great po-\ntential to realize memory disaggregation with low monetary\ncost and high performance, it has not been yet made for pro-\nduction, and there is no platform to integrate memory into a\nmemory pooling network.\nWe demonstrate DIRECT CXL , direct accessible disaggre-\ngated memory that connects host processor complex and\nremote memory resources over CXL\u2019s memory protocol\n(CXL.mem ). To this end, we explore a practical design for\nCXL-based memory disaggregation and make it real. Speci\ufb01-\ncally, we \ufb01rst show how to disaggregate memory over CXL\nand integrate the disaggregated memory into processor-side\nsystem memory. This includes implementing CXL controller\nthat employs multiple DRAM modules on a remote side. We\nthen prototype a set of network infrastructure components\nsuch as a CXL switch in order to make the disaggregated\nmemory connected to the host in a scalable manner. As there\nis no operating system that support CXL, we also offer CXL\nsoftware runtime that allows users to utilize the underlying\ndisaggregated memory resources through sheer load/store in-\nstructions. DIRECT CXL does not require any data copies\nbetween the host memory and remote memory, and therefore,\nit can expose the true performance of remote-side disaggre-\ngated memory resources to the users.\nIn this work, we prototype DIRECT CXL using many cus-\nUSENIX Association 2022 USENIX Annual Technical Conference    287tomized memory add-in-cards, 16 nmFPGA-based processor\nnodes, a switch, and a PCIe backplane. On the other hand, DI-\nRECT CXL software runtime is implemented based on Linux\n5.13. To the best of our knowledge, this is the \ufb01rst work that\nbrings CXL 2.0 into a real system and analyzes the perfor-\nmance characteristics of CXL-enabled disaggregated memory\ndesign. The results of our real system evaluation show that\nthe disaggregated memory resources of DIRECT CXL can ex-\nhibit DRAM-like performance when the workload can enjoy\nthe host processor\u2019s cache. When the load/store instructions\ngo through the CXL network and are served from the disag-\ngregated memory, D IRECT CXL\u2019s latency is shorter than the\nbest latency of RDMA by 6.2 \u21e5, on average. For real-world\napplications, DIRECT CXL exhibits 3 \u21e5better performance\nthan RDMA-based memory disaggregation, on average.\n2 Memory Disaggregation and Related Work\n2.1 Remote Direct Memory Access\nThe basic idea of memory disaggregation is to connect a host\nwith one or more memory nodes, such that it does not restrict\na given job execution because of limited local memory space.\nFor the backend network control, most disaggregation work\nemploy remote direct memory access (RDMA) [ 4,5,11\u201313,\n15,16] or similar customized DMA protocols [ 7,9,10]. Figure\n1shows how RDMA-style data transfers (one-sided RDMA)\nwork.", "start_char_idx": 643146, "end_char_idx": 647511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3e6c2dd-2f73-40a6-bfad-708f8713dbeb": {"__data__": {"id_": "c3e6c2dd-2f73-40a6-bfad-708f8713dbeb", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59e3dc07-2910-4b08-841b-f9992c27f228", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e6a357b63aec81512eeea45e891ed822cdafe604ba8e69340d29bcc6bd2c2cf3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "392e83ab-859f-44eb-ad37-ff068c803e7b", "node_type": "1", "metadata": {}, "hash": "392a4902a1d723fed0f23172a587a94b168085b99daead5dd6723e264009766c", "class_name": "RelatedNodeInfo"}}, "text": "For real-world\napplications, DIRECT CXL exhibits 3 \u21e5better performance\nthan RDMA-based memory disaggregation, on average.\n2 Memory Disaggregation and Related Work\n2.1 Remote Direct Memory Access\nThe basic idea of memory disaggregation is to connect a host\nwith one or more memory nodes, such that it does not restrict\na given job execution because of limited local memory space.\nFor the backend network control, most disaggregation work\nemploy remote direct memory access (RDMA) [ 4,5,11\u201313,\n15,16] or similar customized DMA protocols [ 7,9,10]. Figure\n1shows how RDMA-style data transfers (one-sided RDMA)\nwork. For both the host and memory node sides, RDMA needs\nhardware support such as RDMA NIC ( RNIC [23]), which\nis designed toward removing the intervention of the network\nsoftware stack as much as possible. To move data between\nthem, processes on each side \ufb01rst require de\ufb01ning one or\nmore memory regions ( MRs) and letting the MR(s) to the\nunderlying RNIC. During this time, the RNIC driver checks\nall physical addresses associated with the MR\u2019s pages and\nregisters them to RNIC\u2019s memory translation table ( MTT ).\nSince those two RNICs also exchange their MR\u2019s virtual\naddress at the initialization, the host can simply send the\nmemory node\u2019s destination virtual address with data for a\nwrite. The remote node then translates the address by referring\nto its MTT and copies the incoming data to the target location\nof MR. Reads over RDMA can also be performed in a similar\nmanner. Note that, in addition to the memory copy operations\n(for DMA), each side\u2019s application needs to prepare or retrieve\nthe data into/from MRs for the data transfers, introducing\nadditional data copies within their local DRAM [ 24].\n2.2 Swap: Page-based Memory Pool\nPage-based memory disaggregation [ 4\u201310] achieves memory\nelasticity by relying on virtual memory systems. Speci\ufb01cally,\nthis approach intercepts paging requests when there is a page\nfault, and then it swaps the data to a remote memory node in-\nstead of the underlying storage. To this end, a disaggregation\ndriver underneath the host\u2019s kernel swap daemon ( kswapd )\nconverts the incoming block address to the memory node\u2019s/g44/g381/g400/g410/g90/g69/g47/g18/g68/g286/g373/g381/g396/g455/g3/g374/g381/g282/g286/g17/g437/g296/g296/g286/g396/g4/g393/g393/g68/g286/g373/g381/g396/g455/g3/g396/g286/g336/g349/g381/g374/g90/g437/g374/g410/g349/g373/g286/g68/g100/g100/g90/g69/g47/g18/g3/g282/g396/g349/g448/g286/g396/g115/g4/g894/g373/g286/g373/g895/g47/g374/g349/g410/g47/g374/g349/g410/g18/g381/g393/g455/g24/g68/g4/g115/g4/g882/g410/g381/g882/g87/g4/g920/g3/g24/g68/g4/g100/g396/g258/g374/g400/g296/g286/g396/g3/g1092/g115/g4/g853/g3/g367/g286/g374/g336/g410/g346/g853/g3/g282/g258/g410/g258/g1093/g90/g69/g47/g18/g17/g437/g296/g296/g286/g396/g4/g393/g393/g68/g286/g373/g381/g396/g455/g3/g396/g286/g336/g349/g381/g374/g90/g437/g374/g410/g349/g373/g286/g68/g100/g100/g90/g69/g47/g18/g3/g282/g396/g349/g448/g286/g396/g115/g4/g894/g346/g381/g400/g410/g895/g47/g374/g349/g410/g18/g381/g393/g455Figure 1: Data movement over RDMA.\nvirtual address. It then copies the target page to RNIC\u2019s MR\nand issues the corresponding RDMA request to the mem-\nory node. Since all operations for memory disaggregation is\nmanaged under kswapd , it is easy-to-adopt and transparent\nto all user applications. However, page-based systems suffer\nfrom performance degradation due to the overhead of page\nfault handling, I/O ampli\ufb01cations, and context switching when\nthere are excessive requests for the remote memory [ 16].", "start_char_idx": 646899, "end_char_idx": 650444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "392e83ab-859f-44eb-ad37-ff068c803e7b": {"__data__": {"id_": "392e83ab-859f-44eb-ad37-ff068c803e7b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3e6c2dd-2f73-40a6-bfad-708f8713dbeb", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "50b6547b4646f62de08463a06477a6db128a5666ad2fbde128aa670cbcdf4fcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ae2e016-f5fa-43ea-a216-ecb7cdc0055f", "node_type": "1", "metadata": {}, "hash": "fee4a4d201770198338b852b21a0da67d15037628b2f897f62d548a7f6ff640e", "class_name": "RelatedNodeInfo"}}, "text": "virtual address. It then copies the target page to RNIC\u2019s MR\nand issues the corresponding RDMA request to the mem-\nory node. Since all operations for memory disaggregation is\nmanaged under kswapd , it is easy-to-adopt and transparent\nto all user applications. However, page-based systems suffer\nfrom performance degradation due to the overhead of page\nfault handling, I/O ampli\ufb01cations, and context switching when\nthere are excessive requests for the remote memory [ 16].\nNote that there are several studies that migrate locally\ncached data in a \ufb01ner granular manner [ 4\u20137] or reduce the\npage fault overhead by of\ufb02oading memory management (in-\ncluding page cache coherence) to the network [ 8] or memory\nnodes [ 9,10]. However, all these approaches use RDMA (or a\nsimilar network protocol), which is essential to cache the data\nand pay the cost of memory operations for network handling.\n2.3 KVS: Object-based Memory Pool\nIn contrast, object-based memory disaggregation systems\n[11\u201316] directly intervene in RDMA data transfers using their\nown database such as key-value store (KVS). Object-based\nsystems create two MRs for both host and memory node sides,\neach dealing with buffer data and submission/completion\nqueues (SQ/CQ). Generally, they employ a KV hash-table\nwhose entries point to corresponding (remote) memory ob-\njects. Whenever there is a request of Put(orGet) from an\napplication, the systems place the corresponding value into\nthe host\u2019s buffer MR and submit it by writing the remote\nside of SQ MR over RDMA. Since the memory node keeps\npolling SQ MR, it can recognize the request. The memory\nnode then reads the host\u2019s buffer MR, copies the value to\nits buffer MR over RDMA, and completes the request by\nwriting the host\u2019s CQ MR. As it does not lean on virtual mem-\nory systems, object-based systems can address the overhead\nimposed by page swap. However, the performance of object-\nbased systems varies based on the semantics of applications\ncompared to page-based systems; kswapd fully utilizes local\npage caches, but KVS does not for remote accesses. In addi-\ntion, this approach is unfortunately limited because it requires\nsigni\ufb01cant source-level modi\ufb01cations for legacy applications.\n3 Direct Accessible Memory Aggregation\nWhile caching pages and network-based data exchange are\nessential in the current technologies, they can unfortunately\nsigni\ufb01cantly deteriorate the performance of memory disaggre-\ngation. DIRECT CXL instead directly connects remote mem-\nory resources to the host\u2019s computing complex and allows\nusers to access them through sheer load/store instructions.\n288    2022 USENIX Annual Technical Conference USENIX Association/g28/g374/g282/g393/g381/g349/g374/g410/g18/g396/g381/g400/g400/g271/g258/g396/g44/g381/g400/g410/g3/g4/g44/g381/g400/g410/g3/g18/g87/g104/g90/g87/g62/g381/g272/g258/g367/g3/g24/g90/g4/g68/g18/g87/g104/g24/g90/g4/g68/g18/g121/g62/g856/g373/g286/g373/g18/g121/g62/g3/g400/g449/g349/g410/g272/g346/g44/g381/g400/g410/g3/g17/g44/g24/g68/g28/g87/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g18", "start_char_idx": 649973, "end_char_idx": 653027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ae2e016-f5fa-43ea-a216-ecb7cdc0055f": {"__data__": {"id_": "6ae2e016-f5fa-43ea-a216-ecb7cdc0055f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "392e83ab-859f-44eb-ad37-ff068c803e7b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f981f914a4d11e7a037aa1e6883d44254856c850cfc71d2cc5e10930b34241ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71aef2a4-f1a3-4925-9932-4c2ac63ae0da", "node_type": "1", "metadata": {}, "hash": "b4f623da5bd2d180c725f252438cac5eb234845b7142727bb3944c66778abf24", "class_name": "RelatedNodeInfo"}}, "text": "8/g87/g104/g24/g90/g4/g68/g18/g121/g62/g856/g373/g286/g373/g18/g121/g62/g3/g400/g449/g349/g410/g272/g346/g44/g381/g400/g410/g3/g17/g44/g24/g68/g28/g87/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g44/g24/g68/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g90/g87/g104/g94/g87/g44/g24/g68/g28/g87/g90/g87/g104/g94/g87/g44/g24/g68/g28/g87/g18/g121/g62/g3/g448/g349/g396/g410/g437/g258/g367/g346/g349/g286/g396/g258/g396/g272/g346/g455/g24/g94/g87/g3/g1004/g24/g94/g87/g3/g1005/g38/g68/g104/g94/g87/g3/g1004/g24/g94/g87/g1004/g1005/g104/g94/g87/g1004/g3/g3/g3/g1005/g75/g121/g75/g75/g44/g24/g68/g17/g4/g90/g94/g455/g400/g410/g286/g373/g373/g286/g373/g856\n/g87/g18/g47/g286/g3/g272/g381/g374/g296/g349/g336/g3/g400/g393/g258/g272/g286/g258/g282/g282/g396/g286/g400/g400/g3/g400/g393/g258/g272/g286/g62/g381/g258/g282/g876/g94/g410/g381/g396/g286\n/g24/g286/g448/g349/g272/g286/g3/g373/g286/g373/g381/g396/g455/g18/g121/g62/g3/g296/g367/g349/g410/g87/g18/g47/g286/g3/g894/g38/", "start_char_idx": 652827, "end_char_idx": 653843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71aef2a4-f1a3-4925-9932-4c2ac63ae0da": {"__data__": {"id_": "71aef2a4-f1a3-4925-9932-4c2ac63ae0da", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ae2e016-f5fa-43ea-a216-ecb7cdc0055f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ea9cf5d3286b7a0c8a28deca87c43c718a92413502d0e7617bdb232de57db2b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05dbe910-58b1-43a4-84f0-dc7e660f0e9f", "node_type": "1", "metadata": {}, "hash": "b90a599432ed1ba31d180069f3553bff78da8bad6b43b140aec3485f09feef55", "class_name": "RelatedNodeInfo"}}, "text": "400/g393/g258/g272/g286/g62/g381/g258/g282/g876/g94/g410/g381/g396/g286\n/g24/g286/g448/g349/g272/g286/g3/g373/g286/g373/g381/g396/g455/g18/g121/g62/g3/g296/g367/g349/g410/g87/g18/g47/g286/g3/g894/g38/g367/g286/g454/g17/g437/g400/g895/g17/g4/g90/g3/g400/g349/g460/g286/g17/g4/g90/g3/g271/g258/g400/g286/g44/g24/g68/g3/g400/g349/g460/g286/g18/g121/g62/g856/g373/g286/g373/g104/g94/g87/g3/g1005/g24/g94/g87/g24/g94/g87/g90/g381/g381/g410/g3/g87/g381/g396/g410/g28/g374/g437/g373/g286/g396/g258/g410/g349/g381/g374/g1005/g90/g286/g400/g286/g396/g448/g286/g282/g3/g296/g381/g396/g3/g18/g121/g62/g44/g24/g68/g3/g271/g258/g400/g286/g882/g4/g282/g282/g396/g286/g400/g400/g3/g410/g396/g258/g374/g400/g367/g258/g410/g349/g381/g374/g17/g4/g90/g876/g44/g24/g68/g3/g271/g258/g400/g286/g1007/g17/g4/g90/g876/g44/g24/g68/g3/g400/g349/g460/g286/g1006/g44/g381/g400/g410/g3/g18/g87/g104/g68/g286/g373/g856/g3/g396/g286/g395/g856/g68/g286/g373/g856/g3/g396/g286/g395/g856\n/g104/g400/g286/g396/g94/g286/g336/g373/g286/g374/g410/g44/g24", "start_char_idx": 653643, "end_char_idx": 654659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05dbe910-58b1-43a4-84f0-dc7e660f0e9f": {"__data__": {"id_": "05dbe910-58b1-43a4-84f0-dc7e660f0e9f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71aef2a4-f1a3-4925-9932-4c2ac63ae0da", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "73649c54f07c722d2be87a15b93ed527a82dd8c25861853d8b4bafab511c5498", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f46f385c-a3ed-4a43-805d-f8af0d42f416", "node_type": "1", "metadata": {}, "hash": "46aecaefa099646594e8ac1c976d39f20c2e629433b05eb8c4dc34f9dc31fa18", "class_name": "RelatedNodeInfo"}}, "text": "49/g460/g286/g1006/g44/g381/g400/g410/g3/g18/g87/g104/g68/g286/g373/g856/g3/g396/g286/g395/g856/g68/g286/g373/g856/g3/g396/g286/g395/g856\n/g104/g400/g286/g396/g94/g286/g336/g373/g286/g374/g410/g44/g24/g68/g60/g286/g396/g374/g286/g367/g75/g296/g296/g400/g286/g410/g1085/g400/g349/g460/g286/g373/g134/g135/g152/g373/g133/g154/g142/g350/g144/g149/g613/g18/g121/g62/g282/g286/g448/g349/g272/g286/g143/g143/g131/g146/g75/g296/g296/g400/g286/g410/g139/g145/g133/g150/g142/g24/g349/g396/g286/g272/g410/g18/g121/g62/g3/g396/g437/g374/g410/g349/g373/g286/g373/g134/g135/g152/g373/g134/g139/g148/g135/g133/g150/g133/g154/g142/g1006/g1007/g1008/g44/g24/g68/g3/g400/g286/g336/g373/g286/g374/g410/g3/g410/g258/g271/g367/g286/g94/g286/g336/g381/g296/g296/g400/g286/g410/g400/g349/g460/g286/g951/g396/g286/g296/g4/g393/g393/g367/g349/g272/g258/g410/g349/g381/g374/g1005Figure 2: DIRECT CXL \u2019s connection method.(a) CXL virtual hierarchy. (b) CXL switch.\nFigure 3: D IRECT CXL\u2019s network and switch.Figure 4: DIRECT CXL software\nruntime.\n3.1 Connecting Host and Memory over CXL\nCXL devices and controllers. In practice, existing memory\ndisaggregation techniques still require computing resources\nat the remote memory node side. This is because all DRAM\nmodules and their interfaces are designed as passive peripher-\nals, which require the control computing resources.", "start_char_idx": 654459, "end_char_idx": 655809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f46f385c-a3ed-4a43-805d-f8af0d42f416": {"__data__": {"id_": "f46f385c-a3ed-4a43-805d-f8af0d42f416", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05dbe910-58b1-43a4-84f0-dc7e660f0e9f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "2609347f8e0da78b00f0405497377b5b5a6bb2a1336840015a63fcacbdaaf4a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79401572-9926-4979-b3fe-c2f90ee105f4", "node_type": "1", "metadata": {}, "hash": "8803141f5e96c06b9774376ffbaf9f826cbfaa7d5aa93fda2fd4cff9a2079612", "class_name": "RelatedNodeInfo"}}, "text": "1/g396/g286/g296/g4/g393/g393/g367/g349/g272/g258/g410/g349/g381/g374/g1005Figure 2: DIRECT CXL \u2019s connection method.(a) CXL virtual hierarchy. (b) CXL switch.\nFigure 3: D IRECT CXL\u2019s network and switch.Figure 4: DIRECT CXL software\nruntime.\n3.1 Connecting Host and Memory over CXL\nCXL devices and controllers. In practice, existing memory\ndisaggregation techniques still require computing resources\nat the remote memory node side. This is because all DRAM\nmodules and their interfaces are designed as passive peripher-\nals, which require the control computing resources. CXL.mem\nin contrast allows the host computing resources directly ac-\ncess the underlying memory through PCIe buses ( FlexBus ); it\nworks similar to local DRAM, connected to their system buses.\nThus, we design and implement CXL devices as pure passive\nmodules, each being able to have many DRAM DIMMs with\nits own hardware controllers. Our CXL device employs mul-\ntiple DRAM controllers, connecting DRAM DIMMs over the\nconventional DDR interfaces. Its CXL controller then exposes\nthe internal DRAM modules to FlexBus through many PCIe\nlanes. In the current architecture, the device\u2019s CXL controller\nparses incoming PCIe-based CXL packets, called CXL \ufb02its ,\nconverts their information (address and length) to DRAM\nrequests, and serves them from the underlying DRAMs using\nthe DRAM controllers.\nIntegrating devices into system memory. Figure 2shows\nhow CXL devices\u2019 internal DRAMs are mapped (exposed)\nto a host\u2019s memory space over CXL. The host CPU\u2019s sys-\ntem bus contains one or more CXL root ports ( RPs), which\nconnect one or more CXL devices as endpoint ( EP) devices.\nOur host-side kernel driver \ufb01rst enumerates CXL devices by\nquerying the size of their base address register (BAR) and\ntheir internal memory, called host-managed device memory\n(HDM ), through PCIe transactions. Based on the retrieved\nsizes, the kernel driver maps BAR and HDM in the host\u2019s\nreserved system memory space and lets the underlying CXL\ndevices know where their BAR and HDM (base addresses)\nare mapped in the host\u2019s system memory. When the host CPU\naccesses an HDM system memory through load/store instruc-\ntion, the request is delivered to the corresponding RP, and the\nRP converts the requests to a CXL \ufb02it. Since HDM is mapped\nto a different location of the system memory, the memory\naddress space of HDM is different from that of EP\u2019s internal\nDRAMs. Thus, the CXL controller translates the incoming ad-\ndresses by simply deducting HDM\u2019s base address from them\nand issues the translated request to the underlying DRAM\ncontrollers. The results are returned to the host via a CXL\nswitch and FlexBus. Note that, since HDM accesses have no\nsoftware intervention or memory data copies, DIRECT CXL\ncan expose the CXL device\u2019s memory resources to the host\nwith low access latency.Designing CXL network switch. Figure 3aillustrates how\nDIRECT CXL can disaggregate memory resources from a host\nusing one or more and CXL devices, and Figure 3bshows\nour CXL switch organization therein. The host\u2019s CXL RP is\nconnected to upstream port (USP) of either a CXL switch\nor the CXL device directly. The CXL switch\u2019s downstream\nport(DSP) also connects either another CXL switch\u2019s USP or\nthe CXL device. Note that our CXL switch employs multiple\nUSPs and DSPs. By setting an internal routing table, our CXL\nswitch\u2019s fabric manager (FM) recon\ufb01gures the switch\u2019s cross-\nbar to connect each USP to a different DSP, which creates a\nvirtual hierarchy from a root (host) to a terminal (CXL de-\nvice). Since a CXL device can employ one or more controllers\nand many DRAMs, it can also de\ufb01ne multiple logical devices,\neach exposing its own HDM to a host. Thus, different hosts\ncan be connected to a CXL switch and a CXL device. Note\nthat each CXL virtual hierarchy only offers the path from one\nto another to ensure that no host is sharing an HDM.", "start_char_idx": 655238, "end_char_idx": 659112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79401572-9926-4979-b3fe-c2f90ee105f4": {"__data__": {"id_": "79401572-9926-4979-b3fe-c2f90ee105f4", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f46f385c-a3ed-4a43-805d-f8af0d42f416", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a4ef5c01217f75abe6c064f6d1abd6dce700ae05660cf286ff78c53b082734e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d8f8216-43b3-4568-be9a-cd6b484885b9", "node_type": "1", "metadata": {}, "hash": "23f792275a3a995ccb8c70dd384819a768034f58029a4fd38000c84ba53ad100", "class_name": "RelatedNodeInfo"}}, "text": "The CXL switch\u2019s downstream\nport(DSP) also connects either another CXL switch\u2019s USP or\nthe CXL device. Note that our CXL switch employs multiple\nUSPs and DSPs. By setting an internal routing table, our CXL\nswitch\u2019s fabric manager (FM) recon\ufb01gures the switch\u2019s cross-\nbar to connect each USP to a different DSP, which creates a\nvirtual hierarchy from a root (host) to a terminal (CXL de-\nvice). Since a CXL device can employ one or more controllers\nand many DRAMs, it can also de\ufb01ne multiple logical devices,\neach exposing its own HDM to a host. Thus, different hosts\ncan be connected to a CXL switch and a CXL device. Note\nthat each CXL virtual hierarchy only offers the path from one\nto another to ensure that no host is sharing an HDM.\n3.2 Software Runtime for DirectCXL\nIn contrast to RDMA, once a virtual hierarchy is established\nbetween a host and CXL device(s), applications running on\nthe host can directly access the CXL device by referring to\nHDM\u2019s memory space. However, it requires software run-\ntime/driver to manage the underlying CXL devices and ex-\npose their HDM in the application\u2019s memory space. We thus\nsupport DIRECT CXL runtime that simply splits the address\nspace of HDM into multiple segments, called cxl-namespace .\nDIRECT CXL runtime then allows the applications to access\neach CXL-namespace as memory-mapped \ufb01les ( mmap ).\nFigure 4shows the software stack of our runtime and how\nthe application can use the disaggregated memory through\ncxl-namespaces. When a CXL device is detected (at a PCIe\nenumeration time), DIRECT CXL driver creates an entry de-\nvice (e.g., /dev/directcxl ) to allow users to manage a\ncxl-namespace via ioctl . If users ask a cxl-namespace to\n/dev/directcxl , the driver checks a (physically) contiguous\naddress space on an HDM by referring to its HDM segment\ntable whose entry includes a segment\u2019s offset, size, and refer-\nence count (recording how many cxl-namespaces that indicate\nthis segment). Since multiple processes can access this table,\nits header also keeps necessary information such as spinlock,\nread/write locks, and a summary of table entries (e.g., valid\nentry numbers). Once DIRECT CXL driver allocates a seg-\nment based on the user request, it creates a device for mmap\n(e.g., /dev/cxl-ns0 ) and updates the segment table. The user\nUSENIX Association 2022 USENIX Annual Technical Conference    289/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286\n/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g400/g18/g121/g62/g3/g94/g449/g349/g410/g272/g346/g18/g121/g62/g3/g346/g381/g400/g410/g3/g393/g396/g381/g272/g286/g400/g400/g381/g396/g400/g24/g349/g400/g258/g336/g336/g396/g286/g336/g258/g410/g286/g282/g3/g373/g286/g373/g381/g396/g455/g87/g18/g47/g286/g3/g271/g258/g272/g364/g393/g367/g258/g374/g286/g18/g121/g62/g3/g282/g286/g448/g349/g272/g286/g18/g121/g62/g3/g400/g449/g349/g410/g272/g346/g18/g121/g62/g3/g346/g381/g400/g410/g18/g121/g62/g3/g296/g367/g349/g410\n/g24/g90/g4/g68/g24/g90/g4/g68/g24/g90/g4/g68/g24/g90/g4/g68/g18/g381/g374/g410/g396/g381/g367/g367/g286/g396/g18/g121/g62/g3/g28/g87/g18/g121/g62/g3/g90/g87/g18/g87/g104/g62/g62/g18/g62/g381/g272/g258/g367/g3/g24/g90/g4/g68/g18/g121/g62/g856/g373/g286/g373/g69/g855/g68/g374/g286/g410/g449/g381/g396/g364/g18/g121/g62/g3/g346/g381/g400/g410(a) Network topology. (b) Implementation.\nFigure 5: CXL-enabled cluster.", "start_char_idx": 658375, "end_char_idx": 661710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d8f8216-43b3-4568-be9a-cd6b484885b9": {"__data__": {"id_": "1d8f8216-43b3-4568-be9a-cd6b484885b9", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79401572-9926-4979-b3fe-c2f90ee105f4", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "bbad6d981d39c406ba0f3d51b8c7b990ce0e73a1757259d8233669be858d307b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aefba818-f1c4-4371-aaf0-624bd968fc18", "node_type": "1", "metadata": {}, "hash": "365db9f2e1d4dd8c1aba74a3466faf48882dac8b1b00be5a978c6b2e8e7551a7", "class_name": "RelatedNodeInfo"}}, "text": "(b) Implementation.\nFigure 5: CXL-enabled cluster.\napplication can then map the cxl-namespace to its process\nvirtual memory space using mmap with vm_area_struct .\nNote that DIRECT CXL software runtime is designed for\ndirect access of CXL devices, which is a similar concept to the\nmemory-mapped \ufb01le management of persistent memory de-\nvelopment toolkit (PMDK [ 25]). However, it is much simpler\nand more \ufb02exible for namespace management than PMDK.\nFor example, PMDK\u2019s namespace is very much the same idea\nas NVMe namespace, managed by \ufb01le systems or DAX with\na \ufb01xed size [ 26]. In contrast, our cxl-namespace is more sim-\nilar to the conventional memory segment, which is directly\nexposed to the application without a \ufb01le system employment.\n3.3 Prototype Implementation\nFigure 5aillustrates our design of a CXL network topology\nto disaggregate memory resources, and the corresponding im-\nplementation in a real system is shown in Figure 5b. There\narennumbers of compute hosts connected to mnumber of\nCXL devices through a CXL switch; in our prototype, nand\nmare four, but those numbers can scale by having more CXL\nswitches. Speci\ufb01cally, each CXL device prototype is built on\nour customized add-in-card (AIC) CXL memory blade that\nemploys 16 nmFPGA and 8 different DDR4 DRAM modules\n(64GB). In the FPGA, we fabricate a CXL controller and eight\nDRAM controllers, each managing the CXL endpoint and\ninternal DRAM channels. As yet there is no processor archi-\ntecture supporting CXL, we also build our own in-house host\nprocessor using RISC-V ISAs, which employs four out-of-\norder cores whose last-level cache ( LLC) implements CXL RP.\nEach CXL-enabled host processor is implemented in a high-\nperformance datacenter accelerator card, taking a role of a\nhost, which can individually run Linux 5.13 and DIRECT CXL\nsoftware runtime. We expose four CXL devices (32 DRAM\nmodules) to the four hosts through our PCIe backplane. We\nextended the backplane with one more accelerator card that\nimplements DIRECT CXL \u2019s CXL switch. This switch imple-\nments FM that can create multiple virtual hierarchies, each\nconnecting a host and a CXL device in a \ufb02exible manner.\nTo the best of our knowledge, there are no commercialized\nCXL 2.0 IPs for the processor side\u2019s CXL engines and CXL\nswitch. Thus, we built all DIRECT CXL IPs from the ground.\nThe host-side processors require advanced con\ufb01guration and\npower interface (ACPI [ 27]) for CXL 2.0 enumeration (e.g.,\nRP location and RP\u2019s reserved address space). Since RISC-V\ndoes not support ACPI yet, we enable the CXL enumeration\nby adding such information into the device tree [ 28]. Speci\ufb01-\ncally, we update an MMIO register designated as a property ofthe tree\u2019s node to let the processor know where CXL RP exists.\nOn the other hand, we add a new \ufb01eld ( cxl-reserved-area )\nin the node to indicate where an HDM can be mapped. Our\nin-house softcore processors work at 100MHz while CXL\nand PCIe IPs (RP, EP, and Switch) operate at 250MHz.\n4 Evaluation\nTestbed prototypes for memory disaggregation. In addition\nto the CXL environment that we implemented in Section 3.3\n(DirectCXL ), we set up the same con\ufb01guration with it for our\nRDMA-enabled hardware system ( RDMA ). For RDMA , we use\nMellanox ConnectX-3 VPI In\ufb01niBand RNIC (56Gbps, [ 29])\ninstead of our CXL switch as RDMA network interface card\n(RNIC). In addition, we port Mellanox OpenFabric Enterprise\nDistribution (OFED) v4.9 [ 30] as an RDMA driver to enable\nRNIC in our evaluation testbed. Lastly, we port FastSwap [ 1]\nand HERD [ 12] into RISC-V Linux 5.13.19 computing envi-\nronment atop RDMA , each realizing page-based disaggregation\n(Swap ) and object-based disaggregation ( KVS).\nFor better comparison, we also con\ufb01gure the host proces-\nsors to use only their local DRAM ( Local ) by disabling all\nthe CXL memory nodes.", "start_char_idx": 661660, "end_char_idx": 665478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aefba818-f1c4-4371-aaf0-624bd968fc18": {"__data__": {"id_": "aefba818-f1c4-4371-aaf0-624bd968fc18", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d8f8216-43b3-4568-be9a-cd6b484885b9", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "03f41b0afcc870f2cc7d780d676e0f474f97f828a4741b1bf6dcea0abf01852f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ff0315a-d337-44a6-a6be-f98a5e646955", "node_type": "1", "metadata": {}, "hash": "5bb25877ba06514f466cbf95d5366d943b31b75fa590e8bc9e4c2a3253d67afd", "class_name": "RelatedNodeInfo"}}, "text": "For RDMA , we use\nMellanox ConnectX-3 VPI In\ufb01niBand RNIC (56Gbps, [ 29])\ninstead of our CXL switch as RDMA network interface card\n(RNIC). In addition, we port Mellanox OpenFabric Enterprise\nDistribution (OFED) v4.9 [ 30] as an RDMA driver to enable\nRNIC in our evaluation testbed. Lastly, we port FastSwap [ 1]\nand HERD [ 12] into RISC-V Linux 5.13.19 computing envi-\nronment atop RDMA , each realizing page-based disaggregation\n(Swap ) and object-based disaggregation ( KVS).\nFor better comparison, we also con\ufb01gure the host proces-\nsors to use only their local DRAM ( Local ) by disabling all\nthe CXL memory nodes. Note that we used the same testbed\nhardware mentioned above for both CXL experiments and\nnon-CXL experiments but differently con\ufb01gured the testbed\nfor each reference. For example, our testbed\u2019s FPGA chips\nfor the host (in-house) processors and CXL devices use all\nthe same architecture/technology and product line-up.\nBenchmark and workloads. Since there is no microbench-\nmark that we can compare different memory pooling tech-\nnologies ( RDMA vs.DirectCXL ), we also build an in-house\nmemory benchmark for in-depth analysis of those two tech-\nnologies (Section 4.1). For RDMA , this benchmark allocates a\nlarge size of the memory pool at the remote side in advance.\nThis benchmark allows a host processor to send random mem-\nory requests to a remote node with varying lengths; the re-\nmote node serves the requests using the pre-allocated memory\npool. For DirectCXL andLocal , the benchmark maps cxl\nnamespace oranonymous mmap to user spaces, respectively.\nThe benchmark then generates a group of RISC-V memory\ninstructions, which can cover a given address length in a\nrandom pattern and directly issues them without software\nintervention. For the real workloads, we use Facebook\u2019s deep\nlearning recommendation model (DLRM [ 31]), an in-memory\ndatabase used for the HERD evaluation (MemDB [ 12]), and\nfour graph analysis workloads (MIS [ 32], BFS [ 33], CC [ 34],\nand BC [ 35]) coming from Ligra [ 36]. All their tables and data\nstructures are stored in the remote node, while each host\u2019s lo-\ncal memory handles the execution code and static data. Table\n1summarizes the per-node memory usage and total data sizes\nfor each workload that we tested.\n4.1 In-depth Analysis of RDMA and CXL\nIn this subsection, we compare the performance of RDMA\nand CXL technologies when the host and memory nodes are\n290    2022 USENIX Annual Technical Conference USENIX Association1K4K16K64K256K1M4M16M64M256M1G1101001k10k03 0 0 2 4 0 0 2 7 0 0Latency (cycles)PCIe MemoryNetwork CPU cache RDMADirectCXLDMAx8.3 fasterDMA641282565121K2K4K04k8k12k16kBreakdown(cycles)Payload (bytes)Library Copy Memory Network641282565121K2K4K01k2kBreakdown(cycles)Payload (bytes)MemoryPCIe CPU Cache Latency (cycles)Working set sizeLocal RDMA DirectCXLL1D (4)L2 (24)Local (60)CXL (328)RDMA (2027~2042 cycles)x5.5x510.5x34Figure 6: RDMA vs. CXL.(a) RDMA breakdown. (b) CXL breakdown.\nFigure 7: Sensitivity tests. Figure 8: Memory hierarchy performance.\ncon\ufb01gured through a 1:1 connection. Figure 6shows latency\nbreakdown of RDMA andDirectCXL when reading 64 bytes of\ndata. One can observe from the \ufb01gure that RDMA requires two\nDMA operations, which doubles the PCIe transfer and mem-\nory access latency. In addition, the communication overhead\nof In\ufb01niBand ( Network ) takes 78.7% (2129 cycles) of the\ntotal latency (2705 cycles). In contrast, DirectCXL only takes\n328 cycles for memory load request, which is 8.3 \u21e5faster than\nRDMA . There are two reasons behind this performance differ-\nence.", "start_char_idx": 664862, "end_char_idx": 668434, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ff0315a-d337-44a6-a6be-f98a5e646955": {"__data__": {"id_": "2ff0315a-d337-44a6-a6be-f98a5e646955", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aefba818-f1c4-4371-aaf0-624bd968fc18", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "79f10f83d8512ae7a7c2660ceb37d84617e4285f9090734aa4211311f3cad04d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a460a119-a678-4bc9-81f1-9912a05f19ef", "node_type": "1", "metadata": {}, "hash": "d56d767d737a5b1462013e2bf65899e2c146ff504c0d023b55c55950d38a42e8", "class_name": "RelatedNodeInfo"}}, "text": "CXL.(a) RDMA breakdown. (b) CXL breakdown.\nFigure 7: Sensitivity tests. Figure 8: Memory hierarchy performance.\ncon\ufb01gured through a 1:1 connection. Figure 6shows latency\nbreakdown of RDMA andDirectCXL when reading 64 bytes of\ndata. One can observe from the \ufb01gure that RDMA requires two\nDMA operations, which doubles the PCIe transfer and mem-\nory access latency. In addition, the communication overhead\nof In\ufb01niBand ( Network ) takes 78.7% (2129 cycles) of the\ntotal latency (2705 cycles). In contrast, DirectCXL only takes\n328 cycles for memory load request, which is 8.3 \u21e5faster than\nRDMA . There are two reasons behind this performance differ-\nence. First, DirectCXL straight connects the compute nodes\nand memory nodes using PCIe while RDMA requires proto-\ncol/interface changes between In\ufb01niBand and PCIe. Second,\nDirectCXL can translate memory load/store request from\nLLC into the CXL \ufb02its whereas RDMA must use DMA to\nread/write data from/to memory.\nSensitivity tests. Figure 7adecomposes RDMA latency into es-\nsential hardware ( Memory andNetwork ), software ( Library ),\nand data transfer latencies ( Copy ). In this evaluation, we in-\nstrument two user-level In\ufb01niBand libraries, libibverbs and\nlibmlx4 to measure the software side latency. Library is\nthe primary performance bottleneck in RDMA when the size of\npayloads is smaller than 1KB (4158 cycles, on average). As\nthe payloads increase, Copy gets longer and reaches 28.9% of\ntotal execution time. This is because users must copy all their\ndata into RNIC\u2019s MR, which takes extra overhead in RDMA . On\nthe other hand, Memory andNetwork shows a performance\ntrend similar to RDMA analyzed in Figure 6. Note that the actual\ntimes of Network (Figure 7a) do not decrease as the payload\nincreases; while Memory increases to handle large size of data,\nRNIC can simultaneously transmit the data to the underlying\nnetwork. These overlapped cycles are counted by Memory in\nour analysis. As shown in Figure 7b, the breakdown analysis\nforDirectCXL shows a completely different story; there is\nneither software nor data copy overhead. As the payloads\nincrease, the dominant component of DirectCXL \u2019s latency is\nLLC ( CPU Cache ). This is because LLC can handle 16 con-\ncurrent misses through miss status holding registers (MSHR)\nin our custom CPU. Thus, many memory requests (64B) com-\nposing a large payload data can be stalled at CPU, which\ntakes 67% of the total latency to handle 4KB payloads. PCIe\nshown in Figure 7adoes not decrease as the payloads increase\nbecause of a similar reason of RDMA \u2019sNetwork . However, it\nPer-node usage Total\nusageData stored in\nremote memory Local Remote\nDLRM [ 31]Less than\n100MB17GB 68GB Embedding tables.\nMemDB [ 12] 4GB 16GB Key-value pairs and tree structure.\nLigra [ 36] 7GB 28GB Deserialized graph structure.\nTable 1: Memory usage characteristic of each workload.is not as much as what Network did as only 16 concurrent\nmisses can be overlapped. ote that PCIe shown in Figures 6\nand7bincludes the latency of CXL IPs (RP, EP, and Switch),\nwhich is different from the pure cycles of PCIe physical bus.\nThe pure cycles of PCIe physical bus (FlexBus) account for\n28% of DirectCXL latency. The detailed latency decomposi-\ntion will be analyzed in Section 4.2.\nMemory hierarchy performance. Figure 8shows latency\ncycles of different components in the system\u2019s memory hier-\narchy. While Local andDirectCXL exhibits CPU cache by\nlowering the memory access latency to 4 cycles, RDMA has neg-\nligible impacts on CPU cache as their network overhead is\nmuch higher than that of Local . The best-case performance of\nRDMA was 2027 cycles, which is 6.2 \u21e5and 510.5 \u21e5slower than\nthat of DirectCXL and L1 cache, respectively. DirectCXL\nrequires 328 cycles whereas Local requires only 60 cycles in\nthe case of L2 misses. Note that the performance bottleneck\nofDirectCXL isPCIe including CXL IPs (77.8% of the total\nlatency).", "start_char_idx": 667782, "end_char_idx": 671683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a460a119-a678-4bc9-81f1-9912a05f19ef": {"__data__": {"id_": "a460a119-a678-4bc9-81f1-9912a05f19ef", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ff0315a-d337-44a6-a6be-f98a5e646955", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "0efd67ac134636feae4806f80c9a777f7faa272113990a2c1188a295dbb2d018", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15e8d1b8-6c9b-4ae5-aed4-31ef7bb4d1e8", "node_type": "1", "metadata": {}, "hash": "d1c8ab59c365e6b5854eddd41777cbc46b574c4ae0d7aaf74a33d3750ed2a204", "class_name": "RelatedNodeInfo"}}, "text": "The detailed latency decomposi-\ntion will be analyzed in Section 4.2.\nMemory hierarchy performance. Figure 8shows latency\ncycles of different components in the system\u2019s memory hier-\narchy. While Local andDirectCXL exhibits CPU cache by\nlowering the memory access latency to 4 cycles, RDMA has neg-\nligible impacts on CPU cache as their network overhead is\nmuch higher than that of Local . The best-case performance of\nRDMA was 2027 cycles, which is 6.2 \u21e5and 510.5 \u21e5slower than\nthat of DirectCXL and L1 cache, respectively. DirectCXL\nrequires 328 cycles whereas Local requires only 60 cycles in\nthe case of L2 misses. Note that the performance bottleneck\nofDirectCXL isPCIe including CXL IPs (77.8% of the total\nlatency). This can be accelerated by increasing the working\nfrequency, which will be discussed shortly.\n4.2 Latency Distribution and Scaling Study\nLatency distribution. In addition to the latency trend (av-\nerage) we reported above, we also analyze complete latency\nbehaviors of Local ,RDMA , and DirectCXL . Figure 9shows\nthe latency CDF of memory accesses (64B) for the different\npooling methods. RDMA shows the performance curve, which\nranges from 1790 cycles to 4006 cycles. The reason why there\nis a difference between the minimum and maximum latency\nofRDMA is RNIC\u2019s MTT memory buffer and CPU caches for\ndata transfers. While RDMA cannot take the bene\ufb01ts from di-\nrect load/store instruction with CPU caches, its data transfers\nthemselves utilize CPU caches. Nevertheless, RDMA cannot\navoid the network accesses for remote memory accesses, mak-\ning its latency worse than Local by 36.8 \u21e5, on average. In\ncontrast, the latency behaviors of DirectCXL are similar to\nLocal . Even though the latency of DirectCXL (reported in\nFigures 6and7b) is the average value, its best performance\nis the same as Local (4\u21e024 cycles). This is because, as we\nshowed in the previous section, DirectCXL can take the ben-\ne\ufb01ts of CPU caches directly. The tail latency is 2.8 \u21e5worse\nthan Local , but its latency curve is similar to that of Local .\nThis is because both DirectCXL andLocal use the same\nDRAM (and there is no network access overhead).\nSpeed scaling estimation. The cycle numbers that we re-\nported here are measured at each host\u2019s CPU using register-\nlevel instrumentation. We believe it is suf\ufb01cient and better\nUSENIX Association 2022 USENIX Annual Technical Conference    29102001800200022000255075100CDF (%)Latency (cycles)Local RDMA DirectCXL L1DL2LocalCXL4006Figure 9: Memory-level\nlatency CDF (64B).Measurement\nclock domain !DIRECT CXL PCIe 5.0 x8 (Estimated)\nCPU (100MHz) CPU (1.2GHz) Time delay\nL1/L2 cache 30 30 25 ns\nCXL IPs (2.0)* 165 287 239 nsPCIeFlexBus 91 69 57 ns\nDRAM controller 42 126 105 ns\nTotal 328 512 426 ns\n*Including RP, EP, and Switch Unit: cycles\nTable 2: Latency breakdown and\nestimated 64B load latency.DLRMMemDB0.00.51.0Norm. Exec. TimeSwap KVS DirectCXL MISBFSCCBC0.00.51.0\nSwapKVSCXL0.00.51.0Norm.Exec. TimeRDMA Software Workload DLRMSwapKVSCXLMemDBSwapCXLMISSwapCXLBFSSwapCXLCCSwapCXLBC(a) Execution Time. (b) Execution breakdown.\nFigure 10: Real workload performance.\nthan a cross-time-domain analysis to decompose the system\nlatency. Nevertheless, we estimate a time delay in cases where\nthe target system accelerates the frequency of its processor\ncomplex and CXL IPs (RP, EP, and Switch) by 1.2GHz and\n1GHz, respectively. Table 2decomposes DirectCXL \u2019s latency\nof a 64B memory load and compares it with the estimated time\ndelay. The cycle counts of L1/L2 cache misses are not differ-\nent as they work in all the same clock domain of CPU.", "start_char_idx": 670963, "end_char_idx": 674545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15e8d1b8-6c9b-4ae5-aed4-31ef7bb4d1e8": {"__data__": {"id_": "15e8d1b8-6c9b-4ae5-aed4-31ef7bb4d1e8", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a460a119-a678-4bc9-81f1-9912a05f19ef", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "25138522b2820c54ec25ce77d933a281cda7cd845de4f5bb4cf6a6f4b65e5ca3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fc3ec5a-38dd-40fa-9aef-93cae98bec0e", "node_type": "1", "metadata": {}, "hash": "624a60c4f04e0981f6d1ec06e53484b334c015da985067be57ca1c3a7ca59b1a", "class_name": "RelatedNodeInfo"}}, "text": "TimeRDMA Software Workload DLRMSwapKVSCXLMemDBSwapCXLMISSwapCXLBFSSwapCXLCCSwapCXLBC(a) Execution Time. (b) Execution breakdown.\nFigure 10: Real workload performance.\nthan a cross-time-domain analysis to decompose the system\nlatency. Nevertheless, we estimate a time delay in cases where\nthe target system accelerates the frequency of its processor\ncomplex and CXL IPs (RP, EP, and Switch) by 1.2GHz and\n1GHz, respectively. Table 2decomposes DirectCXL \u2019s latency\nof a 64B memory load and compares it with the estimated time\ndelay. The cycle counts of L1/L2 cache misses are not differ-\nent as they work in all the same clock domain of CPU. While\nother components (FlexBus, CXL IPs, and DRAM controller)\nspeed up by 4 \u21e5(250MHz !1GHz), the number of cycles\nincreases since CPU gets faster by 12 \u21e5. Note that, as the\nversion of PCIe is changed and the number of lanes for PCIe\nincreases by double, FlexBus\u2019s cycles decrease. The table in-\ncludes the time delays corresponding to the estimated system\nfrom the CPU\u2019s viewpoint. While the time delay of FlexBus is\npretty good ( \u21e060ns), the corresponding CXL IPs have room\nto improve further with a higher working frequency.\n4.3 Performance of Real Workloads\nFigure 10ashows the execution latency of Swap ,KVS, and\nDirectCXL when running DLRM, MemDB, and four work-\nloads from Ligra. For better understanding, all the results in\nthis subsection are normalized to those of Swap . For Ligra, we\nonly compare DirectCXL with Swap because Ligra\u2019s graph\nprocessing engines (handling in-/out-edges and vertices) is\nnot compatible with a key-value structure. KVScan reduce the\nlatency of Swap as it addresses the overhead imposed by page-\nbased I/O granularity to access the remote memory. However,\nit has two major issues behind KVS. First, it requires signif-\nicant modi\ufb01cation of the application\u2019s source codes, which\nis often unable to service (e.g., MIS, BFS, CC, BC). Sec-\nond, KVSrequires heavy computation such as hashing at the\nmemory node, which increases monetary costs. In contrast,\nDirectCXL without having a source modi\ufb01cation and remote-\nside resource exhibits 3 \u21e5and 2.2 \u21e5better performance than\nSwap and even KVS, respectively.\nTo better understand this performance improvement of\nDirectCXL , we also decompose the execution times into\nRDMA , network library intervention ( Software ), and appli-\ncation execution itself ( Workload ) latencies, and the results\nare shown in Figure 10b. This \ufb01gure demonstrates where\nSwap degrades the overall performance from its execution;\n51.8% of the execution time is consumed by kernel swap\ndaemon ( kswapd ) and FastSwap driver, on average. This is\nbecause Swap just expands memory with the local and remote\nbased on LRU, which makes its page exchange frequent. Thereason why KVSshows performance better than Swap in the\ncases of DLRM and MemDB is mainly related to workload\ncharacteristics and its service optimization. For DLRM, KVS\nloads the exact size of embeddings rather than a page, which\nreduces Swap \u2019s data transfer overhead as high as 6.9 \u21e5. While\nKVS shows the low overhead in our evaluation, RDMA and\nSoftware can linearly increase as the number of inferences\nincreases; in our case, we only used 13.5MB (0.0008%) of\nembeddings for single inference. For MemDB, as KVSstores\nall key-value pairs into local DRAM, it only accesses remote-\nside DRAM to inquiry values. However, it spends 55.3% and\n24.9% of the execution time for RDMA andSoftware to han-\ndle the remote DRAMs, respectively. In contrast, DirectCXL\nremoves such hardware and software overhead, which ex-\nhibits much better performance than Swap andKVS. Note that\nMemDB contains 2M key-value pairs whose value size is\n2KB, and its host queries 8M Getrequests by randomly gen-\nerating their keys. This workload characteristic roughly makes\nDirectCXL \u2019s memory accesses be faced with a cache miss\nfor every four queries.", "start_char_idx": 673906, "end_char_idx": 677788, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fc3ec5a-38dd-40fa-9aef-93cae98bec0e": {"__data__": {"id_": "5fc3ec5a-38dd-40fa-9aef-93cae98bec0e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15e8d1b8-6c9b-4ae5-aed4-31ef7bb4d1e8", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ef4c87397d6e7bf2d2bbb4e06a643a4e9caab1f5696e48c9fe04340b254a760d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3d3940f-f5cc-41fa-ad2e-2b604ddae237", "node_type": "1", "metadata": {}, "hash": "97399a3d8621acb486a3276c6df3306a305e6f6a1bc13aee45fb14c421276830", "class_name": "RelatedNodeInfo"}}, "text": "For MemDB, as KVSstores\nall key-value pairs into local DRAM, it only accesses remote-\nside DRAM to inquiry values. However, it spends 55.3% and\n24.9% of the execution time for RDMA andSoftware to han-\ndle the remote DRAMs, respectively. In contrast, DirectCXL\nremoves such hardware and software overhead, which ex-\nhibits much better performance than Swap andKVS. Note that\nMemDB contains 2M key-value pairs whose value size is\n2KB, and its host queries 8M Getrequests by randomly gen-\nerating their keys. This workload characteristic roughly makes\nDirectCXL \u2019s memory accesses be faced with a cache miss\nfor every four queries. Note that Workload ofDirectCXL is\nlonger than that of KVS, because DirectCXL places all hash\ntable and tree for key-value pairs whereas KVShas it in local\nDRAM. Lastly, all the four graph workloads show similar\ntrends; Swap is always slower than DirectCXL . They require\nmultiple graph traverses, which frequently generate random\nmemory access patterns. As Swap requires exchanging 4KB\npages to read 8B pointers for graph traversing, it shows 2.2 \u21e5\nworse performance than DirectCXL .\n5 Conclusion\nIn this paper, we propose DIRECT CXL that connects host\nprocessor complex and remote memory resources over CXL\u2019s\nmemory protocol ( CXL.mem ). The results of our real system\nevaluation show that the disaggregated memory resources\nofDIRECT CXL can exhibit DRAM-like performance when\nthe workload can enjoy the host-processor\u2019s cache. For real-\nworld applications, it exhibits 3 \u21e5better performance than\nRDMA-based memory disaggregation, on average.\n6 Future Work and Acknowledgement\nThe authors are extending the kernel for ef\ufb01cient CXL mem-\nory management and consider having an SoC silicon as fu-\nture work of DirectCXL . This work is protected by one or\nmore patents. The authors would like to thank the anonymous\nreviewers for their comments, and Myoungsoo Jung is the\ncorresponding author ( mj@camelab.org ).\n292    2022 USENIX Annual Technical Conference USENIX AssociationReferences\n[1]Emmanuel Amaro, Christopher Branner-Augmon, Zhi-\nhong Luo, Amy Ousterhout, Marcos K Aguilera, Aurojit\nPanda, Sylvia Ratnasamy, and Scott Shenker. Can far\nmemory improve job throughput? In Proceedings of the\nFifteenth European Conference on Computer Systems ,\npages 1\u201316, 2020.\n[2]Ling Liu, Wenqi Cao, Semih Sahin, Qi Zhang, Juhyun\nBae, and Yanzhao Wu. Memory disaggregation: Re-\nsearch problems and opportunities. In 2019 IEEE 39th\nInternational Conference on Distributed Computing Sys-\ntems (ICDCS) , pages 1664\u20131673. IEEE, 2019.\n[3]Kevin Lim, Yoshio Turner, Jose Renato Santos, Alvin\nAuYoung, Jichuan Chang, Parthasarathy Ranganathan,\nand Thomas F Wenisch. System-level implications of\ndisaggregated memory. In IEEE International Sympo-\nsium on High-Performance Comp Architecture , pages\n1\u201312. IEEE, 2012.\n[4]Juncheng Gu, Youngmoon Lee, Yiwen Zhang, Mosharaf\nChowdhury, and Kang G Shin. Ef\ufb01cient memory dis-\naggregation with in\ufb01niswap. In 14th USENIX Sympo-\nsium on Networked Systems Design and Implementation\n(NSDI 17) , pages 649\u2013667, 2017.\n[5]Marcos K Aguilera, Nadav Amit, Irina Calciu, Xavier\nDeguillard, Jayneel Gandhi, Stanko Novakovic, Arun\nRamanathan, Pratap Subrahmanyam, Lalith Suresh, Ki-\nran Tati, et al. Remote regions: a simple abstraction for\nremote memory. In 2018 USENIX Annual Technical\nConference (USENIX ATC 18) , pages 775\u2013787, 2018.\n[6]Chenxi Wang, Haoran Ma, Shi Liu, Yuanqi Li, Zhenyuan\nRuan, Khanh Nguyen, Michael D Bond, Ravi Netravali,\nMiryung Kim, and Guoqing Harry Xu. Semeru: A\nmemory-disaggregated managed runtime.", "start_char_idx": 677160, "end_char_idx": 680725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3d3940f-f5cc-41fa-ad2e-2b604ddae237": {"__data__": {"id_": "c3d3940f-f5cc-41fa-ad2e-2b604ddae237", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fc3ec5a-38dd-40fa-9aef-93cae98bec0e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "7b8431920b46fe7bdd3f8a97546b93049cbcf6b65dfe5ef7b7c0b57747b6804b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2db55bd0-ecf2-4264-9a95-5433d1420db1", "node_type": "1", "metadata": {}, "hash": "398e545f35b4e7a1f092b3ed0a671de7dfae3f4d78d32d155790552012c9e665", "class_name": "RelatedNodeInfo"}}, "text": "[5]Marcos K Aguilera, Nadav Amit, Irina Calciu, Xavier\nDeguillard, Jayneel Gandhi, Stanko Novakovic, Arun\nRamanathan, Pratap Subrahmanyam, Lalith Suresh, Ki-\nran Tati, et al. Remote regions: a simple abstraction for\nremote memory. In 2018 USENIX Annual Technical\nConference (USENIX ATC 18) , pages 775\u2013787, 2018.\n[6]Chenxi Wang, Haoran Ma, Shi Liu, Yuanqi Li, Zhenyuan\nRuan, Khanh Nguyen, Michael D Bond, Ravi Netravali,\nMiryung Kim, and Guoqing Harry Xu. Semeru: A\nmemory-disaggregated managed runtime. In 14th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 20) , pages 261\u2013280, 2020.\n[7]Christian Pinto, Dimitris Syrivelis, Michele Gazzetti,\nPanos Koutsovasilis, Andrea Reale, Kostas Katrinis,\nand H Peter Hofstee. Thymesis\ufb02ow: a software-\nde\ufb01ned, hw/sw co-designed interconnect stack for rack-\nscale memory disaggregation. In 2020 53rd Annual\nIEEE/ACM International Symposium on Microarchitec-\nture (MICRO) , pages 868\u2013880. IEEE, 2020.\n[8]Seung-seob Lee, Yanpeng Yu, Yupeng Tang, Anurag\nKhandelwal, Lin Zhong, and Abhishek Bhattacharjee.\nMind: In-network memory management for disaggre-\ngated data centers. In Proceedings of the ACM SIGOPS\n28th Symposium on Operating Systems Principles , pages\n488\u2013504, 2021.[9]Zhiyuan Guo, Yizhou Shan, Xuhao Luo, Yutong Huang,\nand Yiying Zhang. Clio: A hardware-software co-\ndesigned disaggregated memory system. In Proceedings\nof the 27th ACM International Conference on Architec-\ntural Support for Programming Languages and Operat-\ning Systems , pages 417\u2013433, 2022.\n[10] Irina Calciu, M Talha Imran, Ivan Puddu, Sanidhya\nKashyap, Hasan Al Maruf, Onur Mutlu, and Aasheesh\nKolli. Rethinking software runtimes for disaggregated\nmemory. In Proceedings of the 26th ACM International\nConference on Architectural Support for Programming\nLanguages and Operating Systems , pages 79\u201392, 2021.\n[11] Shin-Yeh Tsai, Yizhou Shan, and Yiying Zhang. Dis-\naggregating persistent memory and controlling them\nremotely: An exploration of passive disaggregated key-\nvalue stores. In 2020 USENIX Annual Technical Con-\nference (USENIX ATC 20) , pages 33\u201348, 2020.\n[12] Anuj Kalia, Michael Kaminsky, and David G Andersen.\nUsing rdma ef\ufb01ciently for key-value services. In Pro-\nceedings of the 2014 ACM Conference on SIGCOMM ,\npages 295\u2013306, 2014.\n[13] Aleksandar Dragojevi \u00b4c, Dushyanth Narayanan, Miguel\nCastro, and Orion Hodson. Farm: Fast remote mem-\nory. In 11th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI 14) , pages 401\u2013414,\n2014.\n[14] Aleksandar Dragojevi \u00b4c, Dushyanth Narayanan, Ed-\nmund B Nightingale, Matthew Renzelmann, Alex\nShamis, Anirudh Badam, and Miguel Castro. No com-\npromises: Distributed transactions with consistency,\navailability, and performance. In Proceedings of the\n25th symposium on operating systems principles , pages\n54\u201370, 2015.\n[15] Jacob Nelson, Brandon Holt, Brandon Myers, Preston\nBriggs, Luis Ceze, Simon Kahan, and Mark Oskin.\nLatency-tolerant software distributed shared memory. In\n2015 USENIX Annual Technical Conference (USENIX\nATC 15) , pages 291\u2013305, 2015.\n[16] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K Aguil-\nera, and Adam Belay. Aifm: High-performance,\napplication-integrated far memory.", "start_char_idx": 680222, "end_char_idx": 683423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2db55bd0-ecf2-4264-9a95-5433d1420db1": {"__data__": {"id_": "2db55bd0-ecf2-4264-9a95-5433d1420db1", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3d3940f-f5cc-41fa-ad2e-2b604ddae237", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "4583fc3ea6c6d5b2c08de14a8c3b44d5ff05dd6c4c8a67fd4a85c7de5d2fdc42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a717654-feba-4956-8460-66b0e3671c4c", "node_type": "1", "metadata": {}, "hash": "0f3015af1e6044e3de6d0c0d5fb7542065a44e08c53db99bd86f07ba46cff319", "class_name": "RelatedNodeInfo"}}, "text": "No com-\npromises: Distributed transactions with consistency,\navailability, and performance. In Proceedings of the\n25th symposium on operating systems principles , pages\n54\u201370, 2015.\n[15] Jacob Nelson, Brandon Holt, Brandon Myers, Preston\nBriggs, Luis Ceze, Simon Kahan, and Mark Oskin.\nLatency-tolerant software distributed shared memory. In\n2015 USENIX Annual Technical Conference (USENIX\nATC 15) , pages 291\u2013305, 2015.\n[16] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K Aguil-\nera, and Adam Belay. Aifm: High-performance,\napplication-integrated far memory. In 14th USENIX\nSymposium on Operating Systems Design and Imple-\nmentation (OSDI 20) , pages 315\u2013332, 2020.\n[17] Gen-Z Consortium. Gen-Z Final Speci\ufb01cations. https:\n//genzconsortium.org/specifications/ .\n[18] CXL Consortium. Compute Express Link Speci\ufb01cation\nRevision 2.0. https://www.computeexpresslink.\norg/download-the-specification .\nUSENIX Association 2022 USENIX Annual Technical Conference    293[19] CXL Consortium. Compute Express\nLink\u2122 2.0 White Paper. https://www.\ncomputeexpresslink.org/_files/ugd/0c1418_\n14c5283e7f3e40f9b2955c7d0f60bebe.pdf .\n[20] Navin Shenoy. A Milestone in Moving Data. https:\n//newsroom.intel.com/editorials/milestone-\nmoving-data .\n[21] Debendra Das Sharma. CXL: Coherency, Memory,\nand I/O Semantics on PCIe Infrastructure. https:\n//www.electronicdesign.com/technologies/\nembedded-revolution/article/21162617/cxl-\ncoherency-memory-and-io-semantics-on-pcie-\ninfrastructure .\n[22] Patrick Kennedy. Compute Express Link\nor CXL What it is and Examples. https:\n//www.servethehome.com/compute-express-\nlink-or-cxl-what-it-is-and-examples/ .\n[23] Hari Subramoni, Ping Lai, Miao Luo, and Dha-\nbaleswar K Panda. Rdma over ethernet\u2014a preliminary\nstudy. In 2009 IEEE International Conference on Clus-\nter Computing and Workshops , pages 1\u20139. IEEE, 2009.\n[24] Philip Werner Frey and Gustavo Alonso. Minimizing the\nhidden cost of rdma. In 2009 29th IEEE International\nConference on Distributed Computing Systems , pages\n553\u2013560. IEEE, 2009.\n[25] Intel. Persistent Memory Developer Kit Version v1.11.0.\nhttps://pmem.io/ .\n[26] Intel. NVDIMM Namespace Speci\ufb01cation.\nhttps://pmem.io/documents/NVDIMM_Namespace_\nSpec.pdf .\n[27] UEFI Forum, Inc. Advanced Con\ufb01guration and Power\nInterface (ACPI) Speci\ufb01cation Version 6.4. https://\nuefi.org/specs/ACPI/6.4/ , 2021.\n[28] Linaro. The devicetree speci\ufb01cation. https://www.\ndevicetree.org/ .[29] Mellanox. Mellanox ConnectX-3 FDR (56Gbps) In\ufb01ni-\nBand VPI. https://www.mellanox.com/related-\ndocs/prod_adapter_cards/PB_ConnectX3_VPI_\nCard_Dell.pdf .\n[30] Xilinx. Mellanox OpenFabrics Enterprise Distri-\nbution. https://www.mellanox.com/products/\ninfiniband-drivers/linux/mlnx_ofed .\n[31] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael\nShi, Jianyu Huang, Narayanan Sundaraman, Jongsoo\nPark, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,\nAlisson G. Azzolini, Dmytro Dzhulgakov, Andrey\nMallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman\nKrishnamoorthi, Ansha Yu, Volodymyr Kondratenko,\nStephanie Pereira, Xianjie Chen, Wenlin Chen, Vi-\njay Rao, Bill Jia, Liang Xiong, and Misha Smelyan-\nskiy.", "start_char_idx": 682868, "end_char_idx": 685974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a717654-feba-4956-8460-66b0e3671c4c": {"__data__": {"id_": "3a717654-feba-4956-8460-66b0e3671c4c", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2db55bd0-ecf2-4264-9a95-5433d1420db1", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "c201088d7a5f760663754f538e29642d1c3c2b7b5e4cd133aeb0117bda835897", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d7b6c9b-6212-46cc-bedc-e2e648660b41", "node_type": "1", "metadata": {}, "hash": "2a571ed372fac44de86752bebd5ed24239ca2db782c5a23c6e2347670631380d", "class_name": "RelatedNodeInfo"}}, "text": "[30] Xilinx. Mellanox OpenFabrics Enterprise Distri-\nbution. https://www.mellanox.com/products/\ninfiniband-drivers/linux/mlnx_ofed .\n[31] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael\nShi, Jianyu Huang, Narayanan Sundaraman, Jongsoo\nPark, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,\nAlisson G. Azzolini, Dmytro Dzhulgakov, Andrey\nMallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman\nKrishnamoorthi, Ansha Yu, Volodymyr Kondratenko,\nStephanie Pereira, Xianjie Chen, Wenlin Chen, Vi-\njay Rao, Bill Jia, Liang Xiong, and Misha Smelyan-\nskiy. Deep learning recommendation model for per-\nsonalization and recommendation systems. CoRR ,\nabs/1906.00091, 2019.\n[32] Michael Luby. A simple parallel algorithm for the max-\nimal independent set problem. SIAM journal on com-\nputing , 15(4):1036\u20131053, 1986.\n[33] Alan Bundy and Lincoln Wallen. Breadth-\ufb01rst search.\nInCatalogue of arti\ufb01cial intelligence tools , pages 13\u201313.\nSpringer, 1984.\n[34] Fan Chung and Linyuan Lu. Connected components in\nrandom graphs with given expected degree sequences.\nAnnals of combinatorics , 6(2):125\u2013145, 2002.\n[35] Ulrik Brandes. A faster algorithm for betweenness cen-\ntrality. Journal of mathematical sociology , 25(2):163\u2013\n177, 2001.\n[36] Julian Shun and Guy E Blelloch. Ligra: a lightweight\ngraph processing framework for shared memory. In\nProceedings of the 18th ACM SIGPLAN symposium on\nPrinciples and practice of parallel programming , pages\n135\u2013146, 2013.\n294    2022 USENIX Annual Technical Conference USENIX AssociationElastic Use of Far Memory for In-Memory Database Management\nSystems\nDonghun Lee\nMinseon Ahn\nJungmin Kim\ndong.hun.lee@sap.com\nminseon.ahn@sap.com\njimmy.kim@sap.com\nSAP Labs Korea\nSeoul, South KoreaDaniel Booss\nDaniel Ritter\nOliver Rebholz\ndaniel.booss@sap.com\ndaniel.ritter@sap.com\noliver.rebholz@sap.com\nSAP SE\nWalldorf, Germany\nThomas Willhalm\nthomas.willhalm@intel.com\nIntel Deutschland GmbH\nFeldkirchen, GermanySuprasad Mutalik Desai\nNavneet Singh\nsuprasad.desai@intel.com\nnavneet.singh@intel.com\nIntel Technology India Pvt. Ltd.\nBengaluru, India\nABSTRACT\nThe separation and independent scalability of compute and mem-\nory is one of the crucial aspects for modern in-memory database\nsystems (IMDBMSs) in the cloud. The new, cache-coherent memory\ninterconnect Compute Express Link (CXL) promises elastic mem-\nory capacity through memory pooling. In this work, we adapt the\nwell-known IMDBMS, SAP HANA, for memory pools by features\nof table data placement and operational heap memory allocation\non far memory, and study the impact of the limited bandwidth and\nhigher latency of CXL. Our results show negligible performance\ndegradation for TPC-C. For the analytical workloads of TPC-H,\na notable impact on query processing is observed due to the lim-\nited bandwidth and long latency of our early CXL implementation.\nHowever, our emulation shows it would be acceptably smaller with\nthe improved CXL memory devices.\nCCS CONCEPTS\n\u2022Hardware !Emerging interfaces ;\u2022Information systems\n!Database management system engines .\nKEYWORDS\nCXL, Far memory, Memory pool, In-Memory Database, DBMS,\nDatabase Management Systems\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro \ufffft or commercial advantage and that copies bear this notice and the full citation\non the \uffffrst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted.", "start_char_idx": 685433, "end_char_idx": 688955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d7b6c9b-6212-46cc-bedc-e2e648660b41": {"__data__": {"id_": "3d7b6c9b-6212-46cc-bedc-e2e648660b41", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a717654-feba-4956-8460-66b0e3671c4c", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "4b09d90a021a8bfa54e7cd2dbb15ed341100b2c9a8a0594aaf8fa0bfc562c4b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ab5b89e-8fbf-45e8-9e38-03c77736fee1", "node_type": "1", "metadata": {}, "hash": "afbf73597ed13060f79c7e2fd1113c91bcd7f4814cddf91811c8ac0904d510c0", "class_name": "RelatedNodeInfo"}}, "text": "For the analytical workloads of TPC-H,\na notable impact on query processing is observed due to the lim-\nited bandwidth and long latency of our early CXL implementation.\nHowever, our emulation shows it would be acceptably smaller with\nthe improved CXL memory devices.\nCCS CONCEPTS\n\u2022Hardware !Emerging interfaces ;\u2022Information systems\n!Database management system engines .\nKEYWORDS\nCXL, Far memory, Memory pool, In-Memory Database, DBMS,\nDatabase Management Systems\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro \ufffft or commercial advantage and that copies bear this notice and the full citation\non the \uffffrst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speci \uffffc permission\nand/or a fee. Request permissions from permissions@acm.org.\nDaMoN \u201923, June 18\u201323, 2023, Seattle, WA, USA\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0191-7/23/06. . . $15.00\nhttps://doi.org/10.1145/3592980.3595311ACM Reference Format:\nDonghun Lee, Minseon Ahn, Jungmin Kim, Daniel Booss, Daniel Ritter,\nOliver Rebholz, Thomas Willhalm, Suprasad Mutalik Desai, and Navneet\nSingh. 2023. Elastic Use of Far Memory for In-Memory Database Manage-\nment Systems. In 19th International Workshop on Data Management on New\nHardware (DaMoN \u201923), June 18\u201323, 2023, Seattle, WA, USA. ACM, New York,\nNY, USA, 9 pages. https://doi.org/10.1145/3592980.3595311\n1 INTRODUCTION\nThe cloud computing industry is constantly evolving, and the needs\nof the elasticity in computing power and memory capacity are be-\ncoming increasingly important. The new memory interconnect\nCompute Express Link (CXL) [ 7] promises large, cache-coherent\nmemory capacity and independent, elastic scalability. CXL enables\ndynamic memory expansion, a disaggregated memory system, and\na memory pool, all of which could help to meet the growing de-\nmand for the \uffffexible system architecture of cloud-based in-memory\ndatabase management systems (IMDBMSs) and reduce their total\ncost ownership (TCO). CXL-attached memory capacity can be more\nelastically scaled, based on the actual demand, which makes it easier\nto manage and optimize the resource utilization.\nIn our previous work [ 2], we laid out the vision of a CXL memory\ndevice for elastic memory expansion of IMDBMSs, and gave initial\nperformance results for standard database benchmarks. We showed\nthat we can easily expand the memory space with nearly no or\nsmall amount of performance impact. Additionally in [ 11], we sug-\ngested the classi \uffffcation of the di \ufffferent memory distances, based on\ntheir actual physical distance but taking the transport layer into ac-\ncount, and introduced general CXL use cases for the disaggregated\nmemory system (e. g., shared memory pooling, multi-socket). Due\nto various latencies and di \ufffferent raw materials of CXL memory\ndevices, there would be diverse con \uffffgurations of tiered memory\nsystems for IMDBMSs. As in [ 1,3,11], we use the term far memory\nfor memory devices connected via network \u2013 without mediation\nby a local processor and better availability due to separate fault\n35\nDaMoN \u201923, June 18\u201323, 2023, Sea \uffffle, WA, USA Donghun Lee, et al.\ndomains \u2013 like CXL 1.1 or CXL-switch based on CXL 2.0, which\ncould consist of DRAM or PMEM to support direct load or store\ncommands within a reasonable duration.\nIn this work, we introduce and describe use cases for enterprise-\nscale IMDBMS that leverage a memory pool, namely use idle com-\npute,near-zero downtime upgrade ,query burst ,distributed compute ,\nandfailover .", "start_char_idx": 688083, "end_char_idx": 691897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ab5b89e-8fbf-45e8-9e38-03c77736fee1": {"__data__": {"id_": "9ab5b89e-8fbf-45e8-9e38-03c77736fee1", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d7b6c9b-6212-46cc-bedc-e2e648660b41", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6f7ab93d430be223fb34686ea3fe070a321b38e628afcb5466f19588139c44e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d25a75be-4b62-4959-869b-399102cf98ab", "node_type": "1", "metadata": {}, "hash": "b4430930f8cc60d8ca08d2a18e721a1d77e5e634b85431a73c4ae93eef3da992", "class_name": "RelatedNodeInfo"}}, "text": "As in [ 1,3,11], we use the term far memory\nfor memory devices connected via network \u2013 without mediation\nby a local processor and better availability due to separate fault\n35\nDaMoN \u201923, June 18\u201323, 2023, Sea \uffffle, WA, USA Donghun Lee, et al.\ndomains \u2013 like CXL 1.1 or CXL-switch based on CXL 2.0, which\ncould consist of DRAM or PMEM to support direct load or store\ncommands within a reasonable duration.\nIn this work, we introduce and describe use cases for enterprise-\nscale IMDBMS that leverage a memory pool, namely use idle com-\npute,near-zero downtime upgrade ,query burst ,distributed compute ,\nandfailover . We extend the well-known SAP HANA IMDBMS by\ntwo main features that enable dynamic memory allocation using\nfar memory for those use cases: (i) moving the main storage of any\ntable data to far memory and (ii) allocating the heap memory of SAP\nHANA execution engine (HEX) [ 25] to far memory. As far memory\nusually has a higher latency and a limited bandwidth compared to\nlocal memory, understanding the performance impact of the two\nfeatures (cf. (i), (ii)) on an IMDBMS is crucial. Hence, we evaluate\ntheir performance impact on far memory using TPC-C and TPC-H.\nThe main contributions of this work are:\n\u2022Speci \uffffcation of CXL use cases for enterprise-scale IMDBMSs,\n\u2022Adaptation of a SAP HANA with far memory by features of\n(i) table data placement and (ii) operational heap memory\nallocation, and\n\u2022Extensive evaluation of the adapted IMDBMS for transac-\ntional and analytical workloads.\nOur experimental analysis shows that transactional workloads like\nTPC-C have nearly no performance degradation, despite longer\nlatencies of far memory. Analytical workloads like TPC-H su \uffffer\nfrom performance degradation due to the limited bandwidth and\nlong latency of our early CXL implementation, but it would be\nacceptable with the improved CXL memory devices.\nThe remainder of this paper is organized as follows: Section 2 in-\ntroduces CXL and explains the elasticity requirement in cloud-based\nIMDBMSs. In Section 3, we introduce use cases of far memory for\nIMDBMSs. The implementation details of far memory are presented\nin Section 4. In Section 5, we conduct the performance evaluation\nand discuss important insights. Section 6 discusses related works\nand Section 7 concludes the paper.\n2 BACKGROUND\nIn this section, we introduce the CXL memory interconnect and\ndescribe requirements on the elasticity of cloud IMDBMSs.\n2.1 Compute Express Link\nCompute Express Link (CXL) is an open standard to support cache-\ncoherent interconnect between a variety of devices [ 7]. After the\nintroduction of CXL in 2019, the standard has evolved and contin-\nues to be enhanced. CXL 1.1 de \uffffnes the protocol for three major\ndevice types: accelerators with cache-only (type 1), cache with at-\ntached memory (type 2), and memory expansion (type 3). CXL 2.0\nexpands the speci \uffffcation \u2013 among other capabilities \u2013 to memory\npools using CXL switches on a device level. CXL 3.0 introduces\nfabric capabilities and management, improved memory sharing and\npooling with dynamic capacity capability, enhanced coherency, and\npeer-to-peer communication.\nSince the market of CXL devices is emerging, several vendors\nhave announced products using CXL. For example, Samsung [ 18]and SK Hynix [ 19] introduce CXL DDR5 modules, AsteraLabs [ 5] an-\nnounced a CXL memory accelerator, and Montage technology [ 28]\nwill o \uffffer a CXL memory expander controller.\n2.2 Elasticity of Cloud IMDBMSs\nOne of the key requirements of modern IMDBMSs in the cloud\nis \u201celasticity\u201d. Current public cloud infrastructures are o \uffffering\nvirtually unlimited computing and storage resources on demand.\nWith the emergence of disaggregated memory technology such as\nCXL, Gen-Z [ 8], OpenCAPI [ 9] (latter two subsumed by CXL), and\nCCIX [ 6], elastic memory capacity in cloud infrastructure will be\navailable in the near future.", "start_char_idx": 691284, "end_char_idx": 695157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d25a75be-4b62-4959-869b-399102cf98ab": {"__data__": {"id_": "d25a75be-4b62-4959-869b-399102cf98ab", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ab5b89e-8fbf-45e8-9e38-03c77736fee1", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "77650bd925801568b338e7e5e1e0bad73a0c023b6d410d1af117ca0205926aed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "631b6cfa-6712-4278-bf12-1133d1ae1807", "node_type": "1", "metadata": {}, "hash": "7df10580e57ea5b2ef244b4c0f17510eb1b47b197c53c790f7215cc58c328e42", "class_name": "RelatedNodeInfo"}}, "text": "For example, Samsung [ 18]and SK Hynix [ 19] introduce CXL DDR5 modules, AsteraLabs [ 5] an-\nnounced a CXL memory accelerator, and Montage technology [ 28]\nwill o \uffffer a CXL memory expander controller.\n2.2 Elasticity of Cloud IMDBMSs\nOne of the key requirements of modern IMDBMSs in the cloud\nis \u201celasticity\u201d. Current public cloud infrastructures are o \uffffering\nvirtually unlimited computing and storage resources on demand.\nWith the emergence of disaggregated memory technology such as\nCXL, Gen-Z [ 8], OpenCAPI [ 9] (latter two subsumed by CXL), and\nCCIX [ 6], elastic memory capacity in cloud infrastructure will be\navailable in the near future.\nHowever, traditional database architectures are designed for\n\uffffxed amounts of resources, which would make it di \uffffcult for them\nto leverage such elasticity o \uffffered in public cloud infrastructures.\nTo ful \uffffll the requirements of elastic computing capabilities in cloud\nenvironments, SAP HANA provides independently, scalable work-\ners, called Elastic Compute Node (ECN). ECNs are similar to SAP\nHANA scale-out instances1, except that ECNs have an ephemeral\npersistence, which does not need to be persisted via NSE [ 25], and is\nexcluded from services like HANA backup and recovery. Therefore,\nECNs do not store normal database tables, while temporary tables or\nreplica tables are populated to ECN instances. With temporary per-\nsistence, ECN instances can be easily added and removed depending\non the incoming workload state or the customer\u2019s demands.\nSince CXL version 2.0, the memory pooling capability is enabled.\nThis feature goes beyond simple memory expansion and allows\nfor the dynamic scaling of memory capacity for multiple systems,\nsupporting a more \uffffexible architecture for systems that use it and\nresulting in the reduced TCO. Memory pooling is particularly use-\nful for IMDBMSs, where large amounts of memory are needed\nto achieve optimal performance. Thus, our use cases will center\naround memory pooling for IMDBs and we need to evaluate their\nimpacts on performance for various workloads.\n2.3 Data Storage in SAP HANA\nSAP HANA is one of the leading Hybrid Transaction / Analytical\nProcessing (HTAP)2supporting OLTP and OLAP workloads in a\nsingle system, which simpli \uffffes the overall system architecture with\nlow TCO [ 22\u201324]. It uses a compressed, columnar storage layout\nfor fast-read accesses and a low memory footprint. The columnar\ndata is stored in the read-optimized main storage and maintains\na separate delta storage for optimized writes [ 10,26]. The delta\nstorage is periodically merged with the main storage [17].\n3 FAR MEMORY USE CASES FOR SAP HANA\nMemory devices are some of the most expensive parts in the mod-\nern computer architectures. Separating memory from compute and\nmemory pooling are promising trends to reduce the memory con-\nsumption of an IMDBMS like SAP HANA. That is achieved by\nallocating only required memory at a particular time and enabling\n1SAP HANA: https://help.sap.com/docs/SAP_HANA_PLATFORM/\n6b94445c94ae495c83a19646e7c3fd56/a165e192ba374c2a8b17566f89fe8419.html\n2Translytical data platforms: https://news.sap.com/2022/12/translytical-data-\nplatforms-forrester-wave-sap-a-leader/\n36Elastic Use of Far Memory for In-Memory Database Management Systems DaMoN \u201923, June 18\u201323, 2023, Sea \uffffle, WA, USA\ndynamic memory scaling. Subsequently, we present use cases lever-\naging memory pooling for IMDBs, before we describe how far\nmemory can be practically used in SAP HANA.\n3.1 Use Cases\nWe argue that the following far memory use cases, leveraging CXL\nmemory pooling, are bene \uffffcial for IMDBs.\n3.1.1 Use idle compute. For workloads running on a SAP HANA\nprocess that utilizes only a portion of the available compute ca-\npability but does not have enough memory to run another SAP\nHANA process, hosts with free CPU resources can execute another\nSAP HANA process using the memory from the memory pool.", "start_char_idx": 694512, "end_char_idx": 698399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "631b6cfa-6712-4278-bf12-1133d1ae1807": {"__data__": {"id_": "631b6cfa-6712-4278-bf12-1133d1ae1807", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d25a75be-4b62-4959-869b-399102cf98ab", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "cd72a73cf06cfd1389915139accac1171c803b24d3ec4f66650c22b2b1559657", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "759a38f6-2a24-499d-92ce-a8cac70e106a", "node_type": "1", "metadata": {}, "hash": "25df431a461ef1166b451b59160720b85303bbab9e055897a9996ac6a613aa57", "class_name": "RelatedNodeInfo"}}, "text": "Subsequently, we present use cases lever-\naging memory pooling for IMDBs, before we describe how far\nmemory can be practically used in SAP HANA.\n3.1 Use Cases\nWe argue that the following far memory use cases, leveraging CXL\nmemory pooling, are bene \uffffcial for IMDBs.\n3.1.1 Use idle compute. For workloads running on a SAP HANA\nprocess that utilizes only a portion of the available compute ca-\npability but does not have enough memory to run another SAP\nHANA process, hosts with free CPU resources can execute another\nSAP HANA process using the memory from the memory pool. The\nmemory of the process is bound to the memory exposed by the\nmemory pool, providing simpli \uffffed life cycle management, since all\nmemory allocated in both local and far by the process gets freed\nwhen it terminates. In a variant of this case, only the main storage\nof a SAP HANA process could be put into far memory, which keeps\ncolumnar tables on the pool using the same techniques used for\nPMem [ 4]. This case could be most promising for SAP HANA ECNs\nto leverage idle compute capacity.\n3.1.2 Near-zero downtime upgrade. SAP provides near-zero down-\ntime upgrades of enterprise software systems such as SAP S/4HANA.\nFor the upgrade, a \u201cbridging\u201d database subsystem is created as a\npart of the existing system where a view is generated for each SAP-\nspeci \uffffc or customer-speci \uffffc table in the bridge\u2019s schema. Business\nusers are transparently transferred and reconnected to the bridge\nsubsystem without any interruption while upgrading the original\nsubsystem. Meanwhile, the bridge subsystem imitates the origi-\nnal system and contains all data of the production system that\nusers need to continue their work. Even though not the entire\ndatabase is cloned but only selected tables based on the changes\nto be performed by the maintenance event, we need additional\nmemory capacity for the bridge subsystem. Therefore, a memory\npool could be utilized, supporting a tighter sizing on the existing\nsystem, regardless of the additional memory requirements during\nthe upgrade.\n3.1.3 \uffffery Burst. Occasionally, more memory is needed during\nphases with more memory-demanding queries. For example, there\nare increasing memory demands for big queries during the \uffffscal\nclosing process at the end of every month, quarter, or year. In ad-\ndition to the memory space for main storage and delta storage,\nSAP HANA needs temporal, operational memory space for keep-\ning intermediate results and data structures for query processing.\nSAP HANA\u2019s execution engine (HEX) [ 25] has a clean memory\nmanagement design and allows for specifying whether memory\nallocations are done in local or far memory. Using this feature, we\nallocate the operational memory in a memory pool. It contributes\nto tight memory sizing and elastic memory capability so that we\ncan allocate more memory dynamically according to the increased\nmemory demands.\n3.1.4 Distributed compute. When additional compute nodes are\nadded for SAP HANA\u2019s system replication, it may be advantageous\nto move table data into a memory pool to avoid unnecessary data\nreplication. This allows data to be read from multiple SAP HANAhosts, with synchronized, on-demand updates of the main or delta\nstorage. This usage has the potential to reduce the overall memory\ncosts and enable e \uffffcient memory usage in SAP HANA. However,\nthis requires CXL 3.0, since it needs synchronization among multi-\nple hosts. As an interim solution, a dedicated, independent memory\nspace for multiple SAP HANA instances in a memory pool could\nbe utilized, which is supported by CXL 2.0. For that, we gather all\nthe table data for each SAP HANA instance in a memory pool and\nallocate only the operational memory in the local memory in each\nhost. This approach contributes to better global memory utilization\nand lower TCO.\n3.1.5 Failover. During the planned or unplanned downtime of a\nSAP HANA instance, a takeover will be performed by another SAP\nHANA instance. If the main storage of columnar tables is allocated\nin the memory pool, the SAP HANA processes, which want to\ntake over, can attach to that memory pool and read the tables after\na failover without loading the data into memory. This scenario\nwill signi \uffffcantly reduce the required failover time. The ownership\nchange of the memory space in a memory pool among multiple\nhosts, however, requires CXL 3.0.", "start_char_idx": 697828, "end_char_idx": 702171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "759a38f6-2a24-499d-92ce-a8cac70e106a": {"__data__": {"id_": "759a38f6-2a24-499d-92ce-a8cac70e106a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "631b6cfa-6712-4278-bf12-1133d1ae1807", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "305f24129a83504c979e86305cf63b4758bc84d1999ebad825286e4bee5eecff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "832ae1c3-1f84-4b41-a045-332652b591b2", "node_type": "1", "metadata": {}, "hash": "3857d2ba7f20cbf28d57cf12f7e69951e3b16404ae36259307cd5cfd92725970", "class_name": "RelatedNodeInfo"}}, "text": "For that, we gather all\nthe table data for each SAP HANA instance in a memory pool and\nallocate only the operational memory in the local memory in each\nhost. This approach contributes to better global memory utilization\nand lower TCO.\n3.1.5 Failover. During the planned or unplanned downtime of a\nSAP HANA instance, a takeover will be performed by another SAP\nHANA instance. If the main storage of columnar tables is allocated\nin the memory pool, the SAP HANA processes, which want to\ntake over, can attach to that memory pool and read the tables after\na failover without loading the data into memory. This scenario\nwill signi \uffffcantly reduce the required failover time. The ownership\nchange of the memory space in a memory pool among multiple\nhosts, however, requires CXL 3.0.\n3.2 Far Memory Usages in SAP HANA\nSimilar to [ 11], we use the term \u201cfar memory\u201d as the memory\ndevice connected via CXL 1.1 or CXL-switch based on CXL 2.0. To\nsupport the use cases of the CXL memory pool mentioned in the\nprevious section, SAP HANA provides some features to enable (i)\ndynamic data movement and (ii) memory allocation to far memory.\nSubsequently, we discuss more details of the two features supported\nby SAP HANA.\n3.2.1 Moving main storage. CXL memory pooling can be used by\nmoving the main storage to far memory. We use SAP HANA\u2019s per-\nsistent memory feature [ 4] to put the main storage of the columnar\ntables on far memory with fairly clear semantics on the lifecycle of\nthe main storage. The far memory could be con \uffffgured as fsdax or\ntempfs. Any speci \uffffed table data can be moved to far memory. This\nfeature can be used in the scenarios addressed in Sects. 3.1.1, 3.1.2,\n3.1.4 and 3.1.5.\n3.2.2 Allocating HEX heap memory. CXL memory pool can be used\nby allocating HEX heap memory in the far memory. As mentioned\nin Section 3.1.3, the new SAP HANA execution engine (HEX) [ 25]\ncan allocate the heap memory required to process a given query\nto near or far memory according to the con \uffffgured memory only\nNUMA node information in SAP HANA. Among the several mem-\nory allocators within SAP HANA, the HEX heap memory is the\nmost dominant part of dynamic memory allocation during its query\nprocessing. This feature can be used in the scenarios addressed in\nSects. 3.1.1 and 3.1.3.\nUsually, far memory has a relatively longer latency than local\nmemory \u2013 probably about three to four times longer than a single\nNUMA hop in modern computer architecture. Therefore, it is impor-\ntant to understand the performance characteristics when we apply\nthe far memory usages in SAP HANA. The accesses to the main\nstorage are usually sequential and mostly done by data-intensive op-\nerations where L2 prefetching scheme may hide the longer latency\nof the far memory. Rather, the accesses to the HEX heap memory\n37DaMoN \u201923, June 18\u201323, 2023, Sea \uffffle, WA, USA Donghun Lee, et al.\nFigure 1: System setup and far memory CXL prototype\nFigure 2: Implementation of the CXL memory pool\nare mainly random and frequently done during compute-intensive\noperations. We will show the performance e \uffffect of the two usages\nagainst the two di \ufffferent workloads (OLTP and OLAP) in Sect. 5.\n4 CXL PROTOTYPE IMPLEMENTATION\nIn this section, we describe the CXL prototype that we integrated\ninto SAP HANA as far memory, give more details on its internal\nimplementation and discuss limitations.\n4.1 CXL Prototype Overview\nFigure 1 shows the system setup used to study the performance\nimpact on IMDBMSs. The experimental memory pool is realized\nusing an Intel \u00aeAgilex \u00aeAGI027 FPGA card which supports PCIe\nGen5 x16 CXL connectivity. This FPGA card is connected to the\nsecond socket by directly inserting it in a PCIe slot at Socket 1 with16 PCIe lanes and connected to Socket 0 through two MCIO 8x\ncables.\n4.2 Memory Pool Implementation Details\nFigure 2 grants a more detailed view into the implementation of our\nCXL memory pool on the FPGA card.", "start_char_idx": 701395, "end_char_idx": 705294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "832ae1c3-1f84-4b41-a045-332652b591b2": {"__data__": {"id_": "832ae1c3-1f84-4b41-a045-332652b591b2", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "759a38f6-2a24-499d-92ce-a8cac70e106a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6389ef5fb710bb62629754f7c56af89257f29f91abc7ff8b1dc14f776febd027", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "577be5fd-a953-43b9-917d-43dc8e883e94", "node_type": "1", "metadata": {}, "hash": "06b4df0186aaace48b91855c2d1d53f2e31dd08383436cc85e226fe1eb756f51", "class_name": "RelatedNodeInfo"}}, "text": "5.\n4 CXL PROTOTYPE IMPLEMENTATION\nIn this section, we describe the CXL prototype that we integrated\ninto SAP HANA as far memory, give more details on its internal\nimplementation and discuss limitations.\n4.1 CXL Prototype Overview\nFigure 1 shows the system setup used to study the performance\nimpact on IMDBMSs. The experimental memory pool is realized\nusing an Intel \u00aeAgilex \u00aeAGI027 FPGA card which supports PCIe\nGen5 x16 CXL connectivity. This FPGA card is connected to the\nsecond socket by directly inserting it in a PCIe slot at Socket 1 with16 PCIe lanes and connected to Socket 0 through two MCIO 8x\ncables.\n4.2 Memory Pool Implementation Details\nFigure 2 grants a more detailed view into the implementation of our\nCXL memory pool on the FPGA card. The R-Tile Intel FPGA IP for\nCXL implements the CXL link and transaction layer management\nfunctions needed to build FPGA-based CXL 1.1/2.0 compliant Type\n1, Type 2, and Type 3 endpoint designs. The CXL IP solution consists\nof a combination of a protocol Soft IP in the FPGA main fabric die\npaired with the Hard IP companion R-Tile. R-Tile manages all CXL\nlink functions. It connects to a CPU host with a PCIe Gen5x16\ninterface and supports up to a 64GB/s theoretical bandwidth. The\ndevice enumerates as a CXL endpoint.\nThe Soft IP manages transaction layer functions. For Type 3,\nthe CXL.mem transaction layer receives CXL.mem requests issued\nfrom a CPU host, and generates host-managed device memory\n(HDM) requests to an HDM subsystem. CXL.io transaction layer\nreceives CXL.io requests, either con \uffffguration space requests or\nmemory space requests issued from a CPU host, and forwards them\nto the targeted control and status registers. The User Streaming\nInterface allows a user design to implement additional custom\nCXL.io features.\nThe FPGA card hosts 2x8GB on-board DDR4 1333 MHz memory,\nwhich is exposed to the host as memory. It is important to note\nthat in this experimental setup, the FPGA allows accesses to exactly\nthe same amount of memory over each of the two CXL links. In\nother words, the same far memory can be exposed to two di \ufffferent\nNUMA nodes of the same size without address overlap between\nthem. The FPGA does not create a single cache-coherent domain,\nand thus cannot support coherency between these two NUMA\nnodes assigned to the far memory. Therefore, applications must\nmanage accesses to the shared memory.\n4.3 Known Limitations of Prototype\nIt is worth noting that the bandwidth is limited due to the current\nimplementation of our CXL prototype, and not inherent to CXL.\nOptions to increase the prototype\u2019s bandwidth would include:\n\u2022Use a faster speed FPGA which supports DDR4 3200 Mbps\nor DDR5 5600 Mbps.\n\u2022Increase the number of slices for the CXL IP.\n\u2022Increase the number of independent DDR channels within\nthe device from one to four channels, for example.\nApart from improving the performance of the device itself, multiple\ndevices can be interleaved to aggregate their bandwidth.\n5 PERFORMANCE EVALUATION\nIn this section, we evaluate SAP HANA on CXL far memory imple-\nmentation for common transactional and analytical workloads.\n5.1 System Con \uffffguration\nOur experimental setup is based on Intel 4thgeneration Xeon pro-\ncessor code-named Sapphire Rapids . The system is equipped with\ntwo processors with a base frequency of 2.0GHz and 56 cores each\n38Elastic Use of Far Memory for In-Memory Database Management Systems DaMoN \u201923, June 18\u201323, 2023, Sea \uffffle, WA, USA\n(a) I-con \uffffg\n(b) Y-con \uffffg\nFigure 3: Far memory setup variants\nFigure 4: Far memory con \uffffgurations in I-con \uffffg\nplus Hyper-Threading. Each processor has 128GBin 8 channels,\none16 GB DDR5 4800 MHz DIMM per channel.\nTo see the performance e \uffffect on various use cases, we use\ntwo di \ufffferent far memory setups, I-con \uffffg and Y-con \uffffg, as shown\nin Fig. 3. First, I-con \uffffg enumerates the entire far memory as NUMA\nnode 2 of size 16GBdirectly connected to Socket 0 as shown\nin Fig. 3a.", "start_char_idx": 704541, "end_char_idx": 708464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "577be5fd-a953-43b9-917d-43dc8e883e94": {"__data__": {"id_": "577be5fd-a953-43b9-917d-43dc8e883e94", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "832ae1c3-1f84-4b41-a045-332652b591b2", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f6780b753a7aa5c014da7f501f1e35574a7c37e1cabfd1dfd1e2b4e70330cb94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "032e2300-232a-489d-ae43-b273d57c7d8b", "node_type": "1", "metadata": {}, "hash": "71b3b81a4aa7f37d557960ebf6f942d06d417ffe72a8229dd5995707132ca387", "class_name": "RelatedNodeInfo"}}, "text": "Each processor has 128GBin 8 channels,\none16 GB DDR5 4800 MHz DIMM per channel.\nTo see the performance e \uffffect on various use cases, we use\ntwo di \ufffferent far memory setups, I-con \uffffg and Y-con \uffffg, as shown\nin Fig. 3. First, I-con \uffffg enumerates the entire far memory as NUMA\nnode 2 of size 16GBdirectly connected to Socket 0 as shown\nin Fig. 3a. We enable the far memory connection only from Socket\n0to see the performance impact on far memory use cases. In this\nsetup, the CXL connection to Socket 1 is disabled and the whole\nfar memory in FPGA is accessed through NUMA node 2. Since the\nfar memory is directly connected to the \uffffrst socket, CPU a \uffffnity of\nSAP HANA is set to CPU0 onSocket 0 to avoid any NUMA e \uffffect.To study the performance e \uffffects on two di \ufffferent usage types\nof far memory, i. e., (i) moving the main storage and (ii) allocating\nHEX heap memory, we divide the far memory into two parts, (A)\nfsdax for the main storage and (B) memory only NUMA node for\nthe HEX heap memory. Then, we evaluate four di \ufffferent con \uffffgura-\ntions by combining these two usage types. The \uffffrst con \uffffguration,\nBoth DRAM , has both the main storage and the HEX heap memory\nin DRAM, which is our baseline in this experiment. The second\ncon\uffffguration, Main FAR , puts the main storage in the far memory\nand the HEX heap memory remaining in DRAM while the third\ncon\uffffguration, HEX FAR , keeps the main storage in DRAM and\nputs the HEX heap memory in the far memory. The last con \uffffgura-\ntion, Both FAR , has both in the far memory. Figure 4 shows the\nlocation of the main storage and the HEX heap memory for each\ncon\uffffguration in this far memory setup.\nThe second far memory setup is Y-con \uffffg where the far memory\nis connected to both CPUs as shown in Fig. 3b. We enable both far\nmemory connections to con \uffffrm the e \uffffect of mixed workloads from\nmultiple sockets to a memory pool. We believe it is a general form\nof the memory pool with CXL 2.0. To avoid any memory access\nviolation, we partition the whole far memory into two halves. The\n\uffffrst half is set to NUMA node 2 directly connected to Socket 0 ,\nand the second half is set to NUMA node 3 directly connected to\nSocket 1 .\n5.2 Experiment on I-Con \uffffg\nOur experiments are conducted using a transactional / OLTP and\nan analytical / OLAP benchmark. First, we use TPC-C [ 29] with 100\nwarehouses for OLTP workloads, with an fsdax size of 12GB, to\nsupport intermediate delta merges, while the memory only NUMA\nnode is set to 4GBfor the HEX heap memory. Second, we use TPC-\nH[30] for evaluating OLAP workloads. Due to the limited capacity\nof the far memory, we use TPC-H SF10, which requires 4.7GB\nfor the main storage. When testing TPC-H, the FPGA memory is\ndivided into 8GBforfsdax and8GBfor memory only NUMA node\nbecause TPC-H requires more HEX heap memory during query\nprocessing. Subsequently, we report our experimental results for\ntransactional and analytical query processing on IMDBs on CXL\nfar memory.\n5.2.1 OLTP (OnLine Transaction Processing). We use TPC-C for\nOLTP workload evaluation with 100 warehouses. To populate the\ntransactions in this benchmark test, the number of client threads\nper process is set to 28. Figure 5 shows the performance and CXL\ntra\uffffc in all four con \uffffgurations of TPC-C. The performance is nor-\nmalized to the value at the 16-process con \uffffguration of the baseline\nBoth DRAM . As shown in Fig. 5, TPC-C has no signi \uffffcant per-\nformance degradation even with the smaller available bandwidth\nand the longer latency of our far memory implementation.", "start_char_idx": 708122, "end_char_idx": 711635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "032e2300-232a-489d-ae43-b273d57c7d8b": {"__data__": {"id_": "032e2300-232a-489d-ae43-b273d57c7d8b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "577be5fd-a953-43b9-917d-43dc8e883e94", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "89361cea810329ed84ba8c95a6564e32815be28db88c4bc3430096dff2b6d230", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af0a0fe2-c935-4b04-aa8d-ea4b96785156", "node_type": "1", "metadata": {}, "hash": "e029b345363f0a817fd8da15c2176ea3b3cf89ee56462c6eb1578fbe017b4f94", "class_name": "RelatedNodeInfo"}}, "text": "Subsequently, we report our experimental results for\ntransactional and analytical query processing on IMDBs on CXL\nfar memory.\n5.2.1 OLTP (OnLine Transaction Processing). We use TPC-C for\nOLTP workload evaluation with 100 warehouses. To populate the\ntransactions in this benchmark test, the number of client threads\nper process is set to 28. Figure 5 shows the performance and CXL\ntra\uffffc in all four con \uffffgurations of TPC-C. The performance is nor-\nmalized to the value at the 16-process con \uffffguration of the baseline\nBoth DRAM . As shown in Fig. 5, TPC-C has no signi \uffffcant per-\nformance degradation even with the smaller available bandwidth\nand the longer latency of our far memory implementation. Since\n8-process con \uffffguration, CPU utilization of SAP HANA shows up\nto 80-90% at maximum. The total number of client connections to\nSAP HANA is 224 with 8 client processes, which is more than twice\nthe total number of available logical cores in a single socket. Even\nwith the 16-process con \uffffguration, where SAP HANA shows nearly\nfull CPU utilization, there is no performance degradation when\nusing far memory.\n39DaMoN \u201923, June 18\u201323, 2023, Sea \uffffle, WA, USA Donghun Lee, et al.\n2 4 8 160.20.40.60.81.01.2\n# Client ProcessesNormalized ThroughputBoth DRAM Main FAR HEX FAR Both FAR\n(a) Normalized Throughput2 4 8 160.00.51.01.52.0\n# Client ProcessesTra\uffffc (GB/s)\n(b) CXL Total Tra \uffffc\n2 4 8 160.00.51.01.52.0\n# Client ProcessesTra\uffffc (GB/s)\n(c) CXL Write Tra \uffffc2 4 8 160.00.51.01.52.0\n# Client ProcessesTra\uffffc (GB/s)\n(d) CXL Read Tra \uffffc\nFigure 5: TPC-C throughput and CXL tra \uffffc\n1 2 4 80.20.40.60.81.0\n# StreamsNormalized ThroughputBoth DRAM Main FAR HEX FAR Both FAR\n(a) Normalized Throughput1 2 4 8051015\n# StreamsTra\uffffc (GB/s)\n(b) CXL Total Tra \uffffc\n1 2 4 8051015\n# StreamsTra\uffffc (GB/s)\n(c) CXL Write Tra \uffffc1 2 4 8051015\n# StreamsTra\uffffc (GB/s)\n(d) CXL Read Tra \uffffc\nFigure 6: TPC-H throughput and CXL tra \uffffc\nOur analysis of CXL tra \uffffc measured with customized Intel \u00ae\nPerformance Counter Monitor [ 21] shows that there is a meaningful\namount of data access to the far memory, but the total amount of\nCXL tra \uffffc is much lower than the maximum bandwidth of our farmemory implementation. HEX FAR always has a similar amount of\nCXL write tra \uffffc to that of CXL read tra \uffffc. It is also observed that\nMain FAR has CXL write tra \uffffc only in the con \uffffgurations with 8\nor more processes. This is due to a delta merge after collecting a\n40Elastic Use of Far Memory for In-Memory Database Management Systems DaMoN \u201923, June 18\u201323, 2023, Sea \uffffle, WA, USA\nBoth DRAM Main FAR HEX FAR Both FAR0.00.51.01.52.01.001.121.261.58Normalized Time\n(a) Normalized execution time at 1 streamBoth DRAM Main FAR HEX FAR Both FAR0.00.20.40.60.81.01.00\n0.600.53\n0.311.000.93 0.960.88Normalized ThroughputMeasured CXL Emulation\n(b) Normalized throughput at 8 streams\nFigure 7: TPC-H performance comparison\ncertain amount of deltas in the host DRAM. Since CXL tra \uffffc is not\nsaturated in TPC-C, the amount of CXL tra \uffffc inBoth FAR appears\nas the sum of those in both Main FAR andHEX FAR .", "start_char_idx": 710937, "end_char_idx": 713981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af0a0fe2-c935-4b04-aa8d-ea4b96785156": {"__data__": {"id_": "af0a0fe2-c935-4b04-aa8d-ea4b96785156", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "032e2300-232a-489d-ae43-b273d57c7d8b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ad378b9530ecb7e47d36dab12965199946a750da3cc9bb9af7ddb445511b4c47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc406730-5119-44c8-9dbd-9cc762f8de33", "node_type": "1", "metadata": {}, "hash": "9b4d1bf06cb943f745e1fe02dab7d94098bb52497b0909a6fe45b10f0ecc73aa", "class_name": "RelatedNodeInfo"}}, "text": "Since CXL tra \uffffc is not\nsaturated in TPC-C, the amount of CXL tra \uffffc inBoth FAR appears\nas the sum of those in both Main FAR andHEX FAR . Our analysis\nwith Intel \u00aeVTune [ 13] shows that the TPC-C workload causes\nmany lock con \ufffficts and a high synchronization overhead within\nSAP HANA. We conclude that this synchronization overhead is\nhiding the longer latency of far memory. Thus, it is not sensitive to\nthe bandwidth or latency of the far memory.\n5.2.2 OLAP (OnLine Analytical Processing). We use TPC-H with\nSF10 for OLAP workload evaluation due to the limited capacity in\nthe CXL memory device. Figure 6 shows the overall performance\nand CXL tra \uffffc in all four con \uffffgurations of TPC-H. The perfor-\nmance is normalized to the value at the 8-stream con \uffffguration of\nthe baseline Both DRAM . We observe that Main FAR has no CXL\nwrite tra \uffffc, but it has much more CXL read tra \uffffc than HEX FAR .\nThus, it has a larger amount of CXL total tra \uffffc than HEX FAR .\nHowever, Main FAR shows less performance degradation than\nHEX FAR . Our analysis with Intel \u00aeVTune reports that Main FAR\nhas higher memory bandwidth bound than HEX FAR . When mov-\ning the main storage, the access pattern is mainly sequential [ 14]\nand the prefetching scheme e \uffffciently exploits the bandwidth. Thus,\nMain FAR is a\uffffected by limited bandwidth of far memory. Prefetch-\ning contributes to the better performance even with the much larger\namount of CXL tra \uffffc. When allocating HEX heap memory, the\naccess pattern is rather random and, thus is a \uffffected by long latency\nof far memory. Note that Both FAR has smaller CXL write tra \uffffc\nthan HEX FAR because the CXL total tra \uffffc is already saturated.\nFigure 7 shows the performance comparison for TPC-H. In the\nexecution time with a single stream in Fig. 7a, the performance\ndegradation compared to the baseline Both DRAM is measured\n12.0% for Main FAR , 26.1% for HEX FAR , and 57.9% for Both FAR .\nIn the throughput with 8 streams, it is measured 39.9% in Main FAR ,\n47.4% in HEX FAR , and 69.2% in Both FAR as shown in Fig. 7b. The\noverall performance in TPC-H is bound by the limited bandwidth\nof our far memory prototype. Since most of the internal bandwidth\nin our far memory prototype is consumed from the 4-stream con \uffffg-\nuration, the performance becomes saturated in all con \uffffgurations\nusing the far memory as shown in Fig. 6b. Thus, TPC-H results\nshow a higher performance degradation because of the bandwidth\nlimitation in our implementation.To project the performance impact on future CXL devices, we\nperform CXL emulation using the memory in the remote NUMA\nnode (cf. Socket1 in Fig. 1), assuming that the access latency to\nthe memory in the remote NUMA node through UPI is similar to\nthe latency of future CXL devices with improved bandwidth. Ac-\ncording to the performance test in CXL emulation, performance\ndegradation can be reduced up to 7.2% in Main FAR , up to 4.1% in\nHEX FAR , and up to 11.9% in Both FAR as shown in Fig. 7b. The\nresults demonstrate that the performance degradation by analyt-\nical workload can be signi \uffffcantly reduced when the latency and\nbandwidth of far memory are improved. Similar results have been\nobserved in our previous work [2].\n5.3 Experiment on Y-Con \uffffg\nIn this experiment, we use only TPC-H SF10 because OLAP work-\nloads are more sensitive to the bandwidth limitation in far memory\ncon\uffffgurations. To generate mixed workloads for Y-con \uffffg, we in-\nstall two separate SAP HANA instances and assign each instance to\na di\ufffferent socket to allow them to run independently and concur-\nrently without any interference except the far memory. The \uffffrst\ninstance running at Socket 0 is set to HEX FAR by allocating the\nHEX heap memory to NUMA node 2.", "start_char_idx": 713844, "end_char_idx": 717547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc406730-5119-44c8-9dbd-9cc762f8de33": {"__data__": {"id_": "bc406730-5119-44c8-9dbd-9cc762f8de33", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af0a0fe2-c935-4b04-aa8d-ea4b96785156", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f265783dd05686ce1b25a3907c7711c38c58755f633c7edb206ec7a77f31f4fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1969d826-5009-4b11-b1c7-2ed8b5f381e5", "node_type": "1", "metadata": {}, "hash": "f391d5c3bebd2075d91d3fb53352e73772ff9d4d1076da8bf97153ddec6609db", "class_name": "RelatedNodeInfo"}}, "text": "7b. The\nresults demonstrate that the performance degradation by analyt-\nical workload can be signi \uffffcantly reduced when the latency and\nbandwidth of far memory are improved. Similar results have been\nobserved in our previous work [2].\n5.3 Experiment on Y-Con \uffffg\nIn this experiment, we use only TPC-H SF10 because OLAP work-\nloads are more sensitive to the bandwidth limitation in far memory\ncon\uffffgurations. To generate mixed workloads for Y-con \uffffg, we in-\nstall two separate SAP HANA instances and assign each instance to\na di\ufffferent socket to allow them to run independently and concur-\nrently without any interference except the far memory. The \uffffrst\ninstance running at Socket 0 is set to HEX FAR by allocating the\nHEX heap memory to NUMA node 2. The second instance running\natSocket 1 is set to Main FAR by moving the main storage to\nfsdax in NUMA node 3.\nFigure 8 shows performance throughput of TPC-H in Y-con \uffffg\nnormalized to the previously measured performance value at the\n8-stream con \uffffguration of the baseline Both DRAM . To see the\nperformance impact on the bandwidth competition in the far mem-\nory, we measure the performance for each instance in both cases\n(1) when they are running separate ly consuming the far memory\nbandwidth exclusively and (2) when they are running concur-\nrent ly competing each other for the limited bandwidth of the far\nmemory. This \uffffgure shows that HEX Far has less performance\ndegradation than Main FAR when comparing the performance\nfrom the separate execution to the concurrent execution for each\ninstance. In the 8-stream con \uffffguration, HEX Far has 42.1% per-\nformance degradation, while Main FAR has 54.8%. When they are\nrunning concurrently, it is observed that HEX Far always has bet-\nter performance than Main FAR . We believe that the performance\ndegradation comes from the CXL tra \uffffc reduction in the concurrent\nexecution because Main FAR has a larger amount of CXL tra \uffffc\n41DaMoN \u201923, June 18\u201323, 2023, Sea \uffffle, WA, USA Donghun Lee, et al.\n1 2 4 80.00.20.40.6\n# StreamsNormalized ThroughputHEX FAR (separate) Main FAR (separate)\nHEX FAR (concurrent) Main FAR (concurrent)\nFigure 8: TPC-H throughput on Y-con \uffffg\nthan HEX Far . Therefore, it is concluded that the prefetch bene \ufffft\ninMain FAR is reduced when workloads are competing for the\nlimited available internal bandwidth in the CXL memory pool.\n5.4 Discussion\nOur experimental results show that the far memory usages have\ndi\ufffferent impacts depending on workloads. TPC-C has negligible\nperformance degradation in any far memory usage because of the\nsmall amount of data accesses and high synchronization overhead,\nwhich hides longer latency of far memory. Unlike TPC-C, TPC-H\nhas a certain amount of performance degradation because of the\nlimited available bandwidth and long latency when accessing the\nlarge amount of data. When the main storage is moved to the far\nmemory, bandwidth dominantly a \uffffects the performance degrada-\ntion because prefetching for the sequential accesses on the main\nstorage saturates the available bandwidth. When the HEX heap\nmemory is allocated in the far memory, latency dominantly a \uffffects\nthe performance degradation because of the random accesses on\nthe HEX heap memory. Our analysis with CXL emulation showed\nthat the performance degradation would be acceptably smaller if\nthe future CXL memory pool has better bandwidth and latency.\nFrom the experimental results on Y-con \uffffg, we observed that\nprefetch bene \ufffft is reduced when the available internal bandwidth\nin CXL memory pool is limited. Therefore, we conclude that CXL\nmemory pool must have enough internal bandwidth to fully exploit\nthe prefetch bene \ufffft of sequential accesses.\n6 RELATED WORK\nMemory disaggregation has been active in recent years, and several\nproposals have been made to address the challenges of managing\nand accessing remote memory. To enable high-performance mem-\nory disaggregation in modern data centers, CXL technology has\nbeen widely adopted. Gouk et al. [ 12] have introduced DirectCXL,\na system that allows for direct access to remote memory and uti-\nlizes intelligent caching and prefetching techniques to achieve high\nperformance.", "start_char_idx": 716801, "end_char_idx": 720950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1969d826-5009-4b11-b1c7-2ed8b5f381e5": {"__data__": {"id_": "1969d826-5009-4b11-b1c7-2ed8b5f381e5", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc406730-5119-44c8-9dbd-9cc762f8de33", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "117706da1257fff62409992d1118e77e27eb5ff963d333429527d1e7698f3555", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bff5e5c2-5657-4588-8f9e-1e56a8582a25", "node_type": "1", "metadata": {}, "hash": "99728dcbc6ba274515905e95f3b369dffb1669cf84b211d94119059356139ca8", "class_name": "RelatedNodeInfo"}}, "text": "From the experimental results on Y-con \uffffg, we observed that\nprefetch bene \ufffft is reduced when the available internal bandwidth\nin CXL memory pool is limited. Therefore, we conclude that CXL\nmemory pool must have enough internal bandwidth to fully exploit\nthe prefetch bene \ufffft of sequential accesses.\n6 RELATED WORK\nMemory disaggregation has been active in recent years, and several\nproposals have been made to address the challenges of managing\nand accessing remote memory. To enable high-performance mem-\nory disaggregation in modern data centers, CXL technology has\nbeen widely adopted. Gouk et al. [ 12] have introduced DirectCXL,\na system that allows for direct access to remote memory and uti-\nlizes intelligent caching and prefetching techniques to achieve high\nperformance. Similarly, Maruf et al. [ 16] have presented a memory\nplacement strategy that utilizes CXL to create a tiered memory\nhierarchy that applications can seamlessly access.CXL for memory expansion and acceleration is one of the hot\nresearch topics. Sim et al. [ 27] have proposed a computational CXL-\nmemory solution that expands memory capacity and improves\nperformance for memory-intensive applications. Park et al. [ 20]\nhave developed a CXL memory expander that scales memory per-\nformance and capacity in modern data centers.\nFurthermore, the performance and e \uffffciency of CXL-enabled\nmemory pooling systems have been investigated. Li et al. [ 15]\nhave introduced a CXL-based memory pooling system that can be\nemployed in cloud platforms to enhance memory utilization and\nperformance. Additionally, Yang et al. [ 31] have presented a CXL-\nenabled hybrid memory pool that combines DRAM and PMEM\nresources to improve memory performance.\n7 CONCLUSION\nThe separation of compute and memory, and their composition\nthrough far memory based on CXL enables new use cases for\nIMDBMSs, shows a path from current resource over-provisioning,\nand reduces their TCO. In this paper, we introduced and discussed\npromising far memory use cases for IMDBMSs and two fundamen-\ntal adaptations (i. e., table data placement and operational heap\nmemory allocation on far memory) to SAP HANA, and conducted\na performance evaluation of these two adaptations using the far\nmemory implemented on an early FPGA-based CXL prototype.\nThe results of our experimental evaluation show that transac-\ntional workloads like TPC-C have nearly no performance degrada-\ntion in any far memory usage because of the small amount of data\naccesses and high synchronization overhead. However, analytical\nworkloads, such as TPC-H, causing large amount of data accesses,\nhave a certain amount of performance degradation because of the\nlimited available bandwidth and long latency of far memory. As\nshown in our CXL emulation results using remote NUMA mem-\nory, we expect an acceptable performance degradation in TPC-H\nwith improved bandwidth and latency of far memory in the future.\nTherefore, we conclude that the far memory can allow elasticity in\nIMDBMSs with the improved CXL memory devices.\nIn future work, we will further develop solutions on how to\navoid performance impacts, especially for data intensive analytic\nworkloads due to the limited bandwidth and long latency of far\nmemory. Moreover, we will collect and specify more far memory\nuse cases for memory pools to fully utilize the elastic memory\ncapacity for cloud IMDBMSs.\nREFERENCES\n[1]Marcos K. Aguilera, Kimberly Keeton, Stanko Novakovic, and Sharad Singhal.\n2019. Designing Far Memory Data Structures: Think Outside the Box. In HotOS .\nACM, 120\u2013126. https://doi.org/10.1145/3317550.3321433\n[2]Minseon Ahn, Andrew Chang, Donghun Lee, Jongmin Gim, Jungmin Kim, Jaemin\nJung, Oliver Rebholz, Vincent Pham, Krishna T. Malladi, and Yang-Seok Ki. 2022.\nEnabling CXL Memory Expansion for In-Memory Database Management Systems.\nInDaMoN . ACM, 8:1\u20138:5. https://doi.org/10.1145/3533737.3535090\n[3]Emmanuel Amaro, Christopher Branner-Augmon, Zhihong Luo, Amy Ouster-\nhout, Marcos K. Aguilera, Aurojit Panda, Sylvia Ratnasamy, and Scott Shenker.\n2020. Can far memory improve job throughput?. In EuroSys .", "start_char_idx": 720171, "end_char_idx": 724262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bff5e5c2-5657-4588-8f9e-1e56a8582a25": {"__data__": {"id_": "bff5e5c2-5657-4588-8f9e-1e56a8582a25", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1969d826-5009-4b11-b1c7-2ed8b5f381e5", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f40281d2e73da20e6ebf9fab09a6f89335c119d44b1e7f889cbdb7389e1a971f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc4da986-c189-4e74-8983-3fcc37d2d951", "node_type": "1", "metadata": {}, "hash": "c6d7bf8b4050b0f10f4bf451a4b9a0231176ff9aefe20a96437ffb27f4a55035", "class_name": "RelatedNodeInfo"}}, "text": "ACM, 120\u2013126. https://doi.org/10.1145/3317550.3321433\n[2]Minseon Ahn, Andrew Chang, Donghun Lee, Jongmin Gim, Jungmin Kim, Jaemin\nJung, Oliver Rebholz, Vincent Pham, Krishna T. Malladi, and Yang-Seok Ki. 2022.\nEnabling CXL Memory Expansion for In-Memory Database Management Systems.\nInDaMoN . ACM, 8:1\u20138:5. https://doi.org/10.1145/3533737.3535090\n[3]Emmanuel Amaro, Christopher Branner-Augmon, Zhihong Luo, Amy Ouster-\nhout, Marcos K. Aguilera, Aurojit Panda, Sylvia Ratnasamy, and Scott Shenker.\n2020. Can far memory improve job throughput?. In EuroSys . ACM, 14:1\u201314:16.\nhttps://doi.org/10.1145/3342195.3387522\n[4]Mihnea Andrei, Christian Lemke, G\u00fcnter Radestock, et al .2017. SAP HANA\nadoption of non-volatile memory. Proceedings of the VLDB Endowment 10, 12\n(2017), 1754\u20131765. https://doi.org/10.14778/3137765.3137780\n[5]AsteraLabs. 2022. CXL Memory Accelerators . https://www.asteralabs.com/\nproducts/cxl-memory-platform/\n[6]CCIX Consortium. 2017. CCIX . https://www.ccixconsortium.com/\n42Elastic Use of Far Memory for In-Memory Database Management Systems DaMoN \u201923, June 18\u201323, 2023, Sea \uffffle, WA, USA\n[7]Compute Express Link Consortium. 2019. CXL. https://www.computeexpresslink.\norg/\n[8]Gen-Z Consortium. 2016. Gen-Z . https://genzconsortium.org/\n[9]OpenCAPI Consortium. 2014. OpenCAPI . https://opencapi.org/\n[10] Franz Faerber, Alfons Kemper, Per-\u00c5ke Larson, Justin J. Levandoski, Thomas\nNeumann, and Andrew Pavlo. 2017. Main Memory Database Systems. Found.\nTrends Databases 8, 1-2 (2017), 1\u2013130. https://doi.org/10.1561/1900000058\n[11] Andreas Geyer, Daniel Ritter, Dong Hun Lee, Minseon Ahn, Johannes Pietrzyk,\nAlexander Krause, Dirk Habich, and Wolfgang Lehner. 2023. Working with\nDisaggregated Systems. What are the Challenges and Opportunities of RDMA\nand CXL?. In BTW (LNI, Vol. P-331) . Gesellschaft f\u00fcr Informatik e.V., 751\u2013755.\nhttps://doi.org/10.18420/BTW2023-47\n[12] Donghyun Gouk, Sangwon Lee, Miryeong Kwon, and Myoungsoo Jung. 2022.\nDirect access, High-Performance memory disaggregation with DirectCXL. In\nUSENIX ATC . 287\u2013294. https://www.usenix.org/conference/atc22/presentation/\ngouk\n[13] Intel\u00ae. 2021. Intel\u00aeVTune \u2122Pro\uffffler. https://www.intel.com/content/www/us/\nen/developer/tools/oneapi/vtune-pro \uffffler.html\n[14] Robert Lasch, Thomas Legler, Norman May, Bernhard Scheirle, and Kai-Uwe\nSattler. 2022. Cost modelling for optimal data placement in heterogeneous\nmain memory. Proceedings of the VLDB Endowment 15, 11 (2022), 2867\u20132880.\nhttps://www.vldb.org/pvldb/vol15/p2867-lasch.pdf\n[15] Huaicheng Li, Daniel S Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko\nNovakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, et al .2023.\nPond: Cxl-based memory pooling systems for cloud platforms. In ASPLOS . ACM,\n574\u2013587.", "start_char_idx": 723707, "end_char_idx": 726466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc4da986-c189-4e74-8983-3fcc37d2d951": {"__data__": {"id_": "dc4da986-c189-4e74-8983-3fcc37d2d951", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bff5e5c2-5657-4588-8f9e-1e56a8582a25", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "009bc9b672e4ce179510c9bf4058e1368559fa20ba7fab4ad3fd15af676f5f99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1939abc-3068-4062-8999-db7f605a6f6f", "node_type": "1", "metadata": {}, "hash": "c4c836be6a14cda854ed2b8180ae42cb2c896b7aa16033685dbf1c85b67de21e", "class_name": "RelatedNodeInfo"}}, "text": "2022. Cost modelling for optimal data placement in heterogeneous\nmain memory. Proceedings of the VLDB Endowment 15, 11 (2022), 2867\u20132880.\nhttps://www.vldb.org/pvldb/vol15/p2867-lasch.pdf\n[15] Huaicheng Li, Daniel S Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko\nNovakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, et al .2023.\nPond: Cxl-based memory pooling systems for cloud platforms. In ASPLOS . ACM,\n574\u2013587. https://doi.org/10.1145/3575693.3578835\n[16] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agarwal,\nPallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit O. Kanaujia,\nand Prakash Chauhan. 2022. TPP: Transparent Page Placement for CXL-Enabled\nTiered Memory. CoRR abs/2206.02878 (2022). https://doi.org/10.48550/arXiv.\n2206.02878 arXiv:2206.02878\n[17] J. McGlone, P. Palazzari, and J. B. Leclere. 2018. Accelerating Key In-memory\nDatabase Functionality with FPGA Technology. In ReConFig . 1\u20138. https://doi.\norg/10.1109/RECONFIG.2018.8641722\n[18] Samsung Newsroom. 2022. Samsung Electronics Introduces Industry\u2019s First 512GB\nCXL Memory Module . https://news.samsung.com/global/samsung-electronics-\nintroduces-industrys- \uffffrst-512gb-cxl-memory-module\n[19] SK Hynix Newsroom. 2022. SK hynix Develops DDR5 DRAM CXLTM Memory\nto Expand the CXL Memory Ecosystem . https://news.skhynix.com/sk-hynix-\ndevelops-ddr5-dram-cxltm-memory-to-expand-the-cxl-memory-ecosystem/\n[20] S. J. Park, H. Kim, K.-S. Kim, J. So, J. Ahn, W.-J. Lee, D. Kim, Y.-J. Kim, J. Seok, J.-G.\nLee, H.-Y. Ryu, C. Y. Lee, J. Prout, K.-C. Ryoo, S.-J. Han, M.-K. Kook, J. S. Choi,J. Gim, Y. S. Ki, S. Ryu, C. Park, D.-G. Lee, J. Cho, H. Song, and J. Y. Lee. 2022.\nScaling of Memory Performance and Capacity with CXL Memory Expander. In\n2022 IEEE HCS . IEEE, 1\u201327. https://doi.org/10.1109/HCS55958.2022.9895633\n[21] Intel\u00aePCM. 2023. Intel\u00aePerformance Counter Monitor . https://github.com/intel/\npcm\n[22] Hasso Plattner. 2009. A common database approach for OLTP and OLAP using\nan in-memory column database. In SIGMOD . ACM, 1\u20132. https://doi.org/10.1145/\n1559845.1559846\n[23] Hasso Plattner. 2014. The Impact of Columnar In-memory Databases on Enter-\nprise Systems: Implications of Eliminating Transaction-maintained Aggregates.\nProc. VLDB Endow. 7, 13 (Aug. 2014), 1722\u20131729. https://doi.org/10.14778/2733004.\n2733074\n[24] Iraklis Psaroudakis, Florian Wolf, Norman May, Thomas Neumann, Alexander\nBoehm, Anastasia Ailamaki, and Kai-Uwe Sattler. 2015. Scaling Up Mixed Work-\nloads: A Battle of Data Freshness, Flexibility, and Scheduling. Performance Char-\nacterization And Benchmarking: Traditional To Big Data 8904 (2015), 16. 97\u2013112.", "start_char_idx": 726029, "end_char_idx": 728689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1939abc-3068-4062-8999-db7f605a6f6f": {"__data__": {"id_": "f1939abc-3068-4062-8999-db7f605a6f6f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc4da986-c189-4e74-8983-3fcc37d2d951", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8022489fe8e2db16317e920309e210638f010c0567da8b5408f1979ef390ba26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e610229-8ee9-42e2-972d-3d37f2fd19f4", "node_type": "1", "metadata": {}, "hash": "e2467b4c913a2005b882a03aa92ff4dbf6fef9dd825e2c345ef2598d96d8dd79", "class_name": "RelatedNodeInfo"}}, "text": "2014. The Impact of Columnar In-memory Databases on Enter-\nprise Systems: Implications of Eliminating Transaction-maintained Aggregates.\nProc. VLDB Endow. 7, 13 (Aug. 2014), 1722\u20131729. https://doi.org/10.14778/2733004.\n2733074\n[24] Iraklis Psaroudakis, Florian Wolf, Norman May, Thomas Neumann, Alexander\nBoehm, Anastasia Ailamaki, and Kai-Uwe Sattler. 2015. Scaling Up Mixed Work-\nloads: A Battle of Data Freshness, Flexibility, and Scheduling. Performance Char-\nacterization And Benchmarking: Traditional To Big Data 8904 (2015), 16. 97\u2013112.\nhttps://doi.org/10.1007/978-3-319-15350-6_7\n[25] Reza Sherkat, Colin Florendo, Mihnea Andrei, Rolando Blanco, Adrian Dragusanu,\nAmit Pathak, Pushkar Khadilkar, Neeraj Kulkarni, Christian Lemke, Sebastian\nSeifert, Sarika Iyer, Sasikanth Gottapu, Robert Schulze, Chaitanya Gottipati,\nNirvik Basak, Yanhong Wang, Vivek Kandiyanallur, Santosh Pendap, Dheren\nGala, Rajesh Almeida, and Prasanta Ghosh. 2019. Native Store Extension for SAP\nHANA. Proc. VLDB Endow. 12, 12 (2019), 2047\u20132058. https://doi.org/10.14778/\n3352063.3352123\n[26] Vishal Sikka, Franz F\u00e4rber, Wolfgang Lehner, Sang Kyun Cha, Thomas Peh,\nand Christof Bornh\u00f6vd. 2012. E \uffffcient Transaction Processing in SAP HANA\nDatabase: The End of a Column Store Myth. In SIGMOD . ACM, 731\u2013742. https:\n//doi.org/10.1145/2213836.2213946\n[27] Joonseop Sim, Soohong Ahn, Taeyoung Ahn, Seungyong Lee, Myunghyun Rhee,\nJooyoung Kim, Kwangsik Shin, Donguk Moon, Euiseok Kim, and Kyoung Park.\n2023. Computational CXL-Memory Solution for Accelerating Memory-Intensive\nApplications. IEEE Comput. Archit. Lett. 22, 1 (2023), 5\u20138. https://doi.org/10.1109/\nLCA.2022.3226482\n[28] Montage Technology. 2022. Montage Technology Delivers the World\u2019s First CXL \u2122\nMemory eXpander Controller . https://www.montage-tech.com/Press_Releases/\n20220506\n[29] TPC-C. 2023. TPC-C . https://www.tpc.org/tpcc/\n[30] TPC-H. 2023. TPC-H . https://www.tpc.org/tpch/\n[31] Qirui Yang, Runyu Jin, Bridget Davis, Devasena Inupakutika, and Ming Zhao.\n2022. Performance Evaluation on CXL-enabled Hybrid Memory Pool. In NAS.\nIEEE, 1\u20135. https://doi.org/10.1109/NAS55553.2022.9925356\n43A Case for CXL-Centric Server Processors\nAlbert Cho\u0003Anish Saxena\u0003Moinuddin Qureshi Alexandros Daglis\nGeorgia Institute of Technology\nAbstract\nThe memory system is a major performance determinant for\nserver processors. Ever-growing core counts and datasets de-\nmand higher bandwidth and capacity as well as lower latency\nfrom the memory system. To keep up with growing demands,\nDDR\u2014the dominant processor interface to memory over the\npast two decades\u2014has offered higher bandwidth with every\ngeneration. However, because each parallel DDR interface\nrequires a large number of on-chip pins, the processor\u2019s mem-\nory bandwidth is ultimately restrained by its pin-count, which\nis a scarce resource. With limited bandwidth, multiple memory\nrequests typically contend for each memory channel, resulting\nin signi\ufb01cant queuing delays that often overshadow DRAM\u2019s\nservice time and degrade performance.\nWe present COAXIAL, a server design that overcomes mem-\nory bandwidth limitations by replacing all DDR interfaces to\nthe processor with the more pin-ef\ufb01cient CXL interface.", "start_char_idx": 728146, "end_char_idx": 731344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e610229-8ee9-42e2-972d-3d37f2fd19f4": {"__data__": {"id_": "9e610229-8ee9-42e2-972d-3d37f2fd19f4", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1939abc-3068-4062-8999-db7f605a6f6f", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "4e219b0c89fde0c279548d30bb3d19dcdfdc135c1ba21ebeb900c6e611fd2d59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12725f5a-45d6-44fd-82dd-2b0c016ac228", "node_type": "1", "metadata": {}, "hash": "23be529e73bc99db6a8ab257d5f8f7f1a9fd39303243a65c84c0e45b30e49297", "class_name": "RelatedNodeInfo"}}, "text": "Ever-growing core counts and datasets de-\nmand higher bandwidth and capacity as well as lower latency\nfrom the memory system. To keep up with growing demands,\nDDR\u2014the dominant processor interface to memory over the\npast two decades\u2014has offered higher bandwidth with every\ngeneration. However, because each parallel DDR interface\nrequires a large number of on-chip pins, the processor\u2019s mem-\nory bandwidth is ultimately restrained by its pin-count, which\nis a scarce resource. With limited bandwidth, multiple memory\nrequests typically contend for each memory channel, resulting\nin signi\ufb01cant queuing delays that often overshadow DRAM\u2019s\nservice time and degrade performance.\nWe present COAXIAL, a server design that overcomes mem-\nory bandwidth limitations by replacing all DDR interfaces to\nthe processor with the more pin-ef\ufb01cient CXL interface. The\nwidespread adoption and industrial momentum of CXL makes\nsuch a transition possible, offering 4\u0002higher bandwidth per\npin compared to DDR at a modest latency overhead. We\ndemonstrate that, for a broad range of workloads, CXL\u2019s la-\ntency premium is more than offset by its higher bandwidth. As\nCOAXIALdistributes memory requests across more channels,\nit drastically reduces queuing delays and thereby both the\naverage value and variance of memory access latency. Our\nevaluation with a variety of workloads shows that COAXIAL\nimproves the performance of manycore throughput-oriented\nservers by 1:52\u0002on average and by up to 3\u0002.\n1. Introduction\nMulticore processor architectures have been delivering per-\nformance gains despite the end of Dennard scaling and the\nslowdown of Moore\u2019s law in the past two decades. At the same\ntime, as the data consumed by processors is increasing expo-\nnentially, technological breakthroughs have enabled higher-\ncapacity memory with new media like non-volatile RAM or\nvia remote memory access over fast networks (e.g., RDMA). A\ncommon technological trade-off with higher-capacity memory\nis signi\ufb01cantly inferior memory access latency and bandwidth\ncompared to the DDR-based main memory. As a result, servers\ncontinue to predominantly rely on DDR-attached memory for\nperformance while optionally retaining a slower memory tier\nlike NVRAM or remote DRAM for capacity expansion.\n\u0003Equal contribution.The emerging Compute Express Link (CXL) standard\nbridges the performance gap between low-bandwidth, high-\ncapacity memory and DDR-based main memory. By attaching\nDRAM modules over the widely deployed high-bandwidth\nPCI Express (PCIe) bus, CXL vastly improves memory capac-\nity and bandwidth, while retaining DDR-like characteristics at\na modest access latency overhead. Consequently, there has re-\ncently been much interest in architecting CXL-based memory\nsystems that enable memory pooling and capacity expansion\n[3, 16, 25, 33].\nCXL owes its high bandwidth to the underlying PCIe-based\nserial interface, which currently delivers about 4\u0002higher band-\nwidth per processor pin compared to the parallel DDR inter-\nface, with technological roadmaps projecting this gap to grow\nfurther. Hence, by repurposing the processor\u2019s DDR-allocated\npins to CXL, it is possible to quadruple the available memory\nbandwidth. However, the higher bandwidth comes at the cost\nof memory access latency overhead, expected to be as low as\n25\u201330ns [ 9,43], although higher in initial implementations\nand systems that multiplex CXL memory devices across mul-\ntiple processors. Low access latency is a key requirement\nfor high-performance memory, which is why CXL\u2019s latency\noverhead has biased the research so far to treat the technology\nexclusively as a memory expansion technique rather than a\nreplacement of local DDR-attached memory.\nHowever, we observe that the overall memory access la-\ntency in a loaded system is dominated by the queuing delay\nat the memory controller, which arbitrates access to the DDR\nchannel. Modern servers feature between 4 and 12 cores per\nmemory channel, resulting in contention and signi\ufb01cant queu-\ning delays even before a request can be launched over the\nmemory bus. Mitigating these queuing delays by provisioning\nmore memory channels requires more processor pins and die\narea, which are scarce resources. Given rigid pin constraints,\nCXL\u2019s bandwidth-per-pin advantage can unlock signi\ufb01cant\nbandwidth and performance gains by rethinking memory sys-\ntems to be CXL-centric rather than DDR-centric.\nIn this paper, we make the key observation that the band-\nwidth boost attainable with CXL drastically reduces memory\naccess queuing delays, which largely dictate the effective ac-\ncess latency of loaded memory systems.", "start_char_idx": 730498, "end_char_idx": 735100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12725f5a-45d6-44fd-82dd-2b0c016ac228": {"__data__": {"id_": "12725f5a-45d6-44fd-82dd-2b0c016ac228", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e610229-8ee9-42e2-972d-3d37f2fd19f4", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "64aacd7bd02aa23d0ad3746c5ceb5f1a38039a76d20405bc7f4b7d72b5a362af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db80f62f-cfa7-4b6a-b805-062abacd7c14", "node_type": "1", "metadata": {}, "hash": "d448fb0825bc2342e18eb2fba1e01ebd2e46fcbbe9dceedafcaa65980d0ce6a9", "class_name": "RelatedNodeInfo"}}, "text": "However, we observe that the overall memory access la-\ntency in a loaded system is dominated by the queuing delay\nat the memory controller, which arbitrates access to the DDR\nchannel. Modern servers feature between 4 and 12 cores per\nmemory channel, resulting in contention and signi\ufb01cant queu-\ning delays even before a request can be launched over the\nmemory bus. Mitigating these queuing delays by provisioning\nmore memory channels requires more processor pins and die\narea, which are scarce resources. Given rigid pin constraints,\nCXL\u2019s bandwidth-per-pin advantage can unlock signi\ufb01cant\nbandwidth and performance gains by rethinking memory sys-\ntems to be CXL-centric rather than DDR-centric.\nIn this paper, we make the key observation that the band-\nwidth boost attainable with CXL drastically reduces memory\naccess queuing delays, which largely dictate the effective ac-\ncess latency of loaded memory systems. In addition to in-\ncreased average memory access latency, queuing delays also\nincrease memory access variance, which we show has detri-\nmental effects to performance. Driven by this insight, we\nargue that a memory system attached to the processor entirely\nover CXL is a key enabler for scalable high-performance server\nprocessors that deploy memory-intensive workloads. Our pro-\n1arXiv:2305.05033v1  [cs.AR]  8 May 2023posed server design, dubbed COAXIAL, replaces all of the\nprocessor\u2019s direct DDR interfaces with CXL.\nBy evaluating COAXIALwith a wide range of workloads,\nwe highlight how CXL-based memory system\u2019s unique char-\nacteristics (i.e., increased bandwidth and higher unloaded la-\ntency) positively impact performance of processors whose\nmemory system is typically loaded. Our analysis relies on a\nsimple but often overlooked fact about memory system be-\nhavior and its impact on overall performance: that a loaded\nmemory system\u2019s effective latency is dominated by the im-\npact of queuing effects and therefore signi\ufb01cantly differs from\nthe unloaded system\u2019s latency, as we demonstrate in \u00a73.1. A\nmemory system that offers higher parallelism reduces queuing\neffects, which in turn results in lower average latency and\nvariance, even if its unloaded access latency is higher com-\npared to existing systems. We argue that CXL-based memory\nsystems offer exactly this design trade-off, which is favorable\nfor loaded server processors handling memory-intensive ap-\nplications, offering strong motivation for a radical change in\nmemory system design that departs from two decades of DDR\nand enables scalable high-performance server architectures.\nIn summary, we make the following contributions:\n\u2022We make the radical proposal of using high-bandwidth CXL\nas acomplete replacement of pin-inef\ufb01cient DDR interfaces\non server processors, showcasing a ground-breaking shift\nthat disrupts decades-long memory system design practices.\n\u2022We show that, despite its higher unloaded memory access la-\ntency, COAXIALreduces the effective memory access time\nin typical scenarios where the memory system is loaded.\n\u2022We demonstrate the promise of COAXIALwith a study of\na wide range of workloads for various CXL bandwidth and\nlatency design points that are likely in the near future.\n\u2022We identify limitations imposed on CXL by the current\nPCIe standard, and highlight opportunities a revised stan-\ndard could leverage for 20% additional speedup.\nPaper outline: \u00a72 motivates the replacement of DDR with\nCXL in server processors. \u00a73 highlights the critical impact\nof queuing delays on a memory system\u2019s performance and\n\u00a74 provides an overview of our proposed COAXIALserver\ndesign, which leverages CXL to mitigate detrimental queuing.\nWe outline the methodology to evaluate COAXIALagainst a\nDDR-based system in \u00a75 and analyze performance results in\n\u00a76. We discuss related work in \u00a77 and conclude in \u00a78.\n2. Background\nIn this section, we highlight how DRAM memory bandwidth\nis bottlenecked by the processor-attached DDR interface and\nprocessor pin-count. We then discuss how CXL can bridge\nthis gap by using PCIe as its underlying physical layer.\n2.1. Low-latency DDR-based Memory\nServers predominantly access DRAM over the Double Data\nRate (DDR) parallel interface. The interface\u2019s processor pinrequirement is determined by the width of data bus, com-\nmand/address bus, and con\ufb01guration pins. A DDR4 and\nDDR5 [ 21] interface is 288 pins wide. While several of\nthose pins are terminated at the motherboard, most of them\n(160+ for an ECC-enabled DDR4 channel [ 20], likely more\nfor DDR5 [45]) are driven to the processor chip.", "start_char_idx": 734186, "end_char_idx": 738711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db80f62f-cfa7-4b6a-b805-062abacd7c14": {"__data__": {"id_": "db80f62f-cfa7-4b6a-b805-062abacd7c14", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12725f5a-45d6-44fd-82dd-2b0c016ac228", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "3d9279f142d907d536957ae236e2903694fd98578f81d4b732e4e68c33f942b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "271daadc-b16d-4439-8562-b15c922217e7", "node_type": "1", "metadata": {}, "hash": "2916d3c49bde95532957040e6bdbf1a5cc836b20ff9040ba57ab423e28ebf5c2", "class_name": "RelatedNodeInfo"}}, "text": "We discuss related work in \u00a77 and conclude in \u00a78.\n2. Background\nIn this section, we highlight how DRAM memory bandwidth\nis bottlenecked by the processor-attached DDR interface and\nprocessor pin-count. We then discuss how CXL can bridge\nthis gap by using PCIe as its underlying physical layer.\n2.1. Low-latency DDR-based Memory\nServers predominantly access DRAM over the Double Data\nRate (DDR) parallel interface. The interface\u2019s processor pinrequirement is determined by the width of data bus, com-\nmand/address bus, and con\ufb01guration pins. A DDR4 and\nDDR5 [ 21] interface is 288 pins wide. While several of\nthose pins are terminated at the motherboard, most of them\n(160+ for an ECC-enabled DDR4 channel [ 20], likely more\nfor DDR5 [45]) are driven to the processor chip.\nThe DDR interface\u2019s 64 data bits directly connect to the\nprocessor and are bit-wise synchronous with the memory con-\ntroller\u2019s clock, enabling a worst-case (unloaded) access latency\nof about 50ns. Scaling a DDR-based memory system\u2019s band-\nwidth requires either clocking the channels at a higher rate,\nor attaching more channels to the processors. The former\napproach results in signal integrity challenges [ 39] and a re-\nduction in supported ranks per channel, limiting rank-level\nparallelism and memory capacity. Accommodating more chan-\nnels requires more on-chip pins, which cost signi\ufb01cant area\nand power, and complicate placement, routing, and packag-\ning [62]. Therefore, the pin-count on processor packages has\nonly been doubling about every six years [51].\nThus, reducing the number of cores that contend over a\nmemory channel is dif\ufb01cult without clean-slate technologies,\nwhich we discuss in \u00a77. To this end, the emerging CXL in-\nterconnect is bound to bridge this gap by leveraging a widely\ndeployed high-bandwidth serial interface, as we discuss next.\n2.2. The High-bandwidth CXL Memory Interconnect\nThe Compute Express Link (CXL) is a recent interconnect\nstandard, designed to present a uni\ufb01ed solution for coherent\naccelerators, non-coherent devices, and memory expansion\ndevices. It represents the industry\u2019s concerted effort for a\nstandardized interconnect to replace a wide motley collection\nof proprietary solutions (e.g., OpenCAPI [ 55], Gen-Z [ 11]).\nCXL is rapidly garnering industry adoption and is bound to be-\ncome a dominant interconnect, as PCIe has been for peripheral\ndevices over the past twenty years.\nCXL brings load-store semantics and coherent memory ac-\ncess to high-capacity, high-bandwidth memory for processors\nand accelerators alike. It also enables attaching DDR-based\nmemory (\u201cType-3\u201d CXL devices) over PCIe to the proces-\nsor with strict timing constraints. In this work, we focus on\nthis capability of CXL. CXL\u2019s underlying PCIe physical layer\naffords higher bandwidth per pin at the cost of increased la-\ntency. Therefore, most recent works thus far perceive CXL as\na technology enabling an auxiliary slower memory tier directly\nattached to the processor. In contrast, we argue that despite its\nassociated latency overhead, CXL can play a central role in\nfuture memory systems design, replacing , rather than simply\naugmenting, DDR-based memory in server processors.\n2.3. Scaling the Memory Bandwidth Wall with CXL\nCXL\u2019s high bandwidth owes to its underlying PCIe physical\nlayer. PCIe [ 47] is a high-speed serial interface featuring\nmultiple independent lanes capable of bi-directional commu-\nnication using just 4 pins per lane: two for transmitting data,\n2PCIe1.0PCIe2.0PCIe3.0PCIe4.0PCIe5.0PCIe6.0PCIe7.0DDR4-2133DDR4-3200DDR5-4800DDR5-8000DDR6-9600DDR6-160001248163264\n2000200520102015202020252030Relative bandwidth per pinYearFigure 1: Bandwidth per processor pin for DDR and CXL (PCIe)\ninterface, norm. to PCIe-1.0. Note that y-axis is in log scale.\nand two for receiving data. Data is sent over each lane as a\nserial bit stream at very high bit rates in an encoded format.\nFig. 1 illustrates the bandwidth per pin for PCIe and DDR.", "start_char_idx": 737940, "end_char_idx": 741894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "271daadc-b16d-4439-8562-b15c922217e7": {"__data__": {"id_": "271daadc-b16d-4439-8562-b15c922217e7", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db80f62f-cfa7-4b6a-b805-062abacd7c14", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "bdb16019da8873f070f24b2be210ad4ee34ffd49e8b5233925df03ac3063de42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d165a60-ef73-4cd8-b9c2-1a04bd8b341b", "node_type": "1", "metadata": {}, "hash": "885f71c9b470817339db1f4430e1a46e3a2e265a92b9b06b29d28fe2b2e3a9ce", "class_name": "RelatedNodeInfo"}}, "text": "to PCIe-1.0. Note that y-axis is in log scale.\nand two for receiving data. Data is sent over each lane as a\nserial bit stream at very high bit rates in an encoded format.\nFig. 1 illustrates the bandwidth per pin for PCIe and DDR.\nThe normalized bandwidth per pin is derived by dividing each\ninterface\u2019s peak interface bandwidth on JEDEC\u2019s and PCI-\nSIG\u2019s roadmap, respectively, by the processor pins required:\n160 for DDR and 4 per lane for PCIe.\nThe 4\u0002bandwidth gap is where we are today (PCIe5.0\nvs. DDR5-4800). The comparison is conservative, given that\nPCIe\u2019s stated bandwidth is per direction , while DDR5-4800\nrequires about 160 processor pins for a theoretical 38.4GB/s\npeak of combined read and write bandwidth. With a third of\nthe pins, 12 PCIe5.0 lanes (over which CXL operates) offer\n48GB/s per direction\u2014i.e., a theoretical peak of 48GB/s for\nreads and48GB/s for writes. Furthermore, Fig. 1\u2019s roadmaps\nsuggest that the bandwidth gap will grow to 8 \u0002by 2025.\n2.4. CXL Latency Concerns\nCXL\u2019s increased bandwidth comes at the cost of increased\nlatency compared to DDR. There is a widespread assumption\nthat this latency cost is signi\ufb01cantly higher than DRAM ac-\ncess latency itself. For instance, recent work on CXL-pooled\nmemory reinforces that expectation, by reporting a latency\noverhead of 70ns [ 25]. The expectation of such high added\nlatency has reasonably led memory system researchers and\ndesigners to predominantly focus on CXL as a technology for\nenabling a secondary tier of slower memory that augments\nconventional DDR-attached memory. However, such a high\nlatency overhead does not represent the minimum attainable\nlatency of the simplest CXL-attached memory and is largely\nan artifact of more complex functionality, such as multiplexing\nmultiple memory devices, enforcing coherence between the\nhost and memory device, etc.\nIn this work, we argue that CXL is a perfect candidate to\ncompletely replace the DDR-attached memory for server pro-\ncessors that handle memory-intensive workloads. The CXL\n3.0 standard sets an 80ns pin-to-pin load latency target for a\nCXL-attached memory device [ 9, Table 13-2], which in turn\nimplies that the interface-added latency over DRAM access in\nupcoming CXL memory devices should be about 30ns. Early\nimplementations of the CXL 2.0 standard demonstrated a 25ns\nlatency overhead per direction [ 42], and in 2021 PLDA an-\nnounced a commercially available CXL 2.0 controller thatonly adds 12ns per direction [ 43]. Such low latency over-\nheads are attainable with the simplest CXL type-3 devices that\nare not multiplexed across multiple hosts and do not need to\ninitiate any coherence transactions. Our key insight is that a\nmemory access latency penalty in the order of 30ns often pales\nin comparison to queuing delays at the memory controller that\nare common in server systems, and such queuing delays can\nbe curtailed by CXL\u2019s considerable bandwidth boost.\n3. Pitfalls of Unloaded and Average Latency\nIt is evident from current technological trends that systems\nwith CXL-attached memory can enjoy signi\ufb01cantly higher\nbandwidth availability compared to conventional systems\nwith DDR-attached memory. A key concern hindering broad\nadoption\u2014and particularly our proposed replacement of DDR\ninterfaces on-chip with CXL\u2014is CXL\u2019s increased memory\naccess latency. However, in any system with a loaded memory\nsubsystem, queuing effects play a signi\ufb01cant role in determin-\ning effective memory access latency. On a loaded system,\nqueuing (i) dominates the effective memory access latency,\nand (ii) introduces variance in accessing memory, degrading\nperformance. We next demonstrate the impact of both effects.\n3.1. Queuing Dictates Effective Memory Access Latency\nFig. 2a shows a DDR5-4800 channel\u2019s memory access latency\nas its load increases. We model the memory using DRAM-\nSim [ 46] and control the load with random memory accesses\nof con\ufb01gurable arrival rate. The resulting load-latency curve\nis shaped by queuing effects at the memory controller.", "start_char_idx": 741665, "end_char_idx": 745665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d165a60-ef73-4cd8-b9c2-1a04bd8b341b": {"__data__": {"id_": "7d165a60-ef73-4cd8-b9c2-1a04bd8b341b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "271daadc-b16d-4439-8562-b15c922217e7", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "13a302b26932404d054b4132669cc42d20a22964a79a6477441fcdcac251b9ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98b04653-5c73-492c-b6c2-9845d7e5c473", "node_type": "1", "metadata": {}, "hash": "6b13ca2620f9842290b6f27287c446275c679c7ec727927a0d8cea9c719d8dfd", "class_name": "RelatedNodeInfo"}}, "text": "A key concern hindering broad\nadoption\u2014and particularly our proposed replacement of DDR\ninterfaces on-chip with CXL\u2014is CXL\u2019s increased memory\naccess latency. However, in any system with a loaded memory\nsubsystem, queuing effects play a signi\ufb01cant role in determin-\ning effective memory access latency. On a loaded system,\nqueuing (i) dominates the effective memory access latency,\nand (ii) introduces variance in accessing memory, degrading\nperformance. We next demonstrate the impact of both effects.\n3.1. Queuing Dictates Effective Memory Access Latency\nFig. 2a shows a DDR5-4800 channel\u2019s memory access latency\nas its load increases. We model the memory using DRAM-\nSim [ 46] and control the load with random memory accesses\nof con\ufb01gurable arrival rate. The resulting load-latency curve\nis shaped by queuing effects at the memory controller.\nWhen the system is unloaded, a hypothetical CXL interface\nadding 30ns to each memory access would correspond to\na seemingly prohibitive 75% latency overhead compared to\nthe approximated unloaded latency of 40ns. However, as\nthe memory load increases, latency rises exponentially, with\naverage latency increasing by 3\u0002and4\u0002at 50% and 60%\nload, respectively. p90 tail latency grows even faster, rising by\n4:7\u0002and7:1\u0002at the same load points. In a loaded system,\ntrading off additional interface latency for considerably higher\nbandwidth availability can yield signi\ufb01cant net latency gain.\nTo illustrate, consider a baseline DDR-based system operat-\ning at 60% of memory bandwidth utilization, corresponding\nto 160ns average and 285ns p90 memory access latency. A\nCXL-based alternative offering a 4\u0002memory bandwidth boost\nwould shrink the system\u2019s bandwidth utilization to 15%, cor-\nresponding to 50% lower average and 68% lower p90 memory\naccess latency compared to baseline, despite the CXL inter-\nface\u2019s 30ns latency premium.\nFig. 2a shows that a system with bandwidth utilization as\nlow as 20% experiences queuing effects, that are initially re-\n\ufb02ected on tail latency; beyond 40% utilization, queuing effects\nalso noticeably affect average latency. Utilization beyond such\nlevel is common, as we show with our simulation of a 12-\ncore processor with 1 DDR5 memory channels over a range\nof server and desktop applications (methodological details in\n30 10 20 30 40 50 60\nMemory Bandwidth Utilization as \n% of Theoretical Peak0100200300400500Memory Access Latency (ns) Average Latency\n90% Latency(a) Average and p90 memory access latency in a DDR5-\n4800 channel (38.4GB/s) at varying bandwidth utiliza-\ntion points. p90 grows faster than the average latency.\n0100200300400Average Memory\nAccess Latency (ns)484429\nQueuing Delay\nAccess Service Time\nST_copyST_scaleST_addST_triad  \nComp-scCompPR-DBCPR\nRadiiBFSCCBellmFBFS\nBFS-BVTriangleMIS  \nfluida\nfacesimraytracesclustercanneal  \nlbm\nbwavescactuBfotonikcam4wrfmcfromspop2\nomnetppxalancgcc  \nmasstreekmeans0255075Memory Bandwidth\nUtilization(b) Memory latency breakdown (DRAM access time and queuing delay) and memory bandwidth\nutilization for a range of workloads. Higher utilization increases queuing delay.\nFigure 2: Queuing drastically affects memory access time on a loaded system.\n\u00a75). Fig. 2b shows that with all processor cores under use, the\nvast majority of workloads exceed 30% memory bandwidth\nutilization, and most exceed 50% utilization (except several\nworkloads from SPEC and PARSEC benchmarks).\nFig. 2b also breaks down the average memory access time\nseen from the LLC miss register into DRAM service time and\nqueuing delay at the memory controller. We observe a trend in\nhigh bandwidth consumption leading to long queuing delays,\nalthough queuing delay doesn\u2019t present itself as a direct func-\ntion of bandwidth utilization. Queuing delay is also affected\nby application characteristics such as read/write pattern and\nspatial and temporal distribution of accesses.", "start_char_idx": 744821, "end_char_idx": 748693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98b04653-5c73-492c-b6c2-9845d7e5c473": {"__data__": {"id_": "98b04653-5c73-492c-b6c2-9845d7e5c473", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d165a60-ef73-4cd8-b9c2-1a04bd8b341b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a80fb59e71c5a4a753550db48d83d63663d4699e0af0285d0860003534b63624", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b49c16f8-767f-4aa5-8171-882aa2c3957b", "node_type": "1", "metadata": {}, "hash": "50d1ec7cf8a97b562483b66fed348a4d5a5c237ee6bc24a7ea25c68bfb87ee74", "class_name": "RelatedNodeInfo"}}, "text": "Higher utilization increases queuing delay.\nFigure 2: Queuing drastically affects memory access time on a loaded system.\n\u00a75). Fig. 2b shows that with all processor cores under use, the\nvast majority of workloads exceed 30% memory bandwidth\nutilization, and most exceed 50% utilization (except several\nworkloads from SPEC and PARSEC benchmarks).\nFig. 2b also breaks down the average memory access time\nseen from the LLC miss register into DRAM service time and\nqueuing delay at the memory controller. We observe a trend in\nhigh bandwidth consumption leading to long queuing delays,\nalthough queuing delay doesn\u2019t present itself as a direct func-\ntion of bandwidth utilization. Queuing delay is also affected\nby application characteristics such as read/write pattern and\nspatial and temporal distribution of accesses. For example, in\nan access pattern where processor makes majority of memory\naccess requests in a short amount of time, followed by a period\nof low memory activity, the system would temporarily be in\na high bandwidth utilization state when memory requests are\nmade, experiencing contention and high queuing delay, even\nthough the average bandwidth consumption would not be as\nhigh. Even in such cases, provisioning more bandwidth would\nlead to better performance, as it would mitigate contention\nfrom the temporary bursts. In Fig. 2b\u2019s workloads, queuing de-\nlay constitutes 72% of the memory access latency on average,\nand up to 91% in the case of lbm.\n3.2. Memory Latency Variance Impacts Performance\nIn addition to their effect on average memory access latency,\nspurious queuing effects at the memory controller introduce\nhigher memory access latency \ufb02uctuations (i.e., variance).\nSuch variance is closely related to the queueing delay stem-\nming from high utilization, as discussed in \u00a73.1. To demon-\nstrate the impact of memory access latency variance on per-\nformance, we conduct a controlled experiment where the av-\nerage memory access latency is kept constant, but the latency\n\ufb02uctuation around the average grows. The baseline is a toy\nmemory system with a 150ns \ufb01xed access latency and we\nevaluate three additional memory systems where memory ac-\nbwaves ST_triad masstreeBFSraytracegm0.00.51.0Performance norm.\nto Fixed Latency(100ns,350ns)\n(75ns,450ns)\n(50ns,550ns)Figure 3: Performance of workloads for synthetic memory ac-\ncess latency following three (X, Y) bimodal distributions with\n4:1 X:Y ratio, all with 150ns average latency, normalized to a\nmemory system with \ufb01xed 150ns latency. \u201cgm\" refers to geo-\nmetric mean. Higher latency variance degrades performance.\ncess latency follows a bimodal distribution with 80%/20%\nprobability of being lower/higher than the average. We keep\naverage latency constant in all cases ( 80%\u0002low_lat+20%\u0002\nhigh_lat=150ns) and we evaluate (low_lat ;high_lat)for\n(100ns;350ns);(75ns;450ns);(50ns;550ns), resulting in dis-\ntributions with increasing standard deviations (stdev) of 100ns,\n150ns, and 200ns. Variance is the square of stdev and denotes\nhow spread out the latency is from the average.\nFig. 3 shows the relative performance of these memory\nsystems for \ufb01ve workloads of decreasing memory bandwidth\nintensity. As variance increases, the average performance rel-\native to the \ufb01xed-latency baseline noticeably drops to 86%,\n78%, and 71%. This experiment highlights that solely rely-\ning on typical average metrics like Average Memory Access\nTime (AMAT) to assess a memory system\u2019s performance is an\nincomplete method of evaluating a memory system\u2019s perfor-\nmance. In addition to average values, the variance of memory\naccess latency is a major performance determinant and there-\nfore an important quality criterion for a memory system.\n4Table 1: Area of processor\ncomponents at TSMC 7nm\n(rel. to 1MB of L3 cache).\nL3 cache (1MB) 1\nZen 3 Core6.5(incl. 512 KB L2)\nx8 PCIe (PHY + ctrl) 5.9\nDDR channel (PHY + ctrl) 10.8Table 2: DDR-based versus alternative C OAXIAL server con\ufb01gurations.\nServer designCore LLC Memory Relative RelativeCommentcount per core interfaces mem.", "start_char_idx": 747878, "end_char_idx": 751913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b49c16f8-767f-4aa5-8171-882aa2c3957b": {"__data__": {"id_": "b49c16f8-767f-4aa5-8171-882aa2c3957b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98b04653-5c73-492c-b6c2-9845d7e5c473", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "cf873025ce865a5873a4dd5b3a7097668c09c5166f8760d8fa1a494622573bab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4357edb1-44a6-4852-b254-7bc057a45b06", "node_type": "1", "metadata": {}, "hash": "5ff08ab871643159a03ac5742c77caedfde153643d6291b344e2f82acbc1ff92", "class_name": "RelatedNodeInfo"}}, "text": "This experiment highlights that solely rely-\ning on typical average metrics like Average Memory Access\nTime (AMAT) to assess a memory system\u2019s performance is an\nincomplete method of evaluating a memory system\u2019s perfor-\nmance. In addition to average values, the variance of memory\naccess latency is a major performance determinant and there-\nfore an important quality criterion for a memory system.\n4Table 1: Area of processor\ncomponents at TSMC 7nm\n(rel. to 1MB of L3 cache).\nL3 cache (1MB) 1\nZen 3 Core6.5(incl. 512 KB L2)\nx8 PCIe (PHY + ctrl) 5.9\nDDR channel (PHY + ctrl) 10.8Table 2: DDR-based versus alternative C OAXIAL server con\ufb01gurations.\nServer designCore LLC Memory Relative RelativeCommentcount per core interfaces mem. BW area\nDDR-based\n1442 MB12 DDR 1\u0002 1 baseline\nCOAXIAL-5\u0002 60 x8 CXL 5\u0002 1.17 iso-pin\nCOAXIAL-2\u0002 24 x8 CXL 2\u0002\n1.01 iso-areaiso-LLC\nCOAXIAL-4\u0002\n1 MB48 x8 CXL 4\u0002 balanced\nCOAXIAL-asym48 x8 CXL-asym asym. R/Wmax BW(see \u00a74.3)\nSummary: The signi\ufb01cant memory bandwidth boost at-\ntainable with CXL-attached memory more than compen-\nsates for the interface\u2019s higher latency in loaded systems,\nwhere queuing dictates the memory system\u2019s effective\naccess latency. By decreasing queuing, a CXL-based\nmemory system reduces average memory access time and\nvariance, both of which improve performance.\n4. The C OAXIAL Server Architecture\nWe leverage CXL\u2019s per-pin bandwidth advantage to replace all\nof the DDR interfaces with PCIe-based CXL interfaces in our\nproposed COAXIALserver. Fig. 4b depicts our architecture\nwhere each on-chip DDR5 channel is replaced by several\nCXL channels, providing 2\u20134 \u0002higher aggregate memory\nbandwidth to the processor. Fig. 4a shows the baseline DDR-\nbased server design for comparison. Each CXL channel is\nattached to a \u201cType-3\u201c CXL device, which features a memory\ncontroller that manages a regular DDR5 channel that connects\nto DRAM. The processor implements the CXL.mem protocol\nof the CXL standard, which orchestrates data consistency and\nmemory semantics management. The implementation of the\ncaches and cores remains unchanged, as the memory controller\nstill supplies 64B cache lines.\n4.1. Processor Pin Considerations\nA DDR5-4800 channel features a peak uni-directional band-\nwidth of 38.4GB/s and requires more than 160 processor pins\nto account for data and ECC bits, command/address bus, data\nstrobes, clock, feature modes, etc., as described in \u00a72.1. A\nfull 16-lane PCIe connection delivers 64GB/s of bi-directional\nbandwidth. Moreover, PCIe is modular, and higher-bandwidth\nchannels can be constructed by grouping independent lanes\ntogether. Each lane requires just four processor pins: two each\nfor transmitting and receiving data.\nThe PCIe standard currently only allows groupings of 1,\n2, 4, 8, 12, 16, or 32 lanes. To match DDR5\u2019s bandwidth\nof 38.4GB/s, we opt for an x8 con\ufb01guration, which requires\n32 pins for a peak bandwidth of 32GB/s, 5\u0002fewer than the\n160 pins required for the DDR5 channel. As PCIe can sus-\ntain 32GB/s bandwidth in each direction, the peak aggregate\nbandwidth of 8 lanes is 64GB/s, much higher than DDR5\u2019s\n38.4GB/s. Considering a typical 2:1 Read:Write ratio, only\n25.6GB/s of a DDR5 channel\u2019s bandwidth would be used in\ncores, caches, NoC, etc.DDR\nDDRDDRDDR\n......(a) Baseline DDR-based server.\n.........type-3CXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLcores, caches, NoC, etc.\ntype-3\n(b)COAXIALreplaces each DDR channel with several CXL channels. Each\nCXL channel connects to a type-3 device with one DDR memory channel.\nFigure 4: Overview of the baseline and C OAXIAL systems.", "start_char_idx": 751183, "end_char_idx": 754758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4357edb1-44a6-4852-b254-7bc057a45b06": {"__data__": {"id_": "4357edb1-44a6-4852-b254-7bc057a45b06", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b49c16f8-767f-4aa5-8171-882aa2c3957b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "f70efff7c04a4c2121144859f027356d80ec75d856ae1743f281cf730217e2ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d5bddb2-5e95-4c83-bd82-5a0c55e89bfc", "node_type": "1", "metadata": {}, "hash": "c0d8f5d4e7eef1574e14475355c13e4eb6594e58db7606f8a088bef37db0608c", "class_name": "RelatedNodeInfo"}}, "text": "Considering a typical 2:1 Read:Write ratio, only\n25.6GB/s of a DDR5 channel\u2019s bandwidth would be used in\ncores, caches, NoC, etc.DDR\nDDRDDRDDR\n......(a) Baseline DDR-based server.\n.........type-3CXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLCXLcores, caches, NoC, etc.\ntype-3\n(b)COAXIALreplaces each DDR channel with several CXL channels. Each\nCXL channel connects to a type-3 device with one DDR memory channel.\nFigure 4: Overview of the baseline and C OAXIAL systems.\nthe DRAM-to-CPU direction, and about 13GB/s in the oppo-\nsite direction. Furthermore, peak sustainable bandwidth for\nDDR controllers typically achieve around 70% to 90% of the\ntheoretical peak. Thus, even after factoring in PCIe and CXL\u2019s\nheader overheads which reduce the practically attainable band-\nwidth [ 48] to 26GB/s in the DRAM-to-CPU direction and\n13GB/s in the other direction, the x8 con\ufb01guration supports a\nfull DDR5 channel without becoming a choke point.\n4.2. Silicon Area Considerations\nWhen it comes to processor pin requirements, COAXIALal-\nlows replacement of each DDR channel (i.e., PHY and mem-\nory controller) with \ufb01ve x8 PCIe PHY and controllers, for\na5\u0002memory bandwidth boost. However, the relative pin\nrequirements of DDR and PCIe are not directly re\ufb02ected in\ntheir relative on-chip silicon area requirements. Lacking pub-\nlicly available information, we derive the relative size of DDR\nand PCIe PHYs and controllers from AMD Rome and Intel\nGolden Cove die shots [29, 53].\nTable 1 shows the relative silicon die area different key\ncomponents of the processor account for. Assuming linear\nscaling of PCIe area with the number of lanes, as appears to be\nthe case from the die shots, an x8 PCIe controller accounts for\n54% of a DDR controller\u2019s area. Hence, replacing each DDR\ncontroller with four x8 PCIe controllers requires 2.19 \u0002more\n5silicon area than what is allocated to DDR. However, DDR\ncontrollers account for a small fraction of the total CPU die.\nLeveraging Table 1\u2019s information, we now consider a num-\nber of alternative COAXIALserver designs, shown in Table 2.\nWe focus on high-core-count servers optimized for throughput,\nsuch as the upcoming AMD EPYC Bergamo (128 cores) [ 37],\nand Intel Granite Rapids (128 cores) and Sierra Forest (144\ncores) [ 38]. All of them feature 12 DDR5 channels, resulting\nin a core-to-memory-controller (core:MC) ratio of 10.7:1 to\n12:1. A common design choice to accommodate such high\ncore counts is a reduced LLC capacity; e.g., moving from the\n96-core Genoa [ 52] to the 128-core Bergamo, AMD halves the\nLLC per core to 2MB. We thus consider a 144-core baseline\nserver processor with 12 DDR5 channels and 2MB of LLC\nper core (Table 2, \ufb01rst row).\nWith pin count as its only limitation, COAXIAL-5\u0002re-\nplaces each DDR channel with 5 x8 CXL interfaces, for a\n5\u0002bandwidth increase. Unfortunately, that results in a 17%\nincrease in die area to accommodate all the PCIe PHYs and\ncontrollers. Hence, we also consider two iso-area alterna-\ntives. COAXIAL-2\u0002leverages CXL to double memory band-\nwidth without any microarchitectural changes. COAXIAL-4\u0002\nquadruples the available memory bandwidth compared to the\nbaseline CPU by halving the LLC from 288MB to 144MB.\n4.3. C OAXIAL Asymmetric Interface Optimization\nA key difference between CXL and DDR is that the former\nprovisions dedicated pins and wires for each data movement\ndirection (RX and TX). The PCIe standard de\ufb01nes a one-to-\none match of TX and RX pins: e.g., an x8 PCIe con\ufb01guration\nimplies 8 TX and 8 RX lanes. We observe that while uniform\nbandwidth provisioning in each direction is reasonable for a\nperipheral device like a NIC, it is not the case for memory traf-\n\ufb01c.", "start_char_idx": 754284, "end_char_idx": 757950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d5bddb2-5e95-4c83-bd82-5a0c55e89bfc": {"__data__": {"id_": "3d5bddb2-5e95-4c83-bd82-5a0c55e89bfc", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4357edb1-44a6-4852-b254-7bc057a45b06", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "fb5f7359101cff72bc8f981d25c9931a9e674ecad4a1a194fcfbb8734d1923bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55a6877f-bba3-4632-afbe-fb596c192eba", "node_type": "1", "metadata": {}, "hash": "48004f762e0950f5797923aa2ad465f572005774b8ba5ac433d62037f5cc06ff", "class_name": "RelatedNodeInfo"}}, "text": "COAXIAL-2\u0002leverages CXL to double memory band-\nwidth without any microarchitectural changes. COAXIAL-4\u0002\nquadruples the available memory bandwidth compared to the\nbaseline CPU by halving the LLC from 288MB to 144MB.\n4.3. C OAXIAL Asymmetric Interface Optimization\nA key difference between CXL and DDR is that the former\nprovisions dedicated pins and wires for each data movement\ndirection (RX and TX). The PCIe standard de\ufb01nes a one-to-\none match of TX and RX pins: e.g., an x8 PCIe con\ufb01guration\nimplies 8 TX and 8 RX lanes. We observe that while uniform\nbandwidth provisioning in each direction is reasonable for a\nperipheral device like a NIC, it is not the case for memory traf-\n\ufb01c. Because (i) most workloads read more data than they write\nand (ii) every cache block that is written must typically be read\n\ufb01rst, R:W ratios are usually in the 3:1 to 2:1 range rather than\n1:1. Thus, in the current 1:1 design, read bandwidth becomes\nthe bottleneck and write bandwidth is underutilized. Given\nthis observation and that serial interfaces do not fundamentally\nrequire 1:1 RX:TX bandwidth provisioning [ 59], we consider\naCOAXIALdesign with asymmetric RX/TX lane provision-\ning to better match memory traf\ufb01c characteristics. While the\nPCIe standard currently disallows doing so, we investigate\nthe potential performance bene\ufb01ts of revisiting that statutory\nrestriction. We call such a channel CXL-asym .\nWe consider a system leveraging such CXL-asym channels\nto compose an additional COAXIAL-asym con\ufb01guration. An\nx8 CXL channel consists of 32 pins, 16 each way. Without\nthe current 1:1 PCIe restriction, CXL-asym repurposes the\nsame pin count to use 20 RX pins and 12 TX pins, resulting in\n40GB/s RX and 24GB/s TX of raw bandwidth. Accounting\nfor PCIe and CXL\u2019s header overheads, the realized bandwidth\nis approximately 32GB/s for reads (compared to 26GB/s in\nx8 CXL channel) and 10GB/s for writes [ 48]. To utilize theTable 3: System parameters used for simulation on\nChampSim.\nDDR baseline CoaXiaL-*\nCPU 12 OoO cores, 2GHz, 4-wide, 256-entry ROB\nL1 32KB L1-I & L1-D, 8-way, 64B blocks, 4-cycle access\nL2 512 KB, 8-way, 12-cycle access\nLLCshared & non-inclusive, 16-way, 46-cycle access\n2 MB/core 1\u20132 MB/core (see Table 2)\nMemoryDDR5-4800 [36], 128 GB per channel, 2 sub-channels\nper channel, 1 rank per sub-channel, 32 banks per rank\n1 channel2\u20134 CXL-attached channels (see Table 2)\n8 channels for C OAXIAL-asym (see \u00a74.3)\nadditional read bandwidth, we provision two DDR controllers\nper CXL-asym channel on the type-3 device. Therefore, the\nnumber of CXL channels on the processor (as well as their\narea overhead) remains unchanged. While the 32GB/s read\nbandwidth of CXL-asym is insuf\ufb01cient to support two DDR\nchannels at their combined read bandwidth of about 52GB/s\n(assuming a 2:1 R:W ratio), queuing delays at the DDR con-\ntroller typically become signi\ufb01cant at a much lower utiliza-\ntion point, as shown in Fig. 2a. Therefore, COAXIAL-asym\nstill provides suf\ufb01cient bandwidth to eliminate contention at\nqueues by lowering the overall bandwidth utilization, while\nproviding higher aggregate bandwidth.\n4.4. Additional Bene\ufb01ts of C OAXIAL\nOur analysis focuses on the performance impact of a CXL-\nbased memory system. While a memory capacity and cost\nanalysis is beyond the scope of this paper, COAXIALcan have\nadditional positive effects on those fronts that are noteworthy.\nServers provisioned for maximum memory capacity deploy\ntwo high-density DIMMs per DDR channel. The implica-\ntions are two-fold. First, two-DIMMs-per-channel (2DPC)\ncon\ufb01gurations increase capacity over 1DPC at the cost of\n\u001815% memory bandwidth.", "start_char_idx": 757266, "end_char_idx": 760888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55a6877f-bba3-4632-afbe-fb596c192eba": {"__data__": {"id_": "55a6877f-bba3-4632-afbe-fb596c192eba", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d5bddb2-5e95-4c83-bd82-5a0c55e89bfc", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "172210867886cddb3fc87dcd694c3bd5a40df274033d088422a27e30f7bbb134", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6f88f6c-0bd8-41c1-8e61-90254833090b", "node_type": "1", "metadata": {}, "hash": "5e92f074c0005dcf8ac432021c39aca434d6dfac285daf1f795726d0c2b411a8", "class_name": "RelatedNodeInfo"}}, "text": "2a. Therefore, COAXIAL-asym\nstill provides suf\ufb01cient bandwidth to eliminate contention at\nqueues by lowering the overall bandwidth utilization, while\nproviding higher aggregate bandwidth.\n4.4. Additional Bene\ufb01ts of C OAXIAL\nOur analysis focuses on the performance impact of a CXL-\nbased memory system. While a memory capacity and cost\nanalysis is beyond the scope of this paper, COAXIALcan have\nadditional positive effects on those fronts that are noteworthy.\nServers provisioned for maximum memory capacity deploy\ntwo high-density DIMMs per DDR channel. The implica-\ntions are two-fold. First, two-DIMMs-per-channel (2DPC)\ncon\ufb01gurations increase capacity over 1DPC at the cost of\n\u001815% memory bandwidth. Second, DIMM cost grows super-\nlinearly with density; for example, 128GB/256GB DIMMs\ncost 5\u0002/20\u0002more than 64GB DIMMs. By enabling more\nDDR channels, COAXIALallows the same or higher DRAM\ncapacity with 1DPC and lower-density DIMMs.\n5. Evaluation Methodology\nSystem con\ufb01gurations. We compare our COAXIALserver\ndesign, which replaces the processor\u2019s DDR channels with\nCXL channels, to a typical DDR-based server processor.\n\u2022DDR-based baseline . We simulate 12 cores and one DDR5-\n4800 memory channel as a scaled-down version of Table 2\u2019s\n144-core CPU.\n\u2022 C OAXIALservers . We evaluate several servers that re-\nplace the on-chip DDR interfaces with CXL: COAXIAL-2\u0002,\nCOAXIAL-4\u0002, and C OAXIAL-asym (Table 2).\nWe simulate the above system con\ufb01gurations using\n6ChampSim [1] coupled with DRAMsim3 [ 26]. Table 3 sum-\nmarizes the con\ufb01guration parameters used.\nCXL performance modeling. ForCOAXIAL, we model\nCXL controllers and PCIe bus on both the processor and the\ntype-3 device. Each CXL controller comprises a CXL port\nthat incurs a \ufb01xed delay of 12ns accounting for \ufb02it-packing,\nencoding-decoding, packet processing, etc. [ 43]. The PCIe\nbus incurs traversal latency due to the limited channel band-\nwidth and bus width. For an x8 channel, the peak 32GB/s\nbandwidth results in 26/13 GB/s RX/TX goodput when header\noverheads are factored in, and 32/10 GB/s RX/TX in the case\nof CXL-asym channels. The corresponding link traversal\nlatency is 2.5/ 5.5 ns RX/TX for an x8 channel and 2/ 9 ns\nRX/TX for CXL-asym. Additionally, the CXL controller main-\ntains message queues to buffer requests. Therefore, in addition\nto minimum latency overhead of about 30ns (or more, in our\nsensitivity analysis), queuing effects at the CXL controller are\nalso captured and re\ufb02ected in the performance.\nWorkloads. We evaluate 35 workloads from various bench-\nmark suites. We deploy the same workload instance on all\ncores and simulate 200 million instructions per core after fast-\nforwarding each application to a region of interest.\n\u2022Graph analytics: We use 12 workloads from the LIGRA\nbenchmark suite [49].\n\u2022STREAM: We run the four kernels ( copy, scale, add, triad )\nfrom the STREAM benchmark [ 34] to represent bandwidth-\nintensive matrix operations in which ML workloads spend a\nsigni\ufb01cant portion of their execution time.\n\u2022SPEC & PARSEC: We evaluate 13 workloads from the\nSPEC-speed 2017 [ 50] benchmark suite in refmode, as\nwell as \ufb01ve PARSEC workloads [5].\n\u2022We evaluate masstree [32] and kmeans [28] to represent key\nvalue store and data analytics workloads, respectively.\nTable 4 summarizes all our evaluated workloads, along with\ntheir IPC and MPKI as measured on the DDR-based baseline.\n6. Evaluation Results\nWe \ufb01rst compare our main COAXIALdesign, COAXIAL-4\u0002,\nwith the DDR-based baseline by analyzing the impact of re-\nduced bandwidth utilization and queuing delays on perfor-\nmance in \u00a76.1. \u00a76.2 highlights the effect of memory access\npattern and distribution on performance.", "start_char_idx": 760185, "end_char_idx": 763849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6f88f6c-0bd8-41c1-8e61-90254833090b": {"__data__": {"id_": "b6f88f6c-0bd8-41c1-8e61-90254833090b", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55a6877f-bba3-4632-afbe-fb596c192eba", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e9229a92273acde2f5827afc86479bf749fd674f42e40434242d803e67260174", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4ccd43c-9507-414f-8209-3cfc13d4c33e", "node_type": "1", "metadata": {}, "hash": "0025d55f5ef9fffff45fb914b7f4391f27548fb96bd24560710a2b3d9247b0fe", "class_name": "RelatedNodeInfo"}}, "text": "\u2022SPEC & PARSEC: We evaluate 13 workloads from the\nSPEC-speed 2017 [ 50] benchmark suite in refmode, as\nwell as \ufb01ve PARSEC workloads [5].\n\u2022We evaluate masstree [32] and kmeans [28] to represent key\nvalue store and data analytics workloads, respectively.\nTable 4 summarizes all our evaluated workloads, along with\ntheir IPC and MPKI as measured on the DDR-based baseline.\n6. Evaluation Results\nWe \ufb01rst compare our main COAXIALdesign, COAXIAL-4\u0002,\nwith the DDR-based baseline by analyzing the impact of re-\nduced bandwidth utilization and queuing delays on perfor-\nmance in \u00a76.1. \u00a76.2 highlights the effect of memory access\npattern and distribution on performance. \u00a76.3 presents the per-\nformance of alternative COAXIALdesigns, COAXIAL-2\u0002\nandCOAXIAL-asym, and \u00a76.4 demonstrates the impact of\na more conservative 50ns CXL latency penalty. \u00a76.5 evalu-\nates C OAXIAL at different server utilization points, and \u00a76.6\nanalyzes C OAXIAL\u2019s power implications.\n6.1. From Queuing Reduction to Performance Gains\nFig. 5 (top) shows the performance of COAXIAL-4\u0002relative\nto the baseline DDR-based system. Most workloads exhibit\nNote that, throughout \u00a76\u2019s evaluation, reference to \u201c COAXIAL\u201d without\na following quali\ufb01er implies the C OAXIAL-4\u0002con\ufb01guration.Table 4: Workload Summary.\nApplication IPCLLC\nMPKIApplication IPCLLC\nMPKI\nLigra SPEC\nPageRank 0.36 40 lbm 0.14 64\nPageRank\nDelta0.31 27 bwaves 0.33 14\nComponents\n-shortcut0.34 48 cactusBSSN 0.68 8\nComponents 0.36 48 fotonik3d 0.33 22\nBC 0.33 34 cam4 0.87 6\nRadii 0.41 33 wrf 0.61 11\nBFSCC 0.68 17 mcf 0.793 13\nBFS 0.69 15 roms 0.783 6\nBFS-Bitvector 0.84 15 pop2 1.55 3\nBellmanFord 0.86 9 omnetpp 0.51 10\nTriangle 0.65 21 xalancbmk 0.55 12\nMIS 1.37 8 gcc 0.31 19\nSTREAM PARSEC\nStream-copy 0.17 58 \ufb02uidanimate 0.78 7\nStream-scale 0.21 48 facesim 0.74 6\nStream-add 0.16 69 raytrace 1.17 5\nStream-triad 0.18 59 streamcluster 0.99 14\nKVS & Data analytics canneal 0.66 7\nMasstree 0.37 21\nKmeans 0.50 36\nsigni\ufb01cant speedup, up to 3\u0002forlbmand1:52\u0002on average.\n10 of the 35 workloads experience more than 2\u0002speedup.\nFour workloads lose performance, with gcc most signi\ufb01cantly\nimpacted at 26% IPC loss. Workloads most likely to suffer\na performance loss are those with low to moderate memory\ntraf\ufb01c and heavy dependencies among memory accesses.\nFig. 5 (bottom) shows memory bandwidth utilization for\nthe DDR-based baseline and COAXIAL-4\u0002, which provides\n4\u0002higher bandwidth than the baseline. COAXIALdistributes\nmemory requests over more channels which reduces the band-\nwidth utilization of the system, in turn reducing contention\nfor the memory bus. The lower bandwidth utilization and con-\ntention drastically reduces the queuing delay in COAXIALfor\nmemory-intensive workloads. Fig. 5 (middle) demonstrates\nthis reduction with a breakdown of the average memory ac-\ncess latency (as measured from the LLC miss register) into the\nDRAM service time, queuing delay, and CXL interface delay\n(only applicable to C OAXIAL).\nIn many cases, COAXIALenables the workload to drive\nsigni\ufb01cantly more aggregate bandwidth from the system.", "start_char_idx": 763189, "end_char_idx": 766243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4ccd43c-9507-414f-8209-3cfc13d4c33e": {"__data__": {"id_": "c4ccd43c-9507-414f-8209-3cfc13d4c33e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6f88f6c-0bd8-41c1-8e61-90254833090b", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "39f7a6fa6dd09b4475eb238a699bb4136e8d6f6225a2153ff9e4733c8f70f0bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d5c4a3d-a0f2-46f5-9c78-3720a8619ddf", "node_type": "1", "metadata": {}, "hash": "de8abcc4155511a89324cd3214a03ea59c4bae16aa801292db64d67bbb9b8364", "class_name": "RelatedNodeInfo"}}, "text": "Fig. 5 (bottom) shows memory bandwidth utilization for\nthe DDR-based baseline and COAXIAL-4\u0002, which provides\n4\u0002higher bandwidth than the baseline. COAXIALdistributes\nmemory requests over more channels which reduces the band-\nwidth utilization of the system, in turn reducing contention\nfor the memory bus. The lower bandwidth utilization and con-\ntention drastically reduces the queuing delay in COAXIALfor\nmemory-intensive workloads. Fig. 5 (middle) demonstrates\nthis reduction with a breakdown of the average memory ac-\ncess latency (as measured from the LLC miss register) into the\nDRAM service time, queuing delay, and CXL interface delay\n(only applicable to C OAXIAL).\nIn many cases, COAXIALenables the workload to drive\nsigni\ufb01cantly more aggregate bandwidth from the system. For\ninstance, stream-copy is bottlenecked by the baseline system\u2019s\nconstrained bandwidth, resulting in average queuing delay\nexceeding 300ns that largely dictates the overall access la-\ntency (the total height of the stacked bars). COAXIALreduces\nqueuing delay to just 55ns for this workload, more than com-\npensating for the 30ns CXL interface latency overhead. The\noverall average access latency for stream-copy reduces from\n348ns in baseline to just 120ns, enabling COAXIALto drive\nmemory requests at a 2:9\u0002higher rate versus the baseline,\n70.00.51.01.52.0Normalized\nPerformanceGM_STRMGM_LIGRAGM_PARSGM_SPECGM_ALL2.45\n1.45\n1.121.551.522.72.6 3.02.6\n050100150200250Average Memory\nAccess Latency (ns)Queuing Delay Access Service Time CXL Interface Delay\nST_copyST_scaleST_addST_triad  \nComp-scCompPR-DBCPR\nRadiiBFSCCBellmFBFS\nBFS-BVTriangleMIS  \nfluidafacesimraytracesclustercanneal  \nlbm\nbwavescactuBfotonikcam4wrfmcfromspop2\nomnetppxalancgcc  \nmasstreekmeans020406080Memory Bandwidth\nUtilization Baseline\nCoaXiaLFigure 5: Normalized performance of C OAXIAL over DDR-based baseline (top), memory access latency breakdown (middle), and\nmemory bandwidth utilization (bottom). Workloads are grouped into their benchmark suite. \u201cgm\u201d refers to geometric mean.\nCOAXIAL offers 1:52\u0002average speedup due to 4\u0002higher bandwidth, lowering utilization and mitigating queuing effects.\nthus achieving commensurate speedup.\nDespite provisioning 4\u0002more bandwidth, COAXIALre-\nduces average bandwidth utilization from 54% to 34% for\nworkloads that have more than 2\u0002performance improvement,\nhighlighting the extra bandwidth is indeed utilized by these\nworkloads. For most of the other workloads, COAXIAL\u2019s aver-\nage memory access latency is much lower than the baseline\u2019s,\ndespite the CXL interface\u2019s latency overhead.\nOn average, workloads experience 144ns in queuing delay\non top of \u001840ns DRAM service time. By slashing queuing\ndelay to just 31ns on average, COAXIALreduces average\nmemory access latency, thereby boosting performance. Over-\nall, Fig. 5\u2019s results con\ufb01rm our key insight (see \u00a73.1): queuing\ndelays largely dictate the average memory access latency.\nTakeaway #1: COAXIALdrastically reduces queuing\ndelays, resulting in lower effective memory access latency\nfor bandwidth-hungry workloads.\n6.2. Beyond Average Bandwidth Utilization and Access\nLatency\nWhile most of COAXIAL\u2019s performance gains can be justi-\n\ufb01ed by the achieved reduction in average memory latency, a\ncompounding positive effect is reduction in latency variance\nas evidenced in \u00a73.2. For each of the four evaluated workload\ngroups, Fig. 6a shows the mean average latency and standard\ndeviation (stdev) for COAXIALand the DDR-based baseline.\nAs already seen in \u00a76.1, COAXIALdelivers a 45\u201360% reduc-tion to average memory access latency. Fig. 6a shows that\nCOAXIALalso achieves a similar reduction in stdev, indicat-\ning lower dispersion and fewer extreme high-latency values.\nTo further demonstrate the impact of access latency distribu-\ntion and temporal effects, we study a few workloads in more\ndepth.", "start_char_idx": 765463, "end_char_idx": 769309, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d5c4a3d-a0f2-46f5-9c78-3720a8619ddf": {"__data__": {"id_": "9d5c4a3d-a0f2-46f5-9c78-3720a8619ddf", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4ccd43c-9507-414f-8209-3cfc13d4c33e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "bc7f80d86b818675735eb5448bfc5dc0cbd26cfa071a3c7de00739e6bb8b46a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd49931a-c2a3-4080-a659-b68803bb0ea6", "node_type": "1", "metadata": {}, "hash": "df192e115ab12ab7507d0a3df51d43019388532b61d4bca169b0fed90d2e8971", "class_name": "RelatedNodeInfo"}}, "text": "6.2. Beyond Average Bandwidth Utilization and Access\nLatency\nWhile most of COAXIAL\u2019s performance gains can be justi-\n\ufb01ed by the achieved reduction in average memory latency, a\ncompounding positive effect is reduction in latency variance\nas evidenced in \u00a73.2. For each of the four evaluated workload\ngroups, Fig. 6a shows the mean average latency and standard\ndeviation (stdev) for COAXIALand the DDR-based baseline.\nAs already seen in \u00a76.1, COAXIALdelivers a 45\u201360% reduc-tion to average memory access latency. Fig. 6a shows that\nCOAXIALalso achieves a similar reduction in stdev, indicat-\ning lower dispersion and fewer extreme high-latency values.\nTo further demonstrate the impact of access latency distribu-\ntion and temporal effects, we study a few workloads in more\ndepth. Streamcluster presents an interesting case because\nits performance improves despite a slightly higher average\nmemory access latency of 76ns compared to the baseline\u2019s\n69ns (see Fig. 5). Fig. 6b shows the Cumulative Distribution\nFunction (CDF) of Streamcluster\u2019s memory access latencies,\nillustrating that the baseline results in a higher variance than\nCOAXIAL(stdev of 88 versus 76), due to imbalanced queu-\ning across DRAM banks. The tighter distribution of memory\naccess latency allows COAXIALto outperform the baseline\ndespite a 10% higher average memory access latency.\nSome workloads bene\ufb01t from COAXIALmore than other\nworkloads with similar or higher memory bandwidth utiliza-\ntion (Fig. 5 (bottom)). For example, bwaves uses a mere\n32% of the baseline\u2019s available bandwidth but suffers an over-\nwhelming 390ns queuing delay. Even though bwaves uses less\nbandwidth on average compared to workloads (e.g., radii with\n65% bandwidth utilization), it exhibits bursty behavior that\nincurs queuing spikes which can be more effectively absorbed\nbyCOAXIAL.Kmeans exhibits the opposite case. Despite\nhaving the highest bandwidth utilization in the baseline sys-\ntem, it experiences a relatively low average queuing delay of\n50ns and exhibits one of the lowest latency variance values\nacross workloads, indicating an even distribution of accesses\n8STREAMLIGRAPARSECSPECALL0100200300400500Average Access Latency \nand Standard Deviation (ns)Baseline\nCoaXiaL(a) Average memory access la-\ntency per workload group, and stdev\nshown as error bars.\nmean stdev\nBaseline 69 88\nCoaXiaL 76 76\n0 100 200 300\nMemory Access Time (ns)0.20.40.60.81.0CDF\nBaseline\nCoaXiaL(b) Cumulative Distribution Func-\ntion (CDF) of memory access\ntime for Streamcluster.\nFigure 6: Memory access latency distribution.\nover time and across DRAM banks. Kmeans is also an outlier\nwith near-zero write traf\ufb01c, thus avoiding the turnaround over-\nhead from the memory controller switching between read and\nwrite mode that results in bandwidth underutilization.\n6.3. Alternative C OAXIAL designs\nFig. 7 evaluates the two alternative COAXIALdesigns intro-\nduced in \u00a74\u2014 COAXIAL-2\u0002andCOAXIAL-asym\u2014in addi-\ntion to our default COAXIAL-4\u0002.COAXIAL-2\u0002achieves\na1:26\u0002average speedup over the baseline, down from\nCOAXIAL-4\u0002\u2019s1:52\u0002gain. This con\ufb01rms our intuition that\ndoubling memory bandwidth availability at the cost of halving\nthe LLC is bene\ufb01cial for virtually all workloads. COAXIAL-\nasym improves performance by 1:67\u0002on average\u2014a consid-\nerable 15% gain on top of COAXIAL-4\u0002\u2014and no workload is\nnegatively affected by COAXIAL-asym\u2019s reduced write band-\nwidth. This result implies an exciting opportunity to improve\nbandwidth ef\ufb01ciency in memory devices attached via serial\ninterconnects by provisioning the interfaces in a manner that\nis aware of the workloads\u2019 read versus write demands.\nTakeaway #2: Provisioning the lanes in read/write de-\nmand aware manner considerably improves performance\ncompared to the default 1:1 read:write provisioning ratio.\n6.4.", "start_char_idx": 768531, "end_char_idx": 772321, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd49931a-c2a3-4080-a659-b68803bb0ea6": {"__data__": {"id_": "cd49931a-c2a3-4080-a659-b68803bb0ea6", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d5c4a3d-a0f2-46f5-9c78-3720a8619ddf", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "b42a1ffe88200f2b389bd10d94d2071b6e32eaf555296b4385aba35da26c4c16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0ab135b-6bbf-4bd5-b69b-962a17564feb", "node_type": "1", "metadata": {}, "hash": "30287960691c74f4bb0a0beb106f2ce986e531661a11d50f7189c4deffcbf45b", "class_name": "RelatedNodeInfo"}}, "text": "This con\ufb01rms our intuition that\ndoubling memory bandwidth availability at the cost of halving\nthe LLC is bene\ufb01cial for virtually all workloads. COAXIAL-\nasym improves performance by 1:67\u0002on average\u2014a consid-\nerable 15% gain on top of COAXIAL-4\u0002\u2014and no workload is\nnegatively affected by COAXIAL-asym\u2019s reduced write band-\nwidth. This result implies an exciting opportunity to improve\nbandwidth ef\ufb01ciency in memory devices attached via serial\ninterconnects by provisioning the interfaces in a manner that\nis aware of the workloads\u2019 read versus write demands.\nTakeaway #2: Provisioning the lanes in read/write de-\nmand aware manner considerably improves performance\ncompared to the default 1:1 read:write provisioning ratio.\n6.4. Sensitivity to CXL\u2019s Latency Overhead\nWhile we base our main evaluation on a 30ns roundtrip CXL\ninterface latency off the CXL 3.0 speci\ufb01cation and current\nindustry expectations (see \u00a72.4), we also evaluate a more pes-\nsimistic latency overhead of 50ns, in case early products do not\nmeet the 30ns target. Such latency may also better represent\nCXL-attached memory devices located at a longer physical\ndistance from the CPU, or devices with an additional multi-\nplexing overhead (e.g., memory devices shared by multiple\nservers\u2014a scenario CXL intends to enable [16, 25]).\nFig. 8 shows COAXIAL\u2019s performance at 30ns (our default)\nand 50ns CXL interface latency overhead, normalized to the\nDDR-based baseline. Although increasing latency overhead to\n50ns reduces COAXIAL\u2019s average speedup, it remains signi\ufb01-\ncant at 1:33\u0002. Memory-intensive workloads continue to enjoy\ndrastic speedups of over 50%, but more workloads (nine, upfrom four with 30ns latency penalty) take a performance hit.\nThese results imply that while a COAXIALwith a higher CXL\nlatency is still worth pursuing, it should be used selectively\nfor memory-intensive workloads. Deploying different classes\nof servers for different optimization goals is common practice\nnot only in public clouds [ 15] but also in private clouds (e.g.,\ndifferent web and backend server con\ufb01gurations) [12, 18].\nTakeaway #3: Even with a 50ns CXL latency overhead,\nCOAXIALachieves a considerable 1:3\u0002average speedup\nacross all workloads.\n6.5. Sensitivity to Core Utilization\nFig. 9 evaluates COAXIAL\u2019s performance under varying lev-\nels of system utilization by provisioning proportionately less\nwork on a fraction of cores of the system. We \ufb01rst study the\nextreme case of using a single core on our 12-core simulated\nsystem ( 8%utilization). In this scenario, virtually all work-\nloads suffer performance degradation with COAXIAL, for a\n17% average slowdown. Xalancbmk exhibits a corner case\nwhere the working set \ufb01ts in the LLC when only one instance\nis running, removing most memory accesses. The extreme\nsingle-core experiment showcases COAXIAL\u2019s worst-case be-\nhavior, where the memory system is the least utilized.\nWe then increase the system utilization to 33% and 66%,\nby deploying workload instances on 4 and 8 cores of the\n12-core CPU, respectively. We also show results for 100%\nutilization (all cores used) again as a point of comparison.\nCOAXIAL\u2019s bandwidth abundance gradually starts paying\noff, by eliminating the slowdown at 33% utilization for most\nworkloads, and then delivering signi\ufb01cant gains\u20141.27 \u0002on\naverage and up to 2.62 \u0002\u2014even at 66% utilization. The 66%\nutilization point can also be considered as a good proxy for\na fully loaded system where cores and DDR controllers are\nprovisioned at an 8:1 ratio. An 8:1 core:MC ratio is the design\npoint of many server processors with fewer than 100 cores\ntoday, such as AMD EPYC Milan and Genoa [ 8,52]. Thus,\nthe66% utilization results imply that COAXIAL\u2019s approach\nis applicable beyond high-end throughput-oriented processors\nthat already exhibit 12:1 core:MC oversubscription.\nTakeaway #4: Even at 66% server utilization\u2014or 8:1\ncore:MC ratio\u2014C OAXIAL delivers a 1 :27\u0002speedup.\n6.6.", "start_char_idx": 771594, "end_char_idx": 775513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0ab135b-6bbf-4bd5-b69b-962a17564feb": {"__data__": {"id_": "c0ab135b-6bbf-4bd5-b69b-962a17564feb", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd49931a-c2a3-4080-a659-b68803bb0ea6", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "a2561b6595c6666c2442ecded94b9e6a5b54ae0a9fbf41ff361a195636ea44d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff82f36a-dc76-4f19-bde9-d91745588e8e", "node_type": "1", "metadata": {}, "hash": "8b6944dcda6a72dd2816c695b6e4cd47e03b6c3af69ab10d5979026af8c46da1", "class_name": "RelatedNodeInfo"}}, "text": "The 66%\nutilization point can also be considered as a good proxy for\na fully loaded system where cores and DDR controllers are\nprovisioned at an 8:1 ratio. An 8:1 core:MC ratio is the design\npoint of many server processors with fewer than 100 cores\ntoday, such as AMD EPYC Milan and Genoa [ 8,52]. Thus,\nthe66% utilization results imply that COAXIAL\u2019s approach\nis applicable beyond high-end throughput-oriented processors\nthat already exhibit 12:1 core:MC oversubscription.\nTakeaway #4: Even at 66% server utilization\u2014or 8:1\ncore:MC ratio\u2014C OAXIAL delivers a 1 :27\u0002speedup.\n6.6. Power Requirements and Energy Ef\ufb01ciency\nAlthough COAXIAL\u2019s added serial links and 4\u0002more\nDIMMs increase the server\u2019s power consumption, our sys-\ntem also affords much higher throughput. To take this power\nincrease into account, we compute the Energy Delay Product\n(EDP = system power \u0002CPI2)of the baseline and COAXIAL-\n4\u0002. A lower EDP value indicates a more ef\ufb01cient system that\n9ST_copyST_scaleST_addST_triad  \nComp-scCompPR-DBCPR\nRadiiBFSCCBellmFBFS\nBFS-BVTriangleMIS  \nfluidafacesimraytracesclustercanneal  \nlbm\nbwavescactuBfotonikcam4wrfmcfromspop2\nomnetppxalancgcc  \nmasstreekmeansGM_STRMGM_LIGRAGM_PARSGM_SPECGM_ALL0.00.51.01.52.02.53.0Norm. PerformanceCoaXiaL-2X CoaXiaL-4X CoaXiaL-asymFigure 7: C OAXIAL\u2019s performance at different design points, norm. to the DDR-based server baseline. C OAXIAL-4\u0002outperforms\nCOAXIAL-2\u0002, despite its halved LLC size. C OAXIAL-asym considerably outperforms our default C OAXIAL-4\u0002design.\nST_copyST_scaleST_addST_triad  \nComp-scCompPR-DBCPR\nRadiiBFSCCBellmFBFS\nBFS-BVTriangleMIS  \nfluidafacesimraytracesclustercanneal  \nlbm\nbwavescactuBfotonikcam4wrfmcfromspop2\nomnetppxalancgcc  \nmasstreekmeansGM_STRMGM_LIGRAGM_PARSGM_SPECGM_ALL0.00.51.01.52.02.53.0Norm. Performance30ns CXL Buffer Delay\n50ns CXL Buffer Delay\nFigure 8: C OAXIAL\u2019s performance for different CXL latency premium, norm. to the DDR-based server. Even with a 50ns interface\nlatency penalty, C OAXIAL yields a 1:33\u0002average speedup.\nconsumes less energy to complete the same work, even if it\noperates at a higher power.\nWe model power for a manycore processor similar to\nAMD EPYC Bergamo (128 cores) [ 37] or Sierra Forest (144\ncores) [ 38]. The latter is expected to have a 500W TDP, which\nis in line with current processors (e.g., 96-core AMD EPYC\nGenoa [ 52] has a TDP of 360W). While the memory controller\nand interface require negligible power compared to the proces-\nsor, we include them for completeness. We estimate controller\nand interface power per DDR5 channel to be 0.5W and 0.6W,\nrespectively [ 57], or 13W in total for a baseline processor with\n12 channels. Similarly, PCIe 5.0\u2019s interface power is \u00180.2W\nper lane [ 4], or 77W for the 384 lanes required to support\nCOAXIAL\u2019s 48 DDR5 channels.\nA signi\ufb01cant fraction of a large-scale server\u2019s power is\nattributed to memory. We use Micron\u2019s power calculator\ntool [ 35] to compute our baseline\u2019s and CXL system\u2019s DRAM\npower requirement by taking the observed average mem-\nory bandwidth utilization of 52% for baseline and 21% for\nCOAXIALinto account.", "start_char_idx": 774935, "end_char_idx": 778025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff82f36a-dc76-4f19-bde9-d91745588e8e": {"__data__": {"id_": "ff82f36a-dc76-4f19-bde9-d91745588e8e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0ab135b-6bbf-4bd5-b69b-962a17564feb", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "b870602ace805e838108d23dd103429cfb89d7d3bda74af7fb51e69822ce1ee4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d48e9fd4-6545-4925-acc9-b09c1084c723", "node_type": "1", "metadata": {}, "hash": "142705e5f59f108f13e183946273381e1647313b88938f976d0a909c878bc001", "class_name": "RelatedNodeInfo"}}, "text": "While the memory controller\nand interface require negligible power compared to the proces-\nsor, we include them for completeness. We estimate controller\nand interface power per DDR5 channel to be 0.5W and 0.6W,\nrespectively [ 57], or 13W in total for a baseline processor with\n12 channels. Similarly, PCIe 5.0\u2019s interface power is \u00180.2W\nper lane [ 4], or 77W for the 384 lanes required to support\nCOAXIAL\u2019s 48 DDR5 channels.\nA signi\ufb01cant fraction of a large-scale server\u2019s power is\nattributed to memory. We use Micron\u2019s power calculator\ntool [ 35] to compute our baseline\u2019s and CXL system\u2019s DRAM\npower requirement by taking the observed average mem-\nory bandwidth utilization of 52% for baseline and 21% for\nCOAXIALinto account. As this tool only computes power\nup to DDR4-3200MT/s modules, we model a 64GB 2-rank\nDDR4-3200 DIMM (16GB 2-rank module for CXL) and dou-ble the power to obtain power consumption of a 128 GB DDR5\nchannel (32 GB channel for CXL). While COAXIALemploys\n4\u0002more DIMMs than the baseline, its power consumption is\nonly 1 :75\u0002higher due to lower memory utilization.\nTable 5 summarizes the key power components for the\nbaseline and COAXIALsystems. The overall system power\nconsumption is 713W for the baseline system and 1.18kW\nforCOAXIAL, a 66% increase. Crucially, COAXIALmas-\nsively boosts performance, reducing CPI by 34%. As a result,\nCOAXIALreduces the baseline\u2019s EDP by a considerable 28%.\nTakeaway #5: In addition to boosting performance,\nCOAXIALaffords a more ef\ufb01cient system with a 28%\nlower energy-delay product.\n6.7. Evaluation Summary\nCXL-based memory systems hold great promise for manycore\nserver processors. Replacing DDR with CXL-based memory\nthat offers 4\u0002higher bandwidth at a 30ns latency premium\nachieves a 1:52\u0002average speedup across various workloads.\nFurthermore, a COAXIAL-asym design demonstrates oppor-\nST_copyST_scaleST_addST_triad  \nComp-scCompPR-DBCPR\nRadiiBFSCCBellmFBFS\nBFS-BVTriangleMIS  \nfluidafacesimraytracesclustercanneal  \nlbm\nbwavescactuBfotonikcam4wrfmcfromspop2\nomnetppxalancgcc  \nmasstreekmeansGM_STRMGM_LIGRAGM_PARSGM_SPECGM_ALL0.00.51.01.52.02.53.0Norm. Performance1 Core  (8%)\n4 Cores (33%) 8  Cores (66%)\n12 Cores (100%)\nFigure 9: Performance of C OAXIAL as a function of active cores, norm. to DDR-based server baseline at the same active cores.\n10Table 5: Energy Delay Product (EDP =System power \u0002CPI2)\ncomparison for target 144-core server. Lower EDP is better.\nEDP Component Baseline COAXIAL\nProcessor Package power 500W 500W\nDDR5 MC & PHY power (all) 13W 52W\nDDR5 DIMM power (static and access) 200W 551W\nCXL\u2019s Interface power (idle and dynamic) N/A 77W\nTotal system power 713W 1,180W\nAverage CPI (all workloads) 2.02 1.33\nEDP (all workloads) 2,909 2,087 (0.72 \u0002)\ntunity for additional gain ( 1:67\u0002average speedup), assuming\na modi\ufb01cation to the PCIe standard to allow departure from\nthe rigid 1:1 read:write bandwidth provisioning to allow an\nasymmetric, workload-aware one. Even if COAXIALincurs\na 50ns latency premium, it promises substantial performance\nimprovement ( 1:33\u0002on average). We show that our bene-\n\ufb01ts stem from reduced memory contention: by reducing the\nutilization of available bandwidth resources, COAXIALmit-\nigates queuing effects, thus reducing both average memory\naccess latency and its variance.\n7.", "start_char_idx": 777297, "end_char_idx": 780589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d48e9fd4-6545-4925-acc9-b09c1084c723": {"__data__": {"id_": "d48e9fd4-6545-4925-acc9-b09c1084c723", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff82f36a-dc76-4f19-bde9-d91745588e8e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5bd55e3ee6e0c280ceb9fbd0b6ad2d9be005091d6ff703f5acb76cacb3fbb7f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcd46c2b-93cc-492f-950a-1e7efd34d570", "node_type": "1", "metadata": {}, "hash": "f87d6500d3b5dc478cfd763ce5bad8e59200d04440661a37e2182db1056f8f1d", "class_name": "RelatedNodeInfo"}}, "text": "Even if COAXIALincurs\na 50ns latency premium, it promises substantial performance\nimprovement ( 1:33\u0002on average). We show that our bene-\n\ufb01ts stem from reduced memory contention: by reducing the\nutilization of available bandwidth resources, COAXIALmit-\nigates queuing effects, thus reducing both average memory\naccess latency and its variance.\n7. Related Work\nWe discuss recent works investigating CXL-based memory\nsystem solutions, prior memory systems leveraging serial in-\nterfaces, as well as circuit-level and alternative techniques to\nimprove bandwidth and optimize the memory system.\nEmerging CXL-based memory systems. Industry is rapidly\nadopting CXL and already investigating its deployment in\nproduction systems to reap the bene\ufb01ts of memory expansion\nand memory pooling. Microsoft leverages CXL to pool mem-\nory across servers, improving utilization and thus reducing\ncost [ 25]. In the same vein, Gouk et al. [ 16] leverage CXL to\nprototype a practical instance of disaggregated memory [ 27].\nAspiring to use CXL as a memory expansion technique that\nwill enable a secondary memory tier of higher capacity than\nDDR, Meta\u2019s recent work optimizes data placement in this\nnew type of two-tier memory hierarchy [ 33]. Using an FPGA-\nbased prototype of a CXL type-3 memory device, Ahn et al.\nevaluate database workloads on a hybrid DDR/CXL memory\nsystem and demonstrate minimal performance degradation,\nsuggesting that CXL-based memory expansion is cost-ef\ufb01cient\nand performant [ 3]. Instead of using CXL-attached memory\nas a memory system extension, our work stands out as the \ufb01rst\none to propose CXL-based memory as a complete replace-\nment of DDR-attached memory for server processors handling\nmemory-intensive workloads.\nMemory systems leveraging serial interfaces. There have\nbeen several prior memory system proposals leveraging serial\nlinks for high-bandwidth, energy-ef\ufb01cient data transfers. Mi-\ncron\u2019s HMC was connected to the host over 16 SerDes lanes,\ndelivering up to 160GB/s [ 41]. IBM\u2019s Centaur is a memory\ncapacity expansion solution, where the host uses SerDes toconnect to a buffer-on-board, which in turn hosts several DDR\nchannels [ 54]. FBDIMM [ 14] leverages a similar concept to\nCentaur\u2019s buffer-on-board to increase memory bandwidth and\ncapacity. An advanced memory buffer (AMB) acts as a bridge\nbetween the processor and the memory modules, connecting\nto the processor over serial links and featuring an abundance\nof pins to enable multiple parallel interfaces to DRAM mod-\nules. Similar to CXL-attached memory, a key concern with\nFBDIMM is its increased latency. Open Memory Interface\n(OMI) is a recent high-bandwidth memory leveraging serial\nlinks, delivering bandwidth comparable to HBM but without\nHBM\u2019s tight capacity limitations [ 7]. Originally a subset of\nOpenCAPI, OMI is now part of the CXL Consortium.\nResearchers have also proposed memory system architec-\ntures making use of high-bandwidth serial interfaces. In MeS-\nSOS\u2019 two-stage memory system, high-bandwidth serial links\nconnect to a high-bandwidth DRAM cache, which is then\nchained to planar DRAM over DDR [ 58]. Ham et al. pro-\npose disintegrated memory controllers attached over SerDes,\naiming to make the memory system more modular and fa-\ncilitate supporting heterogeneous memory technologies [ 17].\nAlloy combines parallel and serial interfaces to access memory,\nmaintaining the parallel interfaces for lower-latency memory\naccess [ 59]. Unlike our proposal of fully replacing DDR\nprocessor interfaces with CXL for memory-intensive servers,\nAlloy\u2019s approach is closer to the hybrid DDR/CXL memory\nsystems that most ongoing CXL-related research envisions.\nCircuit-level techniques to boost memory bandwidth.\nHBM [ 23] and die-stacked DRAM caches offer an order of\nmagnitude higher bandwidth than planar DRAM, but suffer\nfrom limited capacity [ 22,30,44]. BOOM [ 60] buffers out-\nputs from multiple LPDDR ranks to reduce power and sus-\ntain server-level performance, but offers modest gains due to\nlow frequency LPDDR and limited bandwidth improvement.\nChen et al. [ 6] propose dynamic reallocation of power pins to\nboost data transfer capability from memory during memory-\nintensive phases, during which processors are memory bound\nand hence draw less power. Pal et al. [ 40] propose packageless\nprocessors to mitigate pin limitations and boost the memory\nbandwidth that can be routed to the processor.", "start_char_idx": 780244, "end_char_idx": 784646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcd46c2b-93cc-492f-950a-1e7efd34d570": {"__data__": {"id_": "fcd46c2b-93cc-492f-950a-1e7efd34d570", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d48e9fd4-6545-4925-acc9-b09c1084c723", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e74e93ee22a961366ced42670217ba112b859834dd0d23ef709df2d042569aba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6159ad8-db64-4ae7-85c8-929baa03bb63", "node_type": "1", "metadata": {}, "hash": "6f030971084a99811fc44f02e7cbd5eeb3ae5ffb149bc7a18be5b40bde16e397", "class_name": "RelatedNodeInfo"}}, "text": "Circuit-level techniques to boost memory bandwidth.\nHBM [ 23] and die-stacked DRAM caches offer an order of\nmagnitude higher bandwidth than planar DRAM, but suffer\nfrom limited capacity [ 22,30,44]. BOOM [ 60] buffers out-\nputs from multiple LPDDR ranks to reduce power and sus-\ntain server-level performance, but offers modest gains due to\nlow frequency LPDDR and limited bandwidth improvement.\nChen et al. [ 6] propose dynamic reallocation of power pins to\nboost data transfer capability from memory during memory-\nintensive phases, during which processors are memory bound\nand hence draw less power. Pal et al. [ 40] propose packageless\nprocessors to mitigate pin limitations and boost the memory\nbandwidth that can be routed to the processor. Unlike these\nproposals, we focus on conventional processors, packaging,\nand commodity DRAM, aiming to reshape the memory sys-\ntem of server processors by leveraging the widely adopted\nup-and-coming CXL interconnect.\nOther memory system optimizations. Transparent memory\ncompression techniques are a compelling approach to increas-\ning effective memory bandwidth [ 61]. Malladi et al. [ 31]\nleverage mobile LPDDR DRAM devices to design a more\nenergy-ef\ufb01cient memory system for servers without perfor-\nmance loss. These works are orthogonal to our proposed\napproach. Storage-class memory, like Phase-Change Mem-\nory [13] or Intel\u2019s Optane [ 19], has attracted signi\ufb01cant interest\nas a way to boost a server\u2019s memory capacity, triggering re-\nsearch activity on transforming the memory hierarchy to best\n11accommodate such new memories [ 2,10,24,56]. Unlike our\nwork, such systems often trade off bandwidth for capacity.\n8. Conclusion\nTechnological trends motivate a server processor design where\nall memory is attached to the processor over the emerging\nCXL interconnect instead of DDR. CXL\u2019s superior bandwidth\nper pin helps bandwidth-hungry server processors scale the\nbandwidth wall. By distributing memory requests over 4\u0002\nmore memory channels, CXL reduces queueing effects on\nthe memory bus. Because queuing delay dominates access\nlatency in loaded memory systems, such reduction more than\ncompensates for the interface latency overhead introduced by\nCXL. Our evaluation on a diverse range of memory-intensive\nworkloads shows that our proposed COAXIALserver delivers\n1:52\u0002speedup on average, and up to 3 \u0002.\nReferences\n[1]\u201cChampSim.\u201d [Online]. Available: https://github :com/ChampSim/\nChampSim\n[2]N. Agarwal and T. F. Wenisch, \u201cThermostat: Application-transparent\nPage Management for Two-tiered Main Memory,\u201d in Proceedings of\nthe 22nd International Conference on Architectural Support for Pro-\ngramming Languages and Operating Systems (ASPLOS-XXII) , 2017,\npp. 631\u2013644.\n[3]M. Ahn, A. Chang, D. Lee, J. Gim, J. Kim, J. Jung, O. Rebholz,\nV . Pham, K. T. Malladi, and Y .-S. Ki, \u201cEnabling CXL Memory Expan-\nsion for In-Memory Database Management Systems,\u201d in Proceedings\nof the 18th International Workshop on Data Management on New\nHardware (DaMoN) , 2022, pp. 8:1\u20138:5.\n[4]M. Bichan, C. Ting, B. Zand, J. Wang, R. Shulyzki, J. Guthrie,\nK. Tyshchenko, J. Zhao, A. Parsafar, E. Liu, A. Vatankhahghadim,\nS. Shari\ufb01an, A. Tyshchenko, M. D. Vita, S. Rubab, S. Iyer, F. Spagna,\nand N. Dolev, \u201cA 32Gb/s NRZ 37dB SerDes in 10nm CMOS to Sup-\nport PCI Express Gen 5 Protocol,\u201d in Proceedings of the 2020 IEEE\nCustom Integrated Circuits Conference , 2020, pp. 1\u20134.\n[5]C. Bienia, S. Kumar, J. P. Singh, and K. Li, \u201cThe PARSEC benchmark\nsuite: characterization and architectural implications,\u201d in Proceedings\nof the 17th International Conference on Parallel Architecture and\nCompilation Techniques (PACT) , 2008, pp. 72\u201381.\n[6]S. Chen, Y . Hu, Y .", "start_char_idx": 783900, "end_char_idx": 787579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6159ad8-db64-4ae7-85c8-929baa03bb63": {"__data__": {"id_": "e6159ad8-db64-4ae7-85c8-929baa03bb63", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcd46c2b-93cc-492f-950a-1e7efd34d570", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5bd1c188f79883619720908159c3830be973282f2a348e75c730c99768709936", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be6925c6-1cd0-4cf3-ab20-8c6045c2eb31", "node_type": "1", "metadata": {}, "hash": "68dd93ad34a32a8bcd0c9949f8eebaa1edc29af5ca865e1a79d6ae7e00f5bc3b", "class_name": "RelatedNodeInfo"}}, "text": "1\u20134.\n[5]C. Bienia, S. Kumar, J. P. Singh, and K. Li, \u201cThe PARSEC benchmark\nsuite: characterization and architectural implications,\u201d in Proceedings\nof the 17th International Conference on Parallel Architecture and\nCompilation Techniques (PACT) , 2008, pp. 72\u201381.\n[6]S. Chen, Y . Hu, Y . Zhang, L. Peng, J. Ardonne, S. Irving, and A. Sri-\nvastava, \u201cIncreasing off-chip bandwidth in multi-core processors with\nswitchable pins,\u201d in Proceedings of the 41st International Symposium\non Computer Architecture (ISCA) , 2014, pp. 385\u2013396.\n[7]T. M. Coughlin and J. Handy, \u201cHigher Performance and Capacity with\nOMI Near Memory,\u201d in Proceedings of the 2021 Annual Symposium\non High-Performance Interconnects , 2021, pp. 68\u201371.\n[8]D. I. Cutress and A. Frumusanu, \u201cAmd 3rd gen epyc milan review:\nA peak vs per core performance balance,\u201d 2021. [Online]. Available:\nhttps://www :anandtech :com/show/16529/amd-epyc-milan-review\n[9]CXL Consortium, \u201cCompute Express Link (CXL) Spec-\ni\ufb01cation, Revision 3.0, Version 1.0,\u201d 2022. [On-\nline]. Available: https://www :computeexpresslink :org/_\ufb01les/ugd/\n0c1418_1798ce97c1e6438fba818d760905e43a :pdf\n[10] S. Dulloor, A. Roy, Z. Zhao, N. Sundaram, N. Satish, R. Sankaran,\nJ. Jackson, and K. Schwan, \u201cData tiering in heterogeneous memory\nsystems,\u201d in Proceedings of the 2016 EuroSys Conference , 2016, pp.\n15:1\u201315:16.\n[11] EE Times, \u201cCXL will absorb Gen-Z,\u201d 2021. [Online]. Available:\nhttps://www :eetimes :com/cxl-will-absorb-gen-z/\n[12] Engineering at Meta, \u201c Introducing \u201cYosemite\u201d: the \ufb01rst open\nsource modular chassis for high-powered microservers,\u201d 2015.\n[Online]. Available: https://engineering :fb:com/2015/03/10/core-\ndata/introducing-yosemite-the-\ufb01rst-open-source-modular-chassis-\nfor-high-powered-microservers/\n[13] S. W. Fong, C. M. Neumann, and H.-S. P. Wong, \u201cPhase-change mem-\nory\u2014towards a storage-class memory,\u201d IEEE Transactions on Electron\nDevices , vol. 64, no. 11, pp. 4374\u20134385, 2017.[14] B. Ganesh, A. Jaleel, D. Wang, and B. L. Jacob, \u201cFully-Buffered\nDIMM Memory Architectures: Understanding Mechanisms, Over-\nheads and Scaling,\u201d in Proceedings of the 13th IEEE Symposium on\nHigh-Performance Computer Architecture (HPCA) , 2007, pp. 109\u2013120.\n[15] Google Cloud, \u201cMachine families resource and comparison guide.\u201d\n[Online]. Available: https://cloud :google :com/compute/docs/machine-resource\n[16] D. Gouk, S. Lee, M. Kwon, and M. Jung, \u201cDirect Access, High-\nPerformance Memory Disaggregation with DirectCXL,\u201d in Proceed-\nings of the 2022 USENIX Annual Technical Conference (ATC) , 2022,\npp. 287\u2013294.\n[17] T. J. Ham, B. K. Chelepalli, N. Xue, and B. C. Lee, \u201cDisintegrated\ncontrol for energy-ef\ufb01cient and heterogeneous memory systems,\u201d in\nProceedings of the 19th IEEE Symposium on High-Performance Com-\nputer Architecture (HPCA) , 2013, pp. 424\u2013435.\n[18] K. M. Hazelwood, S. Bird, D. M. Brooks, S. Chintala, U. Diril,\nD. Dzhulgakov, M. Fawzy, B. Jia, Y .", "start_char_idx": 787294, "end_char_idx": 790189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be6925c6-1cd0-4cf3-ab20-8c6045c2eb31": {"__data__": {"id_": "be6925c6-1cd0-4cf3-ab20-8c6045c2eb31", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6159ad8-db64-4ae7-85c8-929baa03bb63", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "271dacab9c39213e0d24f5836bb96ed84a7e06141688502782fd20c159940ee2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3d968bb-a506-48da-a5c8-225f5348d10a", "node_type": "1", "metadata": {}, "hash": "b7fff0e36e1854567a6bdff95d2f7317123d8d5aa0c5e6fb636c2f96af7f7a43", "class_name": "RelatedNodeInfo"}}, "text": "287\u2013294.\n[17] T. J. Ham, B. K. Chelepalli, N. Xue, and B. C. Lee, \u201cDisintegrated\ncontrol for energy-ef\ufb01cient and heterogeneous memory systems,\u201d in\nProceedings of the 19th IEEE Symposium on High-Performance Com-\nputer Architecture (HPCA) , 2013, pp. 424\u2013435.\n[18] K. M. Hazelwood, S. Bird, D. M. Brooks, S. Chintala, U. Diril,\nD. Dzhulgakov, M. Fawzy, B. Jia, Y . Jia, A. Kalro, J. Law, K. Lee, J. Lu,\nP. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang, \u201cApplied Ma-\nchine Learning at Facebook: A Datacenter Infrastructure Perspective,\u201d\ninProceedings of the 24th IEEE Symposium on High-Performance\nComputer Architecture (HPCA) , 2018, pp. 620\u2013629.\n[19] Intel Corporation, \u201cIntel Optane DC Persistent Memory.\u201d [On-\nline]. Available: https://www :intel :com/content/www/us/en/products/\nmemory-storage/optane-dc-persistent-memory :html\n[20] J. Jaffari, A. Ansari, and R. Beraha, \u201cSystems and methods for a hybrid\nparallel-serial memory access,\u201d 2015, US Patent 9747038B2.\n[21] JEDEC, \u201cDDR5 SDRAM standard (JESD79-5B),\u201d 2022.\n[22] D. Jevdjic, S. V olos, and B. Falsa\ufb01, \u201cDie-stacked DRAM caches for\nservers: hit ratio, latency, or bandwidth? have it all with footprint\ncache,\u201d in Proceedings of the 40th International Symposium on Com-\nputer Architecture (ISCA) , 2013, pp. 404\u2013415.\n[23] J. Kim and Y . Kim, \u201cHBM: Memory solution for bandwidth-hungry\nprocessors,\u201d in Hot Chips Symposium , 2014, pp. 1\u201324.\n[24] B. C. Lee, E. Ipek, O. Mutlu, and D. Burger, \u201cArchitecting phase\nchange memory as a scalable dram alternative,\u201d in Proceedings of the\n36th International Symposium on Computer Architecture (ISCA) , 2009,\npp. 2\u201313.\n[25] H. Li, D. S. Berger, S. Novakovic, L. Hsu, D. Ernst, P. Zardoshti,\nM. Shah, S. Rajadnya, S. Lee, I. Agarwal, M. D. Hill, M. Fontoura,\nand R. Bianchini, \u201cPond: CXL-Based Memory Pooling Systems for\nCloud Platforms,\u201d Proceedings of the 28th International Conference\non Architectural Support for Programming Languages and Operating\nSystems (ASPLOS-XXVIII) , 2023.\n[26] S. Li, Z. Yang, D. Reddy, A. Srivastava, and B. L. Jacob, \u201cDRAM-\nsim3: A Cycle-Accurate, Thermal-Capable DRAM Simulator,\u201d IEEE\nComput. Archit. Lett. , vol. 19, no. 2, pp. 110\u2013113, 2020.\n[27] K. T. Lim, J. Chang, T. N. Mudge, P. Ranganathan, S. K. Reinhardt,\nand T. F. Wenisch, \u201cDisaggregated memory for expansion and sharing\nin blade servers,\u201d in Proceedings of the 36th International Symposium\non Computer Architecture (ISCA) , 2009, pp. 267\u2013278.\n[28] S. P. Lloyd, \u201cLeast squares quantization in PCM,\u201d IEEE Trans. Inf.\nTheory , vol. 28, no. 2, pp. 129\u2013136, 1982.\n[29] Locuza, \u201cDie walkthrough: Alder Lake-S/P and a touch of Zen\n3,\u201d 2022. [Online]. Available: https://locuza :substack :com/p/die-\nwalkthrough-alder-lake-sp-and\n[30] G. H. Loh and M. D. Hill, \u201cEf\ufb01ciently enabling conventional block\nsizes for very large die-stacked DRAM caches,\u201d in Proceedings of the\n44th Annual IEEE/ACM International Symposium on Microarchitec-\nture (MICRO) , 2011, pp. 454\u2013464.", "start_char_idx": 789827, "end_char_idx": 792778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3d968bb-a506-48da-a5c8-225f5348d10a": {"__data__": {"id_": "a3d968bb-a506-48da-a5c8-225f5348d10a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be6925c6-1cd0-4cf3-ab20-8c6045c2eb31", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "525e4ec0d4513e878712a5b74913769b5ed244a9c718453bc9c2126654865b34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69544297-4a13-4b41-8ce0-aa956e7bddb3", "node_type": "1", "metadata": {}, "hash": "2237a53883d378827be66b1421a1d01e8c9a7f31a90a79a212c9ff35b75489f6", "class_name": "RelatedNodeInfo"}}, "text": "267\u2013278.\n[28] S. P. Lloyd, \u201cLeast squares quantization in PCM,\u201d IEEE Trans. Inf.\nTheory , vol. 28, no. 2, pp. 129\u2013136, 1982.\n[29] Locuza, \u201cDie walkthrough: Alder Lake-S/P and a touch of Zen\n3,\u201d 2022. [Online]. Available: https://locuza :substack :com/p/die-\nwalkthrough-alder-lake-sp-and\n[30] G. H. Loh and M. D. Hill, \u201cEf\ufb01ciently enabling conventional block\nsizes for very large die-stacked DRAM caches,\u201d in Proceedings of the\n44th Annual IEEE/ACM International Symposium on Microarchitec-\nture (MICRO) , 2011, pp. 454\u2013464.\n[31] K. T. Malladi, F. A. Nothaft, K. Periyathambi, B. C. Lee, C. Kozyrakis,\nand M. Horowitz, \u201cTowards energy-proportional datacenter memory\nwith mobile DRAM,\u201d in Proceedings of the 39th International Sympo-\nsium on Computer Architecture (ISCA) , 2012, pp. 37\u201348.\n[32] Y . Mao, E. Kohler, and R. T. Morris, \u201cCache craftiness for fast multi-\ncore key-value storage,\u201d in Proceedings of the 2012 EuroSys Confer-\nence, 2012, pp. 183\u2013196.\n[33] H. A. Maruf, H. Wang, A. Dhanotia, J. Weiner, N. Agarwal, P. Bhat-\ntacharya, C. Petersen, M. Chowdhury, S. O. Kanaujia, and P. Chauhan,\n\u201cTPP: Transparent Page Placement for CXL-Enabled Tiered Memory,\u201d\nProceedings of the 28th International Conference on Architectural Sup-\nport for Programming Languages and Operating Systems (ASPLOS-\nXXVIII) , 2023.\n[34] J. D. McCalpin, \u201cMemory Bandwidth and Machine Balance in Current\nHigh Performance Computers,\u201d IEEE Computer Society Technical\nCommittee on Computer Architecture (TCCA) Newsletter , 1995.\n[35] Micron Technology Inc., \u201cSystem Power Calculators,\u201d https://\nwww :micron :com/support/tools-and-utilities/power-calc.\n12[36] Micron Technology Inc., \u201cDDR5 SDRAM Datasheet,\u201d 2022. [Online].\nAvailable: https://media-www :micron :com/-/media/client/global/\ndocuments/products/data-sheet/dram/ddr5/ddr5_sdram_core :pdf\n[37] H. Mujtaba, \u201cAMD EPYC Bergamo \u2018Zen 4C\u2019 CPUs Being\nDeployed In 1H 2023 To Tackle Arm CPUs, Instinct MI300\nAPU Back In Labs,\u201d 2022. [Online]. Available: https:\n//wccftech :com/amd-epyc-bergamo-zen-4c-cpus-being-deployed-in-\n1h-2023-tackle-arm-instinct-mi300-apu-back-in-labs/amp/\n[38] H. Mujtaba, \u201cIntel Granite Rapids & Sierra Forest Xeon\nCPU Detailed In Avenue City Platform Leak: Up To 500W\nTDP & 12-Channel DDR5,\u201d 2023. [Online]. Available: https:\n//wccftech :com/intel-granite-rapids-sierra-forest-xeon-cpu-detailed-\nin-avenue-city-platform-leak-up-to-500w-tdp-12-channel-ddr5/\n[39] B. Nitin, W. Randy, I. Shinichiro, F. Eiji, R. Shibata, S. Yumiko, and\nO. Megumi, \u201cDDR5 design challenges,\u201d in 2018 IEEE 22nd Workshop\non Signal and Power Integrity (SPI) , 2018, pp. 1\u20134.\n[40] S. Pal, D. Petrisko, A. A. Bajwa, P. Gupta, S. S. Iyer, and R. Kumar,\n\u201cA Case for Packageless Processors,\u201d in Proceedings of the 24th IEEE\nSymposium on High-Performance Computer Architecture (HPCA) ,\n2018, pp. 466\u2013479.\n[41] J. T. Pawlowski, \u201cHybrid memory cube (HMC),\u201d in Hot Chips Sympo-\nsium , 2011, pp. 1\u201324.", "start_char_idx": 792254, "end_char_idx": 795175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69544297-4a13-4b41-8ce0-aa956e7bddb3": {"__data__": {"id_": "69544297-4a13-4b41-8ce0-aa956e7bddb3", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3d968bb-a506-48da-a5c8-225f5348d10a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "7a81202b1d6928edcd889c4f3a51c37bc3c022afedc3c667c2f2c200cf671ff3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85c1ea8c-277e-422c-bbea-ad64e3d9ccf4", "node_type": "1", "metadata": {}, "hash": "5d94425169060ecb0a4ae9c5e9270e0e6650a233e61580277a76072598acfa59", "class_name": "RelatedNodeInfo"}}, "text": "1\u20134.\n[40] S. Pal, D. Petrisko, A. A. Bajwa, P. Gupta, S. S. Iyer, and R. Kumar,\n\u201cA Case for Packageless Processors,\u201d in Proceedings of the 24th IEEE\nSymposium on High-Performance Computer Architecture (HPCA) ,\n2018, pp. 466\u2013479.\n[41] J. T. Pawlowski, \u201cHybrid memory cube (HMC),\u201d in Hot Chips Sympo-\nsium , 2011, pp. 1\u201324.\n[42] PLDA, \u201cBreaking the PCIe Latency Barrier with CXL,\u201d 2020. [Online].\nAvailable: https://www :brighttalk :com/webcast/18357/434922\n[43] PLDA and AnalogX, \u201cPLDA and AnalogX Announce Market-leading\nCXL 2.0 Solution featuring Ultra-low Latency and Power,\u201d\n2021. [Online]. Available: https://www :businesswire :com/news/\nhome/20210602005484/en/PLDA-and-AnalogX-Announce-Market-\nleading-CXL-2 :0-Solution-featuring-Ultra-low-Latency-and-Power\n[44] M. K. Qureshi and G. H. Loh, \u201cFundamental Latency Trade-off in\nArchitecting DRAM Caches: Outperforming Impractical SRAM-Tags\nwith a Simple and Practical Design,\u201d in Proceedings of the 45th Annual\nIEEE/ACM International Symposium on Microarchitecture (MICRO) ,\n2012, pp. 235\u2013246.\n[45] R. Rooney and N. Koyle, \u201cMicron \u00aeDDR5 SDRAM: New Features,\u201d\nMicron Technology Inc., Tech. Rep , 2019.\n[46] P. Rosenfeld, E. Cooper-Balis, and B. L. Jacob, \u201cDRAMSim2: A\nCycle Accurate Memory System Simulator,\u201d IEEE Comput. Archit.\nLett., vol. 10, no. 1, pp. 16\u201319, 2011.\n[47] D. D. Sharma, \u201cPCI Express \u00ae6.0 Speci\ufb01cation at 64.0 GT/s with PAM-\n4 signaling: a low latency, high bandwidth, high reliability and cost-\neffective interconnect,\u201d in Proceedings of the 2020 Annual Symposium\non High-Performance Interconnects , 2020, pp. 1\u20138.\n[48] D. D. Sharma, \u201cCompute Express Link \u00ae: An open industry-standard\ninterconnect enabling heterogeneous data-centric computing,\u201d in Pro-\nceedings of the 2022 Annual Symposium on High-Performance Inter-\nconnects , 2022, pp. 5\u201312.\n[49] J. Shun and G. E. Blelloch, \u201cLigra: a lightweight graph processing\nframework for shared memory,\u201d in Proceedings of the 18th ACM SIG-\nPLAN Symposium on Principles and Practice of Parallel Programming\n(PPoPP) , 2013, pp. 135\u2013146.[50] Standard Performance Evaluation Corporation, \u201cSPEC CPU2017\nBenchmark Suite.\u201d [Online]. Available: http://www :spec :org/cpu2017/\n[51] P. Stanley-Marbell, V . C. Cabezas, and R. P. Luijten, \u201cPinned to the\nwalls: impact of packaging and application properties on the memory\nand power walls,\u201d in Proceedings of the 2011 International Symposium\non Low Power Electronics and Design , 2011, pp. 51\u201356.\n[52] StorageReview, \u201c4th Gen AMD EPYC Review (AMD Genoa),\u201d 2022.\n[Online]. Available: https://www :storagereview :com/review/4th-gen-\namd-epyc-review-amd-genoa\n[53] TechPowerUp, \u201cAMD \"Matisse\" and \"Rome\" IO Con-\ntroller Dies Mapped Out,\u201d 2020. [Online]. Avail-\nable: https://www :techpowerup :com/266287/amd-matisse-and-rome-\nio-controller-dies-mapped-out\n[54] The Next Platform, \u201cIBM POWER Chips Blur the\nLines to Memory and Accelerators,\u201d 2018. [Online].", "start_char_idx": 794854, "end_char_idx": 797760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85c1ea8c-277e-422c-bbea-ad64e3d9ccf4": {"__data__": {"id_": "85c1ea8c-277e-422c-bbea-ad64e3d9ccf4", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69544297-4a13-4b41-8ce0-aa956e7bddb3", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "7803c483760f3d839e3c2ae8da2b93c65678afbcd4a7b3ce7bf1efddd525d402", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9c97d73-0746-42c5-9d92-1e1a92d40f57", "node_type": "1", "metadata": {}, "hash": "7a61a8bd27a79bd1d8f3c467ec19b1790d45cedb665ad1a37a43a56725c06254", "class_name": "RelatedNodeInfo"}}, "text": "51\u201356.\n[52] StorageReview, \u201c4th Gen AMD EPYC Review (AMD Genoa),\u201d 2022.\n[Online]. Available: https://www :storagereview :com/review/4th-gen-\namd-epyc-review-amd-genoa\n[53] TechPowerUp, \u201cAMD \"Matisse\" and \"Rome\" IO Con-\ntroller Dies Mapped Out,\u201d 2020. [Online]. Avail-\nable: https://www :techpowerup :com/266287/amd-matisse-and-rome-\nio-controller-dies-mapped-out\n[54] The Next Platform, \u201cIBM POWER Chips Blur the\nLines to Memory and Accelerators,\u201d 2018. [Online].\nAvailable: https://www :nextplatform :com/2018/08/28/ibm-\npower-chips-blur-the-lines-to-memory-and-accelerators/#:~:text=\nThe%20Centaur%20memory%20adds%20about\n[55] The Register, \u201cCXL absorbs OpenCAPI on the road to interconnect\ndominance,\u201d 2022. [Online]. Available: https://www :theregister :com/\n2022/08/02/cxl_absorbs_opencapi/\n[56] D. Ustiugov, A. Daglis, J. Picorel, M. Sutherland, E. Bugnion,\nB. Falsa\ufb01, and D. N. Pnevmatikatos, \u201cDesign guidelines for high-\nperformance SCM hierarchies,\u201d in Proceedings of the 2018 Interna-\ntional Symposium on Memory Systems (MEMSYS) , 2018, pp. 3\u201316.\n[57] S. V olos, \u201cMemory Systems and Interconnects for Scale-Out Servers,\u201d\nPh.D. dissertation, EPFL, Switzerland, 2015.\n[58] S. V olos, D. Jevdjic, B. Falsa\ufb01, and B. Grot, \u201cFat Caches for Scale-Out\nServers,\u201d IEEE Micro , vol. 37, no. 2, pp. 90\u2013103, 2017.\n[59] H. Wang, C.-J. Park, G. Byun, J. H. Ahn, and N. S. Kim, \u201cAlloy:\nParallel-serial memory channel architecture for single-chip heteroge-\nneous processor systems,\u201d in Proceedings of the 21st IEEE Symposium\non High-Performance Computer Architecture (HPCA) , 2015, pp. 296\u2013\n308.\n[60] D. H. Yoon, J. Chang, N. Muralimanohar, and P. Ranganathan,\n\u201cBOOM: Enabling mobile memory based low-power server DIMMs,\u201d\ninProceedings of the 39th International Symposium on Computer\nArchitecture (ISCA) , 2012, pp. 25\u201336.\n[61] V . Young, S. Kariyappa, and M. K. Qureshi, \u201cEnabling Transparent\nMemory-Compression for Commodity Memory Systems,\u201d in Proceed-\nings of the 25th IEEE Symposium on High-Performance Computer\nArchitecture (HPCA) , 2019, pp. 570\u2013581.\n[62] Q. Zhu, S. Venkataraman, C. Ye, and A. Chandrasekhar, \u201cPackage\ndesign challenges and optimizations in density ef\ufb01cient (Intel \u00aeXeon \u00ae\nprocessor D) SoC,\u201d in 2016 IEEE Electrical Design of Advanced Pack-\naging and Systems (EDAPS) , 2016.\n13CXL and the Return of Scale-Up Database Engines\nAlberto Lerner\neXascale Infolab\nUniversity of Fribourg, Switzerland\nalberto.lerner@unifr.chGustavo Alonso\nSystems Group, Department of Computer Science\nETH Zurich, Switzerland\nalonso@inf.ethz.ch\nABSTRACT\nThe growing trend towards specialization has led to a proliferation\nof accelerators and alternative processing devices. When embedded\nin conventional computer architectures, the PCIe link connecting\nthe CPU to these devices becomes a bottleneck. Several proposals\nfor alternative designs have been put forward, with these e \ufffforts\nhaving now converged into the Compute Express Link (CXL) spec-\ni\uffffcation. CXL is an interconnect protocol on top of PCIe with a\nmore modern and powerful interface. While still on version 1.0 in\nterms of commercial availability, the potential of CXL to radically\nchange the underlying architecture has already attracted consid-\nerable attention.", "start_char_idx": 797297, "end_char_idx": 800516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9c97d73-0746-42c5-9d92-1e1a92d40f57": {"__data__": {"id_": "b9c97d73-0746-42c5-9d92-1e1a92d40f57", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85c1ea8c-277e-422c-bbea-ad64e3d9ccf4", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "835f81653cd9d3e824593786ce6f620ccd80ba440cc4fee0a5b9173243f7daaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee67b3a0-96ab-4e50-9187-5a8f19546007", "node_type": "1", "metadata": {}, "hash": "e7a675fb636e1da1d75237729c0cf27b069e26a1772af99cafb295fc875edc2d", "class_name": "RelatedNodeInfo"}}, "text": "When embedded\nin conventional computer architectures, the PCIe link connecting\nthe CPU to these devices becomes a bottleneck. Several proposals\nfor alternative designs have been put forward, with these e \ufffforts\nhaving now converged into the Compute Express Link (CXL) spec-\ni\uffffcation. CXL is an interconnect protocol on top of PCIe with a\nmore modern and powerful interface. While still on version 1.0 in\nterms of commercial availability, the potential of CXL to radically\nchange the underlying architecture has already attracted consid-\nerable attention. This attention has been focused mainly on the\npossibility of using CXL to build a shared memory system among\nthe machines in a rack. We argue, however, that such bene \uffffts are\njust the beginning of more signi \uffffcant changes that will have a ma-\njor impact on database engines and data processing systems. In a\nnutshell, while the cloud favored scale-out approaches, CXL brings\nback scale-up architectures. In the paper we describe how CXL\nenables such architectures, and the research challenges associated\nwith the emerging scale-up, heterogeneous hardware platforms.\nKeywords: Database Engines, Memory Management, Heterogenous\nComputing, CXL.\n1 INTRODUCTION\nThe cloud, demanding applications, and an ever larger amount\nof data are driving an increasing specialization in hardware [ 37].\nNowadays, many use cases rely on a wide variety of processors\nother than the CPU: smart NICs, FPGAs, GPUs, TPUs, DPUs, etc.\nIn these settings, the conventional CPU-centric architecture is sub-\noptimal. Data movement is one of the most expensive operations in\na data center [ 11,12], often the CPU is the least powerful element\n[22,44], and the interconnect becomes a major bottleneck [ 19,41].\nParallel to these developments, the cloud has become one of\nthe bigger market drivers for computing hardware. Consequently,\nmany current hardware developments are focused on the needs\nof cloud providers such as disaggregation, cost e \uffffcient use of re-\nsources (CPU, memory, storage), and the particular requirements of\nrunning virtualized environments. In such settings, the limitations\nmentioned above become even more acute, e.g., the bottleneck cre-\nated by many VMs running on the same machine trying to access a\nperipheral or an accelerator (the NIC, the GPU, the local disk, etc.).\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing the authors. Copyright is held by the owner/author(s).\nPreprint, under review.The last years have seen several competing proposals to im-\nprove both the interconnect bandwidth issue as well as the many\narchitectural imbalances in the cloud: CCIX [ 9], Gen-Z [ 14], Open-\nCAPI [ 27], and the Compute Express Link (CXL) [ 31]. Today, all\nthese speci \uffffcations have been merged under the CXL umbrella,\nwhich has become the de-facto standard for future interconnects.\nHowever, CXL is much more than just an interconnect protocol\nthat physically builds on top of PCIe. CXL supports three classes of\ninterfaces. The I/O interface extends the capabilities of PCI without\nmajor changes to its semantics (essentially, copying memory re-\ngions from one device to another in a non-coherent manner). The\nmemory interface allows the CPU (host) to coherently access the\nmemory or storage of peripheral devices. And the cache interface\nallows peripheral devices to coherently access and cache data from\nthe memory of the CPU.\nThe last two interfaces represent a signi \uffffcant change in architec-\nture from the last decades. Cache (memory) coherency allows many\nagents to access memory collectively so that no agent misses each\nother\u2019s memory modi \uffffcations. However, cache coherency meth-\nods are mostly proprietary technologies of CPU vendors and the\napplication or the operating system cannot interact with it in any\nmanner. Furthermore, no agents other than CPU cores can partici-\npate in the protocol, thereby establishing coherency domains as clear\nboundaries outside which memory cannot be accessed collectively.\nThis is one of the big architectural barriers that CXL removes,\nenabling what are called Type 2 devices (e.g., CPUs, GPUs, or FPGAs)\nto directly access the host and each other\u2019s memory in a coherent\nfashion.", "start_char_idx": 799963, "end_char_idx": 804346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee67b3a0-96ab-4e50-9187-5a8f19546007": {"__data__": {"id_": "ee67b3a0-96ab-4e50-9187-5a8f19546007", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9c97d73-0746-42c5-9d92-1e1a92d40f57", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ff8d015c0373f70441763dc62ebbe2e27a88dd786e0790f4b88596c8a0beb1d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01f6c210-ed62-4f33-bee4-b1ef79d81c9c", "node_type": "1", "metadata": {}, "hash": "858e6d5c1e0e8ba95cb99d70a5d3aa81cb79cffb64dbaa8001164ad7e6ade0ba", "class_name": "RelatedNodeInfo"}}, "text": "The last two interfaces represent a signi \uffffcant change in architec-\nture from the last decades. Cache (memory) coherency allows many\nagents to access memory collectively so that no agent misses each\nother\u2019s memory modi \uffffcations. However, cache coherency meth-\nods are mostly proprietary technologies of CPU vendors and the\napplication or the operating system cannot interact with it in any\nmanner. Furthermore, no agents other than CPU cores can partici-\npate in the protocol, thereby establishing coherency domains as clear\nboundaries outside which memory cannot be accessed collectively.\nThis is one of the big architectural barriers that CXL removes,\nenabling what are called Type 2 devices (e.g., CPUs, GPUs, or FPGAs)\nto directly access the host and each other\u2019s memory in a coherent\nfashion. Big as this step is, it not the only one. With version 3.0,\nCXL goes well beyond the traditional role of an interconnect and\nbecomes a rack level networking fabric that is both more performant\nthan current Ethernet based systems (CXL 3.0 claims 64 GBytes/s\nbandwidth versus 100 or 200 Gbits/s in data centers) as well as\nmore powerful in terms of its interface (coherent access, memory\nsharing, encryption, etc., versus simple reliable packet sending and\nreceiving in TCP/IP or data copying in RDMA).\nIn this paper we focus on this capability of CXL and discuss\nhow it enables scale-up (or vertically scaled) database engines that\nwill signi \uffffcantly di \uffffer from the scale-out (horizontally scaled) sys-\ntems that have dominated the landscape in the last years due to\nthe constraints imposed by cloud architectures. We \uffffrst provide\na brief introduction to CXL and then discuss in detail new archi-\ntectural possibilities enabled by novel CXL features as well as the\nmany challenges that will have to be addressed before they can be\nexploited to full advantage.arXiv:2401.01150v1  [cs.DB]  2 Jan 2024Alberto Lerner and Gustavo Alonso\nServerPCIePeripheralmemorycontrollercachecontrollerdirectorycontrollermemorycontroller\nServerCXL\nPeripheralmemorycontrollercachecontrollerdirectorycontrollermemorycontrollercachecontroller\ncxl.memcxl.cacheCPU coherencydomain\nextended coherencydomain\nPCIe MRd andMWr transactionsFigure 1: (left) Peripherals connected using PCIe cards are outside the coherency domain even if they contain memory. (right)\nCXL extends the coherency domain by allowing the peripheral to communicate with the directory controller.\n2 BACKGROUND AND MOTIVATION\nIn this section, we introduce CXL and its basic mechanisms and\ndiscuss the fundamental changes these mechanisms allow.\n2.1 CXL Status\nCXL is a public consortium lead by Intel. It has been evolving\nquickly since its inception in 2019, and so far, there have been\nfour major spec releases: 1.1, 2.0, 3.0, and 3.1. The 1.1 version is\ncentered around local memory expansion, i.e., allowing a server\nto access more memory than available in its DIMM slots. Release\n2.0 introduced basic forms of CXL interconnects (e.g., switches),\nso that memory in a remote chassis is accessible, too, and support\nfor memory pooling. A server can aggregate memory from several\nexpanders and vice-versa. Releases 3.0 and 3.1 added support for\nmore sophisticated networking and for sharing memory across\nmultiple servers and peripherals, rather than the latter expanding\nmemory of the former.\nAt the time of writing, the CXL hardware available is overwhelm-\ningly based on version 1.1, although some vendors have already\nadopted selected features of 2.0 (in what some refer to informally\nas 1.1+). Versions 3.0 and 3.1 require improvements in PCIe, which\nwill be accomplished in the already rati \uffffed Gen 6 of that protocol.\nPresently, one can hardly avoid CXL since all major x86server class\nCPU vendors have adopted it.\n2.2 Multicores and Memory Coherency\nA multicore server is a distributed system where CPU cores share\nthe server\u2019s memory. Each core can cache data in small blocks, called\ncache lines , occasionally duplicating that data. Naturally, issues of\ndata consistency may arise.", "start_char_idx": 803549, "end_char_idx": 807583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01f6c210-ed62-4f33-bee4-b1ef79d81c9c": {"__data__": {"id_": "01f6c210-ed62-4f33-bee4-b1ef79d81c9c", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee67b3a0-96ab-4e50-9187-5a8f19546007", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "38783a3f509c3d556f95eaa0116235cccb0e450821be97dff7dc62adf59a853e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2c4c761-7918-4556-9024-0c7898104281", "node_type": "1", "metadata": {}, "hash": "8fd7dbe9d1dec9498caf1f1429434ddd57bf6d15852d8a7f1417375068dba3f8", "class_name": "RelatedNodeInfo"}}, "text": "At the time of writing, the CXL hardware available is overwhelm-\ningly based on version 1.1, although some vendors have already\nadopted selected features of 2.0 (in what some refer to informally\nas 1.1+). Versions 3.0 and 3.1 require improvements in PCIe, which\nwill be accomplished in the already rati \uffffed Gen 6 of that protocol.\nPresently, one can hardly avoid CXL since all major x86server class\nCPU vendors have adopted it.\n2.2 Multicores and Memory Coherency\nA multicore server is a distributed system where CPU cores share\nthe server\u2019s memory. Each core can cache data in small blocks, called\ncache lines , occasionally duplicating that data. Naturally, issues of\ndata consistency may arise. Cache coherency avoids the issues by\nmaintaining two invariants: (1) writes to the same memory location\nare serialized; and (2) every write is eventually made visible to all\ncores [ 34]. This means that if a core wishes to modify a cached line,\nall other copies residing in di \ufffferent cores must be invalidated \uffffrst.\nTherefore, the system keeps track of the state in which each cached\nline is. Read-only copies are deemed Shared (S), while invalidated\ncopies are called Invalid (I). A unique copy is called Exclusive (E)\nwhile it is still intact, or Modi \uffffed(M) after its contents have been\nupdated w.r.t. main memory. Other states may exist, but these four\nare the most common. Often, coherency protocols announce the\nstates they use. What we described above is a MESI protocol, which\nis what CXL uses.\nIn current computers, only caching done by CPU cores is tracked.\nWe say that the coherency domain extends only to the CPU. Data\ncopies outside this domain are not coherent in the sense that the\nhardware will not keep track of whether that copy is exclusive.\nTherefore, a con \uffffict may arise if one updates it.2.3 Coherent and Non-Coherent Data Transfers\nNon-coherent transfers occur between servers and peripherals\nthrough PCIe exchanges called transactions . A transaction is a se-\nquence of messages in the context of a read or a write operation\nbetween a server\u2019s and a peripheral\u2019s memory. Figure 1 (left) shows\nhow this exchange is done vie PCIe. Note that the communication\ntakes place directly between memory controllers.\nIn contrast, coherent transfers between CPU cores and memory\nare not direct. They involve additional components that track the\ninformation necessary to maintain coherency. To load a line, a CPU\ncore uses a local Cache Controller (CC). In turn, the CC noti \uffffes\na (socket central) component, called a Directory Controller (DC),\nof the intent to access that line. The DC keeps track of all lines\ncurrently cached and can evaluate if, to maintain the invariants,\nthis new request should trigger any cache invalidation. Only after\nthe invalidations are performed can the request be forwarded to the\nmemory controller. Figure 1 (left) also depicts this process, showing\nthat the peripherals are not on the coherency domain.\nCXL extends the type of transactions that can occur between\nservers and peripherals. It is backward compatible with PCIe trans-\nactions, which it wraps under a sub-protocol called cxl.io , and it\nadds two other sub-protocols, cxl.mem andcxl.cache . The server\nuses the former to communicate with the peripheral\u2019s memory\ncontroller as if it resided locally on its motherboard. The periph-\neral uses the latter sub-protocol to cache contents managed by the\nserver\u2019s directory controller. Through these two sub-protocols, any\nmemory the device o \uffffers can be seamlessly incorporated into the\noverall system, and the device can cache data that lives in the sys-\ntem\u2019s memory. Figure 1 (right) depicts this scenario. Due to space\nconstraints, this paper will focus on devices that only use cxl.io\nandcxl.mem , called ( Type 3 ) devices. Devices such as CPUs, GPUs,\nor FPGAs ( Type 2 ) use both cxl.mem andcxl.cache , and some\ndevices use cxl.cache only ( Type 1 ).\n2.4 CXL Performance Characterization\nCurrent conventional NUMA servers typically comprise two sock-\nets, each containing a CPU and half of the system\u2019s DRAM. Such\ntight coupling of CPU and memory is problematic as it prevents\nthe two sides to be scaled independently.", "start_char_idx": 806886, "end_char_idx": 811059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2c4c761-7918-4556-9024-0c7898104281": {"__data__": {"id_": "d2c4c761-7918-4556-9024-0c7898104281", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01f6c210-ed62-4f33-bee4-b1ef79d81c9c", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "7a3e9ad8aa786a32e00760b0ef181987319f8f7138818bf3e5e238bc3355c54f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7db2f6fc-9d06-42ad-8294-ed694d34dba9", "node_type": "1", "metadata": {}, "hash": "66f6294abe5ce890e214e3aec4e2222ff238071697ec0daeed6ac875fa0e03a8", "class_name": "RelatedNodeInfo"}}, "text": "Through these two sub-protocols, any\nmemory the device o \uffffers can be seamlessly incorporated into the\noverall system, and the device can cache data that lives in the sys-\ntem\u2019s memory. Figure 1 (right) depicts this scenario. Due to space\nconstraints, this paper will focus on devices that only use cxl.io\nandcxl.mem , called ( Type 3 ) devices. Devices such as CPUs, GPUs,\nor FPGAs ( Type 2 ) use both cxl.mem andcxl.cache , and some\ndevices use cxl.cache only ( Type 1 ).\n2.4 CXL Performance Characterization\nCurrent conventional NUMA servers typically comprise two sock-\nets, each containing a CPU and half of the system\u2019s DRAM. Such\ntight coupling of CPU and memory is problematic as it prevents\nthe two sides to be scaled independently. CXL can be used to add\ncoherent memory without having to increase the number of sockets\nin the machine. Storage vendors such as Micron and Samsung are\nlaunching a new device category called Memory Expander [23,30].\nA memory expander allows a server to map more memory than\nthe DRAM DIMMs (memory slots) it carries on its motherboard\nby simply plugging in what looks like a special type of SSD device\nthat carries DRAM memory instead. Memory expanders not onlyCXL and the Return of Scale-Up Database Engines\nincrease DRAM size, they also improve memory bandwidth because\nthey e \uffffectively add memory controllers to the system.\nPreliminary benchmarks that use actual CXL implementations\nare already available [ 35]. Regarding latency, executing a load in-\nstruction against a given type of CXL-attached memory can be 35%\nlonger than the equivalent NUMA memory access. Executing a\nstore under the same conditions can present slightly lower but\nequivalent overheads. Regarding bandwidth, it helps to measure the\ne\uffffciency of the transfer, i.e., what percentage of the nominal mem-\nory bandwidth capacity can be achieved. Transfers from NUMA\nnodes can be 70% e \uffffcient when considering only load s, compared\nto 46% e \uffffciency when reading the same type of memory through\na CXL interconnect. Curiously, store s can be more bandwidth ef-\n\uffffcient if directed to a CXL device than a neighbor NUMA socket,\n12% higher. The reason is that stores to a CXL device can bypass\nseveral coherency checks that must occur on a NUMA node.\nThese micro-benchmark results have been complemented with\nmore end-to-end studies. In a recent article by Meta [ 21], CXL mem-\nory is used to store cold pages with the operating system swapping\npages back and forth between the host DRAM and the CXL mem-\nory. In essence, CXL memory is used as an additional memory tier\nbetween the DRAM and storage. The results suggest that the band-\nwidth available from CXL memory will be around 64 GB/s with\nlatency only slightly larger than that of NUMA memory. Similarly,\na study from Microsoft analyzed the impact of CXL memory for\ntheir cloud environment [ 18]. While the results di \uffffer from work-\nload to workload, the study found that under the expected latency\nincreases, some 26% of the 158 workloads studied show less than\n1% performance penalty due to it, and an additional 17% show less\nthan 5%. In contrast, 21% of the workloads were a \uffffected by more\nthan 25% of performance decrease. For database workloads, specif-\nically TPC-H, the overheads are highly query-dependent but are\nmostly below 20%. This analysis already considers CXL switches\nand shared memory among a large number of machines in a rack.\n2.5 CXL as Networking\nThe studies above suggest another disruptive change that CXL\ncan bring about [ 18,21,35]. CXL interconnects\u2014fabrics carrying\ncoherency tra \uffffc across servers\u2014are expected to be signi \uffffcantly\nmore e \uffffcient than traditional networks, even RDMA based ones.\nThe reason is that RDMA requires conversions between the In \uffffni-\nband/RoCE protocols and the PCIe protocol. In other words, a NIC\nuses PCIe to talk to its host but In \uffffniband/RoCE is use to interact\nwith the rest of the system. In contrast, all tra \uffffc in CXL is already\ncarried by PCIe-compatible messages.\nSome studies have attempted to quantify these bene \uffffts [15].", "start_char_idx": 810319, "end_char_idx": 814368, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7db2f6fc-9d06-42ad-8294-ed694d34dba9": {"__data__": {"id_": "7db2f6fc-9d06-42ad-8294-ed694d34dba9", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2c4c761-7918-4556-9024-0c7898104281", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "18869e5462614c54268a5844c01fbcd6e99f1b339cc4fd8c483c07ac4d66f4b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "988e432b-37e4-4c42-9522-717f773fc9ca", "node_type": "1", "metadata": {}, "hash": "0590297fab629c94c7832ba9cc9f5da7927dfcf955ba56aeb257be4597951bc0", "class_name": "RelatedNodeInfo"}}, "text": "2.5 CXL as Networking\nThe studies above suggest another disruptive change that CXL\ncan bring about [ 18,21,35]. CXL interconnects\u2014fabrics carrying\ncoherency tra \uffffc across servers\u2014are expected to be signi \uffffcantly\nmore e \uffffcient than traditional networks, even RDMA based ones.\nThe reason is that RDMA requires conversions between the In \uffffni-\nband/RoCE protocols and the PCIe protocol. In other words, a NIC\nuses PCIe to talk to its host but In \uffffniband/RoCE is use to interact\nwith the rest of the system. In contrast, all tra \uffffc in CXL is already\ncarried by PCIe-compatible messages.\nSome studies have attempted to quantify these bene \uffffts [15].\nThe latency of CXL communications was found to be in the three\nhundred nanoseconds range, while the fastest exchanges in RDMA\ntake at least 2.7 microseconds\u2014a di \ufffference of 8.3 \u21e5. The study also\nhints at a structural advantages that gives CXL more bandwidth\nthan traditional networking: NICs are sub-dimensioned w.r.t. the\nnumber of PCIe lanes they occupy. For instance, a 400 Gbps NIC\n(50 GB/s) will occupy 16 PCIe Gen5 lanes that, in the aggregate,\ncan o \uffffer 64 GB/s [ 24]. Put di \ufffferently, over 20% of the available\nPCIe bandwidth does not translate into network bandwidth. This\ndiscrepancy has been consistent through di \ufffferent network speedsand will likely remain the same for 800 Gbps and 1.6 Tbps NICs\nwhen PCIe Gen 6 and 7 servers are available.\nWe note that with CXL versions 3.0 and 3.1 the networking works\nat a peer-to-peer level as well. Two peer peripherals can access each\nother\u2019s memory independently of the server. Most importantly, CXL\n3.1 brings the concept of Global Integrated Memory . This feature\nallows several servers and peripherals to contribute a region of\nmemory mapped globally, i.e., many servers and peripherals share\nthe same area. For those reasons, CXL is poised to become a much\nbetter way to connect CPUs, memory, and storage, at least on a rack\nlevel. CXL strength is allowing coherency domains that encompass\nfull racks.\n3 SHARED MEMORY ARCHITECTURES\nIn this section we discuss how CXL memory could change the\narchitecture of database engines and the advantages it would bring.\n3.1 Single Machine Memory Expansion\nDatabases maintain a pool of shared memory available to the\nthreads where queries or transactions run. The so called Bu\uffffer\nCache orBu\uffffer Pool is used as an intermediate stage and cache\nbetween persistent storage and the private space used by every\nprocessing thread. In the Bu \uffffer Pool the data is stored and read by\nthe processing threads, with the engine implementing diverse cache\nreplacement policies to minimize the number of page misses and\nthe tra \uffffc to storage. This architecture makes sense in an engine\nthat runs on a single machine. However, it is not without problems:\nthe working space of every thread, the storage needed for indexes\nand auxiliary data structures, and the memory used by the engine\nall compete for space with the Bu \uffffer Pool.\nA simple way to exploit CXL memory follows the tiered memory\napproach pursued by Meta and mentioned above [ 21]. The con-\n\uffffguration used is shown in Figure 2(a) where CXL exposes the\nmemory from a peripheral device (in this case a memory device)\nand seamlessly integrates it with the server\u2019s memory. In such a set\nup, the CXL memory is logically allocated above the local DRAM\nmemory and pages are swapped back and forth as needed. The idea\nis the same as with a disk but using DRAM and faster interconnect\nwhich provides much lower latencies. In a database engine, the\npaging system could be adapted to the needs of data processing\ninstead or relying on general purpose page replacement policies [ 8].\nIn general, CXL used in this way brings the bene \uffffts and challenges\nof a deeper memory hierarchy and existing techniques should be\nrevisited accordingly [20, 33].\nWhile a deeper memory hierarchy would facilitate adoption,\nCXL memory does not need to be treated as block device.", "start_char_idx": 813726, "end_char_idx": 817659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "988e432b-37e4-4c42-9522-717f773fc9ca": {"__data__": {"id_": "988e432b-37e4-4c42-9522-717f773fc9ca", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7db2f6fc-9d06-42ad-8294-ed694d34dba9", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d51f69d81b5296b5cd704bff348130509330aa70ab45975834b7a7ee15d769d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edc4c9cb-a263-4cb3-b6ea-cb2d70c81e4d", "node_type": "1", "metadata": {}, "hash": "9d0ecbb4f60a23e4cac17699bd596938886409fec5c051d0f64609a38eaa8864", "class_name": "RelatedNodeInfo"}}, "text": "The con-\n\uffffguration used is shown in Figure 2(a) where CXL exposes the\nmemory from a peripheral device (in this case a memory device)\nand seamlessly integrates it with the server\u2019s memory. In such a set\nup, the CXL memory is logically allocated above the local DRAM\nmemory and pages are swapped back and forth as needed. The idea\nis the same as with a disk but using DRAM and faster interconnect\nwhich provides much lower latencies. In a database engine, the\npaging system could be adapted to the needs of data processing\ninstead or relying on general purpose page replacement policies [ 8].\nIn general, CXL used in this way brings the bene \uffffts and challenges\nof a deeper memory hierarchy and existing techniques should be\nrevisited accordingly [20, 33].\nWhile a deeper memory hierarchy would facilitate adoption,\nCXL memory does not need to be treated as block device. More\ninteresting con \uffffgurations arise when the local DRAM is used to,\ne.g., store indexes for fast traversal with the data actually in the\nCXL memory where much more capacity can be provided and the\ndata can be accessed on a tuple basis rather than as a block.\nAnother important aspect that is often overlooked is that CXL\nmemory frees up the system from inter-dependencies. In existing\nCPUs, the type of DIMM supported is tied to the CPU architecture.\nAlso, the socket organization of CPUs requires memory capacities\nproportional to the number of sockets and cores. CXL memory\nremoves all these constraints. In terms of capacity, the memory onAlberto Lerner and Gustavo Alonso\nBuffer Area\nServerCXL\nPeripheral\nBuffer Area\nServerCXL\nRemotePeripheral\nCXLBuffer MgrBuffer MgrDisaggregated Rack\nCXL\n(a)(b)(c)Figure 2: CXL allows progressive levels of desegregation: (a) local and (b) far memory expansion, and (c) full-rack disaggregation.\nCXL is not tied to the CPU architecture so one could add DIMMs\ndi\ufffferent from those used in the CPU. It can also be used to simply\nadd more memory capacity independently of the number of cores.\nDoing so o \uffffers more \uffffexible options to con \uffffgure the database by\nvarying the proportion of cores to memory.\nFinally, main memory databases and hybrid transactional/analytic\nsystems could greatly bene \ufffft from the architecture. An interesting\ncon\uffffguration to explore would be to place the transactional work-\nload on the local DRAM and use CXL memory for the analytic part,\nalso providing space for more data as well as structures like data\ncubes, materialized tables, de-normalized tables, etc., without that\na\uffffecting the transactional workload in any way.\nFrom these ideas, several research directions open up:\n\u2022How should the memory extension be handled? As a block device\nwith pagination or as addressable memory just like DRAM on a\nCPU socket? The former makes integration easier but the latter\nis likely to being more performance and design opportunities.\n\u2022Is the memory expansion fast enough for OLTP or will be suitable\nmainly for OLAP? Can it be used to perform both on the same\nmachine and what are the implications?\n\u2022What data structures should be kept in the local memory and\nwhich ones on the memory expansion? Are these data structures\nsuitable for the increases non-uniformity in memory accesses\nthat CXL memory creates?\n3.2 Disaggregated Memory\nThe limitations imposed by the memory available in a given ma-\nchine have been recognized long ago. In the cloud, the problem\nbecomes worse due to what has been called stranded memory : the\nfact that, in the cloud, a machine often runs out of virtual CPUs to\nrent before it runs out memory, resulting in parts of its DRAM being\nunused. Since memory is one of the most expensive components in\ntoday\u2019s data centers, this is a major source of ine \uffffciencies [18].\nIn the context of database engines, how much main memory to\nallocate to the engine is crucial for performance purposes. In the\ncloud this is made worse by the fact that storage is disaggregated\nand swapping of pages involves networking. To minimize this e \uffffect,\nmany cloud providers support special con \uffffgurations for databases\nusing network attached block storage and disks that are much\nfaster than those typically used for conventional applications\u2014of\ncourse, with the corresponding increase in price as these services\nare costlier than typical cloud systems.", "start_char_idx": 816791, "end_char_idx": 821072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edc4c9cb-a263-4cb3-b6ea-cb2d70c81e4d": {"__data__": {"id_": "edc4c9cb-a263-4cb3-b6ea-cb2d70c81e4d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "988e432b-37e4-4c42-9522-717f773fc9ca", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "ac349119959246d3fed446be0fadd4beaab58a4d22e8ce72bdad696f3ac1d931", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12f32004-bdcd-4657-8bcd-053633e52ae1", "node_type": "1", "metadata": {}, "hash": "ead7cf70dfb9500c771d7421a95dbdabc5124ec7af80da99e299a710d674ff08", "class_name": "RelatedNodeInfo"}}, "text": "Since memory is one of the most expensive components in\ntoday\u2019s data centers, this is a major source of ine \uffffciencies [18].\nIn the context of database engines, how much main memory to\nallocate to the engine is crucial for performance purposes. In the\ncloud this is made worse by the fact that storage is disaggregated\nand swapping of pages involves networking. To minimize this e \uffffect,\nmany cloud providers support special con \uffffgurations for databases\nusing network attached block storage and disks that are much\nfaster than those typically used for conventional applications\u2014of\ncourse, with the corresponding increase in price as these services\nare costlier than typical cloud systems. The network overhead and\nthe slow storage needed (for economic reasons) when the data is\nvery large has led over time to additional layers in the system such\nas main memory caches and even specialized solutions with SSDs\nand FPGAs directly attached to the network [4].Recently, researchers have starting exploring ways to provide\nadditional DRAM memory to a sever without necessarily attach-\ning it to this server also in the context of databases [ 2,16,40,43].\nThe ideas has led to the notions of remote memory ,far memory , or\ndisaggregated memory . This is implemented in di \ufffferent ways. Far\nmemory is a term typically used to refer to any generic con \uffffgu-\nration where the additional DRAM is not local. Remote memory\ngenerally refers to utilizing unused memory of other machines.\nDisaggregated memory is memory not allocated to any machine\nbut available for servers to use. In most cases, the data is exchanged\nthrough RDMA and the remote memory is treated as a block device\nwith a paging mechanism to move data back and forth between the\nhost and the far memory.\nAs Figure 2(b) shows, a CXL peripheral can be remote, i.e., it\ncan sit outside the server\u2019s chassis and use the CXL interconnect\nfor integration. This feature allows a server to pool memory from\nseveral CXL devices and, conversely, for a remote device to carve its\nmemory across di \ufffferent servers. In this way, CXL enables disaggre-\ngated memory with the advantage of the memory being coherent\nwhich is not the case when disaggregated memory is implemented\nthrough the network. Through the addition of multi-level switches\nto CXL, CPUs on di \ufffferent machines can access a central memory\nexpander containing a pool of memory available to all machines\nin a rack, very much like disaggregated storage is available in the\ncloud. Such an approach allows to create a large scale memory pool\n(with coherency enforced by hardware) that is far more e \uffffcient\nthat what can be accomplished in a distributed system.\nAn intriguing use of such a con \uffffguration is database migration\nand elasticity across machines. Databases have a large state and\nare not very elastic as they are heavily anchored by the data loaded\nin memory so that the system is reasonably fast. Disaggregated\nmemory implemented through CXL would allow to place the Bu \uffffer\nPool on the disaggregated memory and use the local DRAM just\nfor query processing. If more query processing capacity is needed,\nnew engines can be spawned and connected to the disaggregated\nmemory so that these engines are immediately ready to run queries\nas there is no need to warm up the database. The additional latency\nof CXL memory plays only a minor role in databases and is a\ngood trade-o \uffffin return for far more elasticity than it is feasible\nwith engines where the data resides in local memory. Similarly, a\ndatabase engine can be easily migrated when the bu \uffffer pool is in\ndisaggregated memory. If the data structures and state of the engine\nitself are maintained in disaggregated memory, then migrating the\nentire engine to another machine becomes a far simpler operation.\nFrom a research perspective, interesting questions arise:\n\u2022Implementing these features will require rethinking the internal\ndatabase architecture to remove the assumption that everythingCXL and the Return of Scale-Up Database Engines\nis in local memory. What needs to be local and what can be placed\nin disaggregated memory will require extensive experimentation\nto determine which part of the engine can tolerate the additional\nlatency of CXL memory.\n\u2022With the suitable architectural approach, engines can become\nfar more elastic as pointed out above. Should this be done at the\nlevel of entire engines or can the elasticity be pushed down to\nthe level of threads running queries?\n\u2022Threads running queries could be moved from machine to ma-\nchine by keeping their state and working space in disaggregated\nmemory.", "start_char_idx": 820386, "end_char_idx": 824966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12f32004-bdcd-4657-8bcd-053633e52ae1": {"__data__": {"id_": "12f32004-bdcd-4657-8bcd-053633e52ae1", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edc4c9cb-a263-4cb3-b6ea-cb2d70c81e4d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "1342660d631e6dd18ade993d98510b7a6d66074b2e157c3c7e0a0612567a37c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d66d72e-b250-4661-9673-82c0c8a94b2a", "node_type": "1", "metadata": {}, "hash": "a3aafd6403fcfb6c745503185f457b04c5caf30169b1b7d226cefcb94f73e555", "class_name": "RelatedNodeInfo"}}, "text": "If the data structures and state of the engine\nitself are maintained in disaggregated memory, then migrating the\nentire engine to another machine becomes a far simpler operation.\nFrom a research perspective, interesting questions arise:\n\u2022Implementing these features will require rethinking the internal\ndatabase architecture to remove the assumption that everythingCXL and the Return of Scale-Up Database Engines\nis in local memory. What needs to be local and what can be placed\nin disaggregated memory will require extensive experimentation\nto determine which part of the engine can tolerate the additional\nlatency of CXL memory.\n\u2022With the suitable architectural approach, engines can become\nfar more elastic as pointed out above. Should this be done at the\nlevel of entire engines or can the elasticity be pushed down to\nthe level of threads running queries?\n\u2022Threads running queries could be moved from machine to ma-\nchine by keeping their state and working space in disaggregated\nmemory. Alternatively, they can be created as the workload\ndictates. How would an engine operate under a dynamically\nchanging multiprogramming level?\n3.3 Shared Memory = Scale Up\nDistributed databases are a typical way to implement larger sys-\ntems [ 1,28,39]. However, the architecture of the engines discussed\nabove does not scale as expected: cache invalidation now crosses\nmachines; updates imply distributed, more onerous locks; there is\nwasted memory as the same data is copied into the local bu \uffffer cache\nof several machines, etc. These problems are addressed through a\nmix of replication and sharding, increasingly using RDMA to reduce\nthe network overhead [ 5,6,36,42]. Invariably a fragile balance is\nreached between consistency, symmetry in the system (e.g., with\nread-only copies), and data placement to minimize data movement,\nthereby limiting architectural freedom (e.g., [38]).\nCXL supports building a larger database in a truly scaled-up man-\nner rather than through distribution. This is thanks to CXL\u2019s ability\nto integrate devices via a coherency domain that can encompass the\nentire rack. In particular, CXL supports Global Integrated Memory\n(GIM), where each system component contributes a range of its\nown memory to the collective memory, rather than having each\nmachine expand its memory by monopolizing it out of a pool. Since\nnow each server plus the memory expanders can share memory, the\nboundaries of the machines in a rack get blurred. The entire rack\ncan be seen as a single machine. Figure 2(c) depicts this scenario.\nRack-level integration is arguably the most impactful change\nCXL enables. It liberates the database system from managing con-\nsistency across servers by moving the memory uni \uffffcation e \uffffort to\nhardware. Moreover, as discussed above, CXL o \uffffers a bandwidth\nfar larger than what is available with today\u2019s networks and with far\nlower latency. The result is a \u201cblank canvas\u201d for database architects\nwith almost none of the disadvantages of the previous scalabil-\nity methods. We expect this to cause a radical change in the way\nscalable databases and data processing engines are designed.\nPromising as the prospects are, the road ahead is not without\nchallenges. The last two decades have veered away from scale-up\nsystems, focusing instead on cloud-native and scale-out approaches.\nMoreover, many existing large systems carry biases that do not exist\nanymore, e.g., that networks are slow or that I/O is prohibitively\nexpensive. A fully disaggregated system like the one CXL enables\nbreaks new ground regarding the algorithms and data structures.\nHere are some of the questions it opens:\n\u2022Since our fundamental data operations are built around hashing\nand sorting, do we know how to conduct these operations on a\nrack-level scale? Do we fully understand when to use which?\u2022Given that each core now can access one to two orders of magni-\ntude more memory than before, are the data structures we use to\norganize and index the data still e \uffffective at these new scales? In\nparticular, how is the coherency tra \uffffcgenerated by a typical data\nstructure? Given that the invalidation messages can dominate\naccess time, can it be improved?\n\u2022Assuming we now have the freedom to engage a tremendous\namount of resources to solve individual query operators, how do\nwe schedule the machine resources across competing queries?\nThe bene \uffffts of answering these questions are signi \uffffcant. For\ninstance, consider transactional workloads.", "start_char_idx": 823974, "end_char_idx": 828422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d66d72e-b250-4661-9673-82c0c8a94b2a": {"__data__": {"id_": "6d66d72e-b250-4661-9673-82c0c8a94b2a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12f32004-bdcd-4657-8bcd-053633e52ae1", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "5aa4212f00815ebf56af493a16aef8850eae6c34083e03c0ebe76e7193081633", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2c314d4-700e-42a8-b403-662f0008cdaa", "node_type": "1", "metadata": {}, "hash": "38d951644ee41cc75e8484d617c14d43122a89292a3da1b5a0717c94f02ab67e", "class_name": "RelatedNodeInfo"}}, "text": "Do we fully understand when to use which?\u2022Given that each core now can access one to two orders of magni-\ntude more memory than before, are the data structures we use to\norganize and index the data still e \uffffective at these new scales? In\nparticular, how is the coherency tra \uffffcgenerated by a typical data\nstructure? Given that the invalidation messages can dominate\naccess time, can it be improved?\n\u2022Assuming we now have the freedom to engage a tremendous\namount of resources to solve individual query operators, how do\nwe schedule the machine resources across competing queries?\nThe bene \uffffts of answering these questions are signi \uffffcant. For\ninstance, consider transactional workloads. By keeping the data in\none (large) location, transactional updates can be done against a\ncentralized bu \uffffer cache, rather than across a distributed system.\nSimilarly, many of the data structures the database has to main-\ntain can now be centralized in the disaggregated memory instead\nneeding to be synchronized across the machines involved.\n4 NEAR-DATA PROCESSING\nCXL indirectly enables another set of important features beyond in-\ntegrating local, far, and disaggregated DRAM. To understand these\nfeatures, it helps to look into how a CXL device is implemented\u2014\nand a comparison to Persistent Memory (PMem) devices is relevant\nhere. Persistent Memory solutions like Intel\u2019s Optane carry a com-\nplex controller that is embedded into the PMem DIMM. In contrast,\nCXL support involves wrapping existing DRAM DIMMs (or other\ntypes of memory) within a specialized hardware memory controller\n(cf. Figure 1 (right)) using a variety of possible PCIe device form\nfactors. The device\u2019s controller, which is transparent to the appli-\ncation, must implement the CXL protocol and potentially manage\nseveral logical devices carved out of the physical one, to cite one\nfunctionality. The controller will likely utilize a processor that im-\nplements the protocol, performs tra \uffffc management, and arbitrates\nthe devices\u2019 requests. The CXL memory controller is likely to be\nrather sophisticated, probably in the same way that SSD controllers\nare far more than simple data relays to \uffffash storage.\nWe argue that a sophisticated controller, potentially using a\nspecialized processor, opens a tremendous opportunity: part of the\nCXL controller\u2019s computing power can be co-opted to perform near-\ndata processing. Near-data processing is known to optimize away\nunnecessary data movements, making the entire architecture far\nmore e \uffffcient than one that simply integrates memory. Putting it\ndi\ufffferently, a CXL controller can functionally behave as a smart NIC,\nfollowing the idea that CXL could replace the network. However,\nunlike smart NICs, the CXL controller will be sitting not only next\nto compute devices (CPUs, FPGAs, GPUs) but also close to memory\nand storage. Figure 3 (top) depicts this scenario.\nThis idea is very much aligned with current trends. In research\nthere have been e \ufffforts to, e.g., explore the advantages for databases\nof placing a small processor near memory [ 13] or in disaggregated\nmemory [ 16]. Oracle had a processor (SPARC S7) with Data Analyt-\nics Accelerators (DAX) placed between the cores and the memory\nwhere basic relational operators could be o \uffffoaded to \ufffflter data be-\nfore it hits the processor caches [ 29]. Recently, Amazon developed\nAQUA, a network attached SSD caching layer for Redshift that used\nan FPGA to o \uffffoad relational operators to the caching layer [4].Alberto Lerner and Gustavo Alonso\nServerCXL\nAcceleratorQuery Exec\nQuery Exec\nLock TableQuery data\nServerCXL\nAccelerator w/ VirtualizationQuery Exec\nQuery Accel\nLock TableQuery data\nFigure 3: Near-data processing under CXL. (top) A processor\nor FPGA managing the expanded memory can be co-opted to\nexecute a portion of a query. (bottom) A unique opportunity\nfor acceleration exists through virtual memory regions.\nCXL o \uffffers a way to better implement these ideas but not without\nchallenges, some of which we list below:\n\u2022How to perform query processing near the data? This possibility\nis not new, as several query operators have been shown to have\nnear-data implementations.", "start_char_idx": 827736, "end_char_idx": 831876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2c314d4-700e-42a8-b403-662f0008cdaa": {"__data__": {"id_": "f2c314d4-700e-42a8-b403-662f0008cdaa", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d66d72e-b250-4661-9673-82c0c8a94b2a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "e99ae8a7ff2f06b10829001747a14ce7ec332d890e983fb9b1da3f5731ef7346", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6dbd063-7965-4b57-943c-7c8008782357", "node_type": "1", "metadata": {}, "hash": "063daec962b9327b2cbca3ab450d56b01f0911e85a778954f6e313e5818ca393", "class_name": "RelatedNodeInfo"}}, "text": "Recently, Amazon developed\nAQUA, a network attached SSD caching layer for Redshift that used\nan FPGA to o \uffffoad relational operators to the caching layer [4].Alberto Lerner and Gustavo Alonso\nServerCXL\nAcceleratorQuery Exec\nQuery Exec\nLock TableQuery data\nServerCXL\nAccelerator w/ VirtualizationQuery Exec\nQuery Accel\nLock TableQuery data\nFigure 3: Near-data processing under CXL. (top) A processor\nor FPGA managing the expanded memory can be co-opted to\nexecute a portion of a query. (bottom) A unique opportunity\nfor acceleration exists through virtual memory regions.\nCXL o \uffffers a way to better implement these ideas but not without\nchallenges, some of which we list below:\n\u2022How to perform query processing near the data? This possibility\nis not new, as several query operators have been shown to have\nnear-data implementations. Compression and decompression,\nencryption and decryption, selection, projection, \uffffltering with\nLIKE predicates, and a wide range of other relational operators\nthat have been demonstrated to bring substantial advantages in\npractice [4, 7, 13, 16].\n\u2022Do we leverage the controller to implement the functionality or\ndo we attach an accelerator in the CXL path? How does such a\ncontroller or such an accelerator look like?\n\u2022If the lock table is also placed in the shared memory region, then\neven updates to common data structures from both sides of the\nquery could be performed. What are suitable data structures\nthat can be manipulated by both \u201csides?\u201d In other words, are the\ninvalidation tra \uffffc of traditional data structures acceptable, if\nthey are updated by several processors?\n\u2022Lastly, a very intriguing idea is to use the CXL controller to\nimplement a virtual memory region that does not correspond to\nactual content but to a service that takes data from the memory\nand transforms it before sending it to the requester to make\nit look like it was stored in memory all along. Figure 3 (bot-\ntom) illustrates such a use case. In databases this can be used\nto implement, e.g., view materialization on-the- \uffffy; data type\ntransformations; data transpositions from column to row, row to\ncolumn, or matrix transpositions [ 16]; on-the- \uffffy data cubes and\nstatistical summaries; or even to better integrate databases and\nsmart storage systems [17].\n5 HETEROGENEOUS ARCHITECTURES\nGPUs, TPUs, DPUs, smart NICs, and FPGAs con \uffffgured to perform\ndi\ufffferent types of near-data processing or processing in memory\nacceleration have been shown to be advantageous in several real\nsystems scenarios. If anything, this trend has exposed many of the\nlimitations of CPU-centric computer architectures and, as discussed,\nthe limitations of existing interconnects. With around 64 GB/s\nbandwidth per slot and a typical server with 4 to 6 slots, CXL enables\nthe creation of a computing platform much more \uffffexible than a\nrack of servers with shared memory. A federation of heterogeneous\nprocessing nodes can now operate on a unique coherency domain.This composability possibility that CXL creates opens a research\n\uffffeld of its own: if computational devices are independently con-\nnected, what should a heterogeneous machine look like? Can we\n(and should we) build machines that accommodate speci \uffffc work-\nloads better than others? For instance, Machine Learning (ML) is\ntaxing database engines because the data often has to be taken out\nof the database to run it through ML tools. With a heterogeneous\narchitecture that seamlessly integrates CPUs and GPUs, it becomes\npossible to implement ML operators directly on the database engine\nwhile still taking advantage of suitable hardware. This will require\nchanges to the engine design and architecture. However, given the\npowerful compute fabric that CXL enables, it will likely lead to a\nnew generation of scale-up database engines.\n6 RELATED EFFORTS\nCXL is, in no small part, the result of consolidating several projects\nthat came before it, most notably CCIX [ 9], GenZ [ 14], and Open-\nCAPI [ 27]. An overwhelming number of institutions and companies\nare working together to advance the protocol [ 10]. The consolida-\ntion, however, has not been complete. Some proprietary memory\ncoherency interconnects still exist, mainly involving GPGPU ven-\ndors. AMD supports an interconnect called In \uffffnity Architecture\nacross its GPGPUs [ 3].", "start_char_idx": 831046, "end_char_idx": 835333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6dbd063-7965-4b57-943c-7c8008782357": {"__data__": {"id_": "e6dbd063-7965-4b57-943c-7c8008782357", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2c314d4-700e-42a8-b403-662f0008cdaa", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d19ced3b83f71c6dda960e6a76a4d8da41075ceb683098dcf9ee91f963ea82e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f966a2c-1a1b-4965-886f-04e342cbc4be", "node_type": "1", "metadata": {}, "hash": "f9eb11ae06c02051753ac3d694568b638e62b33a80283b5f6d658c6be7d5ce10", "class_name": "RelatedNodeInfo"}}, "text": "This will require\nchanges to the engine design and architecture. However, given the\npowerful compute fabric that CXL enables, it will likely lead to a\nnew generation of scale-up database engines.\n6 RELATED EFFORTS\nCXL is, in no small part, the result of consolidating several projects\nthat came before it, most notably CCIX [ 9], GenZ [ 14], and Open-\nCAPI [ 27]. An overwhelming number of institutions and companies\nare working together to advance the protocol [ 10]. The consolida-\ntion, however, has not been complete. Some proprietary memory\ncoherency interconnects still exist, mainly involving GPGPU ven-\ndors. AMD supports an interconnect called In \uffffnity Architecture\nacross its GPGPUs [ 3]. NVidia has alternative interconnect tech-\nnology in the form of NVLink [25] and NVlink-C2C [26].\nThe argument for developing these highly specialized intercon-\nnects, especially for GPGPUs, is that they can provide (a) a much\nhigher bandwidth than what is possible with PCIe or the network\nand (b) a uni \uffffed memory between the host and the accelerator to\nbe able to exchange pointers and to avoid having to pin memory\nwhen exchanging data. While these arguments may have been valid\nin older versions of PCIe, the latter standard has been upgraded\nat an unprecedented pace. PCIe Gen 7, expected to be available in\n2025, will support 128MT/s per lane, i.e., 242GB/s in a \u21e516 card [ 32].\nEven if the proprietary interconnect improved their bandwidth,\nthey would still carry the usual drawbacks: outside of the few\nCPU/GPGPUs they support, they bring ine \uffffcient memory use, data\ncopying overheads, consistency issues, etc. It remains to be seen\nhow these competing standards will evolve, merge, or co-exists.\n7 CONCLUSION\nIn this paper, we argued that the availability of CXL technology\nwill upend at least two decades of investments in scale-out data-\nbase systems. The reason is that, through a quite natural memory\nintegration, CXL allows database systems to scale up instead. The\nappeal of this direction change is that it turns what is now complex\ndistributed system development into familiar centralized system\ndevelopment instead. We discussed how CXL supports a range of\ndisaggregated\u2014but coherent!\u2014memory settings in a system, from\nlocal- to far-memory expansion to full rack-level disaggregation. At\neach of these steps, we presented several research questions these\nchanges open, along with the bene \uffffts of addressing them. Lastly,\nwe argued that CXL can also support near-data processing and\nheterogeneous platforms in uncomplicated ways never available\nbefore. Given the magnitude of these possibilities, we expect CXL to\nfoster the design of an entirely new generation of database systems\nwith unprecedented scalability, e \uffffciency, and integration.CXL and the Return of Scale-Up Database Engines\nREFERENCES\n[1] Josep Aguilar-Saborit et al .2020. POLARIS: The Distributed SQL Engine in Azure\nSynapse. Proc. VLDB Endow. 13, 12 (2020), 3204\u20133216. https://doi.org/10.14778/\n3415478.3415545\n[2] Marcos K. Aguilera, Emmanuel Amaro, Nadav Amit, Erika Hunho \uffff, Anil Yelam,\nand Gerd Zellweger. 2023. Memory disaggregation: why now and what are the\nchallenges. ACM SIGOPS Oper. Syst. Rev. 57, 1 (2023). https://doi.org/10.1145/\n3606557.3606563\n[3] AMD. [n.d.]. In \uffffnity Architecture: A New Era in Accelerated System Connectiv-\nity. https://www.amd.com/en/technologies/in \uffffnity-architecture.\n[4] Je\uffffBarr. 2021. AQUA (Advanced Query Accelerator) \u2013 A Speed Boost for Your Ama-\nzon Redshift Queries . https://aws.amazon.com/blogs/aws/new-aqua-advanced-\nquery-accelerator-for-amazon-redshift/\n[5] Claude Barthels, Ingo M\u00fcller, Konstantin Taranov, Gustavo Alonso, and Torsten\nHoe\uffffer. 2019.", "start_char_idx": 834635, "end_char_idx": 838317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f966a2c-1a1b-4965-886f-04e342cbc4be": {"__data__": {"id_": "0f966a2c-1a1b-4965-886f-04e342cbc4be", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6dbd063-7965-4b57-943c-7c8008782357", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "22b662bbc4757f6b170f0b30c71eab127b5843b4b3c9b00fe08c7251c91ca5e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80f791aa-a554-4d8b-9d41-4a99a3dfdf02", "node_type": "1", "metadata": {}, "hash": "52197bf6c70b9df653174dc1543425fa75f3709ca84e80a36ecde927012fa415", "class_name": "RelatedNodeInfo"}}, "text": "ACM SIGOPS Oper. Syst. Rev. 57, 1 (2023). https://doi.org/10.1145/\n3606557.3606563\n[3] AMD. [n.d.]. In \uffffnity Architecture: A New Era in Accelerated System Connectiv-\nity. https://www.amd.com/en/technologies/in \uffffnity-architecture.\n[4] Je\uffffBarr. 2021. AQUA (Advanced Query Accelerator) \u2013 A Speed Boost for Your Ama-\nzon Redshift Queries . https://aws.amazon.com/blogs/aws/new-aqua-advanced-\nquery-accelerator-for-amazon-redshift/\n[5] Claude Barthels, Ingo M\u00fcller, Konstantin Taranov, Gustavo Alonso, and Torsten\nHoe\uffffer. 2019. Strong consistency is not hard to get: Two-Phase Locking and\nTwo-Phase Commit on Thousands of Cores. Proc. VLDB Endow. 12, 13 (2019).\nhttps://doi.org/10.14778/3358701.3358702\n[6] Carsten Binnig, Andrew Crotty, Alex Galakatos, Tim Kraska, and Erfan Zamanian.\n2016. The End of Slow Networks: It\u2019s Time for a Redesign. Proc. VLDB Endow. 9,\n7 (2016). https://doi.org/10.14778/2904483.2904485\n[7]Monica Chiosa, Fabio Maschi, Ingo M\u00fcller, Gustavo Alonso, and Norman May.\n2022. Hardware Acceleration of Compression and Encryption in SAP HANA. Proc.\nof the VLDB Endowment 15, 12 (2022). https://doi.org/10.14778/3554821.3554822\n[8] Hong-Tai Chou and David J. DeWitt. 1985. An Evaluation of Bu \uffffer Management\nStrategies for Relational Database Systems. In VLDB\u201985, Proceedings of 11th\nInternational Conference on Very Large Data Bases, August 21-23, 1985, Stockholm,\nSweden . https://doi.org/10.1007/BF01840450\n[9] CCIX Consortium. [n.d.]. An Introduction to CCIX. https://www.ccixconsortium.\ncom/wp-content/uploads/2019/11/CCIX-White-Paper-Rev111219.pdf.\n[10] CXL. [n.d.]. Consortium Member List. https://www.computeexpresslink.org/\nmembers.\n[11] Bill Dally. 2011. Power, programmability, and granularity: The challenges of\nexascale computing. In 2011 IEEE International Test Conference . IEEE Computer\nSociety, 12\u201312. https://doi.org/10.1109/IPDPS.2011.420\n[12] William J Dally, Yatish Turakhia, and Song Han. 2020. Domain-speci \uffffc hardware\naccelerators. Commun. ACM 63, 7 (2020), 48\u201357. https://doi.org/10.1145/3361682\n[13] Yuanwei Fang, Chen Zou, and Andrew A. Chien. 2019. Accelerating Raw Data\nAnalysis with the ACCORDA Software and Hardware Architecture. Proceedings\nof the VLDB Endowment 12, 11 (2019). https://doi.org/10.14778/3342263.3342634\n[14] GenZ. [n.d.]. GenZ Archive. https://www.computeexpresslink.org/projects-3.\n[15] Donghyun Gouk, Sangwon Lee, Miryeong Kwon, and Myoungsoo Jung. 2022.\nDirect Access, High-Performance Memory Disaggregation with DirectCXL\n(USENIX ATC\u201922\u2019) . https://www.usenix.org/conference/atc22/presentation/gouk\n[16] Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin Taranov,\nDejan S. Milojicic, and Gustavo Alonso. 2022. Farview: Disaggregated Memory\nwith Operator O \uffff-loading for Database Engines. In 12th Conference on Innovative\nData Systems Research, CIDR 2022, Chaminade, CA, USA, January 9-12, 2022 .", "start_char_idx": 837795, "end_char_idx": 840672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80f791aa-a554-4d8b-9d41-4a99a3dfdf02": {"__data__": {"id_": "80f791aa-a554-4d8b-9d41-4a99a3dfdf02", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f966a2c-1a1b-4965-886f-04e342cbc4be", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8b2256137e366ec3606e2eca4dca0c2ffb7c0d160f435dff01ab0eb63b72e70b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d72d4d7e-6c62-486c-a766-c62aa4220b80", "node_type": "1", "metadata": {}, "hash": "01b3289d7454978a26be2a3bed55286712e2ce5bd29a2135a2b464562f013f11", "class_name": "RelatedNodeInfo"}}, "text": "[n.d.]. GenZ Archive. https://www.computeexpresslink.org/projects-3.\n[15] Donghyun Gouk, Sangwon Lee, Miryeong Kwon, and Myoungsoo Jung. 2022.\nDirect Access, High-Performance Memory Disaggregation with DirectCXL\n(USENIX ATC\u201922\u2019) . https://www.usenix.org/conference/atc22/presentation/gouk\n[16] Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin Taranov,\nDejan S. Milojicic, and Gustavo Alonso. 2022. Farview: Disaggregated Memory\nwith Operator O \uffff-loading for Database Engines. In 12th Conference on Innovative\nData Systems Research, CIDR 2022, Chaminade, CA, USA, January 9-12, 2022 .\nhttps://www.cidrdb.org/cidr2022/papers/p11-korolija.pdf\n[17] Sangjin Lee, Alberto Lerner, Philippe Bonnet, and Philippe Cudr\u00e9-Mauroux.\n2024. Database Kernels: Seamless Integration of Database Systems and Fast\nStorage via CXL. In 14th Conference on Innovative Data Systems Research, CIDR\n2024, Chaminade, CA, USA, January 9-12, 2022 . http://exascale.info/assets/pdf/\nlee2024cidr.pdf\n[18] Huaicheng Li, Daniel S. Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko\nNovakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, Mark D.\nHill, Marcus Fontoura, and Ricardo Bianchini. 2023. Pond: CXL-Based Memory\nPooling Systems for Cloud Platforms (ASPLOS 2023) . https://doi.org/10.1145/\n3575693.3578835\n[19] Clemens Lutz, Sebastian Bre\u00df, Ste \uffffen Zeuch, Tilmann Rabl, and Volker Markl.\n2020. Pump Up the Volume: Processing Large Data on GPUs with Fast Intercon-\nnects (SIGMOD\u201920) . https://doi.org/10.1145/3318464.3389705\n[20] Stefan Manegold, Peter A. Boncz, and Martin L. Kersten. 2000. Optimizing\nDatabase Architecture for the New Bottleneck: Memory Access. The VLDB\nJournal 9, 3 (dec 2000). https://doi.org/10.1007/s007780000031\n[21] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agar-\nwal, Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanau-\njia, and Prakash Chauhan. 2023. TPP: Transparent Page Placement for CXL-\nEnabled Tiered-Memory (ASPLOS 2023) . https://doi.org/10.1145/3582016.3582063[22] Fabio Maschi and Gustavo Alonso. 2023. The Di \uffffcult Balance Between Modern\nHardware and Conventional CPUs (DaMoN\u201923) . https://doi.org/10.1145/3592980.\n3595314\n[23] Micron. [n.d.]. Flexible memory capacity expansion for data intensive workloads.\nhttps://www.micron.com/solutions/server/cxl.\n[24] NVidia. [n.d.]. NVidia ConnectX-7 400G Ethernet. https://www.nvidia.com/\ncontent/dam/en-zz/Solutions/networking/ethernet-adapters/connectx-7-\ndatasheet-Final.pdf.\n[25] NVidia. [n.d.]. NVLink and NVSwitch: The building blocks of advanced multi-\nGPU communication\u2014within and between servers. https://www.nvidia.com/en-\nus/data-center/nvlink/.\n[26] NVIDIAG. [n.d.]. NVIDIA Opens NVLink for Custom Silicon Integra-\ntion. https://nvidianews.nvidia.com/news/nvidia-opens-nvlink-for-custom-\nsilicon-integration.\n[27] OpenCAPI. [n.d.]. OpenCAPI Archive. https://www.computeexpresslink.org/occ-\narchive.\n[28] Oracle. [n.d.]. Why Oracle Exadata platforms are the best for Oracle Database.", "start_char_idx": 840072, "end_char_idx": 843099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d72d4d7e-6c62-486c-a766-c62aa4220b80": {"__data__": {"id_": "d72d4d7e-6c62-486c-a766-c62aa4220b80", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80f791aa-a554-4d8b-9d41-4a99a3dfdf02", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "68697313275b9bcfd61f96a882ae3ba3e319401f605c63590b2d5fc7701b6342", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37dc5d59-786a-4d97-a00c-decef3c14044", "node_type": "1", "metadata": {}, "hash": "da290f29dfd1a08e479d34a5f669030c8aa9917868f0744a92b782fae20cf6cf", "class_name": "RelatedNodeInfo"}}, "text": "https://www.nvidia.com/\ncontent/dam/en-zz/Solutions/networking/ethernet-adapters/connectx-7-\ndatasheet-Final.pdf.\n[25] NVidia. [n.d.]. NVLink and NVSwitch: The building blocks of advanced multi-\nGPU communication\u2014within and between servers. https://www.nvidia.com/en-\nus/data-center/nvlink/.\n[26] NVIDIAG. [n.d.]. NVIDIA Opens NVLink for Custom Silicon Integra-\ntion. https://nvidianews.nvidia.com/news/nvidia-opens-nvlink-for-custom-\nsilicon-integration.\n[27] OpenCAPI. [n.d.]. OpenCAPI Archive. https://www.computeexpresslink.org/occ-\narchive.\n[28] Oracle. [n.d.]. Why Oracle Exadata platforms are the best for Oracle Database.\nhttps://www.oracle.com/engineered-systems/exadata/.\n[29] Oracle. 2015. SPARC S7 Processor. https://www.oracle.com/a/ocom/docs/servers/\nsparc/sparc-s7-processor-ds-3042417.pdf.\n[30] Samsung. [n.d.]. Samsung Electronics Introduces Industry\u2019s First 512GB\nCXL Memory Module. https://news.samsung.com/global/samsung-electronics-\nintroduces-industrys- \uffffrst-512gb-cxl-memory-module.\n[31] Debendra Das Sharma. [n.d.]. Compute Express Link. https://docs.wixstatic.\ncom/ugd/0c1418_d9878707bbb7427786b70c3c91d5fbd1.pdf.\n[32] PCIe SIG. [n.d.]. Announcing the PCIe 7.0 Speci \uffffcation. https:\n//pcisig.com/blog/announcing-pcie%C2%AE-70-speci \uffffcation-doubling-\ndata-rate-128-gts-next-generation-computing.\n[33] Utku Sirin, Pinar T\u00f6z\u00fcn, Danica Porobic, and Anastasia Ailamaki. 2016. Micro-\nArchitectural Analysis of In-Memory OLTP (SIGMOD \u201916) . https://doi.org/10.\n1145/2882903.2882916\n[34] Daniel J Sorin, Mark D Hill, and David A Wood. 2011. A Primer on Memory\nConsistency and Cache Coherence . Morgan & Claypool Publishers. https://doi.\norg/10.1007/978-3-031-01764-3\n[35] Yan Sun, Yifan Yuan, Zeduo Yu, Reese Kuper, Chihun Song, Jinghan Huang,\nHouxiang Ji, Siddharth Agarwal, Jiaqi Lou, Ipoom Jeong, Ren Wang, Jung Ho\nAhn, Tianyin Xu, and Nam Sung Kim. 2023. Demystifying CXL Memory with\nGenuine CXL-Ready Systems and Devices (MICRO\u201923) . https://doi.org/10.1145/\n3613424.3614256\n[36] Yacine Taleb, Ryan Stutsman, Gabriel Antoniu, and Toni Cortes. 2018. Tailwind:\nFast and Atomic RDMA-Based Replication (USENIX ATC\u201918) . https://www.\nusenix.org/conference/atc18/presentation/taleb\n[37] Neil C. Thompson and Svenja Spanuth. 2021. The Decline of Computers as a\nGeneral Purpose Technology. Commun. ACM 64, 3 (feb 2021). https://doi.org/\n10.1145/3430936\n[38] Alexandre Verbitski et al .2018. Amazon Aurora: On Avoiding Distributed\nConsensus for I/Os, Commits, and Membership Changes (SIGMOD \u201918) . https:\n//doi.org/10.1145/3183713.3196937\n[39] Alexandre Verbitski, Anurag Gupta, Debanjan Saha, Murali Brahmadesam, Ka-\nmal Gupta, Raman Mittal, Sailesh Krishnamurthy, Sandor Maurice, Tengiz\nKharatishvili, and Xiaofeng Bao. 2017. Amazon Aurora: Design Considera-\ntions for High Throughput Cloud-Native Relational Databases (SIGMOD\u201917) .", "start_char_idx": 842470, "end_char_idx": 845317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37dc5d59-786a-4d97-a00c-decef3c14044": {"__data__": {"id_": "37dc5d59-786a-4d97-a00c-decef3c14044", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d72d4d7e-6c62-486c-a766-c62aa4220b80", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "6b5544be409181a4018267a6c54af9a89cd1efc774329897e4a6420ca329be62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45a5ba89-93bb-4830-ab19-9eeb929975aa", "node_type": "1", "metadata": {}, "hash": "e8449a7a16c9c3aa1fb92313f2afac4dbcf0527fa2d9cb2a6467addc5798ee50", "class_name": "RelatedNodeInfo"}}, "text": "Commun. ACM 64, 3 (feb 2021). https://doi.org/\n10.1145/3430936\n[38] Alexandre Verbitski et al .2018. Amazon Aurora: On Avoiding Distributed\nConsensus for I/Os, Commits, and Membership Changes (SIGMOD \u201918) . https:\n//doi.org/10.1145/3183713.3196937\n[39] Alexandre Verbitski, Anurag Gupta, Debanjan Saha, Murali Brahmadesam, Ka-\nmal Gupta, Raman Mittal, Sailesh Krishnamurthy, Sandor Maurice, Tengiz\nKharatishvili, and Xiaofeng Bao. 2017. Amazon Aurora: Design Considera-\ntions for High Throughput Cloud-Native Relational Databases (SIGMOD\u201917) .\nhttps://doi.org/10.1145/3035918.3056101\n[40] Ruihong Wang, Jianguo Wang, Stratos Idreos, M. Tamer \u00d6zsu, and Walid G. Aref.\n2022. The Case for Distributed Shared-Memory Databases with RDMA-Enabled\nMemory Disaggregation. Proc. VLDB Endow. 16, 1 (2022). https://doi.org/10.\n14778/3561261.3561263\n[41] Yuan Yuan, Rubao Lee, and Xiaodong Zhang. 2013. The Yin and Yang of Pro-\ncessing Data Warehousing Queries on GPU Devices. Proceedings of the VLDB\nEndowment 6, 10 (2013), 817\u2013828. https://doi.org/10.14778/2536206.2536210\n[42] Erfan Zamanian, Xiangyao Yu, Michael Stonebraker, and Tim Kraska. 2019.\nRethinking Database High Availability with RDMA Networks. Proc. VLDB\nEndow. 12, 11 (2019). https://doi.org/10.14778/3342263.3342639\n[43] Qizhen Zhang, Philip A. Bernstein, Daniel S. Berger, and Badrish Chandramouli.\n2021. Redy: Remote Dynamic Memory Cache. Proc. VLDB Endow. 15, 4 (2021).\nhttps://doi.org/10.14778/3503585.3503587\n[44] Mark Zhao et al .2022. Understanding data storage and ingestion for large-\nscale deep recommendation model training: industrial product (ISCA\u201922) . https:\n//doi.org/10.1145/3470496.3533044Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for\nMemory Expansion (CXL-SSD)\nMyoungsoo Jung\nComputer Architecture and Memory Systems Laboratory ,\nKorea Advanced Institute of Science and Technology (KAIST)\nhttp://camelab.org\nABSTRACT\nCompute express link (CXL) is the \ufb01rst open multi-protocol\nmethod to support cache coherent interconnect for differ-\nent processors, accelerators, and memory device types. Even\nthough CXL manages data coherency mainly between CPU\nmemory spaces and memory on attached devices, we argue\nthat it can also be useful to reform existing block storage as\ncost-ef\ufb01cient, large-scale working memory. Speci\ufb01cally, this\npaper examines three different sub-protocols of CXL from a\nmemory expander viewpoint. It then suggests which device\ntype can be the best option for PCIe storage to bridge its\nblock semantics to memory-compatible, byte semantics. We\nthen discuss how to integrate a storage-integrated memory\nexpander into an existing system and speculate how much\neffect it does have on the system performance. Lastly, we visit\nvarious CXL network topologies and explore a new opportu-\nnity to ef\ufb01ciently manage the storage-integrated, CXL-based\nmemory expansion.\n1 INTRODUCTION\nCache coherence interconnects are recently emerged to inte-\ngrate different CPUs, accelerators, and memory components\ninto a heterogeneous, single computing domain. Speci\ufb01cally,\nthe interconnect technologies maintain data coherency be-\ntween CPU memory and private memory attached to devices,\nde\ufb01ning a new type of globally shared memory and network\nspace.", "start_char_idx": 844774, "end_char_idx": 848014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45a5ba89-93bb-4830-ab19-9eeb929975aa": {"__data__": {"id_": "45a5ba89-93bb-4830-ab19-9eeb929975aa", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37dc5d59-786a-4d97-a00c-decef3c14044", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "7262d76e7bfbafbac2f1318b6c3308edb87c8d605d55d58b7e01198b63ec174c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee6c29b6-655d-4e6d-a0cd-6a84f2fd266d", "node_type": "1", "metadata": {}, "hash": "23131d2d8048a35600340bee2957a869f24f20a0d1d8fae0ca7ba7244abce664", "class_name": "RelatedNodeInfo"}}, "text": "It then suggests which device\ntype can be the best option for PCIe storage to bridge its\nblock semantics to memory-compatible, byte semantics. We\nthen discuss how to integrate a storage-integrated memory\nexpander into an existing system and speculate how much\neffect it does have on the system performance. Lastly, we visit\nvarious CXL network topologies and explore a new opportu-\nnity to ef\ufb01ciently manage the storage-integrated, CXL-based\nmemory expansion.\n1 INTRODUCTION\nCache coherence interconnects are recently emerged to inte-\ngrate different CPUs, accelerators, and memory components\ninto a heterogeneous, single computing domain. Speci\ufb01cally,\nthe interconnect technologies maintain data coherency be-\ntween CPU memory and private memory attached to devices,\nde\ufb01ning a new type of globally shared memory and network\nspace. While there have been several efforts to coherently\nconnect different hardware components, such as Gen-Z [ 1]\nand CCIX [ 2],Compute Express Link (CXL) is the \ufb01rst open\ninterconnect protocol supporting various types of processors\nand device endpoints [ 3]. CXL has absorbed Gen-Z [ 4] and\nhas become one of the most promising interconnect interfaces\nthanks to its high-speed coherence control and full compati-\nbility with the existing bus standard, PCIe. A broad spectrum\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro\ufb01t or commercial advantage and that copies bear this notice and the full citation\non the \ufb01rst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior speci\ufb01c permission and/or a\nfee. Request permissions from permissions@acm.org.\nHotStorage \u201922, June 27\u201328, 2022, Virtual Event, USA\n\u00a9 2022  Association  for Computing  Machinery.  \nACM  ISBN  978-1-4503-9399-7/22/06.  . . $15.00  \nhttps://doi.org/10.1145/3538643.353974 5of datacenter-scale hardware such as CPU, GPU, FPGA, and\ndomain-speci\ufb01c ASIC is thus expected to take signi\ufb01cant\nadvantage of CXL [ 5\u20137]. CXL consortium announces that it\ncan also disaggregate memory by pooling DRAM and byte-\naddressable persistent memory (PMEM).\nWhile CXL can handle diverse computing resources and\nmemory components, it sets block storage aside and leaves a\nquestion on whether the storage can reap the bene\ufb01ts of CXL\nor not. A primary question that storage designers and system\narchitects may have is i) why and what can the block storage\nbene\ufb01t from CXL? . If there is an advantage, we should be able\nto answer the following questions: ii) how can we connect the\nunderlying block storage to the host\u2019s system memory bus? ,\niii)what kind of CXL device type should be used for the block\nstorage and memory expander? , and iv) what does CXL need\nto improve for better utilization of the block storage? .\nIn this paper, we argue that CXL is helpful in leveraging\nPCIe-based block storage to incarnate a large, scalable work-\ning memory by answering all the four questions mentioned\nabove. We believe CXL is a cost-effective and practical in-\nterconnect technology that can bridge PCIe storage\u2019s block\nsemantics to memory-compatible, byte semantics. To this end,\nwe should carefully integrate the block storage into its inter-\nconnect network by being aware of the diversity of device\ntypes and protocols that CXL supports. This paper \ufb01rst dis-\ncusses what a mechanism makes the PCIe storage impractical\nand unable to be used for a memory expander (\u00a72). Then, we\nexplore all the CXL device types and their protocol interfaces\nto answer which con\ufb01guration would be the best for the PCIe\nstorage to expand the host\u2019s CPU memory (\u00a73).\nEven though CXL can be the most promising interface for\nthe block storage in getting closer to CPU, it is non-trivial\nto speculate how much effect a storage-integrated memory\nexpander does have on system performance.", "start_char_idx": 847183, "end_char_idx": 851216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee6c29b6-655d-4e6d-a0cd-6a84f2fd266d": {"__data__": {"id_": "ee6c29b6-655d-4e6d-a0cd-6a84f2fd266d", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45a5ba89-93bb-4830-ab19-9eeb929975aa", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "dda7fa94e9461ae8c522b58fa129e9862ebdd99d536801c88e8f1505e8d0cdc4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3cb58db-d464-4777-bc31-60da5a2d59b7", "node_type": "1", "metadata": {}, "hash": "3812182e4b52a7ea6c34d76063535403c5b36d1611bb470d846da3465b9d5a99", "class_name": "RelatedNodeInfo"}}, "text": "We believe CXL is a cost-effective and practical in-\nterconnect technology that can bridge PCIe storage\u2019s block\nsemantics to memory-compatible, byte semantics. To this end,\nwe should carefully integrate the block storage into its inter-\nconnect network by being aware of the diversity of device\ntypes and protocols that CXL supports. This paper \ufb01rst dis-\ncusses what a mechanism makes the PCIe storage impractical\nand unable to be used for a memory expander (\u00a72). Then, we\nexplore all the CXL device types and their protocol interfaces\nto answer which con\ufb01guration would be the best for the PCIe\nstorage to expand the host\u2019s CPU memory (\u00a73).\nEven though CXL can be the most promising interface for\nthe block storage in getting closer to CPU, it is non-trivial\nto speculate how much effect a storage-integrated memory\nexpander does have on system performance. As there is no\nCPU and fabric for CXL yet, it is also unclear for the storage\ndesigners and system architects to see how CXL-enabled\nstorage can be implemented and interact with CPU. To answer\nthis, we discuss what a PCIe storage device needs to change,\nhow it can be connected to the host over CXL, and how users\ncan access the device through load/store instructions (\u00a74).\nWe then project the performance of the storage-integrated\nmemory expander by prototyping CXL agents and controllers\nin different FPGA nodes, all connected by a PCIe network.\n45\nHotStorage \u201922, June 27\u201328, 2022, Virtual Event, USA Myoungsoo Jung\nAfter examining the potential bene\ufb01ts of converting the\nblock semantic to byte semantic over CXL, we further discuss\nhow to disaggregate PCIe storage resources from computing\nresources using CXL (\u00a76). In particular, we visit three differ-\nent network topologies and argue the pros and cons of each\ndisaggregation option. We lastly explore a new opportunity\nto manage the PCIe storage ef\ufb01ciently at the host-side (\u00a77).\nSpeci\ufb01cally, we suggest two additional states, determinism\nandbufferability , and debate why these additional states can\nbe bene\ufb01cial to handle the PCIe storage in expanding the\nhost\u2019s CPU memory with CXL.\n2 WHY CXL FOR PCIE STORAGE\nByte-addressability. It is a long-standing dream for PCIe\nstorage to have byte-addressability and be a part of working\nmemory devices [ 8\u201317]. For example, an industry prototype\nand NVMe standard [ 18\u201320] offer the byte-addressability by\nexposing SSD\u2019s internal memory/buffer to PCIe base address\nregisters (BARs). Since BARs can be directly mapped to the\nsystem memory space, the host-side kernel and applications\ncan access the exposed memory/buffer resources just like the\nlocal memory (using load/store instructions) rather than block\nstorage. To hide long latency imposed by SSD\u2019s backend\nblock media (e.g., Z-NAND, Flash, Optane SSD), they can\nuse the internal memory/buffer as a write-back inclusive cache\nof the backend [21\u201325].\nLimits with non-cacheable accesses. PCIe bandwidth is fast\nenough to be far memory (e.g., 63GB/s \u21e0121GB/s for Gen5/6\n16\u21e5[26]). However, PCIe considers the block storage devices\nas just one of the peripherals that the host-side CPU needs\nto manage and communicate with. Thus, while the storage\ndevices can handle load/store requests through the PCIe\u2019s\nBARs, they are all limited to being used as working mem-\nory in a real system. Speci\ufb01cally, as the memory-mapped\nBARs are only the interface for the host to let the under-\nlying storage know what it requests or controls, CPU must\nmake the load/store requests uncached and directly acces-\nsible. This non-cacheable characteristic severely degrades\nthe performance of all memory accesses targeting the BARs.\nIf CPU can cache/buffer the memory requests targeting the\nPCIe address space, there is no way for the PCIe storage to\ncatch their arrivals. This can introduce an unexpected situ-\nation such as a system failure or storage disconnection. To\nprevent such a failure, x86 instruction set architectures of both\nIntel and AMD do not allow PCIe-related memory requests to\nbe cached at the CPU side. Unfortunately, this nature enforces\nthe storage-integrated memory expanders be excluded from\nthe conventional memory hierarchy anddisables them from\ntaking advantage of CPU caches .\nCompute express link.", "start_char_idx": 850358, "end_char_idx": 854575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3cb58db-d464-4777-bc31-60da5a2d59b7": {"__data__": {"id_": "d3cb58db-d464-4777-bc31-60da5a2d59b7", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee6c29b6-655d-4e6d-a0cd-6a84f2fd266d", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "b04663bc7715355a924446fc9f6f5dae1f8c71db19de84457f6efd5fe1e7d6b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a162ddeb-1914-4296-abe0-6421a200c26a", "node_type": "1", "metadata": {}, "hash": "290fb3032157286d87657b36a8ea2d49362bc536bbdf8af3603e19cab9357fae", "class_name": "RelatedNodeInfo"}}, "text": "Speci\ufb01cally, as the memory-mapped\nBARs are only the interface for the host to let the under-\nlying storage know what it requests or controls, CPU must\nmake the load/store requests uncached and directly acces-\nsible. This non-cacheable characteristic severely degrades\nthe performance of all memory accesses targeting the BARs.\nIf CPU can cache/buffer the memory requests targeting the\nPCIe address space, there is no way for the PCIe storage to\ncatch their arrivals. This can introduce an unexpected situ-\nation such as a system failure or storage disconnection. To\nprevent such a failure, x86 instruction set architectures of both\nIntel and AMD do not allow PCIe-related memory requests to\nbe cached at the CPU side. Unfortunately, this nature enforces\nthe storage-integrated memory expanders be excluded from\nthe conventional memory hierarchy anddisables them from\ntaking advantage of CPU caches .\nCompute express link. CXL is a cache coherent interconnec-\ntion technology, designed initially toward supporting variousaccelerators and memory devices therein [ 3]. Speci\ufb01cally,\nCXL can offer one or more memory address spaces in the\nPCIe network domain coherently, which can consistently be\naccessed by different processors and hardware accelerators\nover its multi-protocol technology. The multi-protocol over-\nrides the I/O semantic of the existing PCIe interface, thereby\nmaking all device types of CXL compatible with most exist-\ning PCIe devices, including SSDs. We will explain details of\neach device type in \u00a73.\nEven though CXL is built upon PCIe, it basically guar-\nantees that all the caches across different computing com-\nplexes in the same CXL hierarchy are coherent. This can\nmake the load/store requests (heading to the PCIe address\nspace) cacheable in contrast to PCIe. The current CXL con-\nsiders only DRAM or PMEM for its memory pooling, but we\nadvocate that CXL can open a new door that changes PCIe\nstorage\u2019s block interface to a memory-like, byte interface. As\nCXL\u2019s multi-protocol can integrate PCIe storage into its cache\ncoherent memory space, it can create a much bigger memory\npool than DRAM-based or PMEM-based memory expansion\ntechnologies.\n3MULTI-PROTOCOL AND CXL DEVICES\nCXL provides three different sub-protocols, i) CXL.io , ii)\nCXL.mem , and iii) CXL.cache , which can also de\ufb01ne three\ndifferent types of CXL devices ( Type 1 \u21e0Type 3 ).\nMulti-protocol. CXL.io is the fundamental protocol that all\nCXL-attached devices and host CPUs require to communi-\ncate. Fundamentally, it employs full features of PCIe as a\nnon-coherent load/store interface for I/O devices (e.g., device\ndiscovery/enumeration and host address con\ufb01guration). To\nthis end, CXL.io amends PCIe\u2019s hierarchical communication\nlayers and creates a high-speed I/O channel, called FlexBus .\nFlexBus converts received CXL data to an appropriate for-\nmat to leverage the physical PCIe layers (e.g., transaction,\ndata, and link). On the other hand, CXL.cache and CXL.mem\nrespectively add coherent cache and memory access capa-\nbilities into FlexBus, which can fan out to support multiple\ndevice domains and remote memory management. While\nCXL overrides PCIe, its root port ( CXL RP ) allows one or\nmore memory addresses (exposed by the underlying CXL\ndevices) to be mapped to a target host\u2019s cacheable system\nmemory space. While this capability is designed toward uni-\nfying multi-domain memory devices into a single pool over\ncoherent cache management, it can be leveraged for a memory\nexpander using different storage technologies.\nCXL device types. Based on how to combine the multi-\nprotocol features of CXL, it declares three different device\ntypes, Type 1, Type 2, and Type 3. Figure 1a shows all the\nCXL device types and the protocols each device type uses.", "start_char_idx": 853654, "end_char_idx": 857408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a162ddeb-1914-4296-abe0-6421a200c26a": {"__data__": {"id_": "a162ddeb-1914-4296-abe0-6421a200c26a", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3cb58db-d464-4777-bc31-60da5a2d59b7", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "42b7eefbbebbde912b6bbbd4003ec3bab698c05351664d39c32773a83a4604d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74b69fa3-6432-4f14-bfbc-d242a237d105", "node_type": "1", "metadata": {}, "hash": "76b62cd2f40634c8e6bd7cdb698705c5c3289e13593e00129df70d600cde564f", "class_name": "RelatedNodeInfo"}}, "text": "On the other hand, CXL.cache and CXL.mem\nrespectively add coherent cache and memory access capa-\nbilities into FlexBus, which can fan out to support multiple\ndevice domains and remote memory management. While\nCXL overrides PCIe, its root port ( CXL RP ) allows one or\nmore memory addresses (exposed by the underlying CXL\ndevices) to be mapped to a target host\u2019s cacheable system\nmemory space. While this capability is designed toward uni-\nfying multi-domain memory devices into a single pool over\ncoherent cache management, it can be leveraged for a memory\nexpander using different storage technologies.\nCXL device types. Based on how to combine the multi-\nprotocol features of CXL, it declares three different device\ntypes, Type 1, Type 2, and Type 3. Figure 1a shows all the\nCXL device types and the protocols each device type uses.\n46Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for Memory Expansion HotStorage \u201922, June 27\u201328, 2022, Virtual Event, USA/g18/g121/g62/g90/g87/g100/g455/g393/g286/g1007/g94/g455/g400/g410/g286/g373/g3/g393/g346/g455/g400/g349/g272/g258/g367/g3/g373/g286/g373/g381/g396/g455/g3/g373/g258/g393/g62/g381/g272/g258/g367/g3/g24/g90/g4/g68/g44/g24/g68/g17/g4/g90/g18/g381/g374/g296/g856/g876/g18/g258/g393/g258/g393/g258/g271/g349/g367/g349/g410/g455/g18/g121/g62/g3/g90/g87/g3/g396/g286/g400/g286/g396/g448/g286/g282/g3/g3/g3/g3/g44/g24/g68/g17/g4/g90/g18/g258/g393/g258/g271/g349/g367/g349/g410/g455/g18/g381/g374/g296/g349/g336/g856/g38/g116/g28/g87/g100/g455/g393/g286/g3/g1007/g3/g68/g286/g373/g381/g396/g455/g3/g68/g258/g393/g68/g286/g373/g381/g396/g455/g17/g367/g381/g272/g364/g18/g121/g62/g3/g272/g381/g374/g410/g396/g381/g367/g367/g286/g396/uni2776/uni2777/uni2778/uni2779/g3\n/g18/g87/g104/g3/g400/g455/g400/g410/g286/g373/g3/g373/g286/g373/g381/g396/g455/g3/g400/g393/g258/g272/g286/g100/g455/g393/g286/g1005/g100/g455/g393/g286/g1006/g100/g455/g393/g286/g1007/g68/g286/g373/g381/g396/g455/g68/g286/g373/g381/g396/g455/g87/g18/g44/g18/g87/g104/g44/g381/g400/g410/g3/g18/g87/g104/g3/g18/g381/g373/g393/g367/g286/g454\n/g62/g381/g272/g258/g367/g3/g24/g90/g4/g68/g44/g24/g68/g44/g24/g68/g68/g286/g373/g381/g396/g455(a) Protocols and devices. (b) System memory mapping.\nFigure 1: CXL multi-protocol and system map.\nType 1 is CXL devices that have a local cache without\nemploying an internal DRAM component. Type 1 is valuable\nif the endpoint device requires a fully coherent cache, which\nimplies that the device can access the corresponding host\u2019s\nmemory data through its own caches in an active manner.\nMost domain-speci\ufb01c accelerators for computationally inten-\nsive applications, such as tensor processing units [ 27], can be\nclassi\ufb01ed by this Type 1 device. Note that Type 1 devices use\nCXL.cache and CXL.io to manage their full cache coherence\ncapability.\nType 2 is for discrete acceleration devices that internally\nemploy high-performance memory modules. These memory\ncomponents are referred to as host-managed device memory\n(HDM) in CXL. While the host can, in default, communicate\nwith Type 2 devices using CXL.io (over PCIe), CXL.cache\nand CXL.mem are respectively used for the device to access\nits host-side memory and for the host to access HDM.", "start_char_idx": 856574, "end_char_idx": 859783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74b69fa3-6432-4f14-bfbc-d242a237d105": {"__data__": {"id_": "74b69fa3-6432-4f14-bfbc-d242a237d105", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a162ddeb-1914-4296-abe0-6421a200c26a", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8e4e2bb082eaeba9f5f18375b929a5f388add60e9631760f48446dade700e14e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcfe8f80-2f0f-4bc7-bafd-aef45633a0e0", "node_type": "1", "metadata": {}, "hash": "145c5b7924cdd97a28bb6d15dee909dead77a240e33ed4b97ab607f226bb6d28", "class_name": "RelatedNodeInfo"}}, "text": "Most domain-speci\ufb01c accelerators for computationally inten-\nsive applications, such as tensor processing units [ 27], can be\nclassi\ufb01ed by this Type 1 device. Note that Type 1 devices use\nCXL.cache and CXL.io to manage their full cache coherence\ncapability.\nType 2 is for discrete acceleration devices that internally\nemploy high-performance memory modules. These memory\ncomponents are referred to as host-managed device memory\n(HDM) in CXL. While the host can, in default, communicate\nwith Type 2 devices using CXL.io (over PCIe), CXL.cache\nand CXL.mem are respectively used for the device to access\nits host-side memory and for the host to access HDM. Note\nthat HDM differs from private memory modules employed by\nconventional acceleration devices such as GPUs. Even though\na host can access GPU\u2019s device-side memory (e.g., GDDR),\nit is only performed by legacy memory copies. In contrast, a\nCXL-enabled host can manage HDM using cache-coherent\nload/store instructions. It is also expected for Type 2 devices\nto actively access the host\u2019s CPU memory by utilizing all the\nfeatures that CXL\u2019s multi-protocol supports.\nLastly, Type 3 is designed for non-acceleration devices\nthat only employ HDM without a processing component.\nCXL regulates these Type 3 devices operate primarily with\nCXL.mem to serve load/store requests issued by a host; it does\nnot allow the Type 3 devices to make a request (to the host)\nover CXL.cache. While Type 3 does not employ CXL.cache,\nit can be used for expanding the host-side memory. This is\nbecause CXL.mem includes basic memory read and write\ninterfaces for HDM. We will explain how to use HDM in \u00a73.\nNote that CXL allows the Type 3 devices to manage CXL.io\nat the device side in an attempt to accommodate various I/O\nspeci\ufb01c demands in a \ufb02exible manner.\n4 INTEGRATING STORAGE INTO CXL\nDevice type consideration. Generally speaking, PCIe storage\nis not a simple, passive device. In addition to its backend\u2019sblock media, PCIe storage employs large internal DRAMs\nto buffer/cache incoming requests and corresponding data. It\nalso has computation capability used for diverse data process-\ning tasks such as address translation or reliability management\n[28\u201332]. Type 2 can probably be a good option for utilizing\nHDM and/or integrating data processing capabilities into the\nstorage by being aware of the semantics of host CPUs. In this\npaper, we however advocate Type 3 for a storage-integrated\nmemory expander in CXL.\nThere are three reasons why we believe that Type 3 de-\nvices are better than Type 2 devices for the storage-integrated\nmemory expander. First, even though Type 2 allows the host\nto handle the storage-side HDM directly, Type 2 is designed\nfor computationally intensive applications. Because of this,\nonly one device (per CXL RP) can be connected to a host\nsystem, which makes Type 2 devices not scalable as Type 3 de-\nvices can do. Second, having full features of CXL.cache and\nCXL.mem can introduce another type of communication bur-\nden, thereby degrading the overall performance of the storage-\nintegrated memory expander. Speci\ufb01cally, all load/store re-\nquests require checking the cache states of PCIe storage\u2019s\ncomputing complex, which exhibits multiple CXL transac-\ntions for every I/O service. Even though it is crucial for the\nPCIe storage to manage internal DRAM ef\ufb01ciently, it does not\nrequire coherently managing the host\u2019s CPU caches. Third,\nif we integrate a PCIe storage device into CXL as Type 2,\nthe device should ask permission from the host whenever the\nstorage side computing resources access its memory. This is\nbecause Type 2\u2019s CXL.cache manages both the host\u2019s local\nmemory and HDM in a fully coherent manner, which makes\nthe device-level I/O performance even worse than before.\nStorage-side modi\ufb01cation. PCIe storage devices typically\nemploy a PCIe endpoint and NVMe controllers to parse the in-\ncoming requests and transfer data between a host and SSD\u2019s\ninternal DRAMs [ 33,34]. Thus, a hardware change at the\nstorage side can be simple, which in turn makes most storage\ndevices easily support Type 3 with a minor modi\ufb01cation.", "start_char_idx": 859131, "end_char_idx": 863230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcfe8f80-2f0f-4bc7-bafd-aef45633a0e0": {"__data__": {"id_": "dcfe8f80-2f0f-4bc7-bafd-aef45633a0e0", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74b69fa3-6432-4f14-bfbc-d242a237d105", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "361e766c8732b18a6d8a36e92af975b85df8c43e14015d8748562a0ab04417ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2938a49-c750-4863-8efa-00ed0108f415", "node_type": "1", "metadata": {}, "hash": "3921f11540386681025f0cbe48b0818f249b0c2b77674106abedbf5050e12e18", "class_name": "RelatedNodeInfo"}}, "text": "Third,\nif we integrate a PCIe storage device into CXL as Type 2,\nthe device should ask permission from the host whenever the\nstorage side computing resources access its memory. This is\nbecause Type 2\u2019s CXL.cache manages both the host\u2019s local\nmemory and HDM in a fully coherent manner, which makes\nthe device-level I/O performance even worse than before.\nStorage-side modi\ufb01cation. PCIe storage devices typically\nemploy a PCIe endpoint and NVMe controllers to parse the in-\ncoming requests and transfer data between a host and SSD\u2019s\ninternal DRAMs [ 33,34]. Thus, a hardware change at the\nstorage side can be simple, which in turn makes most storage\ndevices easily support Type 3 with a minor modi\ufb01cation. For\nexample, we can compose a CXL storage controller to handle\nCXL transaction packet formatting and CXL.io control by\nleveraging the existing PCIe endpoint logic. Similarly, the\nexisting NVMe controller\u2019s capabilities, such as command\nparsing and page memory copies, can be simpli\ufb01ed to im-\nplement the read and write interfaces of CXL.mem. Note\nthat the NVMe speci\ufb01cation allows PCIe storage to realize\nits controllers in either \ufb01rmware or hardware [ 35,36]. How-\never, we believe it is better to automate the service routine\nof CXL.mem\u2019s reads and writes over hardware while letting\n\ufb01rmware manage the internal DRAMs and the back-end block\nmedia.\nSystem integration. Figure 1b shows how CXL can connect\na PCIe storage device to a host and explains how the host-side\nusers directly access the storage device through load/store\n47HotStorage \u201922, June 27\u201328, 2022, Virtual Event, USA Myoungsoo Jung\ninstructions. In this example, the host\u2019s system bus employs\na CXL RP connecting a PCIe storage device as Type 3; we\nwill discuss a system option to disaggregate many storage\ndevices from the host resources in \u00a76. When the host boots, it\nenumerates CXL devices connected to its RP and initializes\nthe devices by mapping their internal memory spaces to the\nsystem memory. Speci\ufb01cally, the host retrieves the size of\nCXL BAR and HDM from the PCIe storage devices and then\nmaps them into its system memory space (CXL RP reserved).\nIn particular, HDM is mapped to a cacheable memory ad-\ndress space such that the users can access it using load/store\ninstructions. As CXL BAR and HDM are mapped to differ-\nent addresses from what a Type 3 device initially manages,\nthe CXL RP requires letting the underlying CXL controller\nknow where they have been mapped [ 1]. The host can do\nthis address space synchronization by writing the correspond-\ning information (e.g., remapped address offset) to the target\nstorage\u2019s CXL capability/con\ufb01guration areas.\nWhen an application loads or stores data on the system\nmemory (mapped to HDM), CXL RP generates a message,\ncalled CXL \ufb02it , and sends it to the target\u2019s CXL storage con-\ntroller via CXL.mem [ 2]. The underlying endpoint and CXL\ncontrollers then parse the \ufb02it and extract the request infor-\nmation (e.g., command and target address) [ 3]. Using the\nrequest information, the controllers can serve the data by\ncollaborating with underlying storage \ufb01rmware [ 4].\n5 PERFORMANCE PROJECTION\nPrototype. Since there is unfortunately no processor com-\nplex that yet supports CXL.mem and CXL.io, we prototype\na CXL-enabled CPU and CXL storage, each taking the role\nof a host and storage-integrated memory expander. Speci\ufb01-\ncally, the CPU and storage are fabricated into two separate,\ncustom FPGA boards, which are connected through a tailored\nPCIe backplane. We integrate CXL.mem and CXL.io agents\ninto an in-house RISC-V CPU (64-bit O3 dual-core architec-\nture that uses 128KB L1 and 4MB L2 caches), and 32GB\nOpenExpress-based NVMe storage [ 24] in 16nm FPGA for\nthe host node and storage node, respectively. OpenExpress\u2019s\nbackend media emulates Z-NAND [ 37] while buffering the\nincoming CXL requests into its internal DRAMs.", "start_char_idx": 862527, "end_char_idx": 866387, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2938a49-c750-4863-8efa-00ed0108f415": {"__data__": {"id_": "d2938a49-c750-4863-8efa-00ed0108f415", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcfe8f80-2f0f-4bc7-bafd-aef45633a0e0", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d76f48de1d8655cbaadf3e5345721e3c730545774bef4ce0c91eed10a01154c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07ae80e5-94c5-469f-bef8-9fea1c95fb69", "node_type": "1", "metadata": {}, "hash": "a2feca6ae8a1dd9086c324b76345ce844e2d49e52394312e1ab50abbd998e941", "class_name": "RelatedNodeInfo"}}, "text": "Since there is unfortunately no processor com-\nplex that yet supports CXL.mem and CXL.io, we prototype\na CXL-enabled CPU and CXL storage, each taking the role\nof a host and storage-integrated memory expander. Speci\ufb01-\ncally, the CPU and storage are fabricated into two separate,\ncustom FPGA boards, which are connected through a tailored\nPCIe backplane. We integrate CXL.mem and CXL.io agents\ninto an in-house RISC-V CPU (64-bit O3 dual-core architec-\nture that uses 128KB L1 and 4MB L2 caches), and 32GB\nOpenExpress-based NVMe storage [ 24] in 16nm FPGA for\nthe host node and storage node, respectively. OpenExpress\u2019s\nbackend media emulates Z-NAND [ 37] while buffering the\nincoming CXL requests into its internal DRAMs. In addition\nto this prototype ( CXL), we evaluate a local DRAM-only sys-\ntem ( DRAM ) and PCIe-based memory expander ( PCIe ).PCIe\nandCXLuse the same backend storage, but their RP\u2019s address\nis mapped to different places of the host\u2019s system memory.\nWorkloads. We use Apex-Map , a global memory access\nbenchmark for large-scale computing [ 38]. The benchmark\nallows us to test underlying memory with different locality\nand request size levels (i.e., the parameter, a). We con\ufb01gure\nthe request size as 64B, which is the same as the last-level\ncacheline size of our CPU. In this performance projection, we(a)a= 0.001. (b) Average a. (c) a= 1.\nFigure 2: Performance of different memory systems.\nexclude time-consuming activities of internal tasks such as\ngarbage collection; we will discuss how CXL can alleviate the\nlong latency imposed by such internal tasks in \u00a77. Apex-Map\ngenerates 512 million memory instructions synthetically by\nranging afrom 0.001 (highest locality) to 1 (lowest locality).\nResult analysis. Figures 2a, 2b, and 2c show each system\u2019s\nlatency (in terms of CPU cycles) for the best case ( a=0.001),\nthe average case ( 0.001\uf8ffa\uf8ff1), and the worst case ( a=1),\nrespectively. The best-case performance shows the reason\nwhy CXLcan be more bene\ufb01cial than a PCIe-based memory\nexpander. While most memory requests in this test are hit\nfrom the CPU caches, PCIe cannot take any advantage of the\nhost CPU caches, thereby exhibiting 129.5 \u21e5longer latency\nthanCXL. In contrast, CXLenjoys the CPU caches and shows\nexcellent latency behaviors comparable with DRAM .\nCXLis also better than PCIe by 3\u21e5for the average case.\nNote that, even though the performance of CXLis 9.3 \u21e5worse\nthan that of DRAM , we believe it is still in a reasonable range\nby considering the fact that CXLleverages the block storage.\nWhen there is no locality (the worst-case), CXLcannot hide\nthe underlying Z-NAND latency because of the benchmark\u2019s\naccess pattern (fully random), which exhibits 84.1 \u21e5worse\nthanDRAM . However, CXLshows still better performance com-\npared to PCIe by 1.6 \u21e5as it does not handle all the memory\nrequests (on PCIe\u2019s BAR) in a synchronized fashion.\nWe are somewhat disappointed with the results as CXL\u2019s\nworst-case latency characteristics are far away from DRAM\u2019s\nbehaviors. However, most workloads exhibit high locality\nexcept for a speci\ufb01c application like graph processing. Con-\nsidering the large capacity that the storage-integrated memory\nexpander offers, we believe many applications can reap the\nbene\ufb01ts of CXL. We also believe that there is an opportunity\nto optimize this long latency by wisely using PCIe storage\u2019s\ninternal DRAMs and backend block media (\u00a77).\n6 STORAGE DISAGGREGATION\nThis section discusses how a system can disaggregate CXL\ncontrollers and storage devices from its computing resources\nwhile keeping their byte-addressability.\nPooling storage over the byte interface.", "start_char_idx": 865667, "end_char_idx": 869287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07ae80e5-94c5-469f-bef8-9fea1c95fb69": {"__data__": {"id_": "07ae80e5-94c5-469f-bef8-9fea1c95fb69", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2938a49-c750-4863-8efa-00ed0108f415", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "06edddb1ad2c3107c1bb47cfc44d8d34dbdf862807d94abf2bb0bf21ed2a3fdb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f562a64c-233a-4e75-9927-b4d7b0e26e53", "node_type": "1", "metadata": {}, "hash": "39acc1d069526fe565f79742c99576a7e7dd73bebba7b6266f9d4944d66075df", "class_name": "RelatedNodeInfo"}}, "text": "We are somewhat disappointed with the results as CXL\u2019s\nworst-case latency characteristics are far away from DRAM\u2019s\nbehaviors. However, most workloads exhibit high locality\nexcept for a speci\ufb01c application like graph processing. Con-\nsidering the large capacity that the storage-integrated memory\nexpander offers, we believe many applications can reap the\nbene\ufb01ts of CXL. We also believe that there is an opportunity\nto optimize this long latency by wisely using PCIe storage\u2019s\ninternal DRAMs and backend block media (\u00a77).\n6 STORAGE DISAGGREGATION\nThis section discusses how a system can disaggregate CXL\ncontrollers and storage devices from its computing resources\nwhile keeping their byte-addressability.\nPooling storage over the byte interface. To make the inter-\nconnect network scalable, CXL 2.0 allows FlexBus to employ\none or more CXL switches, each being able to have multiple\n48Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for Memory Expansion HotStorage \u201922, June 27\u201328, 2022, Virtual Event, USA\n(a) Switch. (b) Multi-switch. (c) Virtual hierarchy.\nFigure 3: Storage disaggregation con\ufb01gurations.\nupstream ports (USPs) and downstream ports (DSPs). Even\nthough CXL yet leaves a question undecided on how to imple-\nment a switch and its internal components, USPs and DSPs\ncan be simply interconnected by a recon\ufb01gurable crossbar\nswitch. Speci\ufb01cally, a USP can be connected to a CXL RP or\nanother switch\u2019s DSP over FlexBus, and it internally returns\nthe incoming message to one or more underlying DSPs as\nsoon as possible. In contrast, a DSP links a lower-level hard-\nware module such as a storage device\u2019s CXL endpoint or a\ndifferent switch\u2019s USP. Using a switch buffer, it can control\nmultiple CXL messages going through the DSP(s).\nFigure 3a shows how a host can expand its local mem-\nory by having multiple storage devices. Speci\ufb01cally, each\nDSP connects to a different storage device, whereas a USP\nis linked to all the DSPs and exposes them to the host\u2019s CXL\nRP. For this type of storage-integrated memory expansion, the\nhost should map each HDM to different places of its physical\nmemory and be aware of the mapping information when the\nsystem enumerates CXL devices to con\ufb01gure CXL capabil-\nity/con\ufb01guration (BAR/HDM). While this network topology\nis simple enough to connect multiple PCIe storage devices,\nthe number of lanes that a CXL switch can support is limited.\nTypically, a switch supports 64 \u21e0128 lanes, and thus, only\n4\u21e08 ports are available for high-performance storage devices\n(using 16 lanes). In this case, as shown in Figure 3b, it can\nexpand the host memory by adding one or more switches to\nthe CXL network. The top switch is used to bridge the host\u2019s\nCXL RP and all other lower-level switches, while the leaf\nswitches are employed to manage many PCIe storage devices.\nNote that the number of storage devices that a network can\nhandle varies based on the size of the devices and the memory\ncapacity that CXL deals with (currently, it is 4PB).\nMulti-host connection management. To better utilize the\nstorage resources, we can also connect arbitrary numbers of\nhost CPUs to the CXL network. Since the switch\u2019s crossbar\n(called fabric manager ) remembers each connection between\nUSPs and DSPs, we can fabricate a unique routing path begin-\nning from a host to one or more storage devices, called virtual\nhierarchy (VH). Each VH guarantees that a storage device\ncan be mapped to a host, which is attached anywhere in the\nCXL network. Thus, VHs allow the system to completely\ndisaggregate many PCIe storage devices from its multi-hostcomputing resources for the memory expansion. While these\nrecon\ufb01gurable VHs can realize a fully scale-out architecture,\nmemory resources expanded by the storage devices are un-\nfortunately tricky to control \ufb01nely. Since the storage device\nshould only be associated with a host, it can be underutilized\nand/or unbalanced across different CPUs.\nStorage device virtualization. To address this issue, we can\nvirtualize each storage device to be shared by different hosts.", "start_char_idx": 868541, "end_char_idx": 872587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f562a64c-233a-4e75-9927-b4d7b0e26e53": {"__data__": {"id_": "f562a64c-233a-4e75-9927-b4d7b0e26e53", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07ae80e5-94c5-469f-bef8-9fea1c95fb69", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "eec83d94b3c8a644caa3a40a59030de73a79da800a600f438009e9f3353344af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2cff24f-cf84-412f-b16a-3dbe47592559", "node_type": "1", "metadata": {}, "hash": "20a28bd1128d4f4c87ee59c3f44561a67077f9d1a3290f7d8d259354495a9abb", "class_name": "RelatedNodeInfo"}}, "text": "Each VH guarantees that a storage device\ncan be mapped to a host, which is attached anywhere in the\nCXL network. Thus, VHs allow the system to completely\ndisaggregate many PCIe storage devices from its multi-hostcomputing resources for the memory expansion. While these\nrecon\ufb01gurable VHs can realize a fully scale-out architecture,\nmemory resources expanded by the storage devices are un-\nfortunately tricky to control \ufb01nely. Since the storage device\nshould only be associated with a host, it can be underutilized\nand/or unbalanced across different CPUs.\nStorage device virtualization. To address this issue, we can\nvirtualize each storage device to be shared by different hosts.\nSpeci\ufb01cally, as shown in Figure 3c, CXL allows a system to\nlogically splits each endpoint into multiple Type 3 devices (up\nto 16), called multiple logical device (MLD). Thus, we can\nmake each MLD de\ufb01ne its own HDM, which can be mapped\nto a different place of any host of system memory, similar\nto a physical storage device. As each MLD associated with\nthe same storage device can be a part of different VHs, it is\nexpected to utilize the underlying storage resources better by\nallocating the memory expanders in a \ufb01ne granular manner.\nA disadvantage of multi-host VHs can be bandwidth shar-\ning and/or traf\ufb01c congestion. To support MLDs, PCIe storage\nmay require partitioning the underlying backend and internal\nDRAMs, lowering the level of parallelism, and this can un-\nfortunately reduce the bandwidth of each MLD. In addition,\nas a single storage device (and a CXL switch) can be shared\nby multiple hosts, the endpoint\u2019s fabric can be congested\nmore than before. Since the performance of this multi-host\nmemory expansion varies based on diverse perspectives and\nhardware con\ufb01gurations of CXL, it does need careful network\nand storage designs.\n7 EXTENSION FOR STORAGE CONTROL\nAs CXL\u2019s Type 3 is designed for memory pooling, not block\nstorage, there are two issues that we can further consider and\ndiscuss: i) latency \ufb02uctuation and ii) data persistence .\nCXL.mem and CXL.io do not strictly manage the turn-\naround time of loads/stores as their CXL memory requests\ncan be served asynchronously. However, long latency is still\nundesirable and can degrade the host\u2019s overall performance.\nFor example, the PCIe storage device that we used for the\nperformance projection assumes that there are no internal\ntasks (\u00a75). The latency of internal tasks varies based on how\n\ufb01rmware operates, but all they can make the responsiveness\nsigni\ufb01cantly worse than usual. In addition, if host-side li-\nbraries such as PMDK [ 39] insist on data persistence, the cur-\nrent \ufb02ushing mechanism of CXL can be insuf\ufb01cient to handle\nthe underlying PCIe storage. Speci\ufb01cally, CXL provides a\nglobal persistent \ufb02ush (GPF) register, which enforces all data\nresiding in the CXL network and SSD\u2019s internal DRAMs to\nbe immediately written back to the backend media. This can\nalso make latency behaviors of the storage-integrated memory\nexpander(s) severely longer.\n49HotStorage \u201922, June 27\u201328, 2022, Virtual Event, USA Myoungsoo Jung\nTo overcome this, we suggest two simple features, i) deter-\nminism and ii) bufferability , which can be annotated to CXL\nmessages and hint the host semantic to the underlying CXL\ncontrollers. Note that CXL allows Type 3 to manage CXL.io\nfor diverse I/O speci\ufb01c demands (\u00a73), and CXL.mem has a\nreserved \ufb01eld, which can be used for the annotation.\nLatency and persistence controls. Determinism can be de-\n\ufb01ned by two states, deterministic (DT) and non-deterministic\n(ND). DT means a host wants Type 3 devices to serve the\ntagged request without internal task involvement, whereas\nND makes the corresponding requests \ufb01re-and-forget. For\nexample, if DT is speci\ufb01ed, the target storage can schedule\none or more internal tasks to operate with the subsequent re-\nquests (annotated by ND) or in idles. Bufferability can also be\ncomposed by two states, bufferable (BF) and non-bufferable\n(NB).", "start_char_idx": 871908, "end_char_idx": 875875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2cff24f-cf84-412f-b16a-3dbe47592559": {"__data__": {"id_": "f2cff24f-cf84-412f-b16a-3dbe47592559", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f562a64c-233a-4e75-9927-b4d7b0e26e53", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "42e40d6f42f78f89a391be987d3133cf34f396263bc4e33b42d47a29aea2e5fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1532427d-b090-4254-ae46-36777f93dd5e", "node_type": "1", "metadata": {}, "hash": "54bdc4305e3e277d18cc3e64ed8226247f36fbdae8c1b538dbc328189e3fca73", "class_name": "RelatedNodeInfo"}}, "text": "Note that CXL allows Type 3 to manage CXL.io\nfor diverse I/O speci\ufb01c demands (\u00a73), and CXL.mem has a\nreserved \ufb01eld, which can be used for the annotation.\nLatency and persistence controls. Determinism can be de-\n\ufb01ned by two states, deterministic (DT) and non-deterministic\n(ND). DT means a host wants Type 3 devices to serve the\ntagged request without internal task involvement, whereas\nND makes the corresponding requests \ufb01re-and-forget. For\nexample, if DT is speci\ufb01ed, the target storage can schedule\none or more internal tasks to operate with the subsequent re-\nquests (annotated by ND) or in idles. Bufferability can also be\ncomposed by two states, bufferable (BF) and non-bufferable\n(NB). When BF is annotated, the corresponding requests can\nbe cached or buffered in SSD\u2019s internal DRAMs, while the\nrequests annotated by NB consider persistence as \ufb01rst-class\ncitizens for their service. PCIe storage can then selectively\nwrite the requests back to its block media, which can avoid\nthe situation where globally \ufb02ushing all data (sitting on its\nlarge, internal DRAMs) to the block media at a time.\nUser scenarios. To cover diverse user scenarios, determin-\nism, bufferability, and GPF can be used in either an individ-\nual or a combination (e.g., BF+DT, BF+ND, NB+DT, and\nNB+ND). For example, databases and transactional memories\n(e.g., libpmem andlibpmemobj ) log the data when a trans-\naction begins. Since the data associated with the log is not\nnecessary to be persistent before its commit, the host can log\nthe transactions with either BF+ND or NB+ND. During this\ntime, the storage can secure a time to perform internal tasks\nby buffering all incoming writes. When there is a transaction\ncommit, it can con\ufb01gure GPF to \ufb02ush all buffered data and\nwrite the commit (if needed) with NB+DT.\nSince an operator of most instructions waits for its operand\narrivals, loads can typically take advantage of DT. However,\nif there is no subsequent instruction that uses a result of\nthe instruction issued in the previous (i.e., read-after-write\ndependency), the current loads do not need to be synchronized.\nWe can thus precisely use BF+DT or BF+ND for the loads,\nwhich allows the storage to prefetch the data into its internal\nDRAMs. For example, since data are somewhat engaged with\nspatial/temporal localities in a loop code segment (e.g., matrix\ncalculation), we can let the storage know that the data will be\nhit by the internal DRAMs sooner or later again.\nAnother use scenario to take advantage of the annotation\nis lock and synchronization management. Their mechanisms\n(e.g., spin, fence, and barrier) do not need persistence in\nmost cases, but the latency is the matter. For example, a spin-\nlock uses an atomic instruction such as compare-and-swap or\ncompare-and-exchange, which is composed of a memory read\nand a write. While the spinlock does not place its parametersin the CPU cache, the corresponding atomic instruction keeps\niterating to access the same memory address. Thus, it would\nbe better for both the loads/stores to access Type 3 devices\nwith BF+DT. Memory fences and barriers are also similar\nto the spinlock as their lifetime is bounded to the running\nprocess rather than the data.\n8 CONCLUSION AND FUTURE WORK\nThis paper examines CXL from a memory expander view-\npoint and explores different con\ufb01gurations to transfer PCIe\u2019s\nblock semantic to memory-compatible byte semantic. As our\nperformance projection is imperfect and limited to studying\ndiverse perspectives of a storage-integrated expander, we con-\nsider extending this work by accommodating various software\nand hardware environments. We also believe that the several\ncharacteristics of CXL-based memory expansion that this pa-\nper discussed will lead to many architectural changes in both\nsoftware and hardware, which can be worthwhile to study in\nthe near future.\nREFERENCES\n[1]Gen-Z Consortium. Gen-Z Final Speci\ufb01cations. https://genzconsortium.\norg/speci\ufb01cations/.\n[2]CCIX Consortium. CCIX Base Speci\ufb01cation 1.1. https://www.\nccixconsortium.com/library/speci\ufb01cation/.\n[3]CXL Consortium.", "start_char_idx": 875183, "end_char_idx": 879251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1532427d-b090-4254-ae46-36777f93dd5e": {"__data__": {"id_": "1532427d-b090-4254-ae46-36777f93dd5e", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2cff24f-cf84-412f-b16a-3dbe47592559", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "8c52b12c01339a65838ed193b3da90b1057a583be5da1e0f6a79a8b72b7019de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0d92fc7-fe58-4967-bac4-3288c881f1f2", "node_type": "1", "metadata": {}, "hash": "09e19327ed62b0699f218254fa8672fd90bd106a100eac357302b1c91700b564", "class_name": "RelatedNodeInfo"}}, "text": "As our\nperformance projection is imperfect and limited to studying\ndiverse perspectives of a storage-integrated expander, we con-\nsider extending this work by accommodating various software\nand hardware environments. We also believe that the several\ncharacteristics of CXL-based memory expansion that this pa-\nper discussed will lead to many architectural changes in both\nsoftware and hardware, which can be worthwhile to study in\nthe near future.\nREFERENCES\n[1]Gen-Z Consortium. Gen-Z Final Speci\ufb01cations. https://genzconsortium.\norg/speci\ufb01cations/.\n[2]CCIX Consortium. CCIX Base Speci\ufb01cation 1.1. https://www.\nccixconsortium.com/library/speci\ufb01cation/.\n[3]CXL Consortium. Compute Express Link Speci\ufb01cation Revision 2.0.\nhttps://www.computeexpresslink.org/download-the-speci\ufb01cation.\n[4]Gen-Z Consortium. Exploring the Future: CXL Consortium and Gen-Z\nConsortium Sign Letter of Intent to Advance Interconnect Technology.\nhttps://bit.ly/3tXPIod.\n[5]Zhiyuan Guo, Yizhou Shan, Xuhao Luo, Yutong Huang, and Yiying\nZhang. Clio: A hardware-software co-designed disaggregated memory\nsystem. In Proceedings of the 27th ACM International Conference\non Architectural Support for Programming Languages and Operating\nSystems (ASPLOS) , 2022.\n[6]Irina Calciu, M. Talha Imran, Ivan Puddu, Sanidhya Kashyap, Hasan Al\nMaruf, Onur Mutlu, and Aasheesh Kolli. Rethinking software run-\ntimes for disaggregated memory. In Proceedings of the 26th ACM\nInternational Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS) , 2021.\n[7]Zixuan Wang, Joonseop Sim, Euicheol Lim, and Jishen Zhao. En-\nabling ef\ufb01cient large-scale deep learning training with cache coherent\ndisaggregated memory systems. In Proceedings of The 28th IEEE\nInternational Symposium on High-Performance Computer Architecture\n(HPCA) , 2022.\n[8]Jie Zhang, Miryeong Kwon, Donghyun Gouk, Sungjoon Koh,\nNam Sung Kim, Mahmut Taylan Kandemir, and Myoungsoo Jung.\nRevamping storage class memory with hardware automated memory-\nover-storage solution. In Proceedings of the 48th Annual International\nSymposium on Computer Architecture (ISCA) , 2021.\n[9]Changmin Lee, Wonjae Shin, Dae Jeong Kim, Yongjun Yu, Sung-\nJoon Kim, Taekyeong Ko, Deokho Seo, Jongmin Park, Kwanghee Lee,\nSeongho Choi, Namhyung Kim, Vishak G, Arun George, Vishwas\nV, Donghun Lee, Kangwoo Choi, Changbin Song, Dohan Kim, Insu\nChoi, Ilgyu Jung, Yong Ho Song, and Jinman Han. Nvdimm-c: A\nbyte-addressable non-volatile memory module for compatibility with\n50Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for Memory Expansion HotStorage \u201922, June 27\u201328, 2022, Virtual Event, USA\nstandard ddr memory interfaces. In 2020 IEEE International Sympo-\nsium on High Performance Computer Architecture (HPCA) , 2020.\n[10] Ahmed Abulila, Vikram Sharma Mailthody, Zaid Qureshi, Jian Huang,\nNam Sung Kim, Jinjun Xiong, and Wen-mei Hwu. Flat\ufb02ash: Exploiting\nthe byte-accessibility of ssds within a uni\ufb01ed memory-storage hierar-\nchy. In Proceedings of the Twenty-Fourth International Conference\non Architectural Support for Programming Languages and Operating\nSystems (ASPLOS) , 2019.\n[11] Xiaojian Liao, Youyou Lu, Erci Xu, and Jiwu Shu. Write dependency\ndisentanglement with {HORAE }. In 14th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI) .\n[12] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K Aguilera, and Adam\nBelay. {AIFM }:{High-Performance },{Application-Integrated }far\nmemory. In 14th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI) .", "start_char_idx": 878579, "end_char_idx": 882109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0d92fc7-fe58-4967-bac4-3288c881f1f2": {"__data__": {"id_": "f0d92fc7-fe58-4967-bac4-3288c881f1f2", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1532427d-b090-4254-ae46-36777f93dd5e", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "45b8ade30135a7c2913122eb0d602b19d4d79f01e836a15128f815c9285bf4f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51dbf7a7-f319-4899-9b4c-65245f44fa27", "node_type": "1", "metadata": {}, "hash": "f485475a2d2297be4d795a05d00de9680e8124701fe2f322e178cba24ff70e57", "class_name": "RelatedNodeInfo"}}, "text": "Flat\ufb02ash: Exploiting\nthe byte-accessibility of ssds within a uni\ufb01ed memory-storage hierar-\nchy. In Proceedings of the Twenty-Fourth International Conference\non Architectural Support for Programming Languages and Operating\nSystems (ASPLOS) , 2019.\n[11] Xiaojian Liao, Youyou Lu, Erci Xu, and Jiwu Shu. Write dependency\ndisentanglement with {HORAE }. In 14th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI) .\n[12] Zhenyuan Ruan, Malte Schwarzkopf, Marcos K Aguilera, and Adam\nBelay. {AIFM }:{High-Performance },{Application-Integrated }far\nmemory. In 14th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI) .\n[13] Anastasios Papagiannis, Giorgos Xanthakis, Giorgos Saloustros, Mano-\nlis Marazakis, and Angelos Bilas. Optimizing memory-mapped {I/O}\nfor fast storage devices. In 2020 USENIX Annual Technical Conference\n(USENIX ATC) .\n[14] Anirudh Badam and Vivek S Pai. {SSDAlloc }: Hybrid {SSD/RAM }\nmemory management made easy. In 8th USENIX Symposium on Net-\nworked Systems Design and Implementation (NSDI) , 2011.\n[15] Kan Wu, Zhihan Guo, Guanzhou Hu, Kaiwei Tu, Ramnatthan Alagap-\npan, Rathijit Sen, Kwanghyun Park, Andrea C Arpaci-Dusseau, and\nRemzi H Arpaci-Dusseau. The storage hierarchy is not a hierarchy:\nOptimizing caching on modern storage devices with orthus. In 19th\nUSENIX Conference on File and Storage Technologies (FAST) .\n[16] S Kazama, S Gokita, S Kuwamura, E Yoshida, J Ogawa, and Y Honda.\nMemory expansion technology using software-controlled ssd. In Proc.\nFlash Memory Summit .\n[17] Shin-Yeh Tsai, Yizhou Shan, and Yiying Zhang. Disaggregating Per-\nsistent Memory and Controlling Them Remotely: An Exploration of\nPassive Disaggregated Key-Value Stores . 2020.\n[18] Duck-Ho Bae, Insoon Jo, Youra Adel Choi, Joo-Young Hwang,\nSangyeun Cho, Dong-Gi Lee, and Jaeheon Jeong. 2b-ssd: The case for\ndual, byte- and block-addressable solid-state drives. In Proceedings of\nthe 45th Annual International Symposium on Computer Architecture\n(ISCA) , 2018.\n[19] Chander Chadha. NVMe SSD with Persistent Memory Region . shorturl.\nat/hrPS3.\n[20] Stephan Bates. Enabling Remote Access to Persitent Memory on an\nI/O Subsystem using NVMe and RDMA. https://tinyurl.com/2jykndr6.\n[21] Jaeho Kim, Donghee Lee, and Sam H Noh. Towards {SLO}complying\n{SSDs }through {OPS}isolation. In 13th USENIX Conference on File\nand Storage Technologies (FAST) , 2015.\n[22] Bryan S Kim, Jongmoo Choi, and Sang Lyul Min. Design tradeoffs\nfor{SSD}reliability. In 17th USENIX Conference on File and Storage\nTechnologies (FAST 19) , 2019.\n[23] Jie Zhang, Miryeong Kwon, Michael Swift, and Myoungsoo Jung.\nScalable parallel \ufb02ash \ufb01rmware for many-core architectures. In 18th\nUSENIX Conference on File and Storage Technologies (FAST) , 2020.\n[24] Myoungsoo Jung. {OpenExpress }: Fully hardware automated open\nresearch framework for future fast {NVMe }devices. In 2020 USENIX\nAnnual Technical Conference (USENIX ATC) , 2020.\n[25] Jian Xu and Steven Swanson. {NOV A }: A log-structured \ufb01le system\nfor hybrid {Volatile/Non-volatile }main memories. In 14th USENIX\nConference on File and Storage Technologies (FAST) , 2016.\n[26] PCISIG. PCI Express 6.0 Speci\ufb01cation.", "start_char_idx": 881460, "end_char_idx": 884638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51dbf7a7-f319-4899-9b4c-65245f44fa27": {"__data__": {"id_": "51dbf7a7-f319-4899-9b4c-65245f44fa27", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0d92fc7-fe58-4967-bac4-3288c881f1f2", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "24ed72730f174fa103f4dbad4818b94edcfc01a2327c339ebe77750f6a772b53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bcaacaa-970b-442c-bb6d-0e3b86535d4f", "node_type": "1", "metadata": {}, "hash": "c2ca16ccf79ed341650ab2c12e0159a6a91d211ac6de9734f3dc21b9169526ad", "class_name": "RelatedNodeInfo"}}, "text": "Scalable parallel \ufb02ash \ufb01rmware for many-core architectures. In 18th\nUSENIX Conference on File and Storage Technologies (FAST) , 2020.\n[24] Myoungsoo Jung. {OpenExpress }: Fully hardware automated open\nresearch framework for future fast {NVMe }devices. In 2020 USENIX\nAnnual Technical Conference (USENIX ATC) , 2020.\n[25] Jian Xu and Steven Swanson. {NOV A }: A log-structured \ufb01le system\nfor hybrid {Volatile/Non-volatile }main memories. In 14th USENIX\nConference on File and Storage Technologies (FAST) , 2016.\n[26] PCISIG. PCI Express 6.0 Speci\ufb01cation. https://pcisig.com/pci-express-\n6.0-speci\ufb01cation.\n[27] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav\nAgrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden,\nAl Borchers, et al. In-datacenter performance analysis of a tensorprocessing unit. In Proceedings of the 44th annual international sym-\nposium on computer architecture , 2017.\n[28] Gala Yadgar, Eitan Yaakobi, and Assaf Schuster. Write once, get 50%\nfree: Saving {SSD}erase costs using {WOM }codes. In 13th USENIX\nConference on File and Storage Technologies (FAST) .\n[29] Devesh Tiwari, Simona Boboila, Sudharshan Vazhkudai, Youngjae Kim,\nXiaosong Ma, Peter Desnoyers, and Yan Solihin. Active \ufb02ash: Towards\n{Energy-Ef\ufb01cient },{In-Situ }data analytics on {Extreme-Scale }ma-\nchines. In 11th USENIX Conference on File and Storage Technologies\n(FAST) .\n[30] Miryeong Kwon, Donghyun Gouk, Changrim Lee, Byounggeun Kim,\nJooyoung Hwang, and Myoungsoo Jung. {DC-Store }: Eliminating\nnoisy neighbor containers using deterministic {I/O}performance and\nresource isolation. In 18th USENIX Conference on File and Storage\nTechnologies (FAST) , 2020.\n[31] Qiuping Wang, Jinhong Li, Wen Xia, Erik Kruus, Biplob Debnath,\nand Patrick PC Lee. Austere \ufb02ash caching with deduplication and\ncompression. In 2020 USENIX Annual Technical Conference (USENIX\nATC) , 2020.\n[32] Mohit Saxena, Yiying Zhang, Michael M Swift, Andrea C Arpaci-\nDusseau, and Remzi H Arpaci-Dusseau. Getting real: Lessons in\ntransitioning research simulations into hardware systems. In 11th\nUSENIX Conference on File and Storage Technologies (FAST) , 2013.\n[33] Jie Zhang and Myoungsoo Jung. Flashabacus: a self-governing \ufb02ash-\nbased accelerator for low-power systems. In Proceedings of the Thir-\nteenth EuroSys Conference , 2018.\n[34] Myoungsoo Jung. Exploring design challenges in getting solid state\ndrives closer to cpu. IEEE Transactions on Computers , 2014.\n[35] NVM Express Work Group. NVMe Base Speci\ufb01cation. https:\n//nvmexpress.org/developers/nvme-speci\ufb01cation/.\n[36] Gyuyoung Park and Myoungsoo Jung. Automatic-ssd: full hardware\nautomation over new memory for high performance and energy ef\ufb01cient\npcie storage cards. In Proceedings of the 39th International Conference\non Computer-Aided Design , 2020.\n[37] Wooseong Cheong, Chanho Yoon, Seonghoon Woo, Kyuwook Han,\nDaehyun Kim, Chulseung Lee, Youra Choi, Shine Kim, Dongku Kang,\nGeunyeong Yu, Jaehong Kim, Jaechun Park, Ki-Whan Song, Ki-Tae\nPark, Sangyeun Cho, Hwaseok Oh, Daniel D.G. Lee, Jin-Hyeok Choi,\nand Jaeheon Jeong.", "start_char_idx": 884085, "end_char_idx": 887161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bcaacaa-970b-442c-bb6d-0e3b86535d4f": {"__data__": {"id_": "1bcaacaa-970b-442c-bb6d-0e3b86535d4f", "embedding": null, "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8cdebc3-981b-4217-a17f-fea2b3ce2ae7", "node_type": "4", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "19dafd05944443fc685340fb4c7724a50e79a4d2310070b5f783645472976b3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51dbf7a7-f319-4899-9b4c-65245f44fa27", "node_type": "1", "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}, "hash": "d4cebf247aa36c75d29ffb72ad230349456905773ed2511f0cbc17a169221aaa", "class_name": "RelatedNodeInfo"}}, "text": "NVMe Base Speci\ufb01cation. https:\n//nvmexpress.org/developers/nvme-speci\ufb01cation/.\n[36] Gyuyoung Park and Myoungsoo Jung. Automatic-ssd: full hardware\nautomation over new memory for high performance and energy ef\ufb01cient\npcie storage cards. In Proceedings of the 39th International Conference\non Computer-Aided Design , 2020.\n[37] Wooseong Cheong, Chanho Yoon, Seonghoon Woo, Kyuwook Han,\nDaehyun Kim, Chulseung Lee, Youra Choi, Shine Kim, Dongku Kang,\nGeunyeong Yu, Jaehong Kim, Jaechun Park, Ki-Whan Song, Ki-Tae\nPark, Sangyeun Cho, Hwaseok Oh, Daniel D.G. Lee, Jin-Hyeok Choi,\nand Jaeheon Jeong. A \ufb02ash memory controller for 15us ultra-low-\nlatency ssd using high-speed 3d nand \ufb02ash with 3us read time. In 2018\nIEEE International Solid State Circuits Conference (ISSCC) , 2018.\n[38] E. Strohmaier and Hongzhang Shan. Apex-map: A global data access\nbenchmark to analyze hpc systems and parallel programming paradigms\n(sc). In SC \u201905: Proceedings of the 2005 ACM/IEEE Conference on\nSupercomputing , 2005.\n[39] Piotr Balcer. Persistent Memory Development Kit. https://pmem.io/\npmdk/.\n51", "start_char_idx": 886569, "end_char_idx": 887649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"b8cdebc3-981b-4217-a17f-fea2b3ce2ae7": {"node_ids": ["ed6b6146-adac-499d-816a-1f74002b652c", "5da63216-b706-4685-9b8c-a0ad025c5806", "c4f0d33c-0bbf-4a42-94fe-f53290f97758", "c897fe5f-62f7-4bbf-8043-4ded07eea5e8", "bf48a779-ad85-4a9d-87bf-f94676b6728c", "ec07150a-4831-4ef1-9819-591e267ee9ae", "81836ed1-268a-4395-b138-205ac7f38a9c", "2ace6239-1ed7-4fca-8302-a95d18b1fafb", "312b52ec-ad6b-41e2-856c-5c10cceadd38", "11083665-f5eb-4a69-9a43-7c7255760241", "fe1f3513-cead-4c57-ae2a-68cda690a3d5", "75d812fb-050b-4b65-9b13-29d14f2818a8", "b9a53f0c-581d-4523-a78a-1f19ef79a7d7", "41bea6a3-1276-4edd-b33f-3f418e137bc9", "cfc06553-66c9-49ba-84f3-67616316a58d", "016d1938-7134-46c1-8ecd-cef6acb466f5", "6d8593cb-0fd2-48e5-b80e-80e1d5aba226", "e9e42b0b-217f-4338-9bbb-2f5807b86140", "d45df45a-1dd0-4c95-ba2c-2c68f2c7fa59", "887b03f6-95f2-434f-9bd9-27a1933719b8", "15a50813-ab7e-4446-9bbc-49d70ad11d31", "a0af8f88-b4cf-4e9e-93ea-a875a5690e70", "59363e3b-4ff5-465f-a964-3a75f2d3ae6b", "0f5fa7b7-c0fb-47d4-8277-429cc49db8f1", "ea8b4e8b-002e-4241-95ae-0b649616fc5d", "73af9a00-d3f2-40b5-b2cf-5c45fc4b34ae", "bc32b152-847a-49d4-a552-4a5b9853fd6a", "e6fc7030-49ed-45fa-a00a-d44b3f90001a", "96242707-6156-484e-a363-11d4f6942758", "7dc46b2f-7b61-4d28-b26d-e2a3e672f772", "41ffe974-2995-4f92-b39c-0abb35266fa2", "c45d1533-f68e-4420-88f2-81bec47e3ed1", "bd25aaa0-33b5-4274-baf1-ff09840de5c7", "4fa95e3c-09bf-48ad-8a3a-cbf000a257bb", "9074039c-bf1f-40b7-b8ed-e11dd11a54e2", "e7a70987-015e-4694-ace1-7f054cce01cb", "97faf31f-b25b-4ef8-bc78-7823842030a2", "56c6d236-6baa-4a64-ab27-d608c2c1ebad", "0013c566-9315-4dc6-a738-5a7574f3c2fc", "35f42536-787c-4c6f-925e-995f9ee1e05a", "f647646d-ed60-48f7-a378-2bb5cd7f479f", "ddcb1049-d906-4483-8f9f-8bfe4084dea8", "184d9667-8304-41ba-badd-fd153c1ace58", "f11dba25-2128-4c36-ace8-893438bbc078", "06a72cc0-adc8-412e-8104-032a5b7da43b", "75c0f980-7f75-4323-9278-0a5f07aaa926", "7bf5bdac-2348-40e0-9b47-c2b450dd21c3", "6ccfe094-f374-4f17-aa3d-e3bb886cd1a2", "4d275945-bc9f-4c98-843f-b54d2c2d8027", "ba3feacb-988e-4add-9fc0-12dbb00113e3", "cd66b23d-bb5b-4842-9e96-a7402bee9c83", "a70687a4-4339-4530-8c1d-6692d20abeb4", "ceffa9b3-773c-459c-9dfc-5a0b37c914a9", "87db7a65-c790-4496-ab37-297be4091b39", "2e737753-ff6d-43e0-9d47-d6e09794c88f", "2bdb7f1f-8d70-49a3-aa9e-6cf9cb5cb675", "04e3b033-ce68-4fa3-85ed-208e76d24aa4", "c16220b3-782a-4dca-a043-d67d87e13ef5", "1c22c8b2-917a-4bc4-9b05-ffbc00996994", "c8121bbc-0bcf-4685-ae53-91cd00fee759", "bdbf5660-da8a-45d5-b71c-48b697ac05a0", "adf863d0-375c-4ee2-8f06-32abf56f015e", "fae38f8c-f20d-43f8-8a84-ab06b6ee8e72", "bf71de70-e0dd-4720-9836-00a3a285651a", "ba191825-e457-4f48-8512-4404c8d3d07d", "eed987cc-15d5-45e8-b75f-bcd083e6bd52", "51e4fc95-119f-4c1e-ab3c-dbc6f1d9e016", "f30fca24-6fc0-4279-86a4-2dd10e29cc95", "6b845fca-806e-4a2d-a56e-adef1d1a801e", "9cd57eaf-b137-471c-ad3d-1992d042c545", "8f02924f-892d-4188-98d1-f076d9da3d7b", "677ff985-63dc-4501-92d1-70a7d67bc0da", "a111e300-1b3a-4824-bcb1-959aa96ef9e2", "56f47d77-b474-4ad6-b976-9c2c6ecf7241", "94081e9e-93f6-410a-9549-c877c44fd91f", "e159add2-82e8-47b7-9131-723a58681f56", "4e3f37ab-7553-4323-bf4f-44a7fefc570d", "937b97a3-2a00-4f64-afd0-ba687e2576c5", "eb0df7ac-47f5-44ea-a6a6-6ac85621d23d", "e40169f7-be14-405b-b487-c47098368872", "eb6a610b-80a9-48fc-818a-9cc1462a6ce5", "a5bccb49-caae-496e-83b0-b00a7b7e2a94", "d68afe09-1348-4e69-b047-0b9d33aaebc3", "5f6e8d4e-22e7-45f2-b464-ada1bc70641f", "0790d16d-c003-4978-9ed9-11405ee0326c", "0497b5fd-6d94-4a24-bde8-10e8ece24a24", "58c3e2d1-9ecd-4012-900b-206e9e1dc999", "5a4b1129-a3a2-42a1-90d6-7731f4589f32", "7def49cb-991a-4787-854d-e651725ba458", "3d4cd4f1-ed22-4aba-b0ce-1a8cbee31620", "4779ff58-fafa-48b0-87f6-f5ceba14be18", "ea7b9ea5-a5ff-4798-9593-b5ef1e2b349d", "48ae52fc-9f5f-449c-8859-76fcffb3dbe7", "4575b79e-aa35-4cc0-bcc4-ce7b079d113f", "0cf51d9e-9b84-4b14-8b4b-bc3ee0310262", "b1da21a9-4a9b-469e-9def-821dc49ed7fe", "4021aae7-49d7-4a21-80fc-3d35da841496", "3a430a03-545c-4596-ab32-2cdac57973b7", "bc9d39b9-60a0-4e3e-b480-9da132b25203", "d8896edc-6004-4c68-9228-f16cfeaff75e", "67db5ecd-6e7b-4aee-9e82-48f02b2e03b8", "37b81762-f267-4f8d-9017-9db8d509e975", "0e779ca7-4861-4d33-855b-b3b21063742a", "6641cf14-a7b1-4e2b-807c-99393c7e8ac0", "d17977e2-d35b-462f-ade4-b269e29460e4", "602cd835-3c9c-4857-8f70-c7edb7a955df", "deeedbde-9e19-4aba-a3f7-ddbf39e73d2d", "0efb634c-c11e-4c57-8645-4c5a7ca6d9a9", "37e79361-c5f8-4833-b9a5-794209ac3182", "45199acd-ee0e-4343-aac1-1cd58e81fd8d", "b3d8d233-48d7-4a03-936e-388edd575300", "a4ca2cdf-0048-46af-85f5-fc2137dabd26", "5c993fdc-0b5f-4dd6-947f-b8fcd33a290f", "71c1feb2-4a79-4fa8-8250-d7b035288089", "e234e9ff-705f-460d-9cae-c30e20f143d2", "423a9b60-d907-4785-998a-de5a66e423f6", "3950ad46-d5e6-4104-8928-d0389d63a5a0", "8598ad6f-8201-461a-b638-280ae196404b", "c3241294-0319-40f5-9621-552eed1e6f31", "f4fc44dd-3527-468d-a425-d38d53a4dc60", "044588df-9bf0-4ec1-ad00-fc7beb27d862", "faf07e91-0dfd-40f7-92dc-9e297004be7a", "9c138416-d3d9-4e8c-8d99-45e5a27ec7ef", "1296a4eb-0be6-49ee-90c6-e3cdf452a49f", "90249a5e-78b3-4491-9e0d-44026ffa83b1", "f1ef47a4-90ea-4539-91cf-381e6cc9ecfc", "1eb8427b-71a5-4afc-b935-7c7fdd2381ab", "687a1876-f339-459e-bae0-5720824202fd", "d2a7991e-013d-40df-92bb-55b47af7f667", "639b4890-c430-4b46-80ea-217d668af544", "4ccb9eb1-172d-4e68-99db-dc6a1d19c336", "3b971bea-5642-4838-9732-9c5331254a9c", "77524f49-007e-4937-b8f0-dd689bdd8b87", "259d203e-0612-44ee-931f-5824178bcbf0", "e1a8719f-215b-40bf-9f98-6974070adfc9", "c5e732c1-b84d-466a-95ba-baf106746856", "503456f6-e46d-47f5-bbf9-efbea49c4860", "30375e0b-837a-4a1b-afcf-b29ec201e7d1", "e17d91ef-49a3-4489-a260-247b78a20864", "fd654f44-dcfd-4468-a3b5-f30bb6a8e976", "ae1bfc67-d98d-47f1-88fc-cf43b4791fc7", "07ddc96d-d9c3-4dab-b48f-48225e9d7a34", "0fa91dd3-e817-4c11-b195-03ea4dd8ca61", "a307a07e-b57b-4d32-98e1-bcb7d8794f0b", "5e61b449-85e3-43ab-a1ce-2e0264918692", "3e947418-3303-46b3-8413-10f44f2bd0ca", "48844f4c-16e1-4406-9fcb-2895e6a62933", "cf07094a-e885-4242-ae58-6185d8c2e187", "f07c154e-8ac2-4e81-856e-5e2ac0359d72", "f55a8490-a1f0-4c19-b905-43b4af61b4d3", "d2ec76ca-350f-4e78-8b63-760941b6050f", "d918ccac-5314-4ce4-b20d-7d9e71fef1c5", "2bcdcd5f-8502-4f88-b03c-d4cf1903d2f9", "ce6c6699-936b-4bf0-97c7-d487c74e5449", "0316081b-bcb9-4172-b58e-6ac439bf5871", "4cc89d25-1d1a-440f-a270-5c8c662d68b2", "f4af30c0-29b2-4a8c-b56a-b594d9a18fd7", "467bd94b-41ad-4b0a-a709-c9bd3bf6f5b5", "78451caa-56b9-446f-83db-c4248359c47f", "7e5a5b45-efbe-4187-9e4b-0ba344ea2b4a", "eb2fab86-fbcd-43bf-a248-1045590d6439", "098a79dc-c1f1-412c-adcd-035eb5d1c2b9", "6e0a5b1d-80a0-41de-9c79-563bfb4ceaf3", "0fe6f05b-2220-444b-b458-4f60e288f309", "8c5546bf-644d-4c3d-bc04-821d9fe301d8", "3ad1ca74-d927-4467-9603-8209d5b0c177", "fb57fbb8-c588-4ef3-bfa7-5368b8056ec8", "7063425c-d2a7-437e-b7c7-08499595b055", "32132b13-671e-400b-ac63-5e4e8baf9ebe", "71ea6d5c-e554-49bf-b7cb-ed90207bd8ba", "f8e3e49d-e209-46da-8ca0-2ec2e1651030", "0ade1b98-f700-472b-92bd-2e1ff740d4be", "85904e96-d6fa-4d3e-a12d-30718b7725d0", "6f9b0d31-3ba2-4f83-9a0c-2bcec1c61d97", "84f2cd68-89d1-43e4-9be9-7aeb8b5a54c5", "5494beb7-f295-408e-a525-90452352328e", "3825240b-86c6-4ed6-bc66-9c6f428ac893", "90580101-1853-46bb-8bdb-f1a6905e8017", "7cd155e9-ea68-46a1-a4a5-eec25df2a6cd", "6a6b4007-a624-498d-8967-9012ac02aa30", "ac430d4f-1d10-4fb8-ad2a-7b57b2aec456", "d33fe1bc-f856-494d-893f-4e9b941e93e0", "9a8e8823-6aff-40ca-9ef3-0243719fc978", "4289d59c-11ae-426b-9242-63779a78e1e6", "6c385b07-63a7-47a8-8cb6-28f8b9451eee", "3cbd77c1-40db-4bc4-9d24-fd207b4e9aad", "35e8b006-71a1-4d0a-85d2-77d804f95787", "bdd6d74c-21de-44ed-a1eb-bf097c06ddc3", "37593358-e774-4741-9978-f10dced5d88e", "356aa619-1ff6-42d0-9af2-a450d6e2082f", "163c397a-21c7-4cc0-a296-82647ae298dc", "09c535d1-6be3-4eb9-acdc-dd6a90cec0fc", "3c31b763-6004-43ef-ac2d-5d78e0d62b79", "91245482-9d09-44f6-8934-e9550d494389", "84b35d94-701e-4ee9-90ba-d565c8694fd4", "b5720807-474f-45d7-903e-0bbd458ae2b3", "96e2d7a6-f347-4f7f-9d4a-19d8f735486e", "696b14d9-e527-415a-8d2f-6b3ba25095ad", "ad9d8f3c-92f8-431f-af0a-3b7caa9f335d", "60d4c6d4-91e6-40d5-9b5d-ea8a95590c6f", "9aaf7c42-afb3-4904-9588-745ece879401", "ca1fa7bd-214d-4245-b566-04c9c43df42e", "49dff4df-dac0-4281-9efd-87bab0dc3ac8", "e055fe99-5e2e-4852-9b2a-25289d61df72", "5460990b-5c6b-45df-9565-72a1d63de2ee", "d6ce0406-fac2-41cc-b343-86f73a1db1f1", "0a0beae8-dabe-442d-b522-7cfac485e7ac", "17fabd49-d262-49a2-9f36-8fe70014a2bc", "87d72e2f-7ac5-4e21-baf5-d8ef597aed14", "89d8e7fb-e9cb-42da-9274-f9ed68a60e1a", "2e3a7c3b-4d72-4ea5-b0d2-92bf530d6f9a", "6975f378-2f02-4f65-9436-4a502e37551d", "59e3dc07-2910-4b08-841b-f9992c27f228", "c3e6c2dd-2f73-40a6-bfad-708f8713dbeb", "392e83ab-859f-44eb-ad37-ff068c803e7b", "6ae2e016-f5fa-43ea-a216-ecb7cdc0055f", "71aef2a4-f1a3-4925-9932-4c2ac63ae0da", "05dbe910-58b1-43a4-84f0-dc7e660f0e9f", "f46f385c-a3ed-4a43-805d-f8af0d42f416", "79401572-9926-4979-b3fe-c2f90ee105f4", "1d8f8216-43b3-4568-be9a-cd6b484885b9", "aefba818-f1c4-4371-aaf0-624bd968fc18", "2ff0315a-d337-44a6-a6be-f98a5e646955", "a460a119-a678-4bc9-81f1-9912a05f19ef", "15e8d1b8-6c9b-4ae5-aed4-31ef7bb4d1e8", "5fc3ec5a-38dd-40fa-9aef-93cae98bec0e", "c3d3940f-f5cc-41fa-ad2e-2b604ddae237", "2db55bd0-ecf2-4264-9a95-5433d1420db1", "3a717654-feba-4956-8460-66b0e3671c4c", "3d7b6c9b-6212-46cc-bedc-e2e648660b41", "9ab5b89e-8fbf-45e8-9e38-03c77736fee1", "d25a75be-4b62-4959-869b-399102cf98ab", "631b6cfa-6712-4278-bf12-1133d1ae1807", "759a38f6-2a24-499d-92ce-a8cac70e106a", "832ae1c3-1f84-4b41-a045-332652b591b2", "577be5fd-a953-43b9-917d-43dc8e883e94", "032e2300-232a-489d-ae43-b273d57c7d8b", "af0a0fe2-c935-4b04-aa8d-ea4b96785156", "bc406730-5119-44c8-9dbd-9cc762f8de33", "1969d826-5009-4b11-b1c7-2ed8b5f381e5", "bff5e5c2-5657-4588-8f9e-1e56a8582a25", "dc4da986-c189-4e74-8983-3fcc37d2d951", "f1939abc-3068-4062-8999-db7f605a6f6f", "9e610229-8ee9-42e2-972d-3d37f2fd19f4", "12725f5a-45d6-44fd-82dd-2b0c016ac228", "db80f62f-cfa7-4b6a-b805-062abacd7c14", "271daadc-b16d-4439-8562-b15c922217e7", "7d165a60-ef73-4cd8-b9c2-1a04bd8b341b", "98b04653-5c73-492c-b6c2-9845d7e5c473", "b49c16f8-767f-4aa5-8171-882aa2c3957b", "4357edb1-44a6-4852-b254-7bc057a45b06", "3d5bddb2-5e95-4c83-bd82-5a0c55e89bfc", "55a6877f-bba3-4632-afbe-fb596c192eba", "b6f88f6c-0bd8-41c1-8e61-90254833090b", "c4ccd43c-9507-414f-8209-3cfc13d4c33e", "9d5c4a3d-a0f2-46f5-9c78-3720a8619ddf", "cd49931a-c2a3-4080-a659-b68803bb0ea6", "c0ab135b-6bbf-4bd5-b69b-962a17564feb", "ff82f36a-dc76-4f19-bde9-d91745588e8e", "d48e9fd4-6545-4925-acc9-b09c1084c723", "fcd46c2b-93cc-492f-950a-1e7efd34d570", "e6159ad8-db64-4ae7-85c8-929baa03bb63", "be6925c6-1cd0-4cf3-ab20-8c6045c2eb31", "a3d968bb-a506-48da-a5c8-225f5348d10a", "69544297-4a13-4b41-8ce0-aa956e7bddb3", "85c1ea8c-277e-422c-bbea-ad64e3d9ccf4", "b9c97d73-0746-42c5-9d92-1e1a92d40f57", "ee67b3a0-96ab-4e50-9187-5a8f19546007", "01f6c210-ed62-4f33-bee4-b1ef79d81c9c", "d2c4c761-7918-4556-9024-0c7898104281", "7db2f6fc-9d06-42ad-8294-ed694d34dba9", "988e432b-37e4-4c42-9522-717f773fc9ca", "edc4c9cb-a263-4cb3-b6ea-cb2d70c81e4d", "12f32004-bdcd-4657-8bcd-053633e52ae1", "6d66d72e-b250-4661-9673-82c0c8a94b2a", "f2c314d4-700e-42a8-b403-662f0008cdaa", "e6dbd063-7965-4b57-943c-7c8008782357", "0f966a2c-1a1b-4965-886f-04e342cbc4be", "80f791aa-a554-4d8b-9d41-4a99a3dfdf02", "d72d4d7e-6c62-486c-a766-c62aa4220b80", "37dc5d59-786a-4d97-a00c-decef3c14044", "45a5ba89-93bb-4830-ab19-9eeb929975aa", "ee6c29b6-655d-4e6d-a0cd-6a84f2fd266d", "d3cb58db-d464-4777-bc31-60da5a2d59b7", "a162ddeb-1914-4296-abe0-6421a200c26a", "74b69fa3-6432-4f14-bfbc-d242a237d105", "dcfe8f80-2f0f-4bc7-bafd-aef45633a0e0", "d2938a49-c750-4863-8efa-00ed0108f415", "07ae80e5-94c5-469f-bef8-9fea1c95fb69", "f562a64c-233a-4e75-9927-b4d7b0e26e53", "f2cff24f-cf84-412f-b16a-3dbe47592559", "1532427d-b090-4254-ae46-36777f93dd5e", "f0d92fc7-fe58-4967-bac4-3288c881f1f2", "51dbf7a7-f319-4899-9b4c-65245f44fa27", "1bcaacaa-970b-442c-bb6d-0e3b86535d4f"], "metadata": {"file_path": "papers_data/papers.txt", "file_name": "papers.txt", "file_type": "text/plain", "file_size": 896079, "creation_date": "2024-02-04", "last_modified_date": "2024-02-04", "last_accessed_date": "2024-02-04"}}}}